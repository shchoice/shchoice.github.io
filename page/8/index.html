<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Shawn&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Shawn&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon_sh.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Shawn&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:type" content="blog"><meta property="og:title" content="Shawn&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Shawn&#039;s Blog"><meta property="og:description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Seohwan Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Shawn's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Seohwan Choi"},"publisher":{"@type":"Organization","name":"Shawn's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"차분하고 겸손하지만 확실하게!!"}</script><link rel="icon" href="/img/favicon_sh.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-D7QRVGYDET" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-D7QRVGYDET');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-21T14:58:47.000Z" title="7/21/2023, 11:58:47 PM">2023-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:19.000Z" title="9/6/2024, 12:03:19 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">12 minutes read (About 1774 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Gradient-Descent/">7장. 기초 최적화 방법 Gradient Descent - Gradient Descent</a></h1><div class="content"><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><h3 id="Again-Our-Objective-is"><a href="#Again-Our-Objective-is" class="headerlink" title="Again, Our Objective is"></a>Again, Our Objective is</h3><ul>
<li><p>주어진 데이터에 대해서 출력 값을 똑같이 모사하는 함수를 찾고 싶다.</p>
</li>
<li><p>Loss 값을 최소로 하는 Loss Function의 입력 값(𝜃)를 찾자. How?</p>
<ul>
<li><p>𝜃 값을 하나하나 다 랜덤하게 넣어볼 수가 없다!! 따라서 Loss Function을 최소화하는 𝜃를 얻는 방법이 바로 Gradient Descent!</p>
<p>$D &#x3D; {(x_i, y_i)}_{i&#x3D;1}^N$    &#x2F;&#x2F; (데이터 셋들이 모여져 있을때)</p>
<p>$L(\theta) &#x3D; \sum_{i&#x3D;1}^{N} |y_i - \hat{y_i}|^2_2 &#x3D; \sum_{i&#x3D;1}^{N} |y_i - f_\theta(x_i)|^2_2, \text { where } \theta&#x3D;{W,b}, \text{ }f(x)&#x3D;x \cdot W+b$</p>
<p>$\hat{\theta} &#x3D; \operatorname{argmin}_{\theta} L(\theta)$</p>
<p>Loss 함수의 출력 결과가 최소가 되고 싶어하는 입력값을 점진적으로 찾고 싶겠다. 잘된다면 목적함수를 근사할 수 있음($f^* \approx f_{\hat{\theta}}$)</p>
</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Descent-1D-Case"><a href="#Gradient-Descent-1D-Case" class="headerlink" title="Gradient Descent 1D Case"></a>Gradient Descent 1D Case</h3><ul>
<li><p>𝑥로 미분하여 기울기를 활용하여 좀 더 낮은 곳으로 점차 나아가자</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2" alt="GradientDescent05"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2">https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2</a></p>
<p>$x \gets x - \eta \frac{dy}{dx}, \text{ where } y &#x3D; f(x)$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">⬇️</span><br></pre></td></tr></table></figure>

<p>$\theta \gets \theta - \eta \frac{\partial L(\theta)}{\partial \theta} &#x3D; \theta - \eta \nabla_\theta L(\theta)$</p>
<p>※ 𝜂 : Learning rate(0~1, hyper parameter), ⅆ𝑦&#x2F;ⅆ𝑥 : 기울기</p>
<p>※ 𝐿(𝜃) : 손실 함수(loss function)로써 스칼라 값을 갖음 , 𝜃: 파라미터 또는 가중치 벡터로 vector값을 갖음(고차원의 신경망에서는 𝜃가 벡터 뿐만 아니라 행렬, 또는 고차원 텐서의 형태를 가질 수도 있음, 여기서는 1D로 가정하기에 Vector) 따라서 $∇_θL(θ)$는 벡터 <em>θ에 대한 L</em>(<em>θ</em>)의 그래디언트를 나타내며, 이는 <em>θ의 각 요소에 대해 L</em>(<em>θ</em>)를 편미분한 결과를 벡터 형태로 표현한 것</p>
</li>
<li><p>가장 loss가 낮은 곳이 아닌 골짜기에 빠질 가능성이 있음 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e" alt="GradientDescent06"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e">https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e</a></p>
<ul>
<li>그림은 2차원 이지만, 사실 파라미터의 개수만큼의 차원으로 이루어져 있음 Convex 한 2차 함수가 아닌 이상, Global Minima를 알 수가 없다.</li>
</ul>
</li>
</ul>
<h3 id="Loss-Minimization-using-Gradient-Descent"><a href="#Loss-Minimization-using-Gradient-Descent" class="headerlink" title="Loss Minimization using Gradient Descent"></a>Loss Minimization using Gradient Descent</h3><p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444">https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444">https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444</a></p>
<ul>
<li><p>1D 케이스를 높은 차원의 파라미터(*θ)*로 확장하자</p>
<ul>
<li><p>$\hat{\theta} &#x3D; \operatorname{argmin}_{\theta} L(\theta)$</p>
<p>$W \gets W - \eta \frac{\partial L(\theta)}{\partial W} \text{, }$</p>
<p>$b \gets b - \eta \frac{\partial L(\theta)}{\partial b},$</p>
<p>$\text{where } \theta &#x3D; {W,b}$</p>
</li>
</ul>
</li>
<li><p>Number of Parameters in Linear Layer</p>
<ul>
<li><p>파라미터 수 : n x m + m &#x3D; (n + 1) x m</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942">https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942</a></p>
<ul>
<li><p>|𝜃|&#x3D;(18,) , &#x2F;&#x2F; 18개의 파라미터가 있음! 𝑊 &#x3D; 5x3 &#x3D;15, 𝑏 &#x3D; 3</p>
</li>
<li><p>$y &#x3D; f(k) &#x3D; x \cdot W + b$</p>
<p>$\text{ where } x \in \mathbb{R}^{k \times n}, W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^{n \times m} \text{ and } y \in \mathbb{R}^{n \times m}$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Local-Minima-in-Practice"><a href="#Local-Minima-in-Practice" class="headerlink" title="Local Minima in Practice"></a>Local Minima in Practice</h3><ul>
<li>실제 딥러닝의 경우에 파라미터의 크기가 수백만 단위</li>
<li>수백만 차원의 loss 함수 surface 에서 global minma를 찾는 문제</li>
<li>수 많은 차원에서 동시에 local minima를 위한 조건이 만족되기는 어려움</li>
<li><strong>따라서 local mima에 대한 걱정을 크게 할 필요 없음</strong></li>
</ul>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><ul>
<li><p>Gradient Descent 실습 -  backward() &amp; requires_grad_()</p>
<ul>
<li>$x &#x3D; \begin{bmatrix} x_{(1,1)} &amp; x_{(1,2)} \ x_{(2,1)} &amp; x_{(2,2)} \end{bmatrix}$</li>
<li>$x_1&#x3D;x+2$</li>
<li>$x_2&#x3D;x-2$</li>
<li>$x_3&#x3D;x^2-4$</li>
<li>$y&#x3D;sum(x_3)&#x3D;x_{3(1,1)} + x_{3(1,2)} + x_{3(2,1)} + x_{3(2,2)}$</li>
<li>$x.grad&#x3D;\begin{bmatrix} \frac{\partial y}{\partial x_{(1,1)}} &amp; \frac{\partial y}{\partial x_{(1,2)}} \ \frac{\partial y}{\partial x_{(2,1)}} &amp; \frac{\partial y}{\partial x_{(2,2)}} \end{bmatrix}$</li>
<li>$\frac{dy}{dx}&#x3D;2x$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>]]).requires_grad_(<span class="literal">True</span>)</span><br><span class="line">x1 = x + <span class="number">2</span></span><br><span class="line">x2 = x - <span class="number">2</span></span><br><span class="line">x3 = x1 * x2</span><br><span class="line">y = x3.<span class="built_in">sum</span>() </span><br><span class="line"><span class="comment"># 스칼라 값이어야 미분 가능하기 때문에 .sum()이 붙음 없으면 RuntimeError: grad can be implicitly created only for scalar outputs 에러 발생</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="comment">#tensor([[3., 4.],</span></span><br><span class="line"><span class="comment">#       [5., 6.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="comment"># tensor([[-1.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.]], grad_fn=&lt;SubBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"><span class="comment"># tensor([[-3.,  0.],</span></span><br><span class="line"><span class="comment">#        [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(14., grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며</span></span><br><span class="line"><span class="string">파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨</span></span><br><span class="line"><span class="string">그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? </span></span><br><span class="line"><span class="string">이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 </span></span><br><span class="line"><span class="string">이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음</span></span><br><span class="line"><span class="string">예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)  # 가중치 벡터, y.backward(v)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, </span></span><br><span class="line"><span class="string">이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 </span></span><br><span class="line"><span class="string">이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[2., 4.],</span></span><br><span class="line"><span class="comment">#        [6., 8.]])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">print(x3.numpy())</span></span><br><span class="line"><span class="string"># RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</span></span><br><span class="line"><span class="string">print(x3.detach_().numpy())</span></span><br><span class="line"><span class="string"># array([[-3.,  0.],</span></span><br><span class="line"><span class="string">#       [ 5., 12.]], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, </span></span><br><span class="line"><span class="string">이 속성이 True로 설정되면, </span></span><br><span class="line"><span class="string">해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨</span></span><br><span class="line"><span class="string">따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. </span></span><br><span class="line"><span class="string">이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. </span></span><br><span class="line"><span class="string">따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여</span></span><br><span class="line"><span class="string"> 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. </span></span><br><span class="line"><span class="string">이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="요약-정리"><a href="#요약-정리" class="headerlink" title="요약 정리"></a>요약 정리</h3><ul>
<li>DNN을 통해 문제 해결을 위한 함수를 모사하고 싶을 경우, 우리가 만든 함수가 내뱉는 출력과 실제 정답의 차이는 loss이다.</li>
<li>loss를 loss function으로 만들고 loss function은 파라미터를 입력으로 받음</li>
<li>따라서 loss function의 출력을 최소로 하는 파라미터를 찾는 것이 목적이 됨</li>
<li>loss를 최소화하는 입력 파라미터를 Gradiend Descent로 찾을 수 있음</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-20T14:21:57.000Z" title="7/20/2023, 11:21:57 PM">2023-07-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:32.000Z" title="9/6/2024, 12:03:32 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">21 minutes read (About 3218 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-%ED%8E%B8%EB%AF%B8%EB%B6%84/">7장. 기초 최적화 방법 Gradient Descent - 편미분</a></h1><div class="content"><h2 id="편미분"><a href="#편미분" class="headerlink" title="편미분"></a>편미분</h2><h3 id="다변수-함수-Multivariation-Function"><a href="#다변수-함수-Multivariation-Function" class="headerlink" title="다변수 함수(Multivariation Function)"></a>다변수 함수(Multivariation Function)</h3><ul>
<li>여러 개의 변수(multivariate)를 입력으로 받는 함수<ul>
<li>$z &#x3D; f(x,y)$</li>
<li>$y &#x3D; f(x_1, x_2)$</li>
<li>$x &#x3D; \begin{bmatrix} x_1 \ x_2 \end{bmatrix}$</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2" alt="MultivariateFunction"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2">https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2</a></p>
<h3 id="편미분-1"><a href="#편미분-1" class="headerlink" title="편미분"></a>편미분</h3><ul>
<li><p>다변수 x와 y를 입력으로 받는 함수 f를 x로 미분할 경우</p>
<ul>
<li>하나의 변수만 남겨 놓고 나머지를 상수 취급하는 미분 방법</li>
</ul>
</li>
<li><p>함수 f를 x변수(x축)으로 미분</p>
<ul>
<li>편미분 기호 𝜕 (round 혹은 partial 라고 부름)</li>
</ul>
<p>$\frac{\partial f}{\partial x} &#x3D; \lim_{h \to 0} \frac{f(x+h,y) - f(x,y)}{(x+h) - x}$</p>
</li>
<li><p>Y값에 대해 뚝 잘랐을 때 x축에 대한 기울기</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61" alt="GradientDescent01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61">https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61</a></p>
</li>
</ul>
<h3 id="함수의-입출력-형태"><a href="#함수의-입출력-형태" class="headerlink" title="함수의 입출력 형태"></a>함수의 입출력 형태</h3><ul>
<li><p>함수의 입력이 벡터인 경우</p>
<p>$y&#x3D;f(\begin{bmatrix} x_1 \⦙\x_n \end{bmatrix})&#x3D;f(x), \text{ where } x \in \mathbb{R}^n$</p>
</li>
<li><p>함수의 출력이 벡터인 경우 $y&#x3D;f(\begin{bmatrix} y_1 \⦙\y_n \end{bmatrix})&#x3D;f(x)&#x3D;\begin{bmatrix} f_1(x) \⦙\f_n(x) \end{bmatrix}, \text{ where } y \in \mathbb{R}^n$</p>
</li>
<li><p>함수의 입력이 행렬인 경우</p>
<p>$y&#x3D;f(\begin{bmatrix} x_{1,1} ⋯ x_{1,m} \⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ x_{n,1} ⋯ x_{n,m} \end{bmatrix})&#x3D;f(X), \text{ where } X \in \mathbb{R}^{n \times m}$</p>
</li>
<li><p>함수의 출력이 행렬인 경우 $Y&#x3D;f(\begin{bmatrix} y_{1,1} ⋯ y_{1,m} \⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ y_{n,1} ⋯ y_{n,m} \end{bmatrix})&#x3D;f(x), \text{ where } Y \in \mathbb{R}^{n \times m}$</p>
</li>
<li><p>입력과 출력이 벡터인 함수</p>
<p>$y&#x3D;f(\begin{bmatrix} y_1 \⦙\y_n \end{bmatrix})&#x3D;f(x)&#x3D; f(\begin{bmatrix} x_1 \⦙\x_n \end{bmatrix}), \text{ where } f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</p>
</li>
</ul>
<h3 id="스칼라를-벡터로-스칼라를-행렬로-미분"><a href="#스칼라를-벡터로-스칼라를-행렬로-미분" class="headerlink" title="스칼라를 벡터로, 스칼라를 행렬로 미분"></a>스칼라를 벡터로, 스칼라를 행렬로 미분</h3><ul>
<li>미분 결과는 gradient 벡터가 되어 방향과 크기를 모두 나타냄</li>
</ul>
<ol>
<li><p>스칼라를 벡터로 미분</p>
<ul>
<li><p>스칼라 함수를 벡터로 미분한다는 것을 의미</p>
</li>
<li><p>결과는 벡터가 됨</p>
</li>
<li><p>각 벡터의 요소는 스칼라 함수를 해당 방향을 미분한 결과를 나타냄, 이것을 gradient라고 부르며, 함수의 기울기를 나타나는데 사용</p>
</li>
<li><p>$\frac{\partial f}{\partial x} &#x3D; \nabla_x f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_1} \ ⦙\ \frac{\partial f}{\partial x_n} \end{bmatrix}, \text {where }x \in \mathbb{R}^n$</p>
</li>
<li><p>예제</p>
<ul>
<li><p>$f(x,y)&#x3D;3x2+2xy+y2$ 라는 스칼라 함수와 벡터 변수 $x$ 와 $y$ 가 있음</p>
<ul>
<li><p>이 함수를 각 변수에 대해 편미분 하면 다음과 같음</p>
<p>$\frac{\partial f}{\partial x} &#x3D; 6x + 2y$</p>
<p>$\frac{\partial f}{\partial y} &#x3D; 2x + 2y$</p>
</li>
<li><p>따라서 함수 $f(x,y)$ 에 대한 그래디언트는 다음과 같음</p>
<p>$\nabla f &#x3D; \left[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right] &#x3D; [6x + 2y, 2x + 2y]$</p>
</li>
<li><p>이 그래디언트 벡터는 각 점$(x,y)$에서 함수의 값이 가장 크게 증가하는 방향을 가리킴, 따라서 그래디언트는 최적화 문제에서 가장 중요한 역할을 함 (최적화 알고리즘은 일반적으로 그래디언트의 반대 방향으로 이동하여 함수의 최소값을 찾습니다. 이것이 그래디언트 디센트 방법의 기본 개념)</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>코드로 확인해보기</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sympy <span class="keyword">import</span> symbols, diff</span><br><span class="line"></span><br><span class="line">x, y = symbols(<span class="string">&#x27;x y&#x27;</span>)</span><br><span class="line">f = <span class="number">3</span>*x**<span class="number">2</span> + <span class="number">2</span>*x*y + y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">df_dx = diff(f, x)  <span class="comment"># x에 대한 편미분</span></span><br><span class="line">df_dy = diff(f, y)  <span class="comment"># y에 대한 편미분</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;df/dx: <span class="subst">&#123;df_dx&#125;</span>&#x27;</span>)    <span class="comment"># df/dx: 6*x + 2*y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;df/dy: <span class="subst">&#123;df_dy&#125;</span>&#x27;</span>)    <span class="comment"># df/dy: 2*x + 2*y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 임의의 시작점</span></span><br><span class="line">x_value = <span class="number">1.0</span></span><br><span class="line">y_value = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습률</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 100번 반복</span></span><br><span class="line">    gradient_x = df_dx.evalf(subs=&#123;x: x_value, y: y_value&#125;)  <span class="comment"># x 위치에서의 그래디언트 계산</span></span><br><span class="line">    gradient_y = df_dy.evalf(subs=&#123;x: x_value, y: y_value&#125;)  <span class="comment"># y 위치에서의 그래디언트 계산</span></span><br><span class="line"></span><br><span class="line">    x_value -= eta * gradient_x     <span class="comment"># 그래디언트의 반대 방향으로 이동</span></span><br><span class="line">    y_value -= eta * gradient_y     <span class="comment"># 그래디언트의 반대 방향으로 이동</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimized x: <span class="subst">&#123;x_value&#125;</span>&#x27;</span>)    <span class="comment"># Optimized x: -0.0627121858146143</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimized y: <span class="subst">&#123;y_value&#125;</span>&#x27;</span>)    <span class="comment"># Optimized y: 0.154295508359253</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>스칼라를 행렬로 미분</p>
<ul>
<li>스칼라 함수를 행렬 변수에 대해 미분하는 것을 나타냄</li>
<li>결과는 행렬이 됨</li>
<li>이 행렬의 각 요소는 해당 스칼라 함수를 행렬의 해당 요소로 편미분한 값, 스칼라 함수를 행렬로 미분한 결과는 자코비안 행렬이라고 부르며, 이는 함수의 지역적 변화율을 나타냄</li>
<li>$\frac{\partial f}{\partial x} &#x3D; \nabla_x f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_{1,1}  } ⋯ \frac{\partial f}{\partial x_{1,m}} \ ⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ \frac{\partial f}{\partial x_{n,1}  } ⋯ \frac{\partial f}{\partial x_{n,m}} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n \times m}$</li>
<li>스칼라 함수 f가 행렬 X에 의존한다고 하면, 이 함수를 행렬 X에 대해 미분한 결과는 행렬이 됨<ul>
<li>이 행렬의 (i, j)번째 요소는 f를 X의 (i, j)번째 요소에 대해 편미분한 값</li>
<li>스칼라 함수 f를 행렬 $X &#x3D; \begin{bmatrix} x_{11} &amp; x_{12} \ x_{21} &amp; x_{22} \end{bmatrix}$에 대해 미분하면 다음과 같은 형태의 행렬이 나옴<ul>
<li>$\nabla_X f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_{11}} &amp; \frac{\partial f}{\partial x_{12}} \ \frac{\partial f}{\partial x_{21}} &amp; \frac{\partial f}{\partial x_{22}} \end{bmatrix}$</li>
<li>이때 $\frac{\partial f}{\partial x_{ij}}$는 함수 f를 행렬 X의 (i,j)번째 요소에 대해 편미분한 것으로, 이렇게 구해진 행렬을 자코비안 행렬(Jacobian matrix) 이라고 함</li>
<li>자코비안 행렬의 각 요소는 스칼라 함수가 각 변수를 조금씩 변화시킬 때, 함수의 출력이 얼마나 변화하는지를 나타냄, 따라서 자코비안 행렬은 함수의 지역적인 변화율을 설명하는 데 사용될 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>스칼라-행렬 미분은 딥러닝에서 매우 중요한 개념<ul>
<li>신경망의 <strong>가중치는 일반적으로 행렬 형태</strong>를 가지며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 함. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 오차 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산</li>
</ul>
</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>스칼라, 벡터, 행렬</strong></p>
<ul>
<li><strong>스칼라</strong>는 단일한 수치 값입니다. 예를 들어, 10이나 2.5와 같은 단일 숫자를 스칼라라고 합니다.</li>
<li><strong>벡터</strong>는 숫자들의 배열입니다. 예를 들어, [1, 2]와 같은 1차원 배열이 벡터입니다.</li>
<li><strong>행렬</strong>은 숫자들의 2차원 배열입니다. 예를 들어, [[1, 2], [3, 4]]와 같은 2차원 배열이 행렬입니다.</li>
</ul>
</blockquote>
<blockquote>
<p><strong>자코비안 행렬, 헤시안 행렬</strong></p>
<ul>
<li>자코비안 행렬(Jacobian matrix)<ul>
<li>자코비안 행렬은 벡터 값을 가진 함수를 벡터 변수에 대해 미분할 때 사용</li>
<li>함수의 입력과 출력이 모두 벡터일 때, 각 입력 변수에 대한 각 출력 변수의 편미분을 행렬 형태로 나타낸 것이 자코비안 행렬</li>
<li>즉, 다변수 벡터 함수의 첫 번째 도함수를 나타냅니다.</li>
</ul>
</li>
<li>헤시안 행렬(Hessian matrix)<ul>
<li>헤시안 행렬은 스칼라 값을 가진 함수를 행렬 변수에 대해 두 번 미분할 때 사용</li>
<li>즉, 함수의 두 번째 도함수(2차 미분)를 나타냄</li>
<li>헤시안 행렬의 각 성분은 원래 함수의 두 변수에 대한 두 번째 편미분</li>
<li>헤시안 행렬은 주로 함수의 곡률, 즉 최솟값, 최댓값, 또는 안장점(saddle point) 등을 판별하는 데 사용됨</li>
</ul>
</li>
</ul>
<p>따라서, 자코비안은 함수의 기울기(1차 도함수)를, 헤시안은 곡률(2차 도함수)을 나타내며, 두 행렬 모두 함수의 지역적인 동작을 이해하는 데 중요한 역할을 함</p>
</blockquote>
<h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><ul>
<li><p>상미분과 달리 미분 결과가 벡터</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61" alt="GradientDescent01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61">https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61</a></p>
<p>$\nabla f(x,y) &#x3D; \begin{bmatrix} 2x+y \ x+3y^2 +10y\end{bmatrix} &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x} \ \frac{\partial f}{\partial y} \end{bmatrix}$</p>
</li>
</ul>
<h3 id="벡터를-스칼라로-벡터를-벡터로-미분"><a href="#벡터를-스칼라로-벡터를-벡터로-미분" class="headerlink" title="벡터를 스칼라로, 벡터를 벡터로 미분"></a>벡터를 스칼라로, 벡터를 벡터로 미분</h3><ol>
<li><p>벡터를 스칼라로 미분</p>
<p>$\frac{\partial f}{\partial x} &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x}, … \text { }, \frac{\partial f_n}{\partial x} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n}$</p>
</li>
<li><p>벡터를 벡터로 미분</p>
<p>$\frac{\partial f}{\partial x} &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_1} \ ⦙ \ \frac{\partial f}{\partial x_n} \end{bmatrix} &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x}, … \text { }, \frac{\partial f_m}{\partial x} \end{bmatrix}  &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x_{1}  } ⋯ \frac{\partial f_m}{\partial x_{1}} \ ⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ \frac{\partial f_1}{\partial x_{n}  } ⋯ \frac{\partial f_m}{\partial x_{n}} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n}\text{ } and\text{ } f(x) \in \mathbb{R}^m$</p>
</li>
</ol>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>]]).requires_grad_(<span class="literal">True</span>)</span><br><span class="line">x1 = x + <span class="number">2</span></span><br><span class="line">x2 = x - <span class="number">2</span></span><br><span class="line">x3 = x1 * x2</span><br><span class="line">y = x3.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="comment">#tensor([[3., 4.],</span></span><br><span class="line"><span class="comment">#       [5., 6.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="comment"># tensor([[-1.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.]], grad_fn=&lt;SubBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"><span class="comment"># tensor([[-3.,  0.],</span></span><br><span class="line"><span class="comment">#        [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(14., grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며</span></span><br><span class="line"><span class="string">파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨</span></span><br><span class="line"><span class="string">그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? </span></span><br><span class="line"><span class="string">이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 </span></span><br><span class="line"><span class="string">이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음</span></span><br><span class="line"><span class="string">예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)  # 가중치 벡터, y.backward(v)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, </span></span><br><span class="line"><span class="string">이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 </span></span><br><span class="line"><span class="string">이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[2., 4.],</span></span><br><span class="line"><span class="comment">#        [6., 8.]])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">print(x3.numpy())</span></span><br><span class="line"><span class="string"># RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</span></span><br><span class="line"><span class="string">print(x3.detach_().numpy())</span></span><br><span class="line"><span class="string"># array([[-3.,  0.],</span></span><br><span class="line"><span class="string">#       [ 5., 12.]], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, </span></span><br><span class="line"><span class="string">이 속성이 True로 설정되면, </span></span><br><span class="line"><span class="string">해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨</span></span><br><span class="line"><span class="string">따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. </span></span><br><span class="line"><span class="string">이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. </span></span><br><span class="line"><span class="string">따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여</span></span><br><span class="line"><span class="string"> 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. </span></span><br><span class="line"><span class="string">이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="Why-we-laern-this"><a href="#Why-we-laern-this" class="headerlink" title="Why we laern this?"></a>Why we laern this?</h3><ul>
<li><p>Loss 함수 결과값인 스칼라 힘수를 파라미터 행렬(𝜃)로 미분해야 한다면?</p>
</li>
<li><p>DNN의 중간 결과물 벡터(ℎ)를 파라미터 행렬(𝜃)로 미분해야 한다면?</p>
<p>딥러닝에서는 일반적으로 가중치(파라미터)가 행렬 형태를 가지고 있으며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 합니다. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 손실 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 또한, DNN에서는 각 레이어의 입력(중간 결과 값)을 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 따라서, 이러한 미분 개념을 이해하는 것은 딥러닝 모델을 이해하고 최적화하는 데 매우 중요합니다. 따라서, 이러한 이유들로 인해 파라미터 행렬(𝜃)에 대한 스칼라 함수, 즉 손실 함수의 결과 값을 미분하거나, 파라미터 행렬(𝜃)에 대한 중간 결과 값 벡터 (ℎ)를 미분하는 개념을 배우는 것이 딥러닝에서 매우 중요합니다</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-19T14:56:59.000Z" title="7/19/2023, 11:56:59 PM">2023-07-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:13.000Z" title="9/6/2024, 12:03:13 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">5 minutes read (About 713 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/6%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%B4-%EC%9E%98-%ED%95%99%EC%8A%B5%EB%90%98%EB%8A%94%EC%A7%80-%ED%8C%90%EB%8B%A8%ED%95%98%EA%B8%B0-Loss-Function/">6장. 신경망이 잘 학습되는지 판단하기 - Loss Function</a></h1><div class="content"><h2 id="Again-Our-object-is"><a href="#Again-Our-object-is" class="headerlink" title="Again, Our object is"></a>Again, Our object is</h2><ul>
<li>데이터를 넣었을 때 출력을 반환하는 가상의 함수를 모사하는 것</li>
<li>Linear Layer 함수를 통해 원하는 함수를 모사해보자<ul>
<li>Linear Layer 함수가 얼마나 원하는 만큼 동작하는지 측정해 보자</li>
<li>얼마나 잘 동작하는지, 점수로 나타내보자</li>
</ul>
</li>
</ul>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><ul>
<li>Loss(손실값): 원하는 출력값(target,𝑦)가 실제 출력값(output, $\hat{y}$)의 차이의 합<ul>
<li>$\text{Loss} &#x3D; \sum_{i&#x3D;1}^{N} | y_i - \hat{y}<em>i | &#x3D; \sum</em>{i&#x3D;1}^{N} | y_i - f(x_i) |$</li>
</ul>
</li>
<li>그러므로 우리는 Loss가 작을수록 가상의 함수를 잘 모사하고 있다고 할 수 있음</li>
<li>Loss가 작은 Linear Layer를 선택하면 됨</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul>
<li><p>Linear Layer의 파라미터를 바꿀 때마다 Loss를 계산</p>
</li>
<li><p>Loss Function</p>
<ul>
<li>입력 : Linear Layer의 파라미터(𝜃, 즉, 𝑊,𝑏가 파라미터)</li>
<li>출력 : Looss<ul>
<li>𝐿(𝜃)&#x3D;$\sum_{i&#x3D;1}^{n} | y_i - f_{\theta}(x_j) |, \text{ where } \theta &#x3D; {W, b}$</li>
</ul>
</li>
</ul>
</li>
<li><p>종류</p>
<ul>
<li><p>Euclidean Distance</p>
<ul>
<li><p>$| y - \hat{y} |_2 (L2) &#x3D; \sqrt{(y_1 - \hat{y}_1)^2 + \ldots + (y_n - \hat{y}</p>
<p>n)^2} &#x3D; \sqrt{\sum</p>
<p>{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}, \text{ where } y \in \mathbb{R}^n \text{ and } \hat{y} \in \mathbb{R}^n$</p>
<ul>
<li><strong>딥러닝은 차원제약이 없어서 고차원으로 가면 차이가 굉장히 커질 수 있기 때문에 RMSE 가 등장</strong></li>
<li>cf) $| y - \hat{y} | : L1$, 절대값</li>
</ul>
</li>
</ul>
</li>
<li><p>RMSE(Root Mean Square Error)</p>
<ul>
<li>Euclidean Distance와 비슷한 개념</li>
<li>$\text{RMSE}(y, \hat{y}) &#x3D; \sqrt{\frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}$</li>
</ul>
</li>
<li><p>&#96;&#96;&#96;<br>MSE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    (Mean Square Error)</span><br><span class="line"></span><br><span class="line">    - $\text&#123;MSE&#125;(y, \hat&#123;y&#125;) = \frac&#123;1&#125;&#123;n&#125; \sum_&#123;i=1&#125;^&#123;n&#125; (y_i - \hat&#123;y&#125;_i)^2 = \frac&#123;1&#125;&#123;n&#125;(\| y - \hat&#123;y&#125; \|_2)^2 = \frac&#123;1&#125;&#123;n&#125;\| y - \hat&#123;y&#125; \|_2^2 ∝ \| y - \hat&#123;y&#125; \|_2^2$</span><br><span class="line">    - Root와 상수를 뺏지만 크기 차이로 인한 순서 결과는 바뀌지 않음</span><br><span class="line">    - **손실함수로 가장 많이 사용**</span><br><span class="line"></span><br><span class="line">## 코드 구현하기</span><br><span class="line"></span><br><span class="line">- Loss Function 예제 (1) – 직접 구현하기</span><br><span class="line"></span><br><span class="line">  ```python</span><br><span class="line">  import torch</span><br><span class="line">  import torch.nn as nn</span><br><span class="line">  </span><br><span class="line">  def mse(x_hat, x):</span><br><span class="line">      # |x_hat| = (batch_size, dim)</span><br><span class="line">      # |x| = (batch_size, dim)</span><br><span class="line">      y = ((x - x_hat)**2).mean()</span><br><span class="line">      </span><br><span class="line">      return y</span><br><span class="line">  </span><br><span class="line">  x = torch.FloatTensor([[1, 1],</span><br><span class="line">                         [2, 2]])</span><br><span class="line">  x_hat = torch.FloatTensor([[0, 0],</span><br><span class="line">                             [0, 0]])</span><br><span class="line">  </span><br><span class="line">  print(x.size(), x_hat.size()) # torch.Size([2, 2]) torch.Size([2, 2])</span><br><span class="line">  mse(x_hat, x) # tensor(2.5000)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83" alt="LossFunction"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83">https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83</a></p>
</li>
<li><p>Loss Function 예제 (2) – 라이브러리 활용</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">x_hat = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                           [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.size(), x_hat.size()) <span class="comment"># torch.Size([2, 2]) torch.Size([2, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x)) <span class="comment"># tensor(2.5000)</span></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x, reduction=<span class="string">&#x27;sum&#x27;</span>)) <span class="comment"># tensor(10.)</span></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x, reduction=<span class="string">&#x27;none&#x27;</span>)) <span class="comment"># tensor([[1., 1.], [4., 4.]])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Loss Function 예제 (3) – 라이브러리 활용</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">mse_loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">print</span>(mse_loss(x_hat, x)) <span class="comment"># tensor(2.5000)</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><ul>
<li>우리는 목표로 하는 함수를 모사하기 위해<ul>
<li>학습용 입력 데이터들을 Linear Layer에 넣어 출력 값들을 구하고</li>
<li>출력값($\hat{y}$)들과 목표값(${y}$)들의 차이의 합(Loss)를 최소화 해야함</li>
</ul>
</li>
<li>결국, Linear Layer 파라미터(𝜃)를 바꾸면서 loss를 최소화 해야함</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-16T15:05:57.000Z" title="7/17/2023, 12:05:57 AM">2023-07-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:30.000Z" title="9/6/2024, 12:02:30 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">10 minutes read (About 1436 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-Linear-Layer/">5장. 신경망의 기본 구성요소 살펴보기 - Linear Layer</a></h1><div class="content"><h2 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h2><p>우리는 다음의 이미지를 통해 3이라고 머리가 인식하지만, 컴퓨터가 어떻게 이 이미지를 3으로 근사하도록 함수를 만들어야한다</p>
<p>우리는 $f^*$(f optimal)을 모사하는 최적의 $\hat{f}$ (f hat)을 찾아야한다</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d" alt="DigitMnist"></p>
<h3 id="Linear-Layer-란"><a href="#Linear-Layer-란" class="headerlink" title="Linear Layer 란"></a>Linear Layer 란</h3><ul>
<li><code>신경망의 가장 기본 구성 요소</code>, 딥러닝을 통해 모사하는 함수를 만들때 가장 기본이 되는 것이 Linear Layer</li>
<li>Fully-connected(FC) Layer 라고 불리기도 함<ul>
<li>입력의 모든 노드는 출력의 모든 노드와 컨넥션이 있음</li>
<li>Dense Layer 라고도 불리기도 함</li>
</ul>
</li>
<li>내부 파라미터에 따른 선형 변환을 수행하는 함수<ul>
<li>내부 파라미터를 잘 찾아내면 우리가 원하는 출력을 얻을 수 있음</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<h3 id="Linear-Layer-동작방식"><a href="#Linear-Layer-동작방식" class="headerlink" title="Linear Layer 동작방식"></a>Linear Layer 동작방식</h3><ul>
<li>각 입력 노드들에 weight(가중치)를 곱하고 모두 합친 뒤, bias(편향)을 더함</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/ed702032-9ad9-491c-b1cd-481a7a412907" alt="FCLayer02"></p>
<ul>
<li>|𝜃|&#x3D;(18,) , &#x2F;&#x2F; 18개의 파라미터가 있음! 𝑊 &#x3D; 5x3 &#x3D;15, 𝑏 &#x3D; 3</li>
</ul>
<h3 id="Linear-Layer-Equations"><a href="#Linear-Layer-Equations" class="headerlink" title="Linear Layer Equations"></a>Linear Layer Equations</h3><ul>
<li><p><strong>행렬 곱으로 구현 가능</strong></p>
</li>
<li><p>n차원에서 m차원으로의 <code>선형 변환 함수</code></p>
<ul>
<li>$x \in R^{k \times n}, w \in R^{n \times m} \rightarrow y \in R^{k \times m}$</li>
<li>$y &#x3D; f(k) &#x3D; x \cdot w + b$</li>
</ul>
</li>
<li><p>같은 표현</p>
<ul>
<li><p>𝑥를 미니배치에 관계없이 단순히 벡터로 볼 경우 : (m,n) x (n,1) &#x3D; (m,1)</p>
<ul>
<li><p>$y &#x3D; f(k) &#x3D; W^T \cdot x + b$</p>
<p>$\text{ where } x \in \mathbb{R}^n, W^T \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m \text{ and } y \in \mathbb{R}^m$</p>
</li>
</ul>
</li>
<li><p>𝑥를 미니배치(N개) 텐서로 표현할 경우 : (N,n) x (n,m) &#x3D; (N,m)</p>
<ul>
<li><p>$y &#x3D; f(k) &#x3D; x \cdot W + b$</p>
<p>$\text{ where } x \in \mathbb{R}^{k \times n}, W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^{n \times m} \text{ and } y \in \mathbb{R}^{n \times m}$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<h3 id="코드로-구현해보기"><a href="#코드로-구현해보기" class="headerlink" title="코드로 구현해보기"></a>코드로 구현해보기</h3><ul>
<li><p>parameter 정보 확인 예제</p>
<ul>
<li>gradient에 관해서는 다음 gradient descent 파트에서 다룸</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 간단한 신경망 구조를 정의</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)  <span class="comment"># 10개의 입력을 받아 5개의 출력을 내는 선형 계층</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)  <span class="comment"># 5개의 입력을 받아 1개의 출력을 내는 선형 계층</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 신경망 객체를 생성</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 데이터</span></span><br><span class="line">input_data = torch.FloatTensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>, <span class="number">10.0</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;input_data : <span class="subst">&#123;input_data&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># input_data : tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 신경망을 통해 입력 데이터를 전달</span></span><br><span class="line">output_data = model(input_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;output: <span class="subst">&#123;output_data&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># output: tensor([[2.3264]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 파라미터들에 대한 정보를 출력</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;name: <span class="subst">&#123;name&#125;</span>, param.data: <span class="subst">&#123;param.data&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    name: fc1.weight, param.data: tensor([[-0.2895, -0.1230,  0.1624,  0.0381,  0.2252,  0.2265, -0.1498,  0.0806, -0.1704,  0.2421],</span></span><br><span class="line"><span class="string">        [-0.1162,  0.0786, -0.1140,  0.0178,  0.0470,  0.2920,  0.2933,  0.2919, 0.0493, -0.0025],</span></span><br><span class="line"><span class="string">        [ 0.0196,  0.0492, -0.2049,  0.1628, -0.1038,  0.1221,  0.0516, -0.1309, -0.2128, -0.3086],</span></span><br><span class="line"><span class="string">        [ 0.0129,  0.1872, -0.1641,  0.0406,  0.1779,  0.1346, -0.1623,  0.1618, 0.0410, -0.1538],</span></span><br><span class="line"><span class="string">        [ 0.1166, -0.0591,  0.0349, -0.0866,  0.2066, -0.0777,  0.3119, -0.1021, -0.2297,  0.2657]])</span></span><br><span class="line"><span class="string">    name: fc1.bias, param.data: tensor([ 0.0787, -0.0037, -0.2033,  0.0398, -0.1233])</span></span><br><span class="line"><span class="string">    name: fc2.weight, param.data: tensor([[0.0168, 0.2259, 0.2410, 0.0145, 0.2553]])</span></span><br><span class="line"><span class="string">    name: fc2.bias, param.data: tensor([0.2295])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Gradient 계산을 위한 랜덤 타깃 값 생성</span></span><br><span class="line">target = torch.FloatTensor([<span class="number">0.5</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;target <span class="subst">&#123;target&#125;</span>&quot;</span>) <span class="comment"># target tensor([[0.5000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실 함수로 평균 제곱 오차를 사용</span></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실 계산</span></span><br><span class="line">loss = loss_fn(output_data, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 역전파를 수행하여 그라디언트를 계산</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파라미터들의 그라디언트 정보를 출력</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;name: <span class="subst">&#123;name&#125;</span>, param.grad: <span class="subst">&#123;param.grad&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    name: fc1.weight, param.grad: tensor([[0.0614, 0.1229, 0.1843, 0.2458, 0.3072, 0.3687, 0.4301, 0.4915, 0.5530, 0.6144],</span></span><br><span class="line"><span class="string">        [0.8253, 1.6507, 2.4760, 3.3013, 4.1267, 4.9520, 5.7774, 6.6027, 7.4280, 8.2534],</span></span><br><span class="line"><span class="string">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0529, 0.1057, 0.1586, 0.2114, 0.2643, 0.3171, 0.3700, 0.4228, 0.4757, 0.5285],</span></span><br><span class="line"><span class="string">        [0.9324, 1.8648, 2.7972, 3.7296, 4.6621, 5.5945, 6.5269, 7.4593, 8.3917, 9.3241]])</span></span><br><span class="line"><span class="string">    name: fc1.bias, param.grad: tensor([0.0614, 0.8253, 0.0000, 0.0529, 0.9324])</span></span><br><span class="line"><span class="string">    name: fc2.weight, param.grad: tensor([[11.5118, 23.9645,  0.0000,  2.8603,  7.8742]])</span></span><br><span class="line"><span class="string">    name: fc2.bias, param.grad: tensor([3.6528])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Raw Linear Layer 예제 (1) – nn.Module 추상 클래스를 활용</p>
<ul>
<li><p>$y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">W = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">                       [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b = torch.FloatTensor([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(W.size()) <span class="comment"># torch.Size([3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(b.size()) <span class="comment"># torch.Size([2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, W, b</span>):</span><br><span class="line">    y = torch.matmul(x, W) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">y = linear(x, W, b)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (2) – nn.Module 추상 클래스를 활용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, output_dim=<span class="number">2</span></span>):</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))</span><br><span class="line">        self.b = nn.Parameter(torch.FloatTensor(output_dim))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># |x| = (batch_size, input_dim)</span></span><br><span class="line">        y = torch.matmul(x, self.W) + self.b</span><br><span class="line">        <span class="comment"># |y| = (batch_size, input_dim) * (input_dim, output_dim)</span></span><br><span class="line">        <span class="comment">#     = (batch_size, output_dim)        </span></span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-3.7895e+32,  7.2868e-43],</span></span><br><span class="line"><span class="string">        [ 2.8026e-45,  0.0000e+00],</span></span><br><span class="line"><span class="string">        [-3.7896e+32,  7.2868e-43]], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (3) – nn.Linear 이용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-0.4061,  0.0483,  0.0804],</span></span><br><span class="line"><span class="string">        [ 0.0581,  0.0730,  0.4323]], requires_grad=True)</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0.4551, 0.4209], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (4) – nn.Linear 이용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, output_dim=<span class="number">2</span></span>):</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.linear = nn.Linear(input_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># |x| = (batch_size, input_dim)</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="comment"># |y| = (batch_size, output_dim)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-0.1267,  0.0563,  0.3951],</span></span><br><span class="line"><span class="string">        [ 0.2291,  0.3214,  0.2595]], requires_grad=True)</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0.3659, 0.4013], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Linear Layer 는 선형 함수</li>
<li>내부 가중치 파라미터(weight parameter) 𝑊와 𝑏에 의해 정의됨</li>
<li>우린 이 함수의 파라미터를 잘 조절하면, 주어진 입력에 대해 원하는 출력을 만들 수 있음</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-16T14:58:32.000Z" title="7/16/2023, 11:58:32 PM">2023-07-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:45.000Z" title="9/6/2024, 12:02:45 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 591 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88%EA%B3%BC-%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/">5장. 신경망의 기본 구성요소 살펴보기 - 행렬의 곱셈과 벡터의 곱셈</a></h1><div class="content"><h2 id="행렬의-곱셈-Matrix-Multiplication"><a href="#행렬의-곱셈-Matrix-Multiplication" class="headerlink" title="행렬의 곱셈(Matrix Multiplication)"></a>행렬의 곱셈(Matrix Multiplication)</h2><ul>
<li><p>행렬의 곱셈</p>
<ul>
<li>2개의 행렬을 입력으로 받아 <code>새로운 행렬</code>을 생성</li>
<li>첫 번째 행렬의 각 행과 두 번째 행렬의 각 열 사이의 내적을 요소로 가지는 새로운 행렬을 만듬</li>
<li>행렬 곱셈은 내적의 총합을 사용하지만 자체적으로는 다른 연산</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행렬 곱셈</span></span><br><span class="line">M = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">N = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">matrix_product = torch.mm(M, N) <span class="comment"># 두 텐서가 모두 2차원 이상인 경우, &#x27;@&#x27;는 행렬곱(matrix multiplication)을 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Matrix Product:\\n <span class="subst">&#123;matrix_product&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Matrix Product:</span></span><br><span class="line"><span class="comment">#  tensor([[ 58,  64],</span></span><br><span class="line"><span class="comment">#         [139, 154]])</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-Matrix-Multiplication-행렬-곱"><a href="#1-Matrix-Multiplication-행렬-곱" class="headerlink" title="1. Matrix Multiplication(행렬 곱)"></a>1. Matrix Multiplication(행렬 곱)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/29973390-4e07-4bf5-a650-be12868b240b" alt="MatrixMultiplication"></p>
<h3 id="2-Vector-Matrix-Multiplication-벡터와-행렬의-곱"><a href="#2-Vector-Matrix-Multiplication-벡터와-행렬의-곱" class="headerlink" title="2. Vector Matrix Multiplication (벡터와 행렬의 곱)"></a>2. Vector Matrix Multiplication (벡터와 행렬의 곱)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/321590d0-9da0-44bf-945f-90d828b4f084" alt="VectorMatrixMultiplication01"></p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4e1cd1aa-2d96-4e75-bb3e-6db31d46b176" alt="VectorMatrixMultiplication02"></p>
<h3 id="3-Batch-Matrix-Multiplication"><a href="#3-Batch-Matrix-Multiplication" class="headerlink" title="3. Batch Matrix Multiplication"></a>3. Batch Matrix Multiplication</h3><ul>
<li>같은 갯수의 행렬 쌍들에 대해서 병렬로 행렬 곱 실행</li>
<li>만약 4차원 텐서라면 (N1, N2, n, h) X (N1, N2, h, m) 이 됨</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4baa6300-8ffc-4ee4-bee2-81c14936e11c" alt="BatchMatrixMultiplication"></p>
<h2 id="벡터의-곱셈-Vector-Multiplication"><a href="#벡터의-곱셈-Vector-Multiplication" class="headerlink" title="벡터의 곱셈(Vector Multiplication)"></a>벡터의 곱셈(Vector Multiplication)</h2><p>벡터의 곱셈에는 주로 2가지의 형태로 있음</p>
<ul>
<li><p>내적 (Dot Product, Inner Product, 점곱)</p>
<ul>
<li>두 개의 벡터를 입력으로 받아 <code>스칼라</code>(단일 수치) 값을 출력</li>
<li>벡터의 내적은 같은 위치에 있는 요소들끼리 곱한 후, 그 결과를 모두 더해서 하나의 숫자를 얻음</li>
<li>내적은 벡터들 사이의 <code>유사성</code>을 측정하는 데 사용</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 벡터 내적</span></span><br><span class="line">A = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">B = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">dot_product = torch.mm(A, B) <span class="comment"># 두 텐서가 모두 1차원인 경우, &#x27;@&#x27;는 벡터 내적(dot product)을 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dot Product: <span class="subst">&#123;dot_product&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Dot Product: 32</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>외적 (Cross Product)</p>
<ul>
<li>3차원 벡터에 한정하며, 두 벡터의 외적은 새로운 벡터를 생성</li>
<li>새로운 벡터는 두 입력 벡터에 수직인 방향을 가지며, 그 크기는 두 입력 벡터 사이의 각도에 따라 달라짐</li>
<li>물리학에서 힘의 방향 계산 등에 사용</li>
</ul>
</li>
</ul>
<h2 id="벡터의-내적-vs-코사인-유사도"><a href="#벡터의-내적-vs-코사인-유사도" class="headerlink" title="벡터의 내적 vs 코사인 유사도"></a>벡터의 내적 vs 코사인 유사도</h2><ul>
<li>Dot Product<ul>
<li>$a \cdot b &#x3D; |a| |b| \cos \theta$</li>
<li>얼마나 같은 방향을 가지고 있는지 정보를 담으며,</li>
<li>벡터의 크기에도 영향을 받음</li>
</ul>
</li>
<li>Cosine Similarity<ul>
<li>$\text{cosine-similarity}(a, b) &#x3D; \frac{a \cdot b}{|a| |b|}$</li>
<li>방향성만 고려함</li>
<li>벡터의 크기 고려x</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-15T14:58:24.000Z" title="7/15/2023, 11:58:24 PM">2023-07-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:24.000Z" title="9/6/2024, 12:02:24 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">2 minutes read (About 333 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/4%EC%9E%A5-PyTorch-Tutorials-Tensor/">4장. PyTorch Tutorials - Tensor</a></h1><div class="content"><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><h3 id="Tensor란-무엇인가"><a href="#Tensor란-무엇인가" class="headerlink" title="Tensor란 무엇인가"></a>Tensor란 무엇인가</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5b8d117f-0487-4131-b90b-261a8d54914d" alt="Tensor"></p>
<ul>
<li>Scalar : 점</li>
<li>Vector : 1차원 배열</li>
<li>Matrix : 2차원 배열</li>
<li>Tensor : 3차원 이상의 배열</li>
</ul>
<h3 id="Tensor-Shape"><a href="#Tensor-Shape" class="headerlink" title="Tensor Shape"></a>Tensor Shape</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/1873595c-eae5-4056-b07f-ed0a4158ea28" alt="Tensor Shape"></p>
<p>​	$x \in \mathbb{R}^{k \times n \times m} \rightarrow |x|&#x3D;(k,n,m)$</p>
<h3 id="Matrix-Shape"><a href="#Matrix-Shape" class="headerlink" title="Matrix Shape"></a>Matrix Shape</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/fe5c94b3-57fe-4bd4-832b-cce05c96b097" alt="Matrix Shape"></p>
<ul>
<li>$x \in \mathbb{R}^{k \times n} \rightarrow |x|&#x3D;(k,n)$</li>
</ul>
<h3 id="Typical-Tensor-Shape-Tabular-Dataset"><a href="#Typical-Tensor-Shape-Tabular-Dataset" class="headerlink" title="Typical Tensor Shape : Tabular Dataset"></a>Typical Tensor Shape : Tabular Dataset</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/088e479a-0a60-4b2e-bde5-95873c2bcc55" alt="Tabular Shape"></p>
<ul>
<li>$|x|&#x3D;(n,) \Rightarrow x \in \mathbb{R}^n \text{ (vector)}$</li>
</ul>
<h3 id="Mini-batch-Consider-Parallel-Operations"><a href="#Mini-batch-Consider-Parallel-Operations" class="headerlink" title="Mini batch: Consider Parallel Operations"></a>Mini batch: Consider Parallel Operations</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/3f3b8ada-ec71-404a-9357-782900187dfa" alt="MiniBatch"></p>
<ul>
<li>$|x|&#x3D;(k,n) \Rightarrow x \in \mathbb{R}^{k \times n}$</li>
</ul>
<h3 id="Typical-Tensor-Shape-NLP"><a href="#Typical-Tensor-Shape-NLP" class="headerlink" title="Typical Tensor Shape : NLP"></a>Typical Tensor Shape : NLP</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b90beaa4-dfd0-464d-a463-abb9e9f18353" alt="NLP_Tensor_Shape"></p>
<ul>
<li>$x \in \mathbb{R}^{k \times n \times m} \rightarrow |x|&#x3D;(k,n,m)$</li>
<li>|𝑥|&#x3D;(#𝒔,#𝐰,#𝒇)</li>
<li>$|x_{(i,j)}|$&#x3D;(#𝑓, )</li>
<li>$|x_i|$&#x3D;(#𝑤, #𝑓, )</li>
</ul>
<h3 id="Typical-Tensor-Shape-Computer-Vision-GrayScale"><a href="#Typical-Tensor-Shape-Computer-Vision-GrayScale" class="headerlink" title="Typical Tensor Shape : Computer Vision(GrayScale)"></a>Typical Tensor Shape : Computer Vision(GrayScale)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/802d848b-a788-4bbb-8753-210de6260e4c" alt="CV_GrayScale_Tensor_Shape"></p>
<ul>
<li>|𝑥|&#x3D;(#𝐢𝐦𝐠, 𝐡, 𝒘)</li>
</ul>
<h3 id="Typical-Tensor-Shape-Computer-Vision-Color"><a href="#Typical-Tensor-Shape-Computer-Vision-Color" class="headerlink" title="Typical Tensor Shape : Computer Vision(Color)"></a>Typical Tensor Shape : Computer Vision(Color)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b287b7a4-88de-4dc2-83cb-a58d5da8e751" alt="CV_Color_Tensor_Shape"></p>
<ul>
<li>|𝑥|&#x3D;(#𝐢𝐦𝐠, #𝐜𝐡𝐚𝐧𝐧𝐞𝐥, 𝐡, 𝒘) (4차원이 됨)</li>
<li>$|x_i|$&#x3D;(#𝑐ℎ, ℎ, 𝑤)</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-13T14:51:23.000Z" title="7/13/2023, 11:51:23 PM">2023-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:06.000Z" title="9/6/2024, 12:02:06 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 598 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-Working-Process/">3장. 딥러닝 Overview - Working Process</a></h1><div class="content"><h3 id="우리의-목표"><a href="#우리의-목표" class="headerlink" title="우리의 목표!"></a>우리의 목표!</h3><p>주어진 데이터에 대해서 결과를 내는 가상의 함수를 모사하는 함수를 만드는 것</p>
<ul>
<li><p>예시</p>
<ul>
<li><p>주어진 숫자 그림을 보고 맞추기!</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d" alt="DigitMnist"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d">https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d</a></p>
</li>
</ul>
</li>
</ul>
<h3 id="Working-Process"><a href="#Working-Process" class="headerlink" title="Working Process"></a>Working Process</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3" alt="WorkingProcess"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3">https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3</a></p>
<ul>
<li>문제 정의<ul>
<li>가장 중요한 부분</li>
<li>풀고자 하는 문제를 단계뼐로 나누고 simplify 하여야 한다.</li>
<li>신경망이라는 함수에 넣기 위한 x와 결과값 y가 정의되어야 한다.<ul>
<li>𝑦&#x3D;𝑓(𝑥) : 라면 끓는 이미지를 넣으면 물의 온도가 나온다 등</li>
</ul>
</li>
</ul>
</li>
<li>데이터 수집<ul>
<li>문제 정의에 따라 정해진 𝑥와 𝑦</li>
<li>풀고자 하는 문제의 영역에 따라 수집 방법이 다름<ul>
<li>NLP, CV : crawling</li>
<li>데이터분석 : 실제 수집한 데이터</li>
</ul>
</li>
<li>필요에 따라 레이블링(labeling) 작업을 수행<ul>
<li>자동적으로 label이 y로 주어질 수도 있지만, 대부분 레이블이 따로 필요함, 비지도학습 기대하지 말자</li>
</ul>
</li>
</ul>
</li>
<li>데이터 전처리 및 분석<ul>
<li>수집된 데이터를 신경망에 넣어주기 위한 형태로 가공하는 과정<ul>
<li>입출력 값을 정제(Cleansing &amp; normalization)</li>
</ul>
</li>
<li>이 과정에서 탐험적 분석(EDA)이 필요<ul>
<li>데이터 알맞은 알고리즘 찾기 위함(NLP, CV 생략되기도)</li>
</ul>
</li>
<li>CV의 경우 데이터 증강(augmentation)이 수행됨<ul>
<li>rotation, flipping, shifting 등의 연산</li>
</ul>
</li>
</ul>
</li>
<li>알고리즘 적용<ul>
<li>데이터에 대해 가설을 세우고, 해당 가설을 위한 알고리즘(모델)을 적용</li>
</ul>
</li>
<li>평가<ul>
<li>문제 정의에 따른 공정하고 올바른 평가 방법 필요 (가설을 검증하기 위한 실험 설계)</li>
<li>테스트 셋 구성</li>
<li>너무 쉽거나 어렵다면 판별력이 떨어짐, 실제 데이터와 가장 가깝게 구성되야함</li>
<li>정성적 평가(extrinsic evaluation)와 정성적 평가(intrinsic evaluation)로 나뉨</li>
</ul>
</li>
<li>배포<ul>
<li>학습과 평가가 완료된 모델 weights 파일을 배포함</li>
<li>RESTful API 등을 통해 wrapping 후 배포</li>
<li>데이터 분포의 변화에 따른 모델 업데이트 및 유지보수가 필요할 수 있음</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-12T14:53:08.000Z" title="7/12/2023, 11:53:08 PM">2023-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:19.000Z" title="9/6/2024, 12:02:19 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">2 minutes read (About 363 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-%EC%A2%8B%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%B4%EB%9E%80/">3장. 딥러닝 Overview - 좋은 인공지능이란</a></h1><div class="content"><h2 id="좋은-인공지능이란"><a href="#좋은-인공지능이란" class="headerlink" title="좋은 인공지능이란"></a>좋은 인공지능이란</h2><h3 id="인공지능-모델이란"><a href="#인공지능-모델이란" class="headerlink" title="인공지능 모델이란?"></a>인공지능 모델이란?</h3><ul>
<li>𝑥 가 주어졌을 때, 𝑦 를 반환하는 함수<ul>
<li>𝑦&#x3D;𝑓(𝑥)</li>
</ul>
</li>
<li>파라미터(weight parameter, 𝜃)란<ul>
<li>𝑓가 동작하는 방식(𝑥 가 들어왔을 때, 어떤 𝑦 를 뱉어낼 것인가?)를 결정</li>
</ul>
</li>
<li>학습이란<ul>
<li>𝑥와 𝑦의 쌍으로 이루어진 데이터가 주어졌을 때, 𝑥로부터 𝑦로 가는 관계를 배우는 것</li>
<li><code>𝑥와 𝑦를 통해 적절한 파라미터(𝜃)를 찾아내는 것</code></li>
</ul>
</li>
<li>모델이란<ul>
<li>상황에 따라 <code>알고리즘 자체</code>를 이르거나 <code>파라미터</code>를 이름</li>
</ul>
</li>
</ul>
<h3 id="좋은-인공지능-모델이란"><a href="#좋은-인공지능-모델이란" class="headerlink" title="좋은 인공지능 모델이란?"></a>좋은 인공지능 모델이란?</h3><ul>
<li><code>일반화(Generalization)</code>를 잘 하는 모델</li>
<li>보지 못한(unseen) 데이터에 대해서 좋은 예측(prediction)을 하는 모델<ul>
<li>우리는 모든 경우의 수에 대해서 데이터를 모을 수 없기 때문</li>
<li>보지 못한 경우에 대해서, 모델은 좋은 판단을 내릴 수 있어야 함</li>
</ul>
</li>
</ul>
<h3 id="기존-머신러닝의-한계"><a href="#기존-머신러닝의-한계" class="headerlink" title="기존 머신러닝의 한계"></a>기존 머신러닝의 한계</h3><ul>
<li>기존 머신러닝은 주로 선형 또는 낮은 차원의 데이터를 다루기 위해 설계되었음</li>
<li>Kernel 등을 사용하여 비선형 데이터를 다룰 수 있지만 한계가 명확함<ul>
<li>이미지, 텍스트, 음성과 같은 훨씬 더 높은 차원의 데이터들에 대해 낮은 성능을 보임</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-11T14:44:26.000Z" title="7/11/2023, 11:44:26 PM">2023-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:11.000Z" title="9/6/2024, 12:02:11 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">5 minutes read (About 750 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%B4%EB%9E%80/">3장. 딥러닝 Overview - 딥러닝이란</a></h1><div class="content"><h2 id="딥러닝-개요"><a href="#딥러닝-개요" class="headerlink" title="딥러닝 개요"></a>딥러닝 개요</h2><h3 id="딥러닝이란"><a href="#딥러닝이란" class="headerlink" title="딥러닝이란?"></a>딥러닝이란?</h3><ul>
<li>Deep Neural Network(D NN)을 학습시켜 문제를 해결</li>
<li>인경신경망(Artificial Neural Networks)의 적통을 이어받음<ul>
<li>neuron 들로 구성된 신경망을 학습하여 문제를 해겨하도록 동작하는 함수<ul>
<li>딥러닝은 인공신경망의 부분집합</li>
<li>차이점이라면 ANN은 얇게, DNN은 깊게 쌓음</li>
</ul>
</li>
</ul>
</li>
<li>기존 신경망에 비하여 더 깊은 구조를 갖는 것이 특징<ul>
<li>이유 1. GPU를 활용한 병렬 연산 방법이 대중화되며, 신경망의 학습&#x2F;추론 속도의 비약적 증가</li>
<li>이유 2. 인터넷의 발달로 빅데이터가 널리 활용되고 이를 통해 깊은 신경망 학습시킬 수 있게 됨</li>
</ul>
</li>
</ul>
<h3 id="왜-딥러닝인가"><a href="#왜-딥러닝인가" class="headerlink" title="왜 딥러닝인가?"></a>왜 딥러닝인가?</h3><ul>
<li>비선형 함수로 기존 머신러닝에 비해 패턴 인식 능력이 월등함 ※ 이 세상 어떠한 유형의 함수든 모사할 수 있는 능력이 있다는 것이 수학적으로 증명됨, UAT(Universal Approach Theorem)</li>
<li>이미지나 텍스트, 음성과 같은 분야들에서 비약적인 성능 개선을 만듬<ul>
<li>기존 머신러닝과 달리 hand-crafted feature가 필요없음</li>
<li>단순히 raw값을 넣는 것으로, 자동으로 특징(feature)을 학습함</li>
</ul>
</li>
</ul>
<h3 id="딥러닝의-주요역사"><a href="#딥러닝의-주요역사" class="headerlink" title="딥러닝의 주요역사"></a>딥러닝의 주요역사</h3><ul>
<li><p>1980년대 역전파(Back-propagation) 알고리즘의 개발로 인한 중흥기</p>
</li>
<li><p>하지만 모델을 깊세 쌓지 못함으로써 절망감</p>
</li>
<li><p>2010년 초 ImageNet 우승과 음성 인식(Speech Recognition)의 상용화</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/80f13ae3-fc0f-408b-a6ff-37deb09e19ac" alt="ImageNet"></p>
</li>
<li><p>2015년 기계번역(Machine Translation)의 상용화 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f3fc9ae0-7502-4424-9c1a-3b107a67599c" alt="Machine Translation"></p>
<ul>
<li>자연어 처리(SequenceToSequence, seq2seq)의 시작</li>
</ul>
</li>
<li><p>2017년 알파고의 승리</p>
</li>
<li><p>2018년 GAN을 통한 이미지 합성의 발전<br> <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/80646f7d-686d-4132-afa1-295c966f776c" alt="GAN"></p>
</li>
</ul>
<h3 id="딥러닝-패러다임의-변화"><a href="#딥러닝-패러다임의-변화" class="headerlink" title="딥러닝 패러다임의 변화"></a>딥러닝 패러다임의 변화</h3><ul>
<li><p>기존 패러다임</p>
<ul>
<li><p>Hand-Crafted Feature를 추출하여 머신러닝 모델에 넣고 학습</p>
<p>※ Hand-Crafted Feature : 얼굴은 둥그렇게 되어져 있으며, 눈 코 입의 위치가 있다라는 가정들을 넣는 것 즉, 여러 sub-모듈</p>
</li>
<li><p>여러 단계의 sub-module로 이루어져 있었음</p>
<ul>
<li>여러 sub-module로 구성되어져 있어서 시스템이 무거웠으며, 여러 사람이 작업을 해야했음</li>
</ul>
</li>
<li><p>가정이 들어감, 하지만 사람의 가정이 틀릴 수도 있는 문제점이 있음</p>
</li>
</ul>
</li>
<li><p>새로운 패러다임</p>
<ul>
<li><p>Raw 값을 신경망에 넣으면 자동으로 특징(Feature)을 학습</p>
<ul>
<li>하지만, 사람이 해석하기가 어려움, 얼굴 인식이 안되면 왜 안되는지 알기 어려움(블랙박스)</li>
</ul>
</li>
<li><p>하나의 task에 대해서, </p>
<ul>
<li>하나의 신경망 모델이 존재하는 end-to-end 방식</li>
</ul>
<ul>
<li>훨씬 가볍고 혼자서도 오픈소스로 충분히 작업이 가능함</li>
</ul>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-07T14:46:25.000Z" title="7/7/2023, 11:46:25 PM">2023-07-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-07T16:33:08.652Z" title="9/8/2024, 1:33:08 AM">2024-09-08</time></span><span class="level-item"><a class="link-muted" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/">프로그래밍</a><span> / </span><a class="link-muted" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/">Java&quot;</a><span> / </span><a class="link-muted" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Java8/">Java8</a></span><span class="level-item">6 minutes read (About 920 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Java%208/%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/">인터페이스</a></h1><div class="content"><h1 id="인터페이스-Interface"><a href="#인터페이스-Interface" class="headerlink" title="인터페이스(Interface)"></a>인터페이스(Interface)</h1><p>default 메서드</p>
<ul>
<li><p>자바 8에서부터 인터페이스에 default 메서드를 허용</p>
</li>
<li><p>기존의 인터페이스에서 메서드를 추가하면, 그 인터페이스를 구현하는 모든 클래스에서 새로운 메서드를 구현해야 했음</p>
<p> → 그렇지 않으면 컴파일 에러가 발생</p>
<ul>
<li>큰 시스템에서는 비용이 많이드는 작업이며, 때로는 수정이 불가능 하기도 했음</li>
<li>이러한 문제를 해결하기 위해 default 메소드를 도입</li>
</ul>
</li>
<li><p>default 메서드는 인터페이스에 메서드를 추가하더라도 기존의 클래스를 변경하지 않아도 되게 하는 기능</p>
</li>
<li><p>default 메서드는 인터페이스 내에서 구현이 가능하므로, 이를 구현하는 클래스에서는 선택적으로 해당 메서드를 오바라이드 할 수 있음</p>
<ul>
<li>이러한 원리로 인터페이스가 바뀌어도 기존 코드를 깨뜨리지 않으면서 인터페이스를 확장할 수 있음</li>
</ul>
</li>
<li><p>default 메서드를 이용하면 인터페이스를 구현하는 클래스가 중복 코드를 줄일 수 있게 됨</p>
<ul>
<li>이전에는 상속을 통해 중복 코드를 줄이는 것이 주요 방법이었지만, default 메서드를 이요하면 인터페이스에 중복되는 메서드를 정의하고, 이를 공유할 수 있음</li>
<li>코드의 재사용성을 증가시키고 유지보수를 용이하게 함</li>
</ul>
</li>
<li><p>코드 예시</p>
<ul>
<li><p>default 메소드 사용 이전</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Vehicle</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">honk</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Car</span> <span class="keyword">implements</span> <span class="title class_">Vehicle</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">honk</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Beep Beep!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Vehicle</span> <span class="variable">car</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Car</span>();</span><br><span class="line">        car.honk();  <span class="comment">// 출력: Beep Beep!</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>default 메소드 사용</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Vehicle</span> &#123;</span><br><span class="line">    <span class="keyword">default</span> <span class="keyword">void</span> <span class="title function_">honk</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Beep Beep!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Car</span> <span class="keyword">implements</span> <span class="title class_">Vehicle</span> &#123;</span><br><span class="line">    <span class="comment">// Car 클래스에서는 honk 메소드를 구현하지 않아도 됩니다.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Vehicle</span> <span class="variable">car</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Car</span>();</span><br><span class="line">        car.honk();  <span class="comment">// 출력: Beep Beep!</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>static 메서드</p>
<ul>
<li><p>자바 8에서부터 인터페이스에 static 메서드를 허용</p>
</li>
<li><p><strong>인터페이스가 단일 구현 클래스 없이도</strong> 유틸리티 메서드나 헬퍼 메서드를 제공할 수 있게 하기 위함</p>
</li>
<li><p>기존의 문제점</p>
<ul>
<li>유틸리티 메서드 들은 해당 클래스의 인스턴스가 필요하지 않으며, 클래스 자체는 상태를 유지하지 않음</li>
<li>이러한 메서드들은 종종 특정 인터페이스와 밀접한 관련이 있는데, 이 메서드들이 해당 인터페이스와 같은 클래스에 있지 않아서 메서드를 찾기 어렵다는 문제</li>
<li>예시<ul>
<li>Collections 클래스의 sort 메소드</li>
<li>Arrays 클래스의 asList 메서드 등</li>
</ul>
</li>
</ul>
</li>
<li><p>인터페이스에 static 메서드를 추가함으로써 이러한 문제를 해결</p>
<ul>
<li>별도의 유틸리티 클래스를 만들 필요가 없어지므로 코드의 구조가 더 단순해짐</li>
<li>static 메서드는 인터페이스에서 바로 호출할 수 있으므로, 특정 인터페이스와 관련된 유틸리티 메서드들을 쉽게 찾을 수 있음</li>
</ul>
</li>
<li><p>코드 예시</p>
<ul>
<li><p>static 메소드 사용 이전</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ListUtils</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; List&lt;T&gt; <span class="title function_">reverse</span><span class="params">(List&lt;T&gt; list)</span> &#123;</span><br><span class="line">        List&lt;T&gt; copy = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(list);</span><br><span class="line">        Collections.reverse(copy);</span><br><span class="line">        <span class="keyword">return</span> copy;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        List&lt;Integer&gt; numbers = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">        List&lt;Integer&gt; reversed = ListUtils.reverse(numbers);</span><br><span class="line">        System.out.println(reversed);  <span class="comment">// 출력: [5, 4, 3, 2, 1]</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>static 메소드 사용</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ListUtils</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> &lt;T&gt; List&lt;T&gt; <span class="title function_">reverse</span><span class="params">(List&lt;T&gt; list)</span> &#123;</span><br><span class="line">        List&lt;T&gt; copy = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(list);</span><br><span class="line">        Collections.reverse(copy);</span><br><span class="line">        <span class="keyword">return</span> copy;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        List&lt;Integer&gt; numbers = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">        List&lt;Integer&gt; reversed = ListUtils.reverse(numbers);</span><br><span class="line">        System.out.println(reversed);  <span class="comment">// 출력: [5, 4, 3, 2, 1]</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/7/">Previous</a></div><div class="pagination-next"><a href="/page/9/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/7/">7</a></li><li><a class="pagination-link is-current" href="/page/8/">8</a></li><li><a class="pagination-link" href="/page/9/">9</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/14/">14</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/matterhorn.jpg" alt="Shawn Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shawn Choi</p><p class="is-size-6 is-block">노력 백줌 열정 천줌의 소프트웨어 개발자</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, Republic of Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">134</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">64</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">110</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/shchoice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/shchoice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DevOps/"><span class="level-start"><span class="level-item">DevOps</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/"><span class="level-start"><span class="level-item">CI/CD 파이프라인</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/"><span class="level-start"><span class="level-item">Jenkins</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/"><span class="level-start"><span class="level-item">버전 관리</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5/"><span class="level-start"><span class="level-item">버전 관리 및 배포 전략</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/MLOps/"><span class="level-start"><span class="level-item">MLOps</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/MLOps/Cuda/"><span class="level-start"><span class="level-item">Cuda</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/MLOps/MLflow/"><span class="level-start"><span class="level-item">MLflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Ops/Windows-CMD/"><span class="level-start"><span class="level-item">Windows CMD</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/%EB%82%B4-%EC%BD%94%EB%93%9C%EA%B0%80-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9D%B4%EC%83%81%ED%95%9C%EA%B0%80%EC%9A%94/"><span class="level-start"><span class="level-item">내 코드가 그렇게 이상한가요</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Spring/%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8/"><span class="level-start"><span class="level-item">핵심 원리 - 기본편</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/"><span class="level-start"><span class="level-item">기타</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/Github-Pages/"><span class="level-start"><span class="level-item">Github Pages</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/"><span class="level-start"><span class="level-item">데이터베이스 &amp; 검색엔진</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/OpenSearch/"><span class="level-start"><span class="level-item">OpenSearch</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="level-start"><span class="level-item">딥러닝</span></span><span class="level-end"><span class="level-item tag">47</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/Text-Summarization/"><span class="level-start"><span class="level-item">Text Summarization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/"><span class="level-start"><span class="level-item">Transformers</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/TainingArugments/"><span class="level-start"><span class="level-item">TainingArugments</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/"><span class="level-start"><span class="level-item">논문 리뷰</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 개념</span></span><span class="level-end"><span class="level-item tag">38</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 기본 개념</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-NLP-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝을 활용한 자연어 처리(NLP) 개념</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%86%B5%EA%B3%84%ED%95%99-%EB%B0%8F-%EC%88%98%ED%95%99/"><span class="level-start"><span class="level-item">딥러닝을 위한 통계학 및 수학</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/"><span class="level-start"><span class="level-item">성능과 튜닝</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%B0%8F-%EB%B2%A4%EC%B9%98%EB%A7%88%ED%82%B9/"><span class="level-start"><span class="level-item">테스트 및 벤치마킹</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/UML/"><span class="level-start"><span class="level-item">UML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">소프트웨어 아키텍처</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/API-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">API 설계 및 아키텍처</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">마이크로서비스 아키텍처</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">웹 프로그래밍</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/FastAPI/"><span class="level-start"><span class="level-item">FastAPI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/HTTP-%EB%B0%8F-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/"><span class="level-start"><span class="level-item">HTTP 및 네트워크</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Core/"><span class="level-start"><span class="level-item">Spring Core</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Data-JPA/"><span class="level-start"><span class="level-item">Spring Data JPA</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-MVC/"><span class="level-start"><span class="level-item">Spring MVC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/"><span class="level-start"><span class="level-item">개발 환경 설정</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/"><span class="level-start"><span class="level-item">보안</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%95%94%ED%98%B8%ED%99%94/"><span class="level-start"><span class="level-item">데이터 암호화</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%84%9C%EB%B2%84-%EB%B0%8F-%EC%9D%B8%ED%94%84%EB%9D%BC/"><span class="level-start"><span class="level-item">서버 및 인프라</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/"><span class="level-start"><span class="level-item">클라우드 컴퓨팅</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EB%8F%84%EC%BB%A4-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4/"><span class="level-start"><span class="level-item">도커 &amp; 쿠버네티스</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EC%84%9C%EB%B2%84%EB%A6%AC%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">서버리스 아키텍처</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">프로그래밍</span></span><span class="level-end"><span class="level-item tag">31</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Effective-Java/"><span class="level-start"><span class="level-item">Effective Java</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">함수형 프로그래밍</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java&quot;</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Java8/"><span class="level-start"><span class="level-item">Java8</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%8F%99%EC%8B%9C%EC%84%B1-%EB%B3%91%EB%A0%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">동시성 &amp; 병렬 프로그래밍</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/Agile/"><span class="level-start"><span class="level-item">Agile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%ED%81%B4%EB%A6%B0-%EC%BD%94%EB%93%9C/"><span class="level-start"><span class="level-item">클린 코드</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-20T17:29:08.000Z">2024-09-21</time></p><p class="title"><a href="/DevOps/CICD%20%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/Docker-Installation-CentOS-7/">Docker Installation - CentOS 7</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/">CI/CD 파이프라인</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/">Docker</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-19T14:46:09.000Z">2024-09-19</time></p><p class="title"><a href="/DevOps/CICD%20%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/Jenkins-%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB-%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7-%E1%84%86%E1%85%B5%E1%86%BE-%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5-%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB-%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC/">Jenkins 기본 개념 및 설치 &amp; 기본 설정</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/">CI/CD 파이프라인</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/">Jenkins</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-18T15:12:28.000Z">2024-09-19</time></p><p class="title"><a href="/DevOps/%E1%84%87%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB%20%E1%84%80%E1%85%AA%E1%86%AB%E1%84%85%E1%85%B5%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%87%E1%85%A2%E1%84%91%E1%85%A9%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%85%E1%85%A3%E1%86%A8/Git-Commit-Rule/">Git Commit Rule</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5/">버전 관리 및 배포 전략</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-18T13:40:11.000Z">2024-09-18</time></p><p class="title"><a href="/DevOps/%E1%84%87%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB%20%E1%84%80%E1%85%AA%E1%86%AB%E1%84%85%E1%85%B5%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%87%E1%85%A2%E1%84%91%E1%85%A9%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%85%E1%85%A3%E1%86%A8/Git-Merge%E1%84%8B%E1%85%AA-Rebae%E1%84%8B%E1%85%B4-%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%B5/">Git Merge와 Rebae의 차이</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5/">버전 관리 및 배포 전략</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-18T09:30:38.000Z">2024-09-18</time></p><p class="title"><a href="/DevOps/%E1%84%87%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB%20%E1%84%80%E1%85%AA%E1%86%AB%E1%84%85%E1%85%B5%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%87%E1%85%A2%E1%84%91%E1%85%A9%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%85%E1%85%A3%E1%86%A8/Git-%E1%84%87%E1%85%B3%E1%84%85%E1%85%A2%E1%86%AB%E1%84%8E%E1%85%B5-%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%85%E1%85%A3%E1%86%A8-git-flow-gitlab-flow-github-flow/">Git 브랜치 전략 - git/gitlab/github flow</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5/">버전 관리 및 배포 전략</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/1%EA%B8%89-%EC%8B%9C%EB%AF%BC/"><span class="tag">1급 시민</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AES/"><span class="tag">AES</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASGI/"><span class="tag">ASGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Anonymous-Class/"><span class="tag">Anonymous Class</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoEncoder/"><span class="tag">AutoEncoder</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bind-Mounts/"><span class="tag">Bind Mounts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CGI/"><span class="tag">CGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CORS/"><span class="tag">CORS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Entropy-Loss/"><span class="tag">Cross Entropy Loss</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Curse-of-Dimensionality/"><span class="tag">Curse of Dimensionality</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Volume/"><span class="tag">Data Volume</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker-Orchestration-Tools/"><span class="tag">Docker Orchestration Tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Document-Embedding/"><span class="tag">Document Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-Vectors/"><span class="tag">Embedding Vectors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-vector/"><span class="tag">Embedding vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Entropy/"><span class="tag">Entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FLAN/"><span class="tag">FLAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FastAPI/"><span class="tag">FastAPI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Feature-Vector/"><span class="tag">Feature Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Forward-Proxy/"><span class="tag">Forward Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function-Interface/"><span class="tag">Function Interface</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradient-Descent/"><span class="tag">Gradient Descent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gunicorn/"><span class="tag">Gunicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Representation/"><span class="tag">Hidden Representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instruction-Finetuning/"><span class="tag">Instruction Finetuning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Jenkins/"><span class="tag">Jenkins</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL-Divergence/"><span class="tag">KL Divergence</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KoNLPy/"><span class="tag">KoNLPy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/L4-%EC%8A%A4%EC%9C%84%EC%B9%98/"><span class="tag">L4 스위치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lambda-Expression/"><span class="tag">Lambda Expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latent-Space/"><span class="tag">Latent Space</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learning-Rate/"><span class="tag">Learning Rate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Layer/"><span class="tag">Linear Layer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Load-Testing/"><span class="tag">Load Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Likelihood/"><span class="tag">Log-Likelihood</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MAP/"><span class="tag">MAP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLE/"><span class="tag">MLE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Manifold-hypothesis/"><span class="tag">Manifold hypothesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix/"><span class="tag">Matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mecab/"><span class="tag">Mecab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-Stage-Build/"><span class="tag">Multi Stage Build&quot;</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLL/"><span class="tag">NLL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OSI-7%EA%B3%84%EC%B8%B5/"><span class="tag">OSI 7계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Persistence-Data/"><span class="tag">Persistence Data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probabilistic-Perspective/"><span class="tag">Probabilistic Perspective</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RPS/"><span class="tag">RPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSA/"><span class="tag">RSA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Response-Time/"><span class="tag">Response Time</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reverse-Proxy/"><span class="tag">Reverse Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SOLID/"><span class="tag">SOLID</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scalar/"><span class="tag">Scalar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spring%EC%9D%B4%EB%9E%80/"><span class="tag">Spring이란</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stress-Testing/"><span class="tag">Stress Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword-Embedding/"><span class="tag">Subword Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TCP-IP-4%EA%B3%84%EC%B8%B5/"><span class="tag">TCP/IP 4계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TPS/"><span class="tag">TPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensor/"><span class="tag">Tensor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Testing-Types/"><span class="tag">Testing Types</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Throughput/"><span class="tag">Throughput</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tramsformers/"><span class="tag">Tramsformers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UML/"><span class="tag">UML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Uvicorn/"><span class="tag">Uvicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vector/"><span class="tag">Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WAS/"><span class="tag">WAS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WSGI/"><span class="tag">WSGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/What-to-do/"><span class="tag">What to do</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cross-entropy/"><span class="tag">cross entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/default-method/"><span class="tag">default method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-commit-rule/"><span class="tag">git commit rule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-flow/"><span class="tag">git flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-merge/"><span class="tag">git merge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-rebase/"><span class="tag">git rebase</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github-flow/"><span class="tag">github flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gitlab-flow/"><span class="tag">gitlab flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/max-length/"><span class="tag">max_length</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mlflow/"><span class="tag">mlflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/one-hot-Encoding/"><span class="tag">one-hot Encoding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/packing/"><span class="tag">packing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/padding/"><span class="tag">padding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-%EC%84%A4%EC%B9%98/"><span class="tag">python 설치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/static-method/"><span class="tag">static method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unpacking/"><span class="tag">unpacking</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5/"><span class="tag">객체지향</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8B%A4%ED%98%95%EC%84%B1/"><span class="tag">다형성</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%88%84%EC%88%98/"><span class="tag">데이터 누수</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%9E%8C%EB%8B%A4%EC%8B%9D/"><span class="tag">람다식</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1/"><span class="tag">로드 밸런싱</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A6%AC%EB%B2%84%EC%8A%A4-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">리버스 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">벡터의 곱셈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%EC%A7%80%ED%91%9C/"><span class="tag">성능 지표</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%ED%85%8C%EC%8A%A4%ED%8A%B8/"><span class="tag">성능 테스트</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC/"><span class="tag">엔트로피</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9B%B9-%EC%84%9C%EB%B2%84/"><span class="tag">웹 서버</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EB%9F%89/"><span class="tag">정보량</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/"><span class="tag">정보이론</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%8B%A4%EC%9D%B4%EC%96%B4%EA%B7%B8%EB%9E%A8/"><span class="tag">클래스 다이어그램</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%8F%AC%EC%9B%8C%EB%93%9C-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">포워드 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"><span class="tag">함수형 인터페이스</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="tag">함수형 프로그래밍</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">행렬의 곱셈</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Seohwan Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>