<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Shawn&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Shawn&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon_sh.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Shawn&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:type" content="blog"><meta property="og:title" content="Shawn&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Shawn&#039;s Blog"><meta property="og:description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Seohwan Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Shawn's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Seohwan Choi"},"publisher":{"@type":"Organization","name":"Shawn's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"차분하고 겸손하지만 확실하게!!"}</script><link rel="icon" href="/img/favicon_sh.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-D7QRVGYDET" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-D7QRVGYDET');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-29T14:59:01.000Z" title="7/29/2023, 11:59:01 PM">2023-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:04:02.000Z" title="9/6/2024, 12:04:02 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">18 minutes read (About 2713 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/2%EC%9E%A5-Probabilistic-Perspective-Basic-Statistics/">2장. Probabilistic Perspective - Basic Statistics</a></h1><div class="content"><h2 id="기본-통계학-Basic-Statistics"><a href="#기본-통계학-Basic-Statistics" class="headerlink" title="기본 통계학(Basic Statistics)"></a>기본 통계학(Basic Statistics)</h2><h3 id="Random-Variable-amp-Probability-Distribution"><a href="#Random-Variable-amp-Probability-Distribution" class="headerlink" title="Random Variable &amp; Probability Distribution"></a>Random Variable &amp; Probability Distribution</h3><ul>
<li>확률 변수(Random Variable)는 사건의 시행의 결과(확률)를 하나의 수치로 대응시킬 때의 확률 값, 일반적으로 대문자 X를 사용</li>
<li>어떤 변수(Random Variable) x가 𝒙 라는 값을 가질 확률<ul>
<li>$𝑃(\text{x}&#x3D;𝑥) &#x3D;𝑃(𝑥)$</li>
</ul>
</li>
<li>확률 분포 (함수)<ul>
<li>입력 : 확률 변수 x</li>
<li>출력 : x가 각 값에 해당될 때에 대한 확률값</li>
</ul>
</li>
<li>이산 확률 변수<ul>
<li>확률변수가 취할 수 있는 값의 수가 유한한 변수</li>
<li>ex) 동전 던지기, 주사위 던지기(X&#x3D;{0, 1, 2, 3})</li>
</ul>
</li>
<li>연속 확률 변수<ul>
<li>확률변수가 취할 수 있는 값의 수가 무한한 변수</li>
<li>ex) 키, 몸무게(P(50 ≤ X ≤ 60)) 등</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2aeefb28-c594-4c5c-806e-69d7d571f19f" alt="ProbabiltyDistibution"></p>
<h3 id="Function-or-Value"><a href="#Function-or-Value" class="headerlink" title="Function? or Value?"></a>Function? or Value?</h3><ul>
<li>확률값<ul>
<li>$𝑃(𝑥) &#x3D; P(\text{x}&#x3D;x)$</li>
</ul>
</li>
<li>확률 분포 함수<ul>
<li>$P(\text{x})$<br><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/47210d6e-73d3-4db3-8d14-817b51030869" alt="ProbabilityDensityFunction"></li>
<li>$P(y|x)$<ul>
<li>$P(y|x) &#x3D; P(\text{y}&#x3D;y|\text{x}&#x3D;x)$</li>
<li>Random Variable x가 𝑥 라는 값을 가졌을 때, Random Variable y가 𝑦일 확률 값</li>
</ul>
</li>
<li>$P(\text{y}|x)$<ul>
<li>$P(\text{y}|x) &#x3D; P(\text{y}|\text{x}&#x3D;x)$</li>
<li>Random Variable x가 𝑥 라는 값을 가졌을 때, Random Variable y의 분포</li>
</ul>
</li>
<li>$P(y|\text{x})$<ul>
<li>$P(y|\text{x})&#x3D;f(\text{x})&#x3D;P(\text{y}&#x3D;y|\text{x})$</li>
<li>어떤 Random Variable이 입력으로 주어졌을 때, Random Variable y가 𝑦일 확률 값을 뱉어내는 함수</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="확률-분포-Probability-Distribution"><a href="#확률-분포-Probability-Distribution" class="headerlink" title="확률 분포(Probability Distribution)"></a>확률 분포(Probability Distribution)</h3><p>확률 분포는 수치로 대응된 확률 변수의 개별 값들이 가지는 확률값의 분포</p>
<ul>
<li><p>이산 확률 분포(Discrete Probability Distribution) : 확률 변수가 취할 수 있는 값의 수가 유한한 확률 분포</p>
<ul>
<li><p>확률 질량함수(Probability Mass Function) : 확률 변수에서 특정 값에 대한 확률을 나타내는 함수</p>
<ul>
<li>확률값의 총 합은 1<ul>
<li>$\sum_{x} P(\text{x}&#x3D;x) &#x3D; 1, \text{ where } 0 \leq P(\text{x}&#x3D;x) \leq 1, \forall x \in \chi$</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c555ab55-2167-44ae-b5a1-f67a393b9f46" alt="ProbabilityMassFunction"></p>
</li>
<li><p>주사위가 예시가 될 수 있음(이산 확률 분포)</p>
</li>
</ul>
</li>
<li><p>연속 확률 분포(Continuous Probability Distribution) : 확률 변수가 취할 수 있는 값의 수가 무한한 확률 분포</p>
<ul>
<li><p>확률 밀도 함수(robability Density Function , PDF)  : 확률 변수의 분포를 나타내는 함수</p>
<ul>
<li>면적의 합이 1<ul>
<li>$\int P(x) , dx &#x3D; 1, \text{ where } P(x) \geq 0, \forall x \in \mathbb{R}$</li>
</ul>
</li>
<li>함수값이 1보다 클 수 있음</li>
</ul>
<p>※ 주사위는 확률질량함수(이산), 확률밀도함수는 연속함수</p>
</li>
<li><p>연속 확률 변수의 경우, 어떤 샘플이 주어졌을 때, <strong>확률값을 알 수 없다</strong>. (높이는 단지 확률 밀도)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f5c4716d-50e0-4c88-bc30-3b24db0869bc" alt="ProbabilityDensityFunction"></p>
</li>
</ul>
</li>
<li><p>결합 확률 분포(Joint Probability Distribution)</p>
<ul>
<li><p>두 개 이상의 Random Variable이 결합되었을 때의 확률 분포</p>
</li>
<li><p>두 이벤트 A와 B가 동시에 발생할 확률을 의미합니다. 이를 기호로 나타내면 P(A ∩ B) 또는 P(A, B)로 표현</p>
</li>
<li><p>P(A, B) &#x3D; P(A|B) * P(B) &#x3D; P(B|A) * P(A)</p>
<ul>
<li>P(A, B) : 사건 A와 사건 B가 동시에 발생할 확률</li>
<li>P(A|B) : 사건 B가 발생한 조건 하에서 사건 A가 발생할 확률</li>
<li>P(B) : 사건 B가 발생할 확률</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2e67be1b-9289-44dc-a006-cd221bbec36c" alt="JointProbabilityDistribution"></p>
</li>
</ul>
</li>
</ul>
<h3 id="조건부-확률-Conditional-Probability"><a href="#조건부-확률-Conditional-Probability" class="headerlink" title="조건부 확률(Conditional Probability)"></a>조건부 확률(Conditional Probability)</h3><ul>
<li><p>어떤 사건 A가 일어났을 때 사건 B가 일어날 확률을 의미. 즉, 사건 A가 주어졌을 때의 사건 B의 확률</p>
</li>
<li><p>조건부 확률 분포</p>
<ul>
<li><p>랜덤 변수의 분포가 다른 랜덤 변수의 값에 의해 어떻게 영향을 받는지를 설명하는 확률 분포</p>
</li>
<li><p>즉, 하나의 랜덤 변수 X의 값이 주어졌을 때, 다른 랜덤 변수 Y의 분포를 의미</p>
</li>
<li><p>$P(\text{y}|\text{x}) &#x3D; \frac{P(\text{x}, \text{y})}{P(\text{x})}$ (x가 조건으로 주어졌을 때 y의 확률값)</p>
<p>P y given x 라고 읽음</p>
</li>
<li><p>조금더 친해져야할 형태(<strong>결합확률분포 형태</strong>)</p>
<ul>
<li>$<strong>P(\text{x}, \text{y}) &#x3D; P(\text{y} ,|, \text{x})P(\text{x})</strong>$</li>
</ul>
</li>
<li><p>예를 들어, 랜덤 변수 X가 ‘오늘 비가 올 확률’을 나타낸다고 할 때, 랜덤 변수 Y는 ‘비가 올 때 우산을 가지고 갈 확률’을 나타낼 수 있음. 이 경우, ‘우산을 가지고 갈 확률’은 ‘비가 올 확률’에 의해 영향을 받음. 이런 상황을 모델링한 것이 조건부 확률 분포이다.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/cf745351-0dc1-492d-a970-9369752d8228" alt="CP"></p>
<h3 id="베이즈-정리-Bayes-Theorm"><a href="#베이즈-정리-Bayes-Theorm" class="headerlink" title="베이즈 정리(Bayes Theorm)"></a>베이즈 정리(Bayes Theorm)</h3><ul>
<li><p>베이즈 정리는 확률 이론과 통계학에서 굉장히 중요한 개념으로, 조건부 확률에 관한 정리</p>
</li>
<li><p>조건부 확률의 개념을 확장하고 이를 뒤집는 데 사용</p>
</li>
<li><p>주어진 사건 B가 발생했을 때, 사건 A가 발생할 확률을 알아내는 방법을 제공. 즉, 이미 알고 있는 정보에 기반하여 새로운 사건의 확률을 추정하는 방법</p>
</li>
<li><p>$P(A ,|, B) &#x3D; \frac{P(B,|,A)P(A)}{P(B)}$,</p>
<ul>
<li>(A|B)는 사건 B가 발생했을 때 사건 A가 발생할 조건부 확률을 의미하며, 이를 사후 확률(Posterior probability)이라고 함</li>
<li>P(B|A)는 사건 A가 발생했을 때 사건 B가 발생할 조건부 확률을 의미하며, 이를 가능도(Likelihood)라고 함</li>
<li>P(A)는 사건 A가 발생할 확률을 의미하며, 이를 사전 확률(Prior probability)라고 함</li>
<li>P(B)는 사건 B가 발생할 확률을 의미하며, 이를 증거(Evidence)라고 함</li>
</ul>
<blockquote>
<p>A를 “오늘 비가 올 것”이라는 사건, B를 “오늘 하늘이 흐림”이라는 사건이라고 가정합시다. 이 경우, 베이즈 정리는 다음과 같이 표현될 수 있습니다.</p>
<ul>
<li>사후 확률(P(A|B)), 즉 “하늘이 흐릴 때 비가 올 확률”은 알고 싶은 최종적인 확률입니다. 이는 우리가 이미 알고 있는 “하늘이 흐림”이라는 정보를 바탕으로 “오늘 비가 올 것”이라는 새로운 사건에 대한 확률을 업데이트하는 것을 의미합니다.</li>
<li>가능도(P(B|A)), 즉 “비가 올 때 하늘이 흐릴 확률”은 우리가 이미 알고 있는 “비가 올 것”이라는 사건이 주어졌을 때 “하늘이 흐림”이라는 사건의 확률입니다. 이는 사건 A가 주어졌을 때 사건 B가 발생할 확률을 나타냅니다.</li>
</ul>
</blockquote>
</li>
<li><p>$P(h ,|, D) &#x3D; \frac{P(D,|,h)P(h)}{P(D)}$</p>
<ul>
<li><p>$P(h ,|, D)P(D) &#x3D; P(D,|,h)P(h) &#x3D;P(h,D)$</p>
</li>
<li><p>가설이 있을 때 데이터가 있을 확률을 뒤집어서 알 수 있게 해준다.</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>조건부 확률과 베이즈 정리의 차이를 다음의 예시로 알아보자</strong>예시 : “비가 오는 날에 우산을 들고 나가는 확률”</p>
<p><strong>조건부 확률</strong> ‘비가 오는 날’이라는 사건이 이미 일어났을 때, 그 조건 하에서 ‘우산을 들고 나가는’ 사건이 일어날 확률을 계산하는 것이 조건부 확률</p>
<p><strong>베이즈 정리</strong> ”우산을 들고 나가는 사람 중에서 비가 오는 경우”</p>
</blockquote>
<h3 id="주변-분포-Marginal-Distribution"><a href="#주변-분포-Marginal-Distribution" class="headerlink" title="주변 분포(Marginal Distribution)"></a>주변 분포(Marginal Distribution)</h3><ul>
<li><p>여러 변수의 결합 확률 분포에서 한 개 또는 일부 변수의 분포를 나타냄, 이는 ‘주변화(marginalization)’라는 과정을 통해 얻어짐</p>
</li>
<li><p>주변화는 결합 확률 분포에서 관심 있는 변수를 제외한 나머지 변수들을 모두 합산(이산)하거나 적분(연속)하는 과정을 의미</p>
</li>
<li><p>주변 확률 분포를 통해, 특정 변수가 다른 변수들로부터 독립적으로 어떻게 분포하는지를 파악할 수 있음, 이를 통해 그 변수의 분포를 단독으로 볼 수 있게 됨</p>
</li>
<li><p>주변 분포는 확률론의 핵심 개념인 결합 확률, 조건부 확률, 그리고 이들의 관계를 연결해 보여주며, 주어진 데이터에서 특정 변수의 확률 분포를 독립적으로 파악하는 데 유용</p>
</li>
<li><p>Marginal Distribution의 공식</p>
<ul>
<li><p>$P(x) &#x3D; \int P(x,z) dz$ ( P(x, z)는 x와 z의 결합 확률 분포를 나타내고, 이를 z에 대해 적분하면 x의 주변 확률 분포 x를 얻게 됨)</p>
<p>$&#x3D; \int P(x|z)P(z) dz$ (결합 확률분포를 조건부 확률 P(x | z)와 z의 주변 확률 P(z)의 곱으로 분해하고 이를 z에 대해 적분하여 x의 주변확률 분포 P(x)를 얻게됨)</p>
<p>$&#x3D; \int P(z|x)P(x) dz &#x3D; P(x)\int P(z|x) dz &#x3D; P(x)$   $(∵\int P(z|x) dz&#x3D;1)$ (조건부 확률 $P(z | x)$가 z에 대해 적분하면 1이 됨, 왜냐하면 조건부 확률 P(z|x)는 z의 확률 분포를 나타내기 때문)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5780265a-73fa-41ca-9c58-f82a57308cff" alt="MarginalDistribution"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Expectation-and-Sampling"><a href="#Expectation-and-Sampling" class="headerlink" title="Expectation and Sampling"></a>Expectation and Sampling</h3><ul>
<li><p>기대값(expectation)은 확률 분포와 그에 대응하는 함수의 가중평균을 의미</p>
</li>
<li><p>기대값의 수식 (앞으로 자주 보게될 수식)</p>
<ul>
<li><p>$\mathbb{E}</p>
<p>{\text{x} \sim P(\text{x})}[f(x)] &#x3D; \sum</p>
<p>{x \in \chi} P(x) \cdot f(x)$</p>
<ul>
<li>$\chi$는 모든 가능한  $x$의 집합, $f(x)$는 $x$에 대한 함수, $P(x)$는 $x$의 확률 분포</li>
</ul>
</li>
</ul>
</li>
<li><p>f(x)라는 어떤 함수가 있는데, P(x)에서 샘플링한 x를 함수에 넣어봄</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/16640675-e6f2-4514-810a-2ff446075b79" alt="Expectation"></p>
</li>
<li><p>주사위의 기대값은?</p>
<ul>
<li><p>$\mathbb{E}<em>{x \sim P(x)}[f(x)] &#x3D; \sum</em>{x \in {1,2,3,4,5,6}} P(x &#x3D; x) \cdot f(x)$</p>
<p>$&#x3D;\frac{1}{6}×(f(1)+f(2)+f(3)+f(4)+f(5)+f(6))$</p>
<p>$&#x3D; \frac{1}{6}×(1+2+3+4+5+6)&#x3D;3.5 \text{, where } f(x)&#x3D;x$</p>
</li>
</ul>
<p>cf) 모든 기대값이 같으면 uniform distribution 따라서 시그마 1&#x2F;n *f(x)</p>
</li>
<li><p>$P(x) &#x3D; \int P(x,z) dz$</p>
<p>$&#x3D; \int P(x|z)P(z) dz$</p>
<p>$&#x3D; \mathbb{E}_{\text{z} \sim P(\text{z})}[P(x|\text{z})]$  (P(x)를 z에 대한 P(x|z)의 기대값으로 표현)</p>
</li>
</ul>
<h3 id="Monte-Carlo-방법"><a href="#Monte-Carlo-방법" class="headerlink" title="Monte-Carlo 방법"></a>Monte-Carlo 방법</h3><ul>
<li>무작위 표본을 사용하여 수학적 문제를 해결하는 통계적인 방법</li>
<li>주로 다음과 같은 상황에서 사용됨<ul>
<li>강화 학습에서는 Monte Carlo 방법을 사용하여 에이전트의 정책(policy)을 평가하거나 업데이트</li>
<li>딥러닝에서 복잡한 확률 모델에서 샘플을 추출하기 위해 Monte Carlo 샘플링 기법을 사용</li>
<li>복잡한 수학적 문제 해결<ul>
<li>복잡한 수학적 문제를 직접 해결하는 것이 어려울 때, Monte Carlo 방법을 사용하여 근사적인 해답을 찾을 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>확률 분포로부터 샘플링을 통해 𝒇의 가중 평균을 구해보자<ul>
<li>sample의 크기가 커질수록 보다 정확하게 구할 수 있음</li>
<li>$\mathbb{E}<em>{x \sim P(x)}[f(x)] \approx \frac{1}{n} \sum</em>{i&#x3D;1}^{n} f(x_i) \text{, where } x_i \sim P(x)$</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-27T14:59:02.000Z" title="7/27/2023, 11:59:02 PM">2023-07-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:04:14.000Z" title="9/6/2024, 12:04:14 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">2 minutes read (About 356 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/2%EC%9E%A5-Probabilistic-Perspective-Introduction/">2장. Probabilistic Perspective - Introduction</a></h1><div class="content"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Again-our-objective-is"><a href="#Again-our-objective-is" class="headerlink" title="Again, our objective is"></a>Again, our objective is</h3><ul>
<li>가상의 함수를 모사하여, 원하는 출력값을 반환하는 신경망의 파라미터를 찾자.</li>
<li>그래서 우리는 Deep Neural Networks를 이야기할 때,<ul>
<li>Gradient Descent</li>
<li>Back-Propagation</li>
<li>Feature Vector</li>
<li>and blah blah..</li>
</ul>
</li>
<li>이제는 우리의 생각을 확장시켜야 할 때! → Probabilistic Perspective!</li>
</ul>
<h3 id="Probabilistic-Perspective"><a href="#Probabilistic-Perspective" class="headerlink" title="Probabilistic Perspective"></a>Probabilistic Perspective</h3><ul>
<li><p>이 세상은 확률에 기반</p>
<ul>
<li><p>아래의 그림에 대해서 모두가 같은 대답을 하지는 않을 것 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702" alt="duckrabbit"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702">https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702</a></p>
</li>
<li><p>우리의 새로운 목표: <code>확률 분포</code>를 학습하는 것</p>
</li>
</ul>
</li>
<li><p>Before vs After</p>
<ul>
<li>Before<ul>
<li>함수를 배우자(모사하자)</li>
</ul>
</li>
<li>After<ul>
<li>확률 분포 함수를 배우자<ul>
<li>수학적으로 더 설명이 가능해짐</li>
<li>불확실성까지 학습</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a" alt="BeforeAfter"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a">https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a</a></p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>Neural Networks는 확률 분포 함수를 모델링할 수 있음</li>
<li>이를 통해 가상의 확률 분포 함수 𝑃(𝑦 | 𝑥)를 근사(approximation)할 것</li>
<li>대부분의 최신 기술들은 이 관점에 기반을 두고 만들어짐</li>
<li>DNN을 확률 분포로 보았을 때, 가능한 이론들에 대해서 앞으로 이야기 할 것<ul>
<li>Likelihood</li>
<li>Maximum Likelihood Estimation(MLE)</li>
<li>Maximum A Posterior(MAP) Estimation</li>
<li>Cross Entropy &amp; KL-Divergence</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-26T14:21:28.000Z" title="7/26/2023, 11:21:28 PM">2023-07-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:40.000Z" title="9/6/2024, 12:03:40 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/1%EC%9E%A5-Representation-Learning-AutoEncoders/">1장. Representation Learning - AutoEncoders</a></h1><div class="content"><h2 id="AutoEncoders"><a href="#AutoEncoders" class="headerlink" title="AutoEncoders"></a>AutoEncoders</h2><ul>
<li><p>Overview</p>
<ul>
<li>인코더(encoder)와 디코더(decoder)를 통해 압축과 해제를 실행<ul>
<li>인코더는 입력(𝑥)의 정보를 최대한 보존하도록 손실 압축을 수행</li>
<li>디코더는 중간 결과물(𝑧)의 정보를 입력(𝑥)과 같아지도록 압축 해제(복원)를 수행</li>
</ul>
</li>
<li>복원을 성공적으로 하기 위해, autoencoder는 특<strong>징(feature)을 추출하는 방법을 자동으로 학습</strong></li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f9947a9e-f1f4-4e4c-b8ab-bba2427ffa45" alt="Encoder-Decoder"></p>
</li>
<li><p>Encoder</p>
<ul>
<li><p>복원에 필요한 정보를 중심으로 손실 압축을 수행</p>
</li>
<li><p>필요없는 정보(뻔한 특징)는 버릴 수도 있음</p>
<ul>
<li><p>예시1) 일반적인 사람의 얼굴을 학습할 때: 사람의 얼굴에서 눈은 2개이다 등</p>
</li>
<li><p>예시2)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2e64e11e-6879-4644-bb6c-d416e8cb5ebc" alt="NoMeaningMNIST"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Bottleneck(𝒛)</p>
<ul>
<li>입력(𝒙)에 비해 작은 차원으로 구성</li>
<li>따라서 정보의 선택과 압축이 발생, 차원에 따라 압축의 정도를 결정함<ul>
<li>집에 불이 나서 탈출할 때, 무엇을 들고 나갈 것인가?</li>
</ul>
</li>
<li>그러므로 𝒛 는 입력(𝒙)에 대한 feature vector라고 할 수 있다.</li>
<li>압축의 효율이 높아야 하므로, 입력에 비해 dense vector일 것</li>
</ul>
</li>
<li><p>Decoder</p>
<ul>
<li>압축된 중간 결과물(𝒙)을 바탕으로 최대한 입력(𝒛)과 비슷하게 복원 : $\hat{x}$</li>
<li>보통 MSELoss 를 통해 최적화 수행 ($MSE&#x3D;|\hat{x}-x|^2_2$)</li>
<li>뻔한 정보는 주어지지 않더라도 어차피 알 수 있기에 복원 가능</li>
</ul>
</li>
</ul>
<h2 id="Hidden-Representation"><a href="#Hidden-Representation" class="headerlink" title="Hidden Representation"></a>Hidden Representation</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul>
<li><p>인코더로부터 나온 중간 결과물(𝒛)은 입력(𝒙)에 대한 feature vector이다.</p>
</li>
<li><p>feature vector의 각 차원은 어떤 의미를 내포하고 있을까?</p>
<ul>
<li><p>인코더의 결과물 𝒛를 plot 하였을 때, 비슷한 샘플들은 비슷한 곳에 위치함을 확인</p>
</li>
<li><p>이 plot이 뿌려진 공간을 hidden(latent) space라고 부름(잠재공간, feature vector가 위치하는 곳)</p>
<ul>
<li>Input space의 MNIST 샘플이 latent space에 embedding 된 것</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c02a4df6-74c9-49f3-b269-87abd0764629" alt="LatentSpace"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Mapping-to-Hidden-Latent-Space"><a href="#Mapping-to-Hidden-Latent-Space" class="headerlink" title="Mapping to Hidden(Latent) Space"></a>Mapping to Hidden(Latent) Space</h3><ul>
<li>각 <strong>레이어의 결과물</strong>을 <code>hidden vector</code> 라고 부름</li>
<li>모두 feature vector라고 볼 수 있음</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/118c8c4d-dd4d-41cf-bc22-e524f3c77867" alt="hiddenSpace"></p>
<h3 id="Hidden-Latent-Representation"><a href="#Hidden-Latent-Representation" class="headerlink" title="Hidden(Latent) Representation"></a>Hidden(Latent) Representation</h3><ul>
<li><p>Tabular data의 feature vector와 달리, hidden vector는 해석이 어려움</p>
<ul>
<li>해석하고자 하는 연구(XAI. Explainable AI)들이 이어지고 있으나, 아직 갈 길이 멀다.</li>
</ul>
</li>
<li><p>하지만, 비슷한 특징을 가진 샘플은 비슷한 hidden vector를 가질 것</p>
</li>
<li><p>만약 각 차원이 명확하게 하나의 의미를 지닌다면 각 차원의 숫자를 조절하여 원하는 이미지를 합성해 낼 수 있을 것</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4acbea3d-5fa3-4503-8cd8-4662a03b798f" alt="hiddenSpace02"></p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>오토인코더(AE)는 압축과 해제를 반복하며 특징 추출을 자동으로 학습<ul>
<li>필요한 정보와 필요없는 정보를 구분할 수 있게되는 것</li>
</ul>
</li>
<li>인코더로부터 나온 중간 결과물(𝒛)은 입력(𝑥)에 대한 feature vector이다.<ul>
<li>a.k.a Embedding vector</li>
<li>인코더에 통과시키는 것은 feature vector에 대한 embedding 과정이라고 볼 수 있음</li>
</ul>
</li>
<li>Hidden layer의 결과값들을 hidden vectors라 부르며, 이들은 샘플의 feature를 담고 있음<ul>
<li>여러개의 hidden vector들을 feature vector라 부를 수 있음</li>
<li>딥러닝에서는 label을 classify하기 위한 feature 를 자동으로 추출하여 학습(전통적인 머신러닝과의 차이점)</li>
</ul>
</li>
<li>신경망(또는 레이어)을 통과시키는 것은 입력 공간(input space)에서 잠재 공간(latent space)로의 맵핑 과정<ul>
<li>고차원 공간(high-dimensional space) → 저차원 공간(lower-dimensional space)</li>
</ul>
</li>
<li>Hidden representaion을 해석하는 것은 매우 어려움<ul>
<li>하지만 비슷한 샘플은 비슷한 hidden representation을 지닐 것!</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-25T14:56:47.000Z" title="7/25/2023, 11:56:47 PM">2023-07-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:50.000Z" title="9/6/2024, 12:03:50 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">8 minutes read (About 1171 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/1%EC%9E%A5-Representation-Learning-One-hot-Encoding-%EB%B0%8F-Embedding-Vector/">1장. Representation Learning - One-hot Encoding 및 Embedding Vector</a></h1><div class="content"><h2 id="One-hot-Encoding"><a href="#One-hot-Encoding" class="headerlink" title="One-hot Encoding"></a>One-hot Encoding</h2><h3 id="Categorical-Value-vs-Continuous-Value"><a href="#Categorical-Value-vs-Continuous-Value" class="headerlink" title="Categorical Value vs Continuous Value"></a>Categorical Value vs Continuous Value</h3><ul>
<li>Categorical Value<ul>
<li>보통은 discrete value</li>
<li>단어, 클래스</li>
</ul>
</li>
<li>Continuous Value<ul>
<li>키, 몸무게</li>
</ul>
</li>
<li>Categorical Value와 Continuous Value의 가장 결정적인 차이점<ul>
<li>Continous value는 비슷한 값은 비슷한 의미를 지니지만</li>
<li>Categorical value는 비슷한 값일지라도 상관없는 의미를 지닌다.</li>
</ul>
</li>
</ul>
<h3 id="One-hot-Encoding의-필요성"><a href="#One-hot-Encoding의-필요성" class="headerlink" title="One-hot Encoding의 필요성"></a>One-hot Encoding의 필요성</h3><ul>
<li><p>One-hot Encoding의 필요성을 느끼기 위해 아래의 단어를 사전 순으로 index에 mapping 해보자</p>
<table>
<thead>
<tr>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody><tr>
<td>공책</td>
<td>딱풀</td>
<td>볼펜</td>
<td>샤프</td>
<td>연필</td>
<td>자</td>
<td>필기장</td>
</tr>
</tbody></table>
<ul>
<li>상식적으로는<ul>
<li>distance(연필, 볼펜) &lt; distance(연필, 자)</li>
<li>distance(공책, 필기장) &lt; distance(공책, 딱풀)</li>
</ul>
</li>
<li>하지만 이 테이블에서는 아래와 같은 결과가 나온다.<ul>
<li>|연필 - 볼펜| &#x3D; 2	&gt;	1 &#x3D; |연필 - 자|</li>
<li>|공책 - 필기장| &#x3D; 6	&gt;	1 &#x3D; |공책 - 딱풀|</li>
</ul>
</li>
<li>즉, 범주형 데이터를 임의의 숫자로 표현하게 되면, 텍스트 간에 원래 존재하지 않았던 ‘크기’나 ‘순서’의 개념이 부여됨. 이로 인해 범주형 데이터 간에 실제로는 없는 거리 혹은 차이가 존재하는 것처럼 해석되어, 불필요하거나 잘못된 정보를 학습하는 결과를 초래할 수 있음</li>
</ul>
</li>
</ul>
<h3 id="One-hot-Encoding-이란"><a href="#One-hot-Encoding-이란" class="headerlink" title="One-hot Encoding 이란"></a>One-hot Encoding 이란</h3><ul>
<li><p>One-hot 인코딩은 범주형 데이터를 컴퓨터가 이해할 수 있는 형태로 변환하는 방법</p>
</li>
<li><p>각각의 범주를 벡터의 형태로 표현하며, 각 범주의 위치에 해당하는 인덱스만 1로 표시하고 나머지는 0으로 표시</p>
</li>
<li><p>크기가 의미를 갖는 integer 값 대신, 1개의 1과 n-1개의 0으로 이루어진 n차원의 벡터</p>
<table>
<thead>
<tr>
<th></th>
<th>index</th>
<th>공책</th>
<th>딱풀</th>
<th>볼펜</th>
<th>샤프</th>
<th>연필</th>
<th>자</th>
</tr>
</thead>
<tbody><tr>
<td>딱풀</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>연필</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>자</td>
<td>5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<ul>
<li>n개의 항목 → n차원</li>
<li>즉, 6개의 항목 → 6차원</li>
</ul>
</li>
<li><p>Vector의 대부분의 element가 0인 경우 Sparse Vector라고 부름</p>
<ul>
<li>반대 개념 : Dense Vector ↔ Sparse Vector</li>
</ul>
</li>
<li><p>One-hot Encoding의 장점</p>
<ul>
<li>범주형 데이터를 숫자로 변환</li>
<li>범주 간의 순서 없음</li>
<li>특성 간의 독립성 보장</li>
</ul>
</li>
<li><p>One-hot Encoding의 단점</p>
<ul>
<li>서로 다른 두 벡터는 항상 직교(orthogonal)한다. (element-wise 곱 &#x3D; 0)<ul>
<li>Cosine Similarity가 0, 따라서 두 샘플 사이의 유사도(거리)를 구할 수 없음</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Embedding-Vectors"><a href="#Embedding-Vectors" class="headerlink" title="Embedding Vectors"></a>Embedding Vectors</h2><h3 id="Motivation-of-Embedding-Vectors"><a href="#Motivation-of-Embedding-Vectors" class="headerlink" title="Motivation of Embedding Vectors"></a>Motivation of Embedding Vectors</h3><ul>
<li><p>NLP에서 단어는 categorical value &amp; discrete value의 속성을 갖음</p>
<ul>
<li>따라서 one-hot representation으로 표현</li>
<li>하지만 이는 실제 존재하는 단어 사이의 유사도를 표현할 수 없음</li>
<li>따라서 실제적으로는 좀더 고급화된 Embedding 기법을 사용</li>
</ul>
</li>
<li><p>다른 Embedding Vectors 표현 기법</p>
<ul>
<li><p>Contextual Word Embedding (문맥적 단어 임베딩)</p>
<ul>
<li><p>기존의 단어 임베딩 방법은 단어의 의미가 문맥에 따라 달라질 수 있다는 점을 고려하지 못했는데 이를 해결하기 위해 등장</p>
</li>
<li><p>단어를 임베딩할 때 주변 문맥을 고려해 동일한 단어라도 다른 문맥에서는 다른 임베딩 벡터를 갖게함</p>
</li>
<li><p>BERT, ELMO 등이 해당됨</p>
</li>
<li><p>코드로 구현하기</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># BERT 모델과 토크나이저 초기화</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 문장</span></span><br><span class="line">sentences = [<span class="string">&quot;I went to the store&quot;</span>, <span class="string">&quot;I went to the school&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 토큰화 및 임베딩</span></span><br><span class="line">inputs = tokenizer(sentences, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 각 문장의 [CLS] 토큰에 대한 임베딩 가져오기</span></span><br><span class="line">embeddings = outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line"><span class="built_in">print</span>(embeddings)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Subword Embedding (서브워드 임베딩)</p>
<ul>
<li>특정 언어들은 단어를 더 작은 의미 단위로 분리할 수 있음</li>
<li>이런 경우, 서브워드 임베딩을 사용하여 작은 단위들을 학습할 수 있음</li>
<li>BPE(Byte Pair Encoding), SentencePiece 등이 해당됨</li>
</ul>
</li>
<li><p>Document Embedding (문서 임베딩)</p>
<ul>
<li>문서 전체를 하나의 벡터로 표현하는 방법</li>
<li>문서 임베딩은 문서의 전체적인 의미를 이해하는데 도움이 됨</li>
<li>Doc2Vec, FastText 등이 해당</li>
</ul>
</li>
<li><p>Word Embedding (단어 임베딩)</p>
<ul>
<li>고차원의 One-hot 벡터를 저차원의 실수 벡터로 변환하는 기법</li>
<li>비슷한 의미를 가진 단어들이 벡터 공간에서 가까이 위치하도록 학습</li>
<li>Word2Vec, GloVe 등이 해당됨</li>
</ul>
</li>
</ul>
</li>
<li><p>개인적인 경험으로는 Contextual Word Embedding + Subword Embedding을 결합해서 가장 많이 사용하는 것 같으며, Word Embedding은 김기현 님의 말씀에 의하면 Embedding에 적합하지 않다고 말씀해주신 것으로 알고 있다.</p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>Categorical Value는 One-hot Encoding을 통해 벡터로 표현됨</li>
<li>Sparse Vector는 벡터 간 유사도 계산이 어려움 → One-hot의 단점</li>
<li>따라서 Dense Vector로 표현할 필요가 있음 → Contextual Embedding 등을 사용!</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-24T14:40:25.000Z" title="7/24/2023, 11:40:25 PM">2023-07-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:45.000Z" title="9/6/2024, 12:03:45 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">6 minutes read (About 889 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/1%EC%9E%A5-Representation-Learning-Feature-Vector/">1장. Representation Learning - Feature Vector</a></h1><div class="content"><h2 id="Representation-Learning-표현-학습"><a href="#Representation-Learning-표현-학습" class="headerlink" title="Representation Learning (표현 학습)"></a>Representation Learning (표현 학습)</h2><ul>
<li>머신러닝의 하위 분야로서, 데이터의 숨겨진 구조를 학습하여 복잡한 데이터를 효율적이고 쉽게 이해할 수 있는 표현 형태로 변환하는 방법을 연구하는 분야</li>
<li>이 변환된 형태를 통해 원본 데이터의 중요한 특성이나 패턴을 찾아내고, 이를 사용해 효과적으로 학습 및 예측을 수행</li>
<li>원본 데이터의 복잡성을 줄이고, 노이즈를 제거하며, 데이터의 중요한 특성을 보다 명확하게 강조하는 역할, 이를 통해 머신러닝 모델의 성능을 향상시키고, 학습 과정을 단순화</li>
<li>오토인코더, 딥 비지도 학습, 임베딩 학습 등은 표현 학습의 대표적인 예시<ul>
<li>원본 데이터에서 중요한 특성을 추출하고</li>
<li>차원 공간에서 표현하는 방법을 학습하며</li>
<li>결과적으로 데이터의 가장 핵심적인 특성만을 잘 보존하는 표현을 찾아냄</li>
</ul>
</li>
</ul>
<h2 id="Feature-특징"><a href="#Feature-특징" class="headerlink" title="Feature(특징)"></a>Feature(특징)</h2><h3 id="Feature란"><a href="#Feature란" class="headerlink" title="Feature란?"></a>Feature란?</h3><ul>
<li>샘플을 잘 설명하는 특징</li>
<li>사람을 설명할 때 좋은 특징<ul>
<li>Continuous: 나이, 키, 몸무게, 소득</li>
<li>Categorical: 성별, 직업, 거주지, 출신 학교&#x2F;학과</li>
</ul>
</li>
<li>나쁜 특징<ul>
<li>(생물 분류학적) 종<ul>
<li>모두가 호모 사피엔스이므로 구분이 불가능</li>
</ul>
</li>
<li>주민등록번호, 이름<ul>
<li>전 국민의 수 만큼 momory가 필요할 것</li>
<li>주민등록번호만으로는 (비록 일부 특징이 유사하여도) 두 사람의 유사도를 알 수 없음</li>
<li>Categorical value로 볼 수 있음</li>
</ul>
</li>
</ul>
</li>
<li><strong>특징을 통해 우리는 특정 샘플을 수치화</strong> 할 수 있음</li>
<li>현실에서의 대표적인 Feature의 예 - 몽타주(Montage)<ul>
<li>범인의 얼굴을 특정하기 위해서, 목격자들에게 물어서 나온 특징들을 합쳐 만든 것<ul>
<li>좋은 단서<ul>
<li>뺨에 큰 붉은 반점</li>
<li>쳐진 눈</li>
<li>긴 생머리</li>
</ul>
</li>
<li>나쁜 단서<ul>
<li>눈이 2개</li>
<li>귀가 2개</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Feature-in-Machine-Learning"><a href="#Feature-in-Machine-Learning" class="headerlink" title="Feature in Machine Learning"></a>Feature in Machine Learning</h3><ul>
<li>MNIST Classifcation<ul>
<li>특정 위치에 곧은(휘어진) 선이 얼마나 있는가?</li>
<li>특정 위치에 선이 얼마나 굵은가?</li>
<li>특정 위치에 선이 얼마나 기울어져 있는가?</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de">https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de</a></p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de" alt="MNIST"></p>
<h3 id="No-Need-of-Hand-crafted-Feature-in-Deep-Learning"><a href="#No-Need-of-Hand-crafted-Feature-in-Deep-Learning" class="headerlink" title="No Need of Hand-crafted Feature in Deep Learning"></a>No Need of Hand-crafted Feature in Deep Learning</h3><ul>
<li>Traditional Machine Learning<ul>
<li>사람이 데이터를 면밀히 분석 후, 가정을 세움</li>
<li>가정에 따라 전처리를 하여 feature를 추출</li>
<li>추출된 feature를 model에 넣어 학습</li>
<li>장점 : 사람이 해석하기 쉬움</li>
<li>단점 : 사람이 미처 생각하지 못한 특징의 존재 가능성</li>
</ul>
</li>
<li>Current Deep Learning<ul>
<li>Raw 데이터에 <strong>최소한의 전처리</strong>(e.g. scale)를 수행</li>
<li>데이터를 model에 넣어 학습</li>
<li>장점 : 구현이 용이함, 미처 발견하지 못한 특징도 활용</li>
<li>단점 : 사람이 해석하기 어려움</li>
</ul>
</li>
</ul>
<h3 id="Feature-Vector"><a href="#Feature-Vector" class="headerlink" title="Feature Vector"></a>Feature Vector</h3><ul>
<li>각 <code>특징들을 모아서 하나의 vector</code>로 만든 것<ul>
<li>Tabular Dataset의 각 row도 이에 해당</li>
</ul>
</li>
<li>각 차원(dimension)은 어떤 속성에 대한 level을 나타냄<ul>
<li>각 속성에 대한 level이 비슷할수록 비슷한 샘플이라고 볼 수 있음</li>
</ul>
</li>
<li>우리는 feature vector를 통해 샘플 사이의 거리(유사도)를 계산할 수 있음</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>키</th>
<th>몸무게</th>
<th>나이</th>
<th>월 소득</th>
<th>출신</th>
</tr>
</thead>
<tbody><tr>
<td>로버트</td>
<td>174cm</td>
<td>78kg</td>
<td>42</td>
<td>100</td>
<td>미국 매사추세츠</td>
</tr>
<tr>
<td>캡틴아메리카</td>
<td>183cm</td>
<td>88kg</td>
<td>58</td>
<td>1000</td>
<td>미국 뉴욕</td>
</tr>
</tbody></table>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-23T14:44:53.000Z" title="7/23/2023, 11:44:53 PM">2023-07-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:25.000Z" title="9/6/2024, 12:03:25 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 565 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Learning-Rate/">7장. 기초 최적화 방법 Gradient Descent - Learning Rate</a></h1><div class="content"><h2 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h2><h3 id="Learning-Rate-in-Gradient-Descent"><a href="#Learning-Rate-in-Gradient-Descent" class="headerlink" title="Learning Rate in Gradient Descent"></a>Learning Rate in Gradient Descent</h3><ul>
<li>파라미터가 업데이트 될 때, gradient의 크기에 영향을 받게 됨<ul>
<li>이 때, learning rate가 step-size를 정해주게 됨</li>
</ul>
</li>
<li>Equation<ul>
<li>$\theta \gets \theta - \eta \frac{\partial L(\theta)}{\partial \theta} &#x3D; \theta - \eta \nabla_\theta L(\theta)$</li>
</ul>
</li>
</ul>
<h3 id="Learning-Rate-에-따른-최적화-데이터나-모델-아키텍처에-따라-lr은-변함"><a href="#Learning-Rate-에-따른-최적화-데이터나-모델-아키텍처에-따라-lr은-변함" class="headerlink" title="Learning Rate 에 따른 최적화 (데이터나 모델 아키텍처에 따라 lr은 변함)"></a>Learning Rate 에 따른 최적화 (데이터나 모델 아키텍처에 따라 lr은 변함)</h3><ul>
<li>Large LR<ul>
<li>너무 큰 Loss가 발산할 수 있음</li>
</ul>
</li>
<li>Small LR<ul>
<li>너무 작은 LR은 수렴이 늦음</li>
<li>자칫 local minima에 빠질 수 있음</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589" alt="LearningRate"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589">https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589</a></p>
<h3 id="Learning-Rate-는-중요한-하이퍼파라미터"><a href="#Learning-Rate-는-중요한-하이퍼파라미터" class="headerlink" title="Learning Rate 는 중요한 하이퍼파라미터"></a>Learning Rate 는 중요한 하이퍼파라미터</h3><ul>
<li><strong>실험을 통해 최적화</strong>하는 것이 필요</li>
<li>초보자들은 처음에 어떤 값을 정해야 할지 난감<ul>
<li>고민할 바에 그냥 아주 작은 값(eg. 1e-4)으로 오래 돌려도 괜찮음</li>
</ul>
</li>
<li>나중에 Adam Optimizer를 통해 Learning Rate에 대한 고민을 없앨 수 있음</li>
</ul>
<p>※ 하이퍼파라미터 : 모델 성능에 영향을 끼치지만, 데이터를 통해 학습할 수 없는 파라미터 (우리가 직접 테스트하고 튜닝을 해야함)</p>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><ul>
<li><p>Gradient Descent + Learning Rate 실습</p>
<ul>
<li>$L(x)&#x3D;| targert - x |_2^2$</li>
<li>$x \gets x - \eta \nabla_x L(x)$<ul>
<li>cf) $\nabla_x L(x) &#x3D; x.grad$</li>
</ul>
</li>
<li>$\hat{x} &#x3D; \text{argmin}L(x)$, $x \in \mathbb{R}^{3,3}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">target = torch.FloatTensor([[<span class="number">.1</span>, <span class="number">.2</span>, <span class="number">.3</span>],</span><br><span class="line">                            [<span class="number">.4</span>, <span class="number">.5</span>, <span class="number">.6</span>],</span><br><span class="line">                            [<span class="number">.7</span>, <span class="number">.8</span>, <span class="number">.9</span>]])</span><br><span class="line"></span><br><span class="line">x = torch.rand_like(target)</span><br><span class="line"><span class="comment"># This means the final scalar will be differentiate by x.</span></span><br><span class="line">x.requires_grad = <span class="literal">True</span></span><br><span class="line"><span class="comment"># You can get gradient of x, after differentiation.</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor([[0.4176, 0.6465, 0.3522],</span></span><br><span class="line"><span class="comment">#         [0.9164, 0.7576, 0.0892],</span></span><br><span class="line"><span class="comment">#         [0.4854, 0.0136, 0.9467]], requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">loss = F.mse_loss(x, target)</span><br><span class="line"><span class="built_in">print</span>(loss) <span class="comment"># tensor(0.1737, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"></span><br><span class="line">threshold = <span class="number">1e-5</span></span><br><span class="line">learning_rate = <span class="number">1.</span></span><br><span class="line">iter_cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> loss &gt; threshold:</span><br><span class="line">    iter_cnt += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment"># Calculate gradients.</span></span><br><span class="line"></span><br><span class="line">    x = x - learning_rate * x.grad</span><br><span class="line">    </span><br><span class="line">    x.detach_()</span><br><span class="line">    x.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    loss = F.mse_loss(x, target)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%d-th Loss: %.4e&#x27;</span> % (iter_cnt, loss))</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    1 - th Loss: 1.0510e-01</span></span><br><span class="line"><span class="string">    tensor([[0.3470, 0.5473, 0.3406],</span></span><br><span class="line"><span class="string">            [0.8016, 0.7003, 0.2027],</span></span><br><span class="line"><span class="string">            [0.5331, 0.1883, 0.9363]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    2 - th Loss: 6.3576e-02</span></span><br><span class="line"><span class="string">    tensor([[0.2921, 0.4701, 0.3316],</span></span><br><span class="line"><span class="string">            [0.7124, 0.6558, 0.2910],</span></span><br><span class="line"><span class="string">            [0.5702, 0.3242, 0.9282]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    3 - th Loss: 3.8460e-02</span></span><br><span class="line"><span class="string">    tensor([[0.2494, 0.4101, 0.3246],</span></span><br><span class="line"><span class="string">            [0.6430, 0.6212, 0.3597],</span></span><br><span class="line"><span class="string">            [0.5990, 0.4300, 0.9220]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    19 - th Loss: 1.2370e-05</span></span><br><span class="line"><span class="string">    tensor([[0.1027, 0.2038, 0.3004],</span></span><br><span class="line"><span class="string">            [0.4044, 0.5022, 0.5957],</span></span><br><span class="line"><span class="string">            [0.6982, 0.7934, 0.9004]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    20 - th Loss: 7.4833e-06</span></span><br><span class="line"><span class="string">    tensor([[0.1021, 0.2029, 0.3003],</span></span><br><span class="line"><span class="string">            [0.4034, 0.5017, 0.5966],</span></span><br><span class="line"><span class="string">            [0.6986, 0.7948, 0.9003]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="7장-Wrap-up"><a href="#7장-Wrap-up" class="headerlink" title="7장 Wrap up"></a>7장 Wrap up</h2><h3 id="Why-we-do-gradient-descent"><a href="#Why-we-do-gradient-descent" class="headerlink" title="Why we do gradient descent?"></a>Why we do gradient descent?</h3><ul>
<li>실재하지만 알 수 없는 함수 $f^*$를 근사하고 싶음</li>
<li>나의 모델(함수)의 $f_\theta$ 파라미터 𝜽를 조절</li>
<li><strong>손실 함수(Loss Function)를 최소화 하도록 파라미터 𝜽를 조절</strong></li>
<li>미분을 통해 gradient($\frac{\partial Loss}{\partial \theta}$)를 얻고, loss를 낮추는 방향으로 파라미터를 업데이트</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-22T17:25:07.000Z" title="7/23/2023, 2:25:07 AM">2023-07-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-07-22T17:25:07.000Z" title="7/23/2023, 2:25:07 AM">2023-07-23</time></span><span class="level-item">41 minutes read (About 6189 words)</span></div></div><div class="content"><h2 id="프로세스와-쓰레드란-무엇인가요"><a href="#프로세스와-쓰레드란-무엇인가요" class="headerlink" title="프로세스와 쓰레드란 무엇인가요??"></a>프로세스와 쓰레드란 무엇인가요??</h2><p>프로세스와 쓰레드는 컴퓨터에서 실행되는 작업의 실행 단위로 모두 동시성(concurrency)을 다룹니다.</p>
<h2 id="프로세스와-쓰레드의-차이는-어떻게-되나요"><a href="#프로세스와-쓰레드의-차이는-어떻게-되나요" class="headerlink" title="프로세스와 쓰레드의 차이는 어떻게 되나요??"></a>프로세스와 쓰레드의 차이는 어떻게 되나요??</h2><ul>
<li>프로세스<ul>
<li>운영체제에서 실행 중인 프로그램을 의미</li>
<li>독립적인 메모리 공간과 시스템 자원(CPU, 메모리 등)을 할당받음</li>
<li>따라서 각각의 프로세스는 완전히 분리되어 서로 영향을 미치지 않음</li>
<li>즉, 프로세스 간에는 메모리를 공유할 수 없음(직접적인 접근이 불가능)</li>
<li>만약에 하나의 프로세스에서 다른 프로세스에 접근하려면 IPC(Inter-Process Communication)를 사용</li>
<li>Java에서는 <code>java.lang.Process</code> 클래스를 통해 프로세스를 생성 및 제어</li>
<li>Python에서는 <code>multiprocessing</code> 모듈을 통해 프로세스를 생성 및 제어</li>
</ul>
</li>
<li>쓰레드<ul>
<li>프로세스 내에서 실행되는 작은 실행 단위</li>
<li>하나의 프로세스 내에서 여러 개의 쓰레드를 생성하여 실행할 수 있음</li>
<li>쓰레드는 각각 자신의 실행 스택을 가지고 있지만, 프로세스 내의 다른 쓰레드와 메모리 공간을 공유하며 실행(부모 프로세스의 메모리 공간과 자원을 공유)</li>
<li>따라서 쓰레드는 프로세스보다 가볍고 실행 속도가 빠름(자원의 효율적인 활용 가능)</li>
<li>하지만 여러 쓰레드가 공유된 메모리 공간을 사용하기에 데이터 불일치 문제를 고려해야함<ul>
<li>Java에서는 동기화(synchronization)를 통해 문제를 해결</li>
</ul>
</li>
<li>Java에서는 <code>java.lang.Thread</code> 클래스를 사용하여 쓰레드를 생성 및 제어</li>
<li>Python에서는 <code>threading</code> 모듈을 통해 쓰레드를 생성 및 제어 </li>
<li>모든 Java 프로그램은 기본적으로 하나의 쓰레드(메인 쓰레드)를 갖음, main method를 실행하고, 프로그램이 종료될 때까지 실행됨</li>
</ul>
</li>
</ul>
<h2 id="프로세스와-쓰레드의-공통점은-어떻게-되나요"><a href="#프로세스와-쓰레드의-공통점은-어떻게-되나요" class="headerlink" title="프로세스와 쓰레드의 공통점은 어떻게 되나요??"></a>프로세스와 쓰레드의 공통점은 어떻게 되나요??</h2><ol>
<li>실행 흐름 : 모두 실행 흐름을 나타냄. 즉 프로그램이 실행되어 작업을 처리하는 단위</li>
<li>자원 공유 : CPU, Memory, 파일, 네트워크 등의 시스템 자원을 공유<ul>
<li>다만 Memory의 경우 프로세스는 독립된 메모리 공간을 사용하고, 쓰레드는 프로세스의 메모리 공간을 공유</li>
</ul>
</li>
<li>스케줄링 : 모두 스케줄링의 대상이 됨, 즉 운영체제가 자원을 할당하는 대상이 됨</li>
<li>동시성 : 프로세스와 쓰레드는 모두 동시에 실행될 수 있으며, 이는 멀티태스킹을 구현하기 위한 필수적인 기능</li>
<li>컨텍스트 스위칭 : 실행중인 프로세스나 쓰레드를 일시 중단하고, 다른 프로세스나 쓰레드를 실행할 수 있음</li>
</ol>
<h2 id="동시성-프로그래밍-Concurrency-Programming-이란-무엇인가요"><a href="#동시성-프로그래밍-Concurrency-Programming-이란-무엇인가요" class="headerlink" title="동시성 프로그래밍(Concurrency Programming)이란 무엇인가요??"></a>동시성 프로그래밍(Concurrency Programming)이란 무엇인가요??</h2><ul>
<li>하나의 컴퓨터 시스템에서 여러 개의 작업(task)이 동시에 실행되는 프로그래밍 기법, 즉 여러 작업을 동시에 처리해 시스템의 활용도를 향상시킬 수 있습니다.</li>
<li>동시성 프로그래밍에서는 동시에 실행되는 작업들이 서로 영향을 주지 않도록 관리해야 합니다.<ul>
<li>세마포어(Semaphore), 뮤텍스(Mutex), 락(Lock) 등 동기화 기법을 사용하여 공유 데이터에 대한 접근을 제어해야 합니다.</li>
</ul>
</li>
<li>멀티쓰레드, 멀티프로세싱, 비동기 프로그래밍 등 다양한 기법이 사용됩니다.</li>
</ul>
<h2 id="동시성-프로그맹과-병렬-프로그래밍은-어떠한-차이가-있나요"><a href="#동시성-프로그맹과-병렬-프로그래밍은-어떠한-차이가-있나요" class="headerlink" title="동시성 프로그맹과 병렬 프로그래밍은 어떠한 차이가 있나요??"></a>동시성 프로그맹과 병렬 프로그래밍은 어떠한 차이가 있나요??</h2><p>네. 비슷한 개념으로 생각할 수 있지만, 약간의 차이가 있습니다.</p>
<ul>
<li>병렬 프로그래밍<ul>
<li>여러 개의 프로세서(코어)를 사용하여 한 작업을 분할하여 동시에 실행</li>
<li>하나의 큰 작업을 작은 작업으로 분할하여 각각의 작은 작업을 병렬적으로 실행</li>
<li>병렬 프로그래밍은 대규모 컴퓨팅 자원이 필요하며, 여러 개의 CPU나 GPU를 사용하여 구현됨</li>
</ul>
</li>
<li>동시성 프로그래밍<ul>
<li>단일 프로세서에서 여러 작업을 동시에 실행하는 것</li>
<li>따라서 여러 개의 작업이 빠르게 번갈아 가며 실행되어 실제로는 작업들이 서로 영향을 주지 않도록 관리되어 동시에 실행</li>
</ul>
</li>
</ul>
<h2 id="동시성-프로그래밍에서-멀티쓰레드-멀티프로세싱-비동기-프로그래밍의-차이는-무엇인가요"><a href="#동시성-프로그래밍에서-멀티쓰레드-멀티프로세싱-비동기-프로그래밍의-차이는-무엇인가요" class="headerlink" title="동시성 프로그래밍에서 멀티쓰레드, 멀티프로세싱, 비동기 프로그래밍의 차이는 무엇인가요??"></a>동시성 프로그래밍에서 멀티쓰레드, 멀티프로세싱, 비동기 프로그래밍의 차이는 무엇인가요??</h2><ul>
<li>비동기 프로그래밍<ul>
<li>작업이 완료될 때까지 기다리지 않고 다른 작업을 수행할 수 있는 기술</li>
<li>콜백(callback)함수나 프로미스(promise) 등을 사용하여 작업 완료 시점에 처리를 수행</li>
<li>입출력이 느린 작업이나 네트워크 작업 등에서 더 나은 성능을 얻을 수 있는 이점이 있음</li>
</ul>
</li>
<li>멀티쓰레드<ul>
<li>하나의 프로세스 내에서 여러 개의 쓰레드를 생성하여, 각 쓰레드가 병렬로 작업을 수행</li>
<li>쓰레드는 같은 메모리 공간을 공유하기에, 데이터를 공유할 때 동기화 문제를 고려해야함</li>
</ul>
</li>
<li>멀티프로세싱<ul>
<li>여러 개의 프로세스를 생성하여 각 프로세스가 병렬로 작업을 수행하는 것</li>
<li>각 프로세스는 독립적인 메모리 공간을 가지므로, 데이터를 공유하기 위해서는 IPC를 사용해야 함</li>
<li>멀티쓰레드 보다는 안정적이지만 더 많은 리소스를 사용한다는 단점이 있음</li>
</ul>
</li>
</ul>
<h2 id="자바에서-비동기-프로그래밍은-어떻게-구현하나요"><a href="#자바에서-비동기-프로그래밍은-어떻게-구현하나요" class="headerlink" title="자바에서 비동기 프로그래밍은 어떻게 구현하나요??"></a>자바에서 비동기 프로그래밍은 어떻게 구현하나요??</h2><ol>
<li>콜백(Callback) 기반의 비동기 프로그래밍을 주로 사용<ul>
<li>콜반 기반의 프로그래밍은 콜백 함수를 등록하여 비동기식 작업이 완료되면 해당 콜백 함수가 호출되도록 하는 것</li>
<li>이를 위해 자바에서는 Future나 CompletableFuture 클래스를 사용하여 비동기식 작업을 수행<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// supplyAsync() 메서드를 사용하여 비동기 작업을 수행하고, thenAccept() 메서드를 사용하여 작업이 완료된 후 호출될 콜백 함수를 등록</span></span><br><span class="line"><span class="comment">// 이러한 방식으로 콜백 함수를 등록하여 비동기식 작업을 처리</span></span><br><span class="line">CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123;</span><br><span class="line">    <span class="comment">// 비동기 작업을 수행하는 코드</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;result&quot;</span>;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">future.thenAccept(result -&gt; &#123;</span><br><span class="line">    <span class="comment">// 비동기 작업이 완료된 후 호출될 콜백 함수</span></span><br><span class="line">    System.out.println(<span class="string">&quot;Result: &quot;</span> + result);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Stream API를 사용<ul>
<li>Stream API는 병렬 처리를 지원하므로 멀티코어 CPU에서 더욱 효과적으로 동작<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// parallelStream() 메서드를 사용하여 리스트의 항목을 병렬로 처리하고, forEach() 메서드를 사용하여 각 항목에 대한 비동기식 작업을 수행</span></span><br><span class="line">List&lt;String&gt; list = Arrays.asList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>);</span><br><span class="line"></span><br><span class="line">list.parallelStream().forEach(item -&gt; &#123;</span><br><span class="line">    <span class="comment">// 비동기 작업을 수행하는 코드</span></span><br><span class="line">    System.out.println(<span class="string">&quot;Item: &quot;</span> + item);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h2 id="동기식-처리와-비동기식-처리의-차이점은-무엇인가요"><a href="#동기식-처리와-비동기식-처리의-차이점은-무엇인가요" class="headerlink" title="동기식 처리와 비동기식 처리의 차이점은 무엇인가요??"></a>동기식 처리와 비동기식 처리의 차이점은 무엇인가요??</h2><ul>
<li>프로그램에서 작업이 실행되는 방식의 차이점에 따라 구분이 됨<ul>
<li>동기식 처리<ul>
<li>작업이 순차적으로 실행되어 결과가 반환되기 전까지 다음 작업이 실행되지 않는 방식</li>
<li>즉, 어떤 작업이 완료될 때까지 다음 작업을 기다리는 방식</li>
<li>예를 들면, 웹 서버에서 동기식으로 작성된 코드는 한 번에 하나의 요청만 처리할 수 있음</li>
</ul>
</li>
<li>비동기식 처리<ul>
<li>작업이 동시에 실행되어 결과가 반환되기를 기다리지 않고 다음 작업을 실행할 수 있는 방식</li>
<li>즉, 어떤 작업이 완료되기를 기다리지 않고, 다음 작업을 실행하는 방식</li>
<li>예를 들면 웹 서버에서 비동기식으로 작성된 코드는 여러 요청을 동시에 처리할 수 있음</li>
<li>또 다른 예로 웹 페이지에서 이미지를 로드되는 동안 텍스트 작업을 수행할 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>비동기식 처리는 작업이 완료될 때까지 기다리지 않고 다른 작업을 처리하므로, 더 높은 처리량을 얻을 수 있으나</li>
<li>코드가 복잡해지고 처리 결과를 조합하는 작업이 필요할 경우 처리 방식이 복잡해질 수 있음</li>
<li>반면 동기식 처리는 간단하고 직관적이지만, 대규모의 작업을 처리하는 데 부적합할 수 있음</li>
</ul>
<h2 id="파이썬에서-비동기식-처리를-수행할-때-처리하는-원리가-어떻게-되나요"><a href="#파이썬에서-비동기식-처리를-수행할-때-처리하는-원리가-어떻게-되나요" class="headerlink" title="파이썬에서 비동기식 처리를 수행할 때 처리하는 원리가 어떻게 되나요??"></a>파이썬에서 비동기식 처리를 수행할 때 처리하는 원리가 어떻게 되나요??</h2><blockquote>
<p>파이썬에서 비동기식 처리를 수행할 때, asyncio 라이브러리를 이용하여 이벤트 루프(event loop)를 생성하고, 코루틴(coroutine)을 이용하여 비동기식 처리를 수행합니다.  </p>
<p>이벤트 루프는 이벤트 발생을 대기하다가, 이벤트가 발생하면 그에 대응하는 작업을 처리하는 루프입니다. 즉, 이벤트 루프는 코루틴을 실행하고, I&#x2F;O 작업이 완료될 때까지 기다린 후, 결과를 반환합니다. 이벤트 루프는 코루틴을 실행할 때, 작업을 블로킹하지 않고 비동기적으로 실행합니다.  </p>
<p>코루틴은 제너레이터(generator)와 비슷한 개념으로, 함수 실행 중에 일시 중지하고 다른 작업을 수행한 후, 다시 원래 작업을 재개하는 것을 가능하게 합니다. 이러한 특징을 이용하여 비동기식 처리를 수행할 수 있습니다. 비동기식 작업은 코루틴으로 구현되어, 이벤트 루프에 의해 실행되며, 작업이 완료되기 전에 다른 작업을 실행할 수 있습니다.  </p>
<p>이러한 방식으로 비동기식 처리를 수행하면, I&#x2F;O 작업이 많은 네트워크 프로그램에서 효과적으로 CPU 자원을 활용할 수 있습니다. I&#x2F;O 작업이 끝날 때까지 대기하는 대신, 다른 작업을 실행할 수 있기 때문입니다. 이러한 방식은 네트워크 프로그램뿐만 아니라, 다양한 분야에서 활용할 수 있으며, 파이썬에서는 asyncio 라이브러리를 이용하여 비동기식 처리를 수행할 수 있습니다.</p>
</blockquote>
<ul>
<li>파이썬의 asyncio 라이브러리르 이용하여 Echo 서버를 비동기식으로 구현한 예시 코드<ul>
<li><p>여러 개의 클라이언트의 요청을 동시에 처리하여 Throughput(처리량)을 향상시킬 수 있음</p>
</li>
<li><p>Echo 서버용 코드</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">async def handle_echo(reader, writer):</span><br><span class="line">    data = await reader.read(100)</span><br><span class="line">    message = data.decode()</span><br><span class="line">    addr = writer.get_extra_info(&#x27;peername&#x27;)</span><br><span class="line">    print(f&quot;Received &#123;message!r&#125; from &#123;addr!r&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    writer.write(data)</span><br><span class="line">    await writer.drain()</span><br><span class="line">    print(f&quot;Send &#123;message!r&#125; to &#123;addr!r&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">async def main():</span><br><span class="line">    server = await asyncio.start_server(handle_echo, &#x27;127.0.0.1&#x27;, 8888)</span><br><span class="line">    print(f&quot;Serving on &#123;server.sockets[0].getsockname()&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    async with server:</span><br><span class="line">        await server.serve_forever()</span><br><span class="line"></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>위 코드는 asyncio 모듈의 start_server() 함수를 이용하여 Echo 서버를 구현한 코드입니다. 클라이언트가 연결되면, handle_echo() 코루틴이 실행되어 클라이언트 요청을 처리합니다. 이 코드에서는 클라이언트 요청이 처리될 때마다 새로운 코루틴을 생성하여 요청을 처리하므로, 다수의 클라이언트 요청을 동시에 처리할 수 있습니다.</p>
</blockquote>
</li>
<li><p>클라이언트 요청을 보내는 코드</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">async def tcp_echo_client(message):</span><br><span class="line">    reader, writer = await asyncio.open_connection(&#x27;127.0.0.1&#x27;, 8888)</span><br><span class="line"></span><br><span class="line">    print(f&quot;Send &#123;message!r&#125;&quot;)</span><br><span class="line">    writer.write(message.encode())</span><br><span class="line"></span><br><span class="line">    data = await reader.read(100)</span><br><span class="line">    print(f&quot;Received &#123;data.decode()!r&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line">    await writer.wait_closed()</span><br><span class="line"></span><br><span class="line">asyncio.run(tcp_echo_client(&quot;Hello, World!&quot;))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>위 코드는 asyncio 모듈의 open_connection() 함수를 이용하여 Echo 서버에 접속하는 코드입니다. 클라이언트가 요청을 보내면, 서버에서는 새로운 코루틴을 생성하여 요청을 처리하고, 클라이언트에 응답을 보냅니다. 이러한 방식으로 비동기식 처리를 수행하면, 다수의 클라이언트 요청을 효율적으로 처리할 수 있습니다.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="자바에서-쓰레드를-구현하는-방법은-어떻게-될까요"><a href="#자바에서-쓰레드를-구현하는-방법은-어떻게-될까요" class="headerlink" title="자바에서 쓰레드를 구현하는 방법은 어떻게 될까요??"></a>자바에서 쓰레드를 구현하는 방법은 어떻게 될까요??</h2><ol>
<li>Thread 클래스 상속 <ul>
<li>Runnable을 상속하여 만들어진 클래스이며, 클래스 객체를 생성하고 start() 메소드를 호출함으로써 Thread가 독립적으로 실행</li>
<li>이러한 Thread 클래스를 상속받아서 run() 메서드를 오버라이드하여 구현하는 방법</li>
<li>어떤 객체도 리턴하지 않음</li>
<li>가장 기본적인 쓰레드 구현 방법 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyThread</span> <span class="keyword">extends</span> <span class="title class_">Thread</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 쓰레드에서 수행할 작업 구현</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Runnable 인터페이스 구현<ul>
<li>Runnable 인터페이스를 구현하여 run() 메서드를 오버라이드하여 구현하는 방법</li>
<li>상속을 사용하지 않기 때문에 코드 재사용성이 더 높음</li>
<li>따라서 다른 클래스를 확장하거나 다른 인터페이스를 구현하고 있는 경우에 사용</li>
<li>어떤 객체도 리턴하지 않음</li>
<li>ExecutorService의 submit() 메소드로 작업을 실행하고 결과 값을 받을 수 있음 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyRunnable</span> <span class="keyword">implements</span> <span class="title class_">Runnable</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 쓰레드에서 수행할 작업 구현</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Callable 인터페이스 구현<ul>
<li>Callable 인터페이스를 구현하여 call() 메서드를 오버라이드하여 구현하는 방법</li>
<li>Runnable과 비슷하지만, 작업 결과인 특정 타입의 객체를 반환할 수 있음</li>
<li>ExecutorService의 submit() 메소드로 작업을 전달하고 작업이 완료되면 Future 객체를 반환해 결과값을 받을 수 있음 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyCallable</span> <span class="keyword">implements</span> <span class="title class_">Callable</span>&lt;Integer&gt; &#123;</span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 쓰레드에서 수행할 작업 구현</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>ExecutorService 를 이용한 쓰레드 풀 사용<ul>
<li><p>ExecutorService 인터페이스를 사용하여 쓰레드 풀을 생성하고, 큐에 작업을 추가하고 실행</p>
</li>
<li><p>여러 쓰레드를 동시에 실행할 수 있음</p>
</li>
<li><p>주로 비동기적인 작업을 수행할 때 사용</p>
</li>
<li><p>submit() 메소드로 작업을 전달하고 작업이 완료되면 Future 객체를 반환해 결과값을 받을 수 있음</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ExecutorService</span> <span class="variable">executorService</span> <span class="operator">=</span> Executors.newFixedThreadPool(<span class="number">10</span>);</span><br><span class="line"><span class="type">Runnable</span> <span class="variable">myRunnable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MyRunnable</span>();</span><br><span class="line">executorService.execute(myRunnable);</span><br></pre></td></tr></table></figure>
</li>
<li><p>ExecutorService의 여러 쓰레드 풀 생성 방법</p>
<ul>
<li><code>new FixedThreadPool(int)</code><ul>
<li>인자 개수만큼 고정된 쓰레드풀 생성</li>
</ul>
</li>
<li><code>new CachedThreadPoll()</code><ul>
<li>필요할 때, 필요한 만큼 쓰레드풀 생성</li>
<li>이미 생성된 쓰레드를 재활용할 수 있어 성능상 이점이 있을 수 있다.</li>
</ul>
</li>
<li><code>new ScheduledThreadPool(int)</code> <ul>
<li>일정 시간 뒤 실행되는 작업이나, 주기적으로 수행되는 작업이 있을 때 고려</li>
</ul>
</li>
<li><code>new SingleThreadExecutor()</code><ul>
<li>쓰레드 한 개인 <code>ExecutorService</code>를 리턴한다.</li>
<li>싱글 쓰레드에서 동작해야 하는 작업을 처리할 때 사용</li>
</ul>
</li>
</ul>
</li>
<li><p><code>submit()</code>메서드를 통해 <code>Callable</code>을 실행하고 <code>Future&lt;V&gt;</code>를 반환 받을 수 있음</p>
</li>
<li><p>실행된 Callable의 반환값을 받는 Future의 메소드 소개</p>
<ul>
<li>get()<ul>
<li>비동기 작업이 완료될 때까지 대기하고 결과를 가져옴<ul>
<li>&#x3D; get() 메소드 호출 시 Blocking Call이 발생하여 반환값을 가지고 올 때까지 멈춤</li>
</ul>
</li>
</ul>
</li>
<li>isDone()<ul>
<li>비동기 작업이 완료되었는지 여부를 확인 </li>
<li>Thread Pool에 submit되어 실행 중인 Callable 작업이 언제 끝난 지를 확인할 수 있음</li>
</ul>
</li>
<li>cancel()<ul>
<li>비동기 작업을 멈춤</li>
</ul>
</li>
<li>invokeAll()<ul>
<li>모든 작업이 완료될 때까지 대기하고 모든 작업의 결과를 가져옴</li>
</ul>
</li>
<li>invokeAny()<ul>
<li>작업 중 하나라도 완료되면 대기를 취소하고 완료된 작업 중 하나의 결과를 가져옴</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Thread-Runnable-Callable-ExecutorService-는-어떠한-경우에-사용하는-것이-좋을까요"><a href="#Thread-Runnable-Callable-ExecutorService-는-어떠한-경우에-사용하는-것이-좋을까요" class="headerlink" title="Thread(), Runnable(), Callable(), ExecutorService() 는 어떠한 경우에 사용하는 것이 좋을까요??"></a>Thread(), Runnable(), Callable(), ExecutorService() 는 어떠한 경우에 사용하는 것이 좋을까요??</h2><ul>
<li>일반적으로는 <code>Runnable</code>을 구현한 객체를 <code>Thread</code> 생성자의 인자로 전달하는 방식으로 새로운 쓰레드를 생성</li>
<li><code>Callable</code> 은 작업 수행 결과가 필요한 경우에 사용</li>
<li><code>ExecutorService</code>는 쓰레드 풀을 생성하고 비동기 작업을 수행</li>
</ul>
<h2 id="파이썬에서-쓰레드를-구현하는-방법은-어떻게-될까요"><a href="#파이썬에서-쓰레드를-구현하는-방법은-어떻게-될까요" class="headerlink" title="파이썬에서 쓰레드를 구현하는 방법은 어떻게 될까요??"></a>파이썬에서 쓰레드를 구현하는 방법은 어떻게 될까요??</h2><ol>
<li><p>threading 모듈 사용: 파이썬에서는 threading 모듈을 사용하여 쓰레드를 구현할 수 있습니다. Thread 클래스를 상속하거나, Runnable 인터페이스와 비슷한 역할을 하는 target 매개변수를 사용하여 쓰레드를 생성할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyThread</span>(threading.Thread):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 쓰레드에서 수행할 작업 구현</span></span><br><span class="line"></span><br><span class="line">my_thread = MyThread()</span><br><span class="line">my_thread.start()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_function</span>():</span><br><span class="line">    <span class="comment"># 쓰레드에서 수행할 작업 구현</span></span><br><span class="line"></span><br><span class="line">my_thread = threading.Thread(target=my_function)</span><br><span class="line">my_thread.start()</span><br></pre></td></tr></table></figure></li>
<li><p>concurrent.futures 모듈 사용: concurrent.futures 모듈을 사용하여 ThreadPoolExecutor나 ProcessPoolExecutor 클래스를 이용하여 쓰레드 풀을 생성하고, 여러 쓰레드를 동시에 실행할 수 있습니다. ThreadPoolExecutor는 쓰레드 기반, ProcessPoolExecutor는 프로세스 기반으로 쓰레드를 실행합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_function</span>():</span><br><span class="line">    <span class="comment"># 쓰레드에서 수행할 작업 구현</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">    future = executor.submit(my_function)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_function</span>():</span><br><span class="line">    <span class="comment"># 쓰레드에서 수행할 작업 구현</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">    future = executor.submit(my_function)</span><br></pre></td></tr></table></figure>
</li>
<li><p>asyncio 모듈 사용: asyncio 모듈을 사용하여 코루틴 기반의 비동기 쓰레드를 구현할 수 있습니다. 이 방법은 비동기 I&#x2F;O 작업에 특화되어 있으며, 단일 스레드에서 여러 작업을 동시에 처리할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">my_coroutine</span>():</span><br><span class="line">    <span class="comment"># 비동기적으로 수행할 작업 구현</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">await</span> asyncio.gather(my_coroutine())</span><br><span class="line"></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure></li>
<li><p>queue 모듈을 이용한 Producer-Consumer 패턴 사용: queue 모듈을 사용하여 여러 쓰레드 간에 데이터를 공유하고, Producer-Consumer 패턴을 구현할 수 있습니다. 이 방법은 여러 쓰레드가 동시에 작업을 처리해야 하는 경우에 유용합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">producer</span>(<span class="params">q</span>):</span><br><span class="line">    <span class="comment"># 데이터를 생산하여 큐에 추가하는 작업 구현</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">consumer</span>(<span class="params">q</span>):</span><br><span class="line">    <span class="comment"># 큐에서 데이터를 가져와서 처리하는 작업 구현</span></span><br><span class="line"></span><br><span class="line">q = queue.Queue()</span><br><span class="line">p = threading.Thread(target=producer, args=(q,))</span><br><span class="line">c = threading.Thread(target=consumer, args=(q,))</span><br><span class="line">p.start()</span><br><span class="line">c.start()</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="쓰레드를-사용하면서-주의해야할-점은-어떻게-되나요"><a href="#쓰레드를-사용하면서-주의해야할-점은-어떻게-되나요" class="headerlink" title="쓰레드를 사용하면서 주의해야할 점은 어떻게 되나요??"></a>쓰레드를 사용하면서 주의해야할 점은 어떻게 되나요??</h2><ol>
<li>Race Condition(경쟁 상태)<ul>
<li>2개 이상의 쓰레드가 공유 데이터에 접근하여 변경하는 경우 경쟁 상태가 발생할 수 있음<ul>
<li>자료의 일관성을 해치는 결과가 나타날 수 있기 때문</li>
</ul>
</li>
<li>이를 해결하기 위해서는 동기화 기법을 사용</li>
</ul>
</li>
<li>DeadLock(교착 상태)<ul>
<li>2개 이상의 쓰레드가 서로 대기하며 무한정 기다리는 상황</li>
<li>교착 상태를 방지하기 위해서는 상호배제, 점유대기, 비선점, 순환 대기 등의 조건 중 하나가 만족되지 않도록 해야함</li>
</ul>
</li>
<li>Thread.stop() 메소드 사용 금지<ul>
<li>Thread.stop() 메소드는 쓰레드를 강제로 종료시키는 메소드이지만 안정적으로 쓰레드를 종료시키지 못하고 데이터 불일치(Data Inconsistency) 등의 문제를 발생시킬 수 있음</li>
<li>대신 쓰레드를 종료시키기 위해서는 플래그 변수나 interrupt() 메소드를 사용하는 것이 좋음</li>
</ul>
</li>
<li>Thread 우선순위 지정 주의<ul>
<li>Thread 우선순위는 시스템마다 다를 수 있음</li>
<li>우선순위에 따라 쓰레드가 실행되는 것을 보장하지 않음</li>
</ul>
</li>
<li>ThreadLocal 사용 주의<ul>
<li>쓰레드별로 독립적인 데이터를 저장하는데 사용되는 ThreadLocal을 남발하면 쓰레드 간에 데이터 불일치 문제가 발생할 수 있음</li>
</ul>
</li>
<li>성능 문제<ul>
<li>쓰레드를 사용하면 Context Switching과 메모리 사용량 등의 부가적인 성능 문제가 발생할 수 있음<ul>
<li>Context Switching을 할 때는 현재 진행 중인 작업의 상태(ex. 다음에 실행해야할 위치(Program Counter)) 등의 정보를 저장하고 읽어오는 시간이 소요</li>
</ul>
</li>
<li>따라서 필요한 만큼의 Thread만 생성하고 Thread의 생명주기를 관리하여 성능 문제를 최소화해야함</li>
<li>단순한 작업일 경우에는 Single Thread로 프로그래밍 하는 것이 더 효율적</li>
</ul>
</li>
</ol>
<h2 id="그렇다면-경쟁상태를-해소하기-위해-쓰레드-동기화는-어떻게-동작하나요"><a href="#그렇다면-경쟁상태를-해소하기-위해-쓰레드-동기화는-어떻게-동작하나요" class="headerlink" title="그렇다면 경쟁상태를 해소하기 위해 쓰레드 동기화는 어떻게 동작하나요"></a>그렇다면 경쟁상태를 해소하기 위해 쓰레드 동기화는 어떻게 동작하나요</h2><p><code>Critical Section(임계영역)</code>, <code>Lock(잠금)</code></p>
<ul>
<li>한 Thread가 진행 중인 작업을 다른 Thread가 간섭하지 못하도록 막는 것을 쓰레드의 동기화라 함</li>
<li>공유 데이터를 사용하는 코드 영역을 임계영역으로 지정해놓고, 공유 데이터(객체)가 가지고 있는 lock을 획득한 단 하나의 쓰레드만 이 영역 내의 코드를 수행할 수 있도록 함</li>
<li>해당 쓰레드가 임계 영역 내의 모든 코드를 수행하고 벗어나서 lock을 반납해야 비로소 다른 쓰레드가 반납된 lock을 획득하여 임계 영역의 코드를 수행할 수 있음<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 메소드 전체를 임계 영역으로 지정</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">sum</span><span class="params">()</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 특정한 영역을 임계 영역으로 지정</span></span><br><span class="line">synchronzied(객체의 참조변수) &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="자바에서의-프로세스와-쓰레드의-동시성-문제를-어떻게-해결하나요"><a href="#자바에서의-프로세스와-쓰레드의-동시성-문제를-어떻게-해결하나요" class="headerlink" title="자바에서의 프로세스와 쓰레드의 동시성 문제를 어떻게 해결하나요??"></a>자바에서의 프로세스와 쓰레드의 동시성 문제를 어떻게 해결하나요??</h2><ol>
<li>Lock, Semaphore 등의 동기화 기법<ul>
<li>java.util.concurrent 패키지의 Lock, ReentrantLock, Semaphore, Condition 등을 이용 (Synchronized 키워드 보다 더욱 세밀한 동기화 제어 제공)</li>
</ul>
</li>
<li>Thread-safe한 자료구조를 이용<ul>
<li>java.util.concurrent 패키지의 BlockingQueue, ConcurrentHashMap, CopyOnWriteArrayList 등</li>
</ul>
</li>
<li>쓰레드 풀(Thread pool)을 이용하여 쓰레드 생성 비용을 줄이고 작업을 분배</li>
</ol>
<ul>
<li>java.util.concurrent 패키지의 ThreadPoolExecutor</li>
</ul>
<ol start="4">
<li>비동기식 처리를 이용(I&#x2F;O 작업이 많은 경우에 효과적으로 CPU 자원을 활용)<ul>
<li>java.util.concurrent 패키지의 CompletableFuture, Executor 등</li>
</ul>
</li>
<li>Synchronized : Synchronized 키워드는 메서드나 블록 단위에서 사용할 수 있음</li>
<li>java.util.concurrent 패키지의 Atomic  사용</li>
<li>Fork&#x2F;Join 프레임워크 : 병렬 처리를 위한 고수준의 도구로, RecursiveTask나 RecursiveAction 클래스를 사용하여 작업을 분할하고, 각각의 작업을 병렬로 처리한 후 결과를 합쳐서 반환</li>
</ol>
<h2 id="파이썬에서의-프로세스와-쓰레드의-동시성-문제를-어떻게-해결하나요"><a href="#파이썬에서의-프로세스와-쓰레드의-동시성-문제를-어떻게-해결하나요" class="headerlink" title="파이썬에서의 프로세스와 쓰레드의 동시성 문제를 어떻게 해결하나요??"></a>파이썬에서의 프로세스와 쓰레드의 동시성 문제를 어떻게 해결하나요??</h2><ol>
<li>Lock, Semaphore 등의 동기화 기법<ul>
<li>threading 모듈의 Lock, RLock, Semaphore, Condition, Event 등의 클래스를 제공</li>
</ul>
</li>
<li>Thread-safe한 자료구조를 이용<ul>
<li>queue 모듈의 Queue, LifoQueue, PriorityQueue 등의 클래스를 제공</li>
</ul>
</li>
<li>쓰레드 풀(Thread pool)을 이용하여 쓰레드 생성 비용을 줄임</li>
</ol>
<ul>
<li>concurrent.futures 모듈의 ThreadPoolExecutor, ProcessPoolExecutor 등의 클래스</li>
</ul>
<ol start="4">
<li>비동기식 처리를 이용 (I&#x2F;O 작업이 많은 경우에 효과적으로 CPU 자원을 활용)</li>
</ol>
<ul>
<li>asyncio 라이브러리를 이용<ul>
<li>코루틴 기반의 비동기 프로그래밍을 구현할 수 있음</li>
<li>여러 작업을 동시에 처리할 수 있으며, IO 작업이 많은 프로그램에서 성능을 향상</li>
</ul>
</li>
</ul>
<ol start="5">
<li>Queue 모듈을 사용한 작업자 스레드 패턴 <ul>
<li>Queue 모듈을 사용하여 여러 쓰레드가 공유하는 작업 큐를 생성하고, 작업자 스레드들이 이 큐에서 작업을 꺼내어 처리하도록 하는 방법</li>
<li>이를 사용하여 작업을 분산 처리하고, 쓰레드 간의 경쟁을 방지</li>
</ul>
</li>
<li>multiprocessing 모듈을 사용한 프로세스 기반 병렬 처리<ul>
<li>여러 개의 프로세스를 생성하고, 이들 간에 작업을 분산하여 병렬 처리</li>
<li>이를 사용하여 여러 코어를 활용하여 작업을 처리</li>
</ul>
</li>
<li>concurrent.futures 모듈을 사용한 스레드나 프로세스 기반 병렬 처리<ul>
<li>concurrent.futures 모듈을 사용하여 스레드나 프로세스 기반의 병렬 처리를 구현</li>
<li>이를 사용하여 여러 작업을 동시에 처리하고, 블로킹 작업을 효율적으로 처리할 수 있음</li>
</ul>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-21T14:58:47.000Z" title="7/21/2023, 11:58:47 PM">2023-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:19.000Z" title="9/6/2024, 12:03:19 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">12 minutes read (About 1774 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Gradient-Descent/">7장. 기초 최적화 방법 Gradient Descent - Gradient Descent</a></h1><div class="content"><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><h3 id="Again-Our-Objective-is"><a href="#Again-Our-Objective-is" class="headerlink" title="Again, Our Objective is"></a>Again, Our Objective is</h3><ul>
<li><p>주어진 데이터에 대해서 출력 값을 똑같이 모사하는 함수를 찾고 싶다.</p>
</li>
<li><p>Loss 값을 최소로 하는 Loss Function의 입력 값(𝜃)를 찾자. How?</p>
<ul>
<li><p>𝜃 값을 하나하나 다 랜덤하게 넣어볼 수가 없다!! 따라서 Loss Function을 최소화하는 𝜃를 얻는 방법이 바로 Gradient Descent!</p>
<p>$D &#x3D; {(x_i, y_i)}_{i&#x3D;1}^N$    &#x2F;&#x2F; (데이터 셋들이 모여져 있을때)</p>
<p>$L(\theta) &#x3D; \sum_{i&#x3D;1}^{N} |y_i - \hat{y_i}|^2_2 &#x3D; \sum_{i&#x3D;1}^{N} |y_i - f_\theta(x_i)|^2_2, \text { where } \theta&#x3D;{W,b}, \text{ }f(x)&#x3D;x \cdot W+b$</p>
<p>$\hat{\theta} &#x3D; \operatorname{argmin}_{\theta} L(\theta)$</p>
<p>Loss 함수의 출력 결과가 최소가 되고 싶어하는 입력값을 점진적으로 찾고 싶겠다. 잘된다면 목적함수를 근사할 수 있음($f^* \approx f_{\hat{\theta}}$)</p>
</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Descent-1D-Case"><a href="#Gradient-Descent-1D-Case" class="headerlink" title="Gradient Descent 1D Case"></a>Gradient Descent 1D Case</h3><ul>
<li><p>𝑥로 미분하여 기울기를 활용하여 좀 더 낮은 곳으로 점차 나아가자</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2" alt="GradientDescent05"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2">https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2</a></p>
<p>$x \gets x - \eta \frac{dy}{dx}, \text{ where } y &#x3D; f(x)$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">⬇️</span><br></pre></td></tr></table></figure>

<p>$\theta \gets \theta - \eta \frac{\partial L(\theta)}{\partial \theta} &#x3D; \theta - \eta \nabla_\theta L(\theta)$</p>
<p>※ 𝜂 : Learning rate(0~1, hyper parameter), ⅆ𝑦&#x2F;ⅆ𝑥 : 기울기</p>
<p>※ 𝐿(𝜃) : 손실 함수(loss function)로써 스칼라 값을 갖음 , 𝜃: 파라미터 또는 가중치 벡터로 vector값을 갖음(고차원의 신경망에서는 𝜃가 벡터 뿐만 아니라 행렬, 또는 고차원 텐서의 형태를 가질 수도 있음, 여기서는 1D로 가정하기에 Vector) 따라서 $∇_θL(θ)$는 벡터 <em>θ에 대한 L</em>(<em>θ</em>)의 그래디언트를 나타내며, 이는 <em>θ의 각 요소에 대해 L</em>(<em>θ</em>)를 편미분한 결과를 벡터 형태로 표현한 것</p>
</li>
<li><p>가장 loss가 낮은 곳이 아닌 골짜기에 빠질 가능성이 있음 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e" alt="GradientDescent06"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e">https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e</a></p>
<ul>
<li>그림은 2차원 이지만, 사실 파라미터의 개수만큼의 차원으로 이루어져 있음 Convex 한 2차 함수가 아닌 이상, Global Minima를 알 수가 없다.</li>
</ul>
</li>
</ul>
<h3 id="Loss-Minimization-using-Gradient-Descent"><a href="#Loss-Minimization-using-Gradient-Descent" class="headerlink" title="Loss Minimization using Gradient Descent"></a>Loss Minimization using Gradient Descent</h3><p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444">https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444">https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444</a></p>
<ul>
<li><p>1D 케이스를 높은 차원의 파라미터(*θ)*로 확장하자</p>
<ul>
<li><p>$\hat{\theta} &#x3D; \operatorname{argmin}_{\theta} L(\theta)$</p>
<p>$W \gets W - \eta \frac{\partial L(\theta)}{\partial W} \text{, }$</p>
<p>$b \gets b - \eta \frac{\partial L(\theta)}{\partial b},$</p>
<p>$\text{where } \theta &#x3D; {W,b}$</p>
</li>
</ul>
</li>
<li><p>Number of Parameters in Linear Layer</p>
<ul>
<li><p>파라미터 수 : n x m + m &#x3D; (n + 1) x m</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942">https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942</a></p>
<ul>
<li><p>|𝜃|&#x3D;(18,) , &#x2F;&#x2F; 18개의 파라미터가 있음! 𝑊 &#x3D; 5x3 &#x3D;15, 𝑏 &#x3D; 3</p>
</li>
<li><p>$y &#x3D; f(k) &#x3D; x \cdot W + b$</p>
<p>$\text{ where } x \in \mathbb{R}^{k \times n}, W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^{n \times m} \text{ and } y \in \mathbb{R}^{n \times m}$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Local-Minima-in-Practice"><a href="#Local-Minima-in-Practice" class="headerlink" title="Local Minima in Practice"></a>Local Minima in Practice</h3><ul>
<li>실제 딥러닝의 경우에 파라미터의 크기가 수백만 단위</li>
<li>수백만 차원의 loss 함수 surface 에서 global minma를 찾는 문제</li>
<li>수 많은 차원에서 동시에 local minima를 위한 조건이 만족되기는 어려움</li>
<li><strong>따라서 local mima에 대한 걱정을 크게 할 필요 없음</strong></li>
</ul>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><ul>
<li><p>Gradient Descent 실습 -  backward() &amp; requires_grad_()</p>
<ul>
<li>$x &#x3D; \begin{bmatrix} x_{(1,1)} &amp; x_{(1,2)} \ x_{(2,1)} &amp; x_{(2,2)} \end{bmatrix}$</li>
<li>$x_1&#x3D;x+2$</li>
<li>$x_2&#x3D;x-2$</li>
<li>$x_3&#x3D;x^2-4$</li>
<li>$y&#x3D;sum(x_3)&#x3D;x_{3(1,1)} + x_{3(1,2)} + x_{3(2,1)} + x_{3(2,2)}$</li>
<li>$x.grad&#x3D;\begin{bmatrix} \frac{\partial y}{\partial x_{(1,1)}} &amp; \frac{\partial y}{\partial x_{(1,2)}} \ \frac{\partial y}{\partial x_{(2,1)}} &amp; \frac{\partial y}{\partial x_{(2,2)}} \end{bmatrix}$</li>
<li>$\frac{dy}{dx}&#x3D;2x$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>]]).requires_grad_(<span class="literal">True</span>)</span><br><span class="line">x1 = x + <span class="number">2</span></span><br><span class="line">x2 = x - <span class="number">2</span></span><br><span class="line">x3 = x1 * x2</span><br><span class="line">y = x3.<span class="built_in">sum</span>() </span><br><span class="line"><span class="comment"># 스칼라 값이어야 미분 가능하기 때문에 .sum()이 붙음 없으면 RuntimeError: grad can be implicitly created only for scalar outputs 에러 발생</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="comment">#tensor([[3., 4.],</span></span><br><span class="line"><span class="comment">#       [5., 6.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="comment"># tensor([[-1.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.]], grad_fn=&lt;SubBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"><span class="comment"># tensor([[-3.,  0.],</span></span><br><span class="line"><span class="comment">#        [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(14., grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며</span></span><br><span class="line"><span class="string">파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨</span></span><br><span class="line"><span class="string">그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? </span></span><br><span class="line"><span class="string">이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 </span></span><br><span class="line"><span class="string">이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음</span></span><br><span class="line"><span class="string">예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)  # 가중치 벡터, y.backward(v)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, </span></span><br><span class="line"><span class="string">이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 </span></span><br><span class="line"><span class="string">이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[2., 4.],</span></span><br><span class="line"><span class="comment">#        [6., 8.]])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">print(x3.numpy())</span></span><br><span class="line"><span class="string"># RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</span></span><br><span class="line"><span class="string">print(x3.detach_().numpy())</span></span><br><span class="line"><span class="string"># array([[-3.,  0.],</span></span><br><span class="line"><span class="string">#       [ 5., 12.]], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, </span></span><br><span class="line"><span class="string">이 속성이 True로 설정되면, </span></span><br><span class="line"><span class="string">해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨</span></span><br><span class="line"><span class="string">따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. </span></span><br><span class="line"><span class="string">이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. </span></span><br><span class="line"><span class="string">따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여</span></span><br><span class="line"><span class="string"> 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. </span></span><br><span class="line"><span class="string">이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="요약-정리"><a href="#요약-정리" class="headerlink" title="요약 정리"></a>요약 정리</h3><ul>
<li>DNN을 통해 문제 해결을 위한 함수를 모사하고 싶을 경우, 우리가 만든 함수가 내뱉는 출력과 실제 정답의 차이는 loss이다.</li>
<li>loss를 loss function으로 만들고 loss function은 파라미터를 입력으로 받음</li>
<li>따라서 loss function의 출력을 최소로 하는 파라미터를 찾는 것이 목적이 됨</li>
<li>loss를 최소화하는 입력 파라미터를 Gradiend Descent로 찾을 수 있음</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-20T14:21:57.000Z" title="7/20/2023, 11:21:57 PM">2023-07-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:32.000Z" title="9/6/2024, 12:03:32 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">21 minutes read (About 3218 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-%ED%8E%B8%EB%AF%B8%EB%B6%84/">7장. 기초 최적화 방법 Gradient Descent - 편미분</a></h1><div class="content"><h2 id="편미분"><a href="#편미분" class="headerlink" title="편미분"></a>편미분</h2><h3 id="다변수-함수-Multivariation-Function"><a href="#다변수-함수-Multivariation-Function" class="headerlink" title="다변수 함수(Multivariation Function)"></a>다변수 함수(Multivariation Function)</h3><ul>
<li>여러 개의 변수(multivariate)를 입력으로 받는 함수<ul>
<li>$z &#x3D; f(x,y)$</li>
<li>$y &#x3D; f(x_1, x_2)$</li>
<li>$x &#x3D; \begin{bmatrix} x_1 \ x_2 \end{bmatrix}$</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2" alt="MultivariateFunction"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2">https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2</a></p>
<h3 id="편미분-1"><a href="#편미분-1" class="headerlink" title="편미분"></a>편미분</h3><ul>
<li><p>다변수 x와 y를 입력으로 받는 함수 f를 x로 미분할 경우</p>
<ul>
<li>하나의 변수만 남겨 놓고 나머지를 상수 취급하는 미분 방법</li>
</ul>
</li>
<li><p>함수 f를 x변수(x축)으로 미분</p>
<ul>
<li>편미분 기호 𝜕 (round 혹은 partial 라고 부름)</li>
</ul>
<p>$\frac{\partial f}{\partial x} &#x3D; \lim_{h \to 0} \frac{f(x+h,y) - f(x,y)}{(x+h) - x}$</p>
</li>
<li><p>Y값에 대해 뚝 잘랐을 때 x축에 대한 기울기</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61" alt="GradientDescent01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61">https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61</a></p>
</li>
</ul>
<h3 id="함수의-입출력-형태"><a href="#함수의-입출력-형태" class="headerlink" title="함수의 입출력 형태"></a>함수의 입출력 형태</h3><ul>
<li><p>함수의 입력이 벡터인 경우</p>
<p>$y&#x3D;f(\begin{bmatrix} x_1 \⦙\x_n \end{bmatrix})&#x3D;f(x), \text{ where } x \in \mathbb{R}^n$</p>
</li>
<li><p>함수의 출력이 벡터인 경우 $y&#x3D;f(\begin{bmatrix} y_1 \⦙\y_n \end{bmatrix})&#x3D;f(x)&#x3D;\begin{bmatrix} f_1(x) \⦙\f_n(x) \end{bmatrix}, \text{ where } y \in \mathbb{R}^n$</p>
</li>
<li><p>함수의 입력이 행렬인 경우</p>
<p>$y&#x3D;f(\begin{bmatrix} x_{1,1} ⋯ x_{1,m} \⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ x_{n,1} ⋯ x_{n,m} \end{bmatrix})&#x3D;f(X), \text{ where } X \in \mathbb{R}^{n \times m}$</p>
</li>
<li><p>함수의 출력이 행렬인 경우 $Y&#x3D;f(\begin{bmatrix} y_{1,1} ⋯ y_{1,m} \⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ y_{n,1} ⋯ y_{n,m} \end{bmatrix})&#x3D;f(x), \text{ where } Y \in \mathbb{R}^{n \times m}$</p>
</li>
<li><p>입력과 출력이 벡터인 함수</p>
<p>$y&#x3D;f(\begin{bmatrix} y_1 \⦙\y_n \end{bmatrix})&#x3D;f(x)&#x3D; f(\begin{bmatrix} x_1 \⦙\x_n \end{bmatrix}), \text{ where } f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</p>
</li>
</ul>
<h3 id="스칼라를-벡터로-스칼라를-행렬로-미분"><a href="#스칼라를-벡터로-스칼라를-행렬로-미분" class="headerlink" title="스칼라를 벡터로, 스칼라를 행렬로 미분"></a>스칼라를 벡터로, 스칼라를 행렬로 미분</h3><ul>
<li>미분 결과는 gradient 벡터가 되어 방향과 크기를 모두 나타냄</li>
</ul>
<ol>
<li><p>스칼라를 벡터로 미분</p>
<ul>
<li><p>스칼라 함수를 벡터로 미분한다는 것을 의미</p>
</li>
<li><p>결과는 벡터가 됨</p>
</li>
<li><p>각 벡터의 요소는 스칼라 함수를 해당 방향을 미분한 결과를 나타냄, 이것을 gradient라고 부르며, 함수의 기울기를 나타나는데 사용</p>
</li>
<li><p>$\frac{\partial f}{\partial x} &#x3D; \nabla_x f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_1} \ ⦙\ \frac{\partial f}{\partial x_n} \end{bmatrix}, \text {where }x \in \mathbb{R}^n$</p>
</li>
<li><p>예제</p>
<ul>
<li><p>$f(x,y)&#x3D;3x2+2xy+y2$ 라는 스칼라 함수와 벡터 변수 $x$ 와 $y$ 가 있음</p>
<ul>
<li><p>이 함수를 각 변수에 대해 편미분 하면 다음과 같음</p>
<p>$\frac{\partial f}{\partial x} &#x3D; 6x + 2y$</p>
<p>$\frac{\partial f}{\partial y} &#x3D; 2x + 2y$</p>
</li>
<li><p>따라서 함수 $f(x,y)$ 에 대한 그래디언트는 다음과 같음</p>
<p>$\nabla f &#x3D; \left[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right] &#x3D; [6x + 2y, 2x + 2y]$</p>
</li>
<li><p>이 그래디언트 벡터는 각 점$(x,y)$에서 함수의 값이 가장 크게 증가하는 방향을 가리킴, 따라서 그래디언트는 최적화 문제에서 가장 중요한 역할을 함 (최적화 알고리즘은 일반적으로 그래디언트의 반대 방향으로 이동하여 함수의 최소값을 찾습니다. 이것이 그래디언트 디센트 방법의 기본 개념)</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>코드로 확인해보기</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sympy <span class="keyword">import</span> symbols, diff</span><br><span class="line"></span><br><span class="line">x, y = symbols(<span class="string">&#x27;x y&#x27;</span>)</span><br><span class="line">f = <span class="number">3</span>*x**<span class="number">2</span> + <span class="number">2</span>*x*y + y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">df_dx = diff(f, x)  <span class="comment"># x에 대한 편미분</span></span><br><span class="line">df_dy = diff(f, y)  <span class="comment"># y에 대한 편미분</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;df/dx: <span class="subst">&#123;df_dx&#125;</span>&#x27;</span>)    <span class="comment"># df/dx: 6*x + 2*y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;df/dy: <span class="subst">&#123;df_dy&#125;</span>&#x27;</span>)    <span class="comment"># df/dy: 2*x + 2*y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 임의의 시작점</span></span><br><span class="line">x_value = <span class="number">1.0</span></span><br><span class="line">y_value = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습률</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 100번 반복</span></span><br><span class="line">    gradient_x = df_dx.evalf(subs=&#123;x: x_value, y: y_value&#125;)  <span class="comment"># x 위치에서의 그래디언트 계산</span></span><br><span class="line">    gradient_y = df_dy.evalf(subs=&#123;x: x_value, y: y_value&#125;)  <span class="comment"># y 위치에서의 그래디언트 계산</span></span><br><span class="line"></span><br><span class="line">    x_value -= eta * gradient_x     <span class="comment"># 그래디언트의 반대 방향으로 이동</span></span><br><span class="line">    y_value -= eta * gradient_y     <span class="comment"># 그래디언트의 반대 방향으로 이동</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimized x: <span class="subst">&#123;x_value&#125;</span>&#x27;</span>)    <span class="comment"># Optimized x: -0.0627121858146143</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimized y: <span class="subst">&#123;y_value&#125;</span>&#x27;</span>)    <span class="comment"># Optimized y: 0.154295508359253</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>스칼라를 행렬로 미분</p>
<ul>
<li>스칼라 함수를 행렬 변수에 대해 미분하는 것을 나타냄</li>
<li>결과는 행렬이 됨</li>
<li>이 행렬의 각 요소는 해당 스칼라 함수를 행렬의 해당 요소로 편미분한 값, 스칼라 함수를 행렬로 미분한 결과는 자코비안 행렬이라고 부르며, 이는 함수의 지역적 변화율을 나타냄</li>
<li>$\frac{\partial f}{\partial x} &#x3D; \nabla_x f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_{1,1}  } ⋯ \frac{\partial f}{\partial x_{1,m}} \ ⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ \frac{\partial f}{\partial x_{n,1}  } ⋯ \frac{\partial f}{\partial x_{n,m}} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n \times m}$</li>
<li>스칼라 함수 f가 행렬 X에 의존한다고 하면, 이 함수를 행렬 X에 대해 미분한 결과는 행렬이 됨<ul>
<li>이 행렬의 (i, j)번째 요소는 f를 X의 (i, j)번째 요소에 대해 편미분한 값</li>
<li>스칼라 함수 f를 행렬 $X &#x3D; \begin{bmatrix} x_{11} &amp; x_{12} \ x_{21} &amp; x_{22} \end{bmatrix}$에 대해 미분하면 다음과 같은 형태의 행렬이 나옴<ul>
<li>$\nabla_X f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_{11}} &amp; \frac{\partial f}{\partial x_{12}} \ \frac{\partial f}{\partial x_{21}} &amp; \frac{\partial f}{\partial x_{22}} \end{bmatrix}$</li>
<li>이때 $\frac{\partial f}{\partial x_{ij}}$는 함수 f를 행렬 X의 (i,j)번째 요소에 대해 편미분한 것으로, 이렇게 구해진 행렬을 자코비안 행렬(Jacobian matrix) 이라고 함</li>
<li>자코비안 행렬의 각 요소는 스칼라 함수가 각 변수를 조금씩 변화시킬 때, 함수의 출력이 얼마나 변화하는지를 나타냄, 따라서 자코비안 행렬은 함수의 지역적인 변화율을 설명하는 데 사용될 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>스칼라-행렬 미분은 딥러닝에서 매우 중요한 개념<ul>
<li>신경망의 <strong>가중치는 일반적으로 행렬 형태</strong>를 가지며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 함. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 오차 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산</li>
</ul>
</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>스칼라, 벡터, 행렬</strong></p>
<ul>
<li><strong>스칼라</strong>는 단일한 수치 값입니다. 예를 들어, 10이나 2.5와 같은 단일 숫자를 스칼라라고 합니다.</li>
<li><strong>벡터</strong>는 숫자들의 배열입니다. 예를 들어, [1, 2]와 같은 1차원 배열이 벡터입니다.</li>
<li><strong>행렬</strong>은 숫자들의 2차원 배열입니다. 예를 들어, [[1, 2], [3, 4]]와 같은 2차원 배열이 행렬입니다.</li>
</ul>
</blockquote>
<blockquote>
<p><strong>자코비안 행렬, 헤시안 행렬</strong></p>
<ul>
<li>자코비안 행렬(Jacobian matrix)<ul>
<li>자코비안 행렬은 벡터 값을 가진 함수를 벡터 변수에 대해 미분할 때 사용</li>
<li>함수의 입력과 출력이 모두 벡터일 때, 각 입력 변수에 대한 각 출력 변수의 편미분을 행렬 형태로 나타낸 것이 자코비안 행렬</li>
<li>즉, 다변수 벡터 함수의 첫 번째 도함수를 나타냅니다.</li>
</ul>
</li>
<li>헤시안 행렬(Hessian matrix)<ul>
<li>헤시안 행렬은 스칼라 값을 가진 함수를 행렬 변수에 대해 두 번 미분할 때 사용</li>
<li>즉, 함수의 두 번째 도함수(2차 미분)를 나타냄</li>
<li>헤시안 행렬의 각 성분은 원래 함수의 두 변수에 대한 두 번째 편미분</li>
<li>헤시안 행렬은 주로 함수의 곡률, 즉 최솟값, 최댓값, 또는 안장점(saddle point) 등을 판별하는 데 사용됨</li>
</ul>
</li>
</ul>
<p>따라서, 자코비안은 함수의 기울기(1차 도함수)를, 헤시안은 곡률(2차 도함수)을 나타내며, 두 행렬 모두 함수의 지역적인 동작을 이해하는 데 중요한 역할을 함</p>
</blockquote>
<h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><ul>
<li><p>상미분과 달리 미분 결과가 벡터</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61" alt="GradientDescent01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61">https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61</a></p>
<p>$\nabla f(x,y) &#x3D; \begin{bmatrix} 2x+y \ x+3y^2 +10y\end{bmatrix} &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x} \ \frac{\partial f}{\partial y} \end{bmatrix}$</p>
</li>
</ul>
<h3 id="벡터를-스칼라로-벡터를-벡터로-미분"><a href="#벡터를-스칼라로-벡터를-벡터로-미분" class="headerlink" title="벡터를 스칼라로, 벡터를 벡터로 미분"></a>벡터를 스칼라로, 벡터를 벡터로 미분</h3><ol>
<li><p>벡터를 스칼라로 미분</p>
<p>$\frac{\partial f}{\partial x} &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x}, … \text { }, \frac{\partial f_n}{\partial x} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n}$</p>
</li>
<li><p>벡터를 벡터로 미분</p>
<p>$\frac{\partial f}{\partial x} &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_1} \ ⦙ \ \frac{\partial f}{\partial x_n} \end{bmatrix} &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x}, … \text { }, \frac{\partial f_m}{\partial x} \end{bmatrix}  &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x_{1}  } ⋯ \frac{\partial f_m}{\partial x_{1}} \ ⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ \frac{\partial f_1}{\partial x_{n}  } ⋯ \frac{\partial f_m}{\partial x_{n}} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n}\text{ } and\text{ } f(x) \in \mathbb{R}^m$</p>
</li>
</ol>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>]]).requires_grad_(<span class="literal">True</span>)</span><br><span class="line">x1 = x + <span class="number">2</span></span><br><span class="line">x2 = x - <span class="number">2</span></span><br><span class="line">x3 = x1 * x2</span><br><span class="line">y = x3.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="comment">#tensor([[3., 4.],</span></span><br><span class="line"><span class="comment">#       [5., 6.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="comment"># tensor([[-1.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.]], grad_fn=&lt;SubBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"><span class="comment"># tensor([[-3.,  0.],</span></span><br><span class="line"><span class="comment">#        [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(14., grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며</span></span><br><span class="line"><span class="string">파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨</span></span><br><span class="line"><span class="string">그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? </span></span><br><span class="line"><span class="string">이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 </span></span><br><span class="line"><span class="string">이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음</span></span><br><span class="line"><span class="string">예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)  # 가중치 벡터, y.backward(v)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, </span></span><br><span class="line"><span class="string">이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 </span></span><br><span class="line"><span class="string">이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[2., 4.],</span></span><br><span class="line"><span class="comment">#        [6., 8.]])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">print(x3.numpy())</span></span><br><span class="line"><span class="string"># RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</span></span><br><span class="line"><span class="string">print(x3.detach_().numpy())</span></span><br><span class="line"><span class="string"># array([[-3.,  0.],</span></span><br><span class="line"><span class="string">#       [ 5., 12.]], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, </span></span><br><span class="line"><span class="string">이 속성이 True로 설정되면, </span></span><br><span class="line"><span class="string">해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨</span></span><br><span class="line"><span class="string">따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. </span></span><br><span class="line"><span class="string">이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. </span></span><br><span class="line"><span class="string">따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여</span></span><br><span class="line"><span class="string"> 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. </span></span><br><span class="line"><span class="string">이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="Why-we-laern-this"><a href="#Why-we-laern-this" class="headerlink" title="Why we laern this?"></a>Why we laern this?</h3><ul>
<li><p>Loss 함수 결과값인 스칼라 힘수를 파라미터 행렬(𝜃)로 미분해야 한다면?</p>
</li>
<li><p>DNN의 중간 결과물 벡터(ℎ)를 파라미터 행렬(𝜃)로 미분해야 한다면?</p>
<p>딥러닝에서는 일반적으로 가중치(파라미터)가 행렬 형태를 가지고 있으며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 합니다. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 손실 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 또한, DNN에서는 각 레이어의 입력(중간 결과 값)을 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 따라서, 이러한 미분 개념을 이해하는 것은 딥러닝 모델을 이해하고 최적화하는 데 매우 중요합니다. 따라서, 이러한 이유들로 인해 파라미터 행렬(𝜃)에 대한 스칼라 함수, 즉 손실 함수의 결과 값을 미분하거나, 파라미터 행렬(𝜃)에 대한 중간 결과 값 벡터 (ℎ)를 미분하는 개념을 배우는 것이 딥러닝에서 매우 중요합니다</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-19T14:56:59.000Z" title="7/19/2023, 11:56:59 PM">2023-07-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:13.000Z" title="9/6/2024, 12:03:13 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">5 minutes read (About 713 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/6%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%B4-%EC%9E%98-%ED%95%99%EC%8A%B5%EB%90%98%EB%8A%94%EC%A7%80-%ED%8C%90%EB%8B%A8%ED%95%98%EA%B8%B0-Loss-Function/">6장. 신경망이 잘 학습되는지 판단하기 - Loss Function</a></h1><div class="content"><h2 id="Again-Our-object-is"><a href="#Again-Our-object-is" class="headerlink" title="Again, Our object is"></a>Again, Our object is</h2><ul>
<li>데이터를 넣었을 때 출력을 반환하는 가상의 함수를 모사하는 것</li>
<li>Linear Layer 함수를 통해 원하는 함수를 모사해보자<ul>
<li>Linear Layer 함수가 얼마나 원하는 만큼 동작하는지 측정해 보자</li>
<li>얼마나 잘 동작하는지, 점수로 나타내보자</li>
</ul>
</li>
</ul>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><ul>
<li>Loss(손실값): 원하는 출력값(target,𝑦)가 실제 출력값(output, $\hat{y}$)의 차이의 합<ul>
<li>$\text{Loss} &#x3D; \sum_{i&#x3D;1}^{N} | y_i - \hat{y}<em>i | &#x3D; \sum</em>{i&#x3D;1}^{N} | y_i - f(x_i) |$</li>
</ul>
</li>
<li>그러므로 우리는 Loss가 작을수록 가상의 함수를 잘 모사하고 있다고 할 수 있음</li>
<li>Loss가 작은 Linear Layer를 선택하면 됨</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul>
<li><p>Linear Layer의 파라미터를 바꿀 때마다 Loss를 계산</p>
</li>
<li><p>Loss Function</p>
<ul>
<li>입력 : Linear Layer의 파라미터(𝜃, 즉, 𝑊,𝑏가 파라미터)</li>
<li>출력 : Looss<ul>
<li>𝐿(𝜃)&#x3D;$\sum_{i&#x3D;1}^{n} | y_i - f_{\theta}(x_j) |, \text{ where } \theta &#x3D; {W, b}$</li>
</ul>
</li>
</ul>
</li>
<li><p>종류</p>
<ul>
<li><p>Euclidean Distance</p>
<ul>
<li><p>$| y - \hat{y} |_2 (L2) &#x3D; \sqrt{(y_1 - \hat{y}_1)^2 + \ldots + (y_n - \hat{y}</p>
<p>n)^2} &#x3D; \sqrt{\sum</p>
<p>{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}, \text{ where } y \in \mathbb{R}^n \text{ and } \hat{y} \in \mathbb{R}^n$</p>
<ul>
<li><strong>딥러닝은 차원제약이 없어서 고차원으로 가면 차이가 굉장히 커질 수 있기 때문에 RMSE 가 등장</strong></li>
<li>cf) $| y - \hat{y} | : L1$, 절대값</li>
</ul>
</li>
</ul>
</li>
<li><p>RMSE(Root Mean Square Error)</p>
<ul>
<li>Euclidean Distance와 비슷한 개념</li>
<li>$\text{RMSE}(y, \hat{y}) &#x3D; \sqrt{\frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}$</li>
</ul>
</li>
<li><p>&#96;&#96;&#96;<br>MSE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    (Mean Square Error)</span><br><span class="line"></span><br><span class="line">    - $\text&#123;MSE&#125;(y, \hat&#123;y&#125;) = \frac&#123;1&#125;&#123;n&#125; \sum_&#123;i=1&#125;^&#123;n&#125; (y_i - \hat&#123;y&#125;_i)^2 = \frac&#123;1&#125;&#123;n&#125;(\| y - \hat&#123;y&#125; \|_2)^2 = \frac&#123;1&#125;&#123;n&#125;\| y - \hat&#123;y&#125; \|_2^2 ∝ \| y - \hat&#123;y&#125; \|_2^2$</span><br><span class="line">    - Root와 상수를 뺏지만 크기 차이로 인한 순서 결과는 바뀌지 않음</span><br><span class="line">    - **손실함수로 가장 많이 사용**</span><br><span class="line"></span><br><span class="line">## 코드 구현하기</span><br><span class="line"></span><br><span class="line">- Loss Function 예제 (1) – 직접 구현하기</span><br><span class="line"></span><br><span class="line">  ```python</span><br><span class="line">  import torch</span><br><span class="line">  import torch.nn as nn</span><br><span class="line">  </span><br><span class="line">  def mse(x_hat, x):</span><br><span class="line">      # |x_hat| = (batch_size, dim)</span><br><span class="line">      # |x| = (batch_size, dim)</span><br><span class="line">      y = ((x - x_hat)**2).mean()</span><br><span class="line">      </span><br><span class="line">      return y</span><br><span class="line">  </span><br><span class="line">  x = torch.FloatTensor([[1, 1],</span><br><span class="line">                         [2, 2]])</span><br><span class="line">  x_hat = torch.FloatTensor([[0, 0],</span><br><span class="line">                             [0, 0]])</span><br><span class="line">  </span><br><span class="line">  print(x.size(), x_hat.size()) # torch.Size([2, 2]) torch.Size([2, 2])</span><br><span class="line">  mse(x_hat, x) # tensor(2.5000)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83" alt="LossFunction"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83">https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83</a></p>
</li>
<li><p>Loss Function 예제 (2) – 라이브러리 활용</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">x_hat = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                           [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.size(), x_hat.size()) <span class="comment"># torch.Size([2, 2]) torch.Size([2, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x)) <span class="comment"># tensor(2.5000)</span></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x, reduction=<span class="string">&#x27;sum&#x27;</span>)) <span class="comment"># tensor(10.)</span></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x, reduction=<span class="string">&#x27;none&#x27;</span>)) <span class="comment"># tensor([[1., 1.], [4., 4.]])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Loss Function 예제 (3) – 라이브러리 활용</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">mse_loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">print</span>(mse_loss(x_hat, x)) <span class="comment"># tensor(2.5000)</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><ul>
<li>우리는 목표로 하는 함수를 모사하기 위해<ul>
<li>학습용 입력 데이터들을 Linear Layer에 넣어 출력 값들을 구하고</li>
<li>출력값($\hat{y}$)들과 목표값(${y}$)들의 차이의 합(Loss)를 최소화 해야함</li>
</ul>
</li>
<li>결국, Linear Layer 파라미터(𝜃)를 바꾸면서 loss를 최소화 해야함</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/6/">Previous</a></div><div class="pagination-next"><a href="/page/8/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/6/">6</a></li><li><a class="pagination-link is-current" href="/page/7/">7</a></li><li><a class="pagination-link" href="/page/8/">8</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/13/">13</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/matterhorn.jpg" alt="Shawn Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shawn Choi</p><p class="is-size-6 is-block">노력 백줌 열정 천줌의 소프트웨어 개발자</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, Republic of Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">128</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">62</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">102</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/shchoice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/shchoice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DevOps/"><span class="level-start"><span class="level-item">DevOps</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/"><span class="level-start"><span class="level-item">CI/CD 파이프라인</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/"><span class="level-start"><span class="level-item">버전 관리</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">함수형 프로그래밍</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/MLOps/"><span class="level-start"><span class="level-item">MLOps</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/MLOps/Cuda/"><span class="level-start"><span class="level-item">Cuda</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/MLOps/MLflow/"><span class="level-start"><span class="level-item">MLflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Ops/Windows-CMD/"><span class="level-start"><span class="level-item">Windows CMD</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/%EB%82%B4-%EC%BD%94%EB%93%9C%EA%B0%80-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9D%B4%EC%83%81%ED%95%9C%EA%B0%80%EC%9A%94/"><span class="level-start"><span class="level-item">내 코드가 그렇게 이상한가요</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Spring/%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8/"><span class="level-start"><span class="level-item">핵심 원리 - 기본편</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/"><span class="level-start"><span class="level-item">기타</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/Github-Pages/"><span class="level-start"><span class="level-item">Github Pages</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/"><span class="level-start"><span class="level-item">데이터베이스 &amp; 검색엔진</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/OpenSearch/"><span class="level-start"><span class="level-item">OpenSearch</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="level-start"><span class="level-item">딥러닝</span></span><span class="level-end"><span class="level-item tag">44</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/Text-Summarization/"><span class="level-start"><span class="level-item">Text Summarization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/"><span class="level-start"><span class="level-item">Transformers</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/TainingArugments/"><span class="level-start"><span class="level-item">TainingArugments</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/"><span class="level-start"><span class="level-item">논문 리뷰</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 개념</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 기본 개념</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-NLP-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝을 활용한 자연어 처리(NLP) 개념</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%86%B5%EA%B3%84%ED%95%99-%EB%B0%8F-%EC%88%98%ED%95%99/"><span class="level-start"><span class="level-item">딥러닝을 위한 통계학 및 수학</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"><span class="level-start"><span class="level-item">자연어처리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/"><span class="level-start"><span class="level-item">성능과 튜닝</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%B0%8F-%EB%B2%A4%EC%B9%98%EB%A7%88%ED%82%B9/"><span class="level-start"><span class="level-item">테스트 및 벤치마킹</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/UML/"><span class="level-start"><span class="level-item">UML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">소프트웨어 아키텍처</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/API-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">API 설계 및 아키텍처</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">웹 프로그래밍</span></span><span class="level-end"><span class="level-item tag">16</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/FastAPI/"><span class="level-start"><span class="level-item">FastAPI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/HTTP-%EB%B0%8F-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/"><span class="level-start"><span class="level-item">HTTP 및 네트워크</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Core/"><span class="level-start"><span class="level-item">Spring Core</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Data-JPA/"><span class="level-start"><span class="level-item">Spring Data JPA</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-MVC/"><span class="level-start"><span class="level-item">Spring MVC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/"><span class="level-start"><span class="level-item">개발 환경 설정</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/"><span class="level-start"><span class="level-item">보안</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%95%94%ED%98%B8%ED%99%94/"><span class="level-start"><span class="level-item">데이터 암호화</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%84%9C%EB%B2%84-%EB%B0%8F-%EC%9D%B8%ED%94%84%EB%9D%BC/"><span class="level-start"><span class="level-item">서버 및 인프라</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="level-start"><span class="level-item">인공지능</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EA%B0%9C%EB%85%90-%EC%A0%95%EB%A6%AC/"><span class="level-start"><span class="level-item">개념 정리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/"><span class="level-start"><span class="level-item">자연어 처리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/"><span class="level-start"><span class="level-item">클라우드 컴퓨팅</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EB%8F%84%EC%BB%A4-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4/"><span class="level-start"><span class="level-item">도커 &amp; 쿠버네티스</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">프로그래밍</span></span><span class="level-end"><span class="level-item tag">20</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Agile/"><span class="level-start"><span class="level-item">Agile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Effective-Java/"><span class="level-start"><span class="level-item">Effective Java</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java&quot;</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Java8/"><span class="level-start"><span class="level-item">Java8</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%ED%81%B4%EB%A6%B0-%EC%BD%94%EB%93%9C/"><span class="level-start"><span class="level-item">클린 코드</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-07T16:11:09.000Z">2024-09-08</time></p><p class="title"><a href="/%EC%9B%B9%20%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/HTTP%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3/OSI-7%EA%B3%84%EC%B8%B5%EA%B3%BC-TCP-IP-4%EA%B3%84%EC%B8%B5%EC%9D%98-%EC%B0%A8%EC%9D%B4/">OSI 7계층과 TCP/IP 4계층의 차이</a></p><p class="categories"><a href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/">웹 프로그래밍</a> / <a href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/HTTP-%EB%B0%8F-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/">HTTP 및 네트워크</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-07T15:45:01.000Z">2024-09-08</time></p><p class="title"><a href="/%EC%9B%B9%20%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%91%E1%85%B3%E1%84%85%E1%85%A1/L4-%EC%8A%A4%EC%9C%84%EC%B9%98%EC%9D%98-%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1-%EA%B3%BC-%EC%9B%B9-%EC%84%9C%EB%B2%84%EC%9D%98-%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1-%EC%B0%A8%EC%9D%B4/">L4 스위치의 로드 밸런싱 과 웹 서버의 로드 밸런싱 차이</a></p><p class="categories"><a href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/">웹 프로그래밍</a> / <a href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%84%9C%EB%B2%84-%EB%B0%8F-%EC%9D%B8%ED%94%84%EB%9D%BC/">서버 및 인프라</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-04T14:49:11.000Z">2024-09-04</time></p><p class="title"><a href="/%E1%84%86%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8F%E1%85%B3%E1%84%85%E1%85%A9%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3-%E1%84%8B%E1%85%A1%E1%84%8F%E1%85%B5%E1%84%90%E1%85%A6%E1%86%A8%E1%84%8E%E1%85%A5/">마이크로서비스 아키텍</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-04T14:48:00.000Z">2024-09-04</time></p><p class="title"><a href="/Stateless/">Stateless</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-04T14:47:50.000Z">2024-09-04</time></p><p class="title"><a href="/%EC%84%9C%EB%B2%84%EB%A6%AC%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/">서버리스 아키텍처</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/1%EA%B8%89-%EC%8B%9C%EB%AF%BC/"><span class="tag">1급 시민</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AES/"><span class="tag">AES</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASGI/"><span class="tag">ASGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Anonymous-Class/"><span class="tag">Anonymous Class</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoEncoder/"><span class="tag">AutoEncoder</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bind-Mounts/"><span class="tag">Bind Mounts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CGI/"><span class="tag">CGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CORS/"><span class="tag">CORS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Entropy-Loss/"><span class="tag">Cross Entropy Loss</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Curse-of-Dimensionality/"><span class="tag">Curse of Dimensionality</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Volume/"><span class="tag">Data Volume</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker-Orchestration-Tools/"><span class="tag">Docker Orchestration Tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Document-Embedding/"><span class="tag">Document Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-Vectors/"><span class="tag">Embedding Vectors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-vector/"><span class="tag">Embedding vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Entropy/"><span class="tag">Entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FLAN/"><span class="tag">FLAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FastAPI/"><span class="tag">FastAPI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Feature-Vector/"><span class="tag">Feature Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Forward-Proxy/"><span class="tag">Forward Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function-Interface/"><span class="tag">Function Interface</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradient-Descent/"><span class="tag">Gradient Descent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gunicorn/"><span class="tag">Gunicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Representation/"><span class="tag">Hidden Representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instruction-Finetuning/"><span class="tag">Instruction Finetuning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL-Divergence/"><span class="tag">KL Divergence</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KoNLPy/"><span class="tag">KoNLPy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/L4-%EC%8A%A4%EC%9C%84%EC%B9%98/"><span class="tag">L4 스위치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lambda-Expression/"><span class="tag">Lambda Expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latent-Space/"><span class="tag">Latent Space</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learning-Rate/"><span class="tag">Learning Rate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Layer/"><span class="tag">Linear Layer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Load-Testing/"><span class="tag">Load Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Likelihood/"><span class="tag">Log-Likelihood</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MAP/"><span class="tag">MAP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLE/"><span class="tag">MLE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Manifold-hypothesis/"><span class="tag">Manifold hypothesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix/"><span class="tag">Matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mecab/"><span class="tag">Mecab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-Stage-Build/"><span class="tag">Multi Stage Build&quot;</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLL/"><span class="tag">NLL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OSI-7%EA%B3%84%EC%B8%B5/"><span class="tag">OSI 7계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Persistence-Data/"><span class="tag">Persistence Data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probabilistic-Perspective/"><span class="tag">Probabilistic Perspective</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RPS/"><span class="tag">RPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSA/"><span class="tag">RSA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Response-Time/"><span class="tag">Response Time</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reverse-Proxy/"><span class="tag">Reverse Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SOLID/"><span class="tag">SOLID</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scalar/"><span class="tag">Scalar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spring%EC%9D%B4%EB%9E%80/"><span class="tag">Spring이란</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stress-Testing/"><span class="tag">Stress Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword-Embedding/"><span class="tag">Subword Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TCP-IP-4%EA%B3%84%EC%B8%B5/"><span class="tag">TCP/IP 4계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TPS/"><span class="tag">TPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensor/"><span class="tag">Tensor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Testing-Types/"><span class="tag">Testing Types</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Throughput/"><span class="tag">Throughput</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tramsformers/"><span class="tag">Tramsformers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UML/"><span class="tag">UML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Uvicorn/"><span class="tag">Uvicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vector/"><span class="tag">Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WAS/"><span class="tag">WAS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WSGI/"><span class="tag">WSGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/What-to-do/"><span class="tag">What to do</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cross-entropy/"><span class="tag">cross entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/default-method/"><span class="tag">default method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/max-length/"><span class="tag">max_length</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mlflow/"><span class="tag">mlflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/one-hot-Encoding/"><span class="tag">one-hot Encoding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/packing/"><span class="tag">packing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/padding/"><span class="tag">padding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-%EC%84%A4%EC%B9%98/"><span class="tag">python 설치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/static-method/"><span class="tag">static method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unpacking/"><span class="tag">unpacking</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5/"><span class="tag">객체지향</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8B%A4%ED%98%95%EC%84%B1/"><span class="tag">다형성</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%9E%8C%EB%8B%A4%EC%8B%9D/"><span class="tag">람다식</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1/"><span class="tag">로드 밸런싱</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A6%AC%EB%B2%84%EC%8A%A4-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">리버스 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">벡터의 곱셈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%EC%A7%80%ED%91%9C/"><span class="tag">성능 지표</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%ED%85%8C%EC%8A%A4%ED%8A%B8/"><span class="tag">성능 테스트</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC/"><span class="tag">엔트로피</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9B%B9-%EC%84%9C%EB%B2%84/"><span class="tag">웹 서버</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EB%9F%89/"><span class="tag">정보량</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/"><span class="tag">정보이론</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%8B%A4%EC%9D%B4%EC%96%B4%EA%B7%B8%EB%9E%A8/"><span class="tag">클래스 다이어그램</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%8F%AC%EC%9B%8C%EB%93%9C-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">포워드 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"><span class="tag">함수형 인터페이스</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="tag">함수형 프로그래밍</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">행렬의 곱셈</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Seohwan Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>