{"posts":[{"title":"1. 객체 지향 설계와 스프링","text":"스프링은 자바 언어 기반의 프레임워크 이다. 자바 언어의 가장 큰 특징은 객체 지향 언어로 스프링은 객체 지향 언어가 가진 강력한 특징을 살려내는 프레임워크 이다. 즉, **스프링은 좋은 객체 지향 애플리케이션을 개발할 수 있게 도와주는 프레임워크**이다. 따라서 해당 페이지에서는 **객체 지향 프로그래밍이 무엇**인지 알아보고 **스프링이 객체 지향 패러다임을 지키기 위해 어떻게 노력했는지**를 알아보도록 하자 객체 지향 프로그래밍이란 객체들의 모임으로 파악하는 것 객체는 메시지를 주고받고 데이터를 처리할 수 있음(협력) 프로그램을 유연하고 변경이 용이하게 만들어줌 컴퍼넌트를 쉽고 유연하게 변경하면서 개발할 수 있는 방법 ex) 컴퓨터 GPU를 업그레이드 하고 싶으면 새로운 GPU로 교체하면 되듯.. 객체 지향의 특징1. 다형성(Polymorphism) (중요!) **역할(Interface)과 구현(Interface를 구현한 구현체)**으로 세상을 구분 운전자와 자동차 예시로 알아보는 다형성(Interface, Implements) 자동차 역할을 인터페이스, 자동차 구현은 구현체로 보자 운전자는 자동차의 인터페이스 즉 역할만 알고 있으면 어떠한 차종(구현체)도 운전할 수 있다. 그로인해 운전자는 자동차의 역할만 의존하고 있음을 알 수 있다. 운전자를 클라이언트라고 한다면 자동차 구현체가 내부적으로 바뀌어도 클라이언트에 영향을 주지 않는다. 따라서 새로운 차(새로운 구현체)가 나와도 자동차 역할(인터페이스)을 그대로 따라갈 수 있다면 자동차 세상에서 무한적으로 확장이 가능하다. 즉 클라이언트(운전자)에 영향을 주지 않고 새로운 기능을 제공할 수 있음.(세상을 역할과 구현으로 구분했기 때무임) 로미오와 줄리엣 공연 로미오와 줄리엣 역할을 인터페이스 배우들(장동건, 원빈, 김태희, 송혜교)을 구현체로 보자 클라이언트인 우리는 로미오와 줄리엣의 역할만 알면 배우가 누구든(뮤지컬의 요일별 배우가 바뀌는 것 처럼) 로미오와 줄리엣 공연을 즐길 수가 있다. 역할과 구현으로 분리하는 이유 세상이 단순해지고 변경이 편리해진다. 클라이언트는 대상의 역할 만 알면된다 클라이언트는 구현 대상의 내부 구조를 몰라도 된다 클라이언트는 구현 대상의 내부 구조가 변경되어도 영향을 받지 않음 클라이언트는 구현 대상을 변경해도 영향을 받지 않음 클라이언트를 변경하지 않고 서버의 구현 기능을 유연하게 변경할 수 있음 그외 추상화, 캡슐화, 상속 등이 있다.자바와 다형성 객체를 설계할 때 역할과 구현을 명확히 분리 역할 = 인터페이스 구현 = 인터페이스를 구현한 클래스, 구현 객체 객체 설계 시, 역할(인터페이스)을 먼저 부여하고, 그역할을 수행하는 구현 객체 만들기 객체의 협력이라는 관계부터 생각 혼자 있는 객체는 없음, 수 많은 객체 클라이언트와 객체 서버는 서로 협력 관계를 가짐 오버라이딩처럼 인터펭슬 구현한 객체를 실행 시점에 유연하게 변경할 수 있음 다형성 정리장점 실세계의 역할과 구현이라는 컨셉을 다형성을 통해 객체 세상으로 가져올 수 있음 유연하고, 변경이 용이 확장 가능한 설계 클라이언트에 영향을 주지 않는 변경 가능 인터페이스를 안정적으로 잘 설계하는 것이 중요 단점 역할(인터페이스) 자체가 변하면, 클라이언트, 서버 모두에 큰 변경이 발생함 자동차를 비행기로 변경해야 한다면? 대본 자체가 변경 되다면? USB 인터페이스가 변경된다면? → 인터페이스를 안정적으로 잘 설계하는 것이 중요 스프링과 객체 지향 다형성이 가장 중요! (객체지향의 꽃!) 스프링은 **다형성을 극대화**해서 이용할 수 있게 도와줌 스프링의 IoC(제어의 역전) , 의존관계 주입(DI) 은 다형성을 활용해서 역할과 구현을 편리하게 다룰 수 있도록 지원 스프링을 사용하면 레고 블럭 조립하듯, 공연무대의 배우를 선택하듯 구현을 편리하게 변경할 수 있음 객체 지향 5대 원칙 - SOLID클린코드로 유명한 로버트 마틴이 좋은 객체 지향 설계의 5가지 원칙을 정리 SRP(Single Response Principle, 단일 책임 원칙) 한 클래스는 하나의 책임만 가져야 한다 중요한 기준은 변경, 변경이 있을 때 파급 효과가 적으면 SRP를 잘 지킨 것 OCP(Open-Closed Principle, 개방-폐쇄 원칙) 확장에는 열려있고 수정에는 닫혀있어야 한다. 인터페이스 자체는 수정을 하지 않고 인터페이스의 구현체를 통해서 기능 확장 즉, 자동차(역할)는 수정하면 안되고 포르쉐(구현)는 추가해도 된다는 의미 OCP의 문제점의 예시를 들어보기 구현 객체를 변경하려면 클라이언트 코드를 변경해야 함 ← 다형성은 지켰지만 OCP 원칙은 지킬 수 없음, 즉 객체를 생성하고 연관관계를 맺어주는 별도의 조립/설정자가 필요 1234public class MemberService { // private MemberRepository memberRepository = new MemoryMemberRepository(); // 기존 코드 private MemberRepository memberRepository = new JdbcMemberRepository(); // 변경 코드} LSP(Liskov Substitute Principle) 객체는 프로그램의 정확성을 깨뜨리지 않으면서 하위 타입의 인스턴스로 바꿀 수 있음 다형성에서 하위 클래스는 인터페이스 규약을 다 지켜야 한다는 것 ex) 정사각형은 직사각형을 상속 받으면 안되고, 정사각형은 사격형으로 상속 받는 것은 가능 ISP(Interface Segation Principle) 특정 클라이언트를 위한 인터페이스 여러개가 범용 인터페이스 하나 보다 낫다. 인터페이스가 명확해지고 대체 가능성이 높아지기 때문 예제 자동차 인터페이스 를 운전 인터페이스, 정비 인터페이스로 분리 사용자 인터페이스를 운전자 클라이언트, 정비사 클라이언트로 분리 DIP(Dependency Inversion Principle) 프로그래머는 추상화에 의존하고 구체화에 의존하면 안됨 구현 클래스에 의존하지 말고 **인터페이스에 의존**하라는 뜻 역할에 의존해야 한다는 것, 구현체에 의존하면 변경이 아주 어려워짐 문제 상황 123MemberRepository m = new MemoryMemberRepository();// MemberRepository 및 MemoryMemberRepository 즉 인터페이스 및 구현체 모두 의존// DIP 위반 다형성 만으로는 OCP, DIP를 지킬 수 없음 즉, 다형성 만으로는 쉽게 부품을 갈아 끼우듯이 개발할 수 없음 또한 구현 객체를 변경할 때 클라이언트 코드도 함께 변경됨 이러한 기법들의 총 집합체 + DI + IoC = Spring!! 스프링은 DI, DI 컨테이너를 통해 다형성 + OCP, DIP를 가능하게 지원 클라이언트 코드의 변경 없이 기능 확장 가능하며 쉽게 부품을 교체하듯 개발이 가능하다! 스프링의 탄생 배경 옛날 어떤 개발자가 좋은 객체 지향 개발을 하려고 OCP, DIP 원칙을 지키면서 개발을 해보니, 너무 할일이 많았음 → 배보다 배꼽이 컸음, 그래서 프레임워크로!! 순수하게 자바로 OCP, DIP 원칙들을 지키면서 개발을 해보면 결국 스프링 프레임워크를 만들게 됨 (더 정확히는 DI 컨테이너) Spring 이라는 단어?문맥에 따라 다르게 사용이 됨 스프링 DI 컨테이너 기술 스프링 프레임워크 스프링 부트, 스프링 프레임워크 등을 모두 포함한 스프링 생태계 참고사항Spring Ecosystem Spring Framework? 핵심 기술 : 스프링 DI 컨테이너, AOP, 이벤트, 기타 웹 기술 : 스프링 MVC, 스프링 WebFlux 데이터 접근 기술 : Transaction, JDBC, ORM 지원, XML 지원 시룻 통합 : 캐시, 이메일, 원격접근, 스케줄링 테스트 : 스프링 기반 테스트 지원 언어: 코틀린, 그루비 최근에는 Spring boot를 통해서 스프링 프레임워크의 기술들을 편하게 사용 Spring Boot 스프링을 편리하게 사용할 수 있도록 지원, 최근에는 기본으로 사용 단독으로 실행할 수 있는 스프링 애플리케이션을 쉽게 생성 Tomcat 같은 웹 서버를 내장해서 별도의 웹 서버를 설치하지 않아도 됨 손쉬운 빌드 구성을 위한 starter 종속성 제공 스프링과 3rd party(외부) 라이브러리 자동 구성 Metric, 상태 확인, 외부 구성 같은 프로덕션 준비 기능 제공 관례에 의한 간결한 설정","link":"/1-%EA%B0%9D%EC%B2%B4-%EC%A7%80%ED%96%A5-%EC%84%A4%EA%B3%84%EC%99%80-%EC%8A%A4%ED%94%84%EB%A7%81/"},{"title":"1. 함수형 프로그래밍이란","text":"프로그래밍 세계에서 함수형 프로그래밍은 복잡한 문제를 해결하는 데 있어 새로운 차원을 제공합니다. 이 글에서는 함수형 프로그래밍의 정의부터 그 필요성까지를 탐구해보자 개요 함수형 프로그래밍이란 무엇인가 1급 시민으로서의 함수(Function as First-Class Citizen) 함수를 1급 시민으로 취급하는 이유 함수형 프로그래밍의 필요성 함수는 ‘역할을 수행하는 동사’에 가까우며, 객체는 ‘명사의 형태로 이름을 지닌 사물’에 비유됨 1. 함수형 프로그래밍이란 무엇인가명령형 프로그래밍 vs 선언형 프로그래밍 명령형 프로그래밍(Imperative Programming) 대표적으로 객체 지향 프로그래밍(OOP)이 이에 속함 어떻게 하여야 하는가(How to do) 12345에 초점을 맞춤- 예시를 통해 알아보자 - 어떻게에 초점을 맞추기에 군더더기가 많음 유저 리스트가 주어졌을 때, 검증되지 않은(unverified) 유저들의 이메일을 리스트로 주세요. 1. 이메일을 담을 리스트 선언 2. 루프 3. 유저 선언 4. 검증되지 않았는지 체크 5. 않았다면 변수에 이메일 추출 6. 이메일 리스트에 넣기 123456789- 선언형 프로그래밍(Declarative Programming) - Functional Programming (함수형 프로그래밍)이 이에 해당 - `무엇을 하여야 하는가(What to do)` 에 집중 - 예시를 통해 알아보자 - 무엇을 해야 하는지 집중하기에 동사 형태의 문제를 해결함에 깔끔하고 간단히 해결 가능 유저 리스트가 주어졌을 때, 검증되지 않은(unverified) 유저들의 이메일을 리스트로 주세요. 1. 유저 리스트에서 1. 검증되지 않은 유저만 골라내서 2. 이메일을 추출해서 2. 리스트로 받기 다만 함수는 OOP 에서보다 더 많은 역할을 해줘야 함 → 1급 시민으로서의 함수! 2. 1급 시민으로서의 함수(Function as First-Class Citizen)1급 시민의 조건함수는 다음 조건을 만족할 때 1급 시민이라고 할 수 있음 함수/메소드의 매개변수로서 전달 가능 함수/메소드의 반환값으로 사용 가능 변수에 할당 가능 이는 객체 지향 프로그래밍에서는 직관적이지 않지만, 함수를 객체의 형태로 나타내면 가능해집니다(예: 자바 8). 3. 함수를 1급 시민으로 취급하는 이유함수를 1급 시민(First-Class Citizen)으로 취급하는 이유는 프로그래밍에서의 유연성, 표현력, 그리고 코드의 재사용성과 유지보수성을 향상시키기 위함입니다. 함수를 1급 시민으로 취급할 때의 주요 이점은 다음과 같습니다. 유연성과 간결성 함수를 변수에 할당하거나 다른 함수의 인자로 전달할 수 있게 되면, 코드를 더 유연하고 간결하게 작성할 수 있음 이는 특히 복잡한 로직을 구현할 때 코드의 가독성을 크게 향상시킴 고차 함수(Higher-Order Functions) 함수를 인자로 받거나 반환값으로 함수를 사용할 수 있게 되면, 고차 함수를 구현할 수 있음 이는 함수형 프로그래밍의 핵심 개념 중 하나로, 복잡한 연산을 간단한 함수의 조합으로 표현할 수 있게 해줍니다. 재사용성 및 유지보수성 증가: 함수를 1급 시민으로 취급하면, 특정 기능을 수행하는 코드 조각을 함수로 분리하고 이를 여러 곳에서 재사용할 수 있게 됨. 이는 코드 중복을 줄이고 유지보수를 용이하게 만듬 선언적 프로그래밍: 함수를 데이터와 마찬가지로 취급하면, 프로그램을 보다 선언적으로 표현할 수 있. 이는 프로그램의 흐름을 더 명확하게 하고, 개발자가 의도하는 바를 직관적으로 전달할 수 있게 해 추상화 레벨의 향상: 함수를 1급 시민으로 사용함으로써, 코드를 보다 추상적인 수준에서 표현할 수 있게됨. 이는 복잡한 문제를 간결하고 명확한 방식으로 해결할 수 있는 방법을 제공. 함수형 프로그래밍 지원: 함수를 1급 시민으로 다루는 언어들은 일반적으로 함수형 프로그래밍 패러다임을 더 잘 지원. 함수형 프로그래밍은 부수 효과(side effects)를 최소화하고 불변성(immutability)을 지향하여, 프로그램의 안정성과 예측 가능성을 높이는 데 도움을 줌 4. 함수형 프로그래밍의 필요성함수형 프로그래밍은 다음과 같은 이점을 제공 역할에 충실한 코드: ‘무엇을 할 것인가’에 중점을 두기 때문에 가독성이 높고 유지보수가 쉬움 버그 감소: 각 기능이 명확히 분리되어 있어 버그 발생 가능성이 줄어듬 확장성: 새로운 기능 추가 시 기존 코드에 미치는 영향이 적어 확장이 용이 패러다임의 전환: Stream, Optional 등의 자바 8 기능을 통해 더 강력하고 간결한 코드 작성이 가능해짐 결론 함수형 프로그래밍은 객체 지향 프로그래밍에 새로운 차원을 추가해 줌 문제에 따라 객체 지향적 접근이 더 적합할 수도 있고, 함수형 프로그래밍이 더 적합할 수도 있음 대용량 데이터 처리와 같은 경우에는 함수형 프로그래밍이 효과적인 해결책을 제시할 수 있음","link":"/1-%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EC%9D%B4%EB%9E%80/"},{"title":"2. 람다 표현식과 함수형 인터페이스","text":"자바에서의 함수형 프로그래밍은 코드의 간결성과 가독성을 높이고, 유지보수를 용이하게 만드는 중요한 패러다임입이다. 이 글에서는 자바의 람다 표현식과 함수형 인터페이스에 대해 살펴보자 개요 Function Interface Lambda Expression Anonymous Class 1. Function Interface 자바에서 함수는 객체로 표현될 수 있으며, 이를 1급 객체라고 함 이를 가능하게 하는 것 중 하나가 Function Interface 예제) Function 인터페이스를 구현하여 10을 더하는 함수를 객체로 나타냄 123456789101112public class Adder implements Function&lt;Integer, Integer&gt; { @Override public Integer apply(Integer x) { return x + 10; } public static void main(String[] args) { Function&lt;Integer, Integer&gt; myAdder = new Adder(); int result = myAdder.apply(5); // 변수에 함수를 넣어서 사용 System.out.println(result); }} 하지만 이 방식은 매번 새로운 객체를 정의해야 하는 번거로움이 있음. 이를 해결하기 위해 람다 표현식이 도입되었음 2. Lambda Expression 람다 표현식은 이름 없는 함수(Anonymous function)를 간결하게 표현할 수 있는 방법 이를 통해 함수를 보다 직관적으로 표현할 수 있음 예제) 람다 표현식을 사용하여 10을 더하는 함수 1234567public class AdderLambda { public static void main(String[] args) { Function&lt;Integer, Integer&gt; myAdder = x -&gt; x + 10; int result = myAdder.apply(5); System.out.println(result); }} 람다 표현식은 매개변수의 타입 유추가 가능할 때 타입을 생략할 수 있고, 매개변수가 하나일 경우 괄호도 생략할 수 있음 또한, 함수 몸체가 단일 반환문으로 구성되어 있을 경우 중괄호도 생략할 수 있음 2.1 BiFunction Interface 매개변수가 2개인 경우에는 BiFunction 인터페이스를 사용합 예제) 두 개의 정수를 더하는 함수 123456789public class AdderLambda { public static void main(String[] args) { BiFunction&lt;Integer, Integer, Integer&gt; myAdder = (Integer x, Integer y) -&gt; { return x + y; }; int result = myAdder.apply(3, 7); System.out.println(result); }} 2.2 Functional Interface 함수형 인터페이스는 단 하나의 추상 메소드를 가지며, 이를 통해 람다 표현식과 연결 Runnable, Comparator, Callable 등이 대표적인 예 2.3 3개 이상의 매개변수 매개변수가 3개 이상인 경우, 사용자가 직접 함수형 인터페이스를 정의 예제) 세 개의 정수를 더하는 함수형 인터페이스와 그 구현 123456789101112131415@FunctionalInterfacepublic interface TriFunction&lt;T, U, V, R&gt; { R apply(T t, U u, V v);}public class Main { public static void main(String[] args) { TriFunction&lt;Integer, Integer, Integer, Integer&gt; addThreeNumbers = (Integer x, Integer y, Integer z) -&gt; { return x + y + z; }; int result = addThreeNumbers.apply(3, 7, 10); System.out.println(result); }} 3. Anonymous Class 익명 클래스는 인터페이스의 구현체를 생성할 때 클래스를 명시적으로 정의하지 않고 즉석에서 만들어 사용하는 방법 12345678Function&lt;Integer, Integer&gt; myAdder = new Function&lt;Integer, Integer&gt;() { @Override public Integer apply(Integer x) { return x + 10; }};int result = myAdder.apply(5);System.out.println(result); 결론 자바에서 람다 표현식과 함수형 인터페이스는 코드를 간결하고 명확하게 작성하는 데 큰 도움을 줌 이들은 함수를 객체로 취급할 수 있게 하며, 이로 인해 프로그래밍이 보다 유연해지며 이러한 기법은 현대 자바 프로그래밍에서 필수적인 요소가 되었음 참고 함수의 구성요소 함수의 이름 반환값의 타입(return type) 매개변수(parameters) 함수의 내용(body)","link":"/2-%EB%9E%8C%EB%8B%A4-%ED%91%9C%ED%98%84%EC%8B%9D%EA%B3%BC-%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"},{"title":"2. 객체지향 원리 적용","text":"스프링을 POJO로 구현한다면 DI, IoC 기술이 없기 때문에 객체지향 5대원칙을 다 지킬 수 없게 된다. OCP(Open Closed Principle), DIP(Dependency Inversion Principle) 원칙에서 어긋나기 때문이다. 그렇다면 스프링에서 어떻게 이를 만족시키는지 알아보자. 관심사의 분리AppConfig 의 등장 어플리케이션의 전체 동작 방식을 구성(config)하기 위해 **구현 객체를 생성하고 연결하는 책임**을 갖는 별도의 설정 클래스 의존관계를 외부 즉 AppConfig에 맡기는 것 의존관계를 마치 외부에서 주입해주는 것 같다고 해서 DI(Dependency Injection) 즉 **의존관계(의존성) 주입**이라고 함 코드 예시 1234567891011121314151617181920public class AppConfig { public MemberService memberService { return new MemberServiceImpl(memberRepository()); } public OrderService orderService { return OrderServiceImpl( memberRepository(), discountpolicy() ); } public MemberRepository memberRepository() { return new MemoryMemberRepository(); } public DiscountPolicy discountpolicy() { return new FixDiscountPolicy(); }} 1234567891011121314public class OrderApp { public static void main(String[] args) { AppConfig appConfig = new AppConfig(); MemberService memberService = appConfig.memberService(); OrderService orderService = appConfig.orderService(); long memberId = 1L; Member member = new Member(memberId, &quot;memberA&quot;, Grade.VIP); memberService.join(member); Order order = orderService.createOrder(memberId, &quot;itemA&quot;, 10000); System.out.println(&quot;order = &quot; + order); }} AppConfig 는 애플리케이션의 실제 동작이 필요한 구현 객체를 생성 MemberServiceImpl MemoryMemberRepository OrderServiceImpl FixDisconutPolicy AppConfig는 생성한 객체 레퍼런스를 ** 1생성자를 통해서 주입(연결) **해줌 MemoryServiceImpl → MemoryMemberRepository OrderServiceImpl → MemoryMemberRepository, FixDiscountPolicy 사용 영역의 코드는 어떤 코드도 변경할 필요가 없다 구성 영역은 당연히 변경 된다. AppConfig를 어플리케이션이라는 공연의 기획자로 생각하자 AppConfig는 애플리케이션의 전체 동작 방식을 구성(config)하기 위해 **구현 객체를 생성하고 연결**하는 책임 이제부터 클라이언트 객체는 자신의 역할을 실행하는 것만 집중하고 권한이 줄어듬(책임이 명확해짐) IoC, DI 그리고 컨테이너IoC(Inversion of Control, 제어의 역전) IoC는 프로그램의 제어 흐름 구조에 관한 디자인 원칙 전통적인 프로그래밍에서는 프로그램의 흐름을 사용자가 직접 제어 구현 객체가 프로그램의 제어 흐름을 스스로 조종 (클라이언트 구현 객체가 스스로 필요한 서버 구현 객체를 생성하고 연결하고 실행 그러나 IoC에서는 이러한 제어가 프레임워크나 라이브러리 같은 외부 컴포넌트에 위임 이는 코드의 결합도를 낮추고 유연성 및 확장성을 높이는 데 도움이 됨 AppConfig 처럼 구현 객체는 자신의 로직을 실행하는 역할만 담당 DI (Dependency Injection, 의존관계 주입) DI는 IoC의 한 형태로, 컴포넌트 간의 의존성을 관리하는 구체적인 방법을 제공 DI를 통해, 객체는 자신의 의존성(즉, 다른 객체에 대한 참조)을 직접 생성하거나 찾지 않고, 외부(예: 프레임워크나 컨테이너)에서 제공받음 이 방식은 객체가 필요로 하는 의존성을 변경하거나 모의 객체로 대체하기 쉬워서 테스트 용이성이 높아지고, 코드의 유지보수성이 향상됨 의존관계는 정적인 클래스 의존 관계와 실행 시점에 결정되는 동적인 객체(인스턴스) 의존 관계 등을 분리해서 생각해야한다. 정적인 클래스 의존관계 개념 정적인 클래스 의존관계는 컴파일 시간에 결정되며, 프로그램 실행 후에는 변경되지 않음 클래스가 다른 클래스를 사용하는 방식(예: 메서드 호출, 변수 선언)을 통해 정의됨 목적 애플리케이션의 모듈 간 결합도를 관리하기 위해 명확하게 의존성을 설정 이를 통해 코드의 가독성과 관리성이 향상됨 예제 12345678import java.util.List;class ProductRepository { // ProductRepository 클래스는 Product 클래스에 정적으로 의존합니다. List&lt;Product&gt; findAllProducts() { // 제품 목록 반환 로직 }} 이 예제에서 ProductRepository 클래스는 Product 클래스에 정적으로 의존 **ProductRepository**의 메서드 내에서 Product 타입을 사용하므로, 이 두 클래스 사이의 의존관계는 컴파일 시간에 결정됨 동적인 클래스 의존관계 개념 동적인 클래스 의존관계는 프로그램 실행 도중, 즉 런타임에 결정되고 변경될 수 있는 의존관계 의존관계는 실행 시점에 객체의 상태나 프로그램의 로직에 따라 결정되고, 변경될 수 있음 동적 의존관계는 런타임에 인터페이스, 추상 클래스, 리플렉션, 또는 의존성 주입 같은 기술을 사용하여 구체적인 구현체를 결정하게 됨 실행 시점(런타임)에 외부에서 실제 구현 객체를 생성하고 클라이언트에 전달해서 클라이언트와 서버의 실제 의존관계가 연결 되는 것을 의존관계 주입이라 함 목적 의존관계 주입을 사용하면 클라이언트 코드를 변경하지 않고, 즉 정적인 클래스 의존관계를 변경하지 않고, 동적인 객체 인스턴스 의존관계를 쉽게 변경할 수 있음 예제 12345678910111213141516171819202122interface PaymentProcessor { void processPayment(PaymentData data);}class PaypalPaymentProcessor implements PaymentProcessor { public void processPayment(PaymentData data) { // PayPal 결제 처리 로직 }}class PaymentService { private PaymentProcessor paymentProcessor; // 의존성 주입을 통해 런타임에 구체적인 PaymentProcessor 구현체를 주입받음 public PaymentService(PaymentProcessor paymentProcessor) { this.paymentProcessor = paymentProcessor; } void processPayment(PaymentData data) { paymentProcessor.processPayment(data); }} 이 예제에서 PaymentService 클래스는 PaymentProcessor 인터페이스에 의존 실제 사용되는 **PaymentProcessor**의 구현체는 런타임에 **PaymentService**의 생성자를 통해 주입되므로, 동적인 클래스 의존관계의 예가 되며 이를 통해 다양한 결제 처리 방식을 PaymentService 클래스 변경 없이 쉽게 적용할 수 있음 IoC 컨테이너, DI 컨테이너, Spring 컨테이너의 차이IoC 컨테이너 개념 제어의 역전 원칙(IoC)을 구현하는 프레임워크 또는 라이브러리 이 원칙에 따라, 애플리케이션의 흐름을 사용자 코드가 아닌 프레임워크가 관리 목적 IoC 컨테이너의 주된 목적은 객체의 생명 주기와 의존성 관리를 자동화하여, 개발자가 이러한 부분에 신경 쓰지 않고 비즈니스 로직 구현에 집중할 수 있도록 하는 것 예시 Spring의 ApplicationContext 등이 IoC 컨테이너 Java EE의 CDI (Contexts and Dependency Injection) 컨테이너 DI 컨테이너 개념 IoC의 한 형태로, 객체 간의 의존성을 외부에서 주입하는 방식으로 관리 이를 통해 객체 간의 결합도를 낮추고 코드의 유연성 및 테스트 용이성을 높임 목적 실행 시점에 객체 간의 의존성을 자동으로 주입함으로써, 개발자가 수동으로 의존성을 설정하는 번거로움을 줄여줌 예시 스프링 프레임워크의 BeanFactory와 ApplicationContext Spring 컨테이너 개념 스프링 프레임워크 내에 구현된 IoC 컨테이너로 스프링 애플리케이션 내의 객체(Bean)들의 생성, 관리, 의존성 주입 등을 담당 IoC(Inversion of Control, 제어의 역전)와 DI(Dependency Injection, 의존성 주입)의 원칙을 기반으로 작동하여, 애플리케이션의 결합도를 낮추고, 코드의 재사용성 및 유지보수성을 높이는 데 중추적인 역할을 함 목적 애플리케이션의 결합도를 낮추어 유연성과 확장성을 제공 코드 재사용성과 유지보수성을 향상시킴 애플리케이션 개발 과정에서 반복되는 코드 작성을 줄여줌 애플리케이션의 설정과 구성을 중앙에서 관리하여 개발자가 비즈니스 로직에 더 집중할 수 있게 도와줌 주요 기능 Bean의 생성과 관리: 스프링 컨테이너는 애플리케이션 구성 요소를 Bean으로 등록하고, 이들의 생성, 초기화, 소멸 과정을 관리. 의존성 주입: 컨테이너는 Bean 간의 의존성을 자동으로 주입하여, 개발자가 수동으로 설정할 필요 없이 객체 간의 관계를 자동으로 설정 Bean의 생명 주기 관리: 스프링 컨테이너는 Bean의 전체 생명 주기를 관리하며, 초기화 및 소멸 시 커스텀 로직을 실행할 수 있는 콜백 인터페이스를 제공합 애플리케이션 설정의 중앙 집중화: XML, 애너테이션, 자바 기반 설정을 통해 애플리케이션의 구성 정보를 중앙에서 관리할 수 있음 예시 BeanFactory와 ApplicationContext는 스프링 컨테이너의 두 가지 주요 형 BeanFactory: 기본적인 DI 기능을 제공하며, 필요할 때 Bean을 로딩하는 지연 로딩 방식을 사용 ApplicationContext: BeanFactory의 모든 기능을 포함하며, 메시지 리소스 처리, 이벤트 발행 등 애플리케이션 개발에 필요한 추가적인 기능을 제공 프레임워크 vs 라이브러리프레임워크 개념 애플리케이션의 기본 구조와 실행 흐름을 정의하는 포괄적인 도구 세트 사용자는 프레임워크가 정의된 규칙과 인터페이스에 맞추어 코드를 작성하며 프레임워크가 애플리케이션의 실행 흐름을 제어 내가 작성한 코드를 제어하고 대신 실행해 줄 경우 목적 애플리케이션 개발의 복잡성을 줄이고, 개발 과정을 표준화하는 것 이를 통해 개발자는 반복적인 작업을 줄이고, 일관된 방식으로 애플리케이션을 구축할 수 있음 제어의 역전(IoC) 원칙을 통해 프레임워크는 개발자가 작성한 코드를 적절한 시점에 호출 예시 Spring, JUnit, FastAPI 라이브러리 개념 특정 작업을 수행하는 데 도움을 주는 재사용 가능한 코드의 집합 개발자가 필요에 따라 선택적으로 사용할 수 있는 함수나 클래스 등으로 구성 내가 작성한 코드가 직접 제어의 흐름을 담당할 경우 목적 특정 기능이나 작업을 수행하는 코드를 제공함으로써, 개발자가 직접 그 기능을 구현하는 데 드는 시간과 노력을 줄이는 것 애플리케이션의 흐름은 여전히 개발자가 제어 예시 PyTorch, React, JQuery, Numpy","link":"/2-%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5-%EC%9B%90%EB%A6%AC-%EC%A0%81%EC%9A%A9/"},{"title":"3. Functional Interface","text":"자바에서 함수형 인터페이스는 코드의 간결성과 유연성을 높이는 데 크게 기여한다. 이 글에서는 자바의 주요 함수형 인터페이스인 Supplier, Consumer, BiConsumer, Predicate, **Comparator**에 대해 알아보자 개요 Supplier Consumer BiConsumer Predicate Comparator 1. Supplier Supplier&lt;T&gt; : 매개변수 없이 T 타입의 객체를 반환하는 함수형 인터페이스 예제 123456789101112131415161718192021222324252627282930import java.util.function.Supplier;public class Main { // 랜덤한 double 값을 생성하는 Supplier와 원하는 개수를 받아 해당 개수만큼 랜덤 double 값을 출력 public static void printRandomDoubles(Supplier&lt;Double&gt; randomSupplier, int count) { for (int i = 0; i &lt; count; i++) { System.out.println(randomSupplier.get()); } } public static void main(String[] args) { // 예제 1: 문자열을 제공하는 Supplier를 생성하고 사용 // 여기서 Supplier는 'Hello world!' 문자열을 반환하는 람다 표현식 Supplier&lt;String&gt; myStringSupplier = () -&gt; &quot;Hello world!&quot;; System.out.println(myStringSupplier.get()); // 예제 2: 랜덤 double 값을 제공하는 Supplier를 생성하고 사용 // 이 Supplier는 Math.random()을 호출하여 랜덤 값을 반환 Supplier&lt;Double&gt; myRandomDoubleSupplier = () -&gt; Math.random(); System.out.println(myRandomDoubleSupplier.get()); System.out.println(myRandomDoubleSupplier.get()); System.out.println(myRandomDoubleSupplier.get()); // 예제 3: printRandomDoubles 메소드를 사용하여 랜덤 double 값을 5번 출력 // 여기서 myRandomDoubleSupplier는 랜덤 값을 생성하는 함수로 사용됨 System.out.println(); printRandomDoubles(myRandomDoubleSupplier, 5); }} 2. Consumer Consumer&lt;T&gt; : T 타입의 객체를 매개변수로 받고 반환값 없이 처리하는 함수형 인터페이스 예제 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.Arrays;import java.util.List;import java.util.function.Consumer;public class Main { // Integer 타입의 리스트와 Consumer를 받아 각 요소에 대해 Consumer의 accept 메소드를 실행 public static void process(List&lt;Integer&gt; inputs, Consumer&lt;Integer&gt; processor) { for (Integer input: inputs) { processor.accept(input); } } // 제네릭 타입의 리스트와 Consumer를 받아 각 요소에 대해 Consumer의 accept 메소드를 실행 public static &lt;T&gt; void processGenerics(List&lt;T&gt; inputs, Consumer&lt;T&gt; processor) { for (T input : inputs) { processor.accept(input); } } public static void main(String[] args) { // 예제 1. 문자열을 출력하는 Consumer 예제 Consumer&lt;String&gt; myStringConsumer = (String str) -&gt; { System.out.println(str); }; myStringConsumer.accept(&quot;Hello World!&quot;); // 예제 2. Integer 처리를 위한 Consumer 예제 List&lt;Integer&gt; integerInputs = Arrays.asList(5, 6, 7); // Arrays.asList로 선언하면 불변형 이어서 add 하면 에러 나옴 // integerInputs.add(5); 에러 발생 확인 Consumer&lt;Integer&gt; myIntegerProcessor = x -&gt; System.out.println(&quot;Porcessing integer: &quot; + x); process(integerInputs, myIntegerProcessor); System.out.println(); // 예제 3. Double 처리를 위한 Consumer의 예제 Consumer&lt;Double&gt; myDoubleProcessor = x -&gt; System.out.println(&quot;Processing double: &quot; + x); List&lt;Double&gt; doubleInputs = Arrays.asList(1.1, 2.2, 3.3); processGenerics(doubleInputs, myDoubleProcessor); }} 3. BiConsumer BiConsumer&lt;T, U&gt; : 두 매개변수 T와 U를 받아 반환값 없이 처리하는 함수형 인터페이스 예제 123456789101112131415161718192021import java.util.Arrays;import java.util.List;import java.util.function.BiConsumer;public class Main { public static &lt;T&gt; void process(List&lt;T&gt; inputs, BiConsumer&lt;Integer, T&gt; processor) { for (int i = 0; i &lt; inputs.size(); i++) { processor.accept(i, inputs.get(i)); } } public static void main(String[] args) { BiConsumer&lt;Integer, Double&gt; myDoubleProcessor = (Integer index, Double input) -&gt; System.out.println(&quot;Processing &quot; + input + &quot; at index &quot; + index); List&lt;Double&gt; inputs = Arrays.asList(1.1, 2.2, 3.3); process(inputs, myDoubleProcessor); }} 4. Predicate Predicate&lt;T&gt; : T 타입의 객체를 매개변수로 받아 boolean 값을 반환하는 함수형 인터페이스 예제 1234567891011121314151617181920212223242526272829import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class Main { public static &lt;T&gt; List&lt;T&gt; filter(List&lt;T&gt; inputs, Predicate&lt;T&gt; condition) { List&lt;T&gt; output = new ArrayList&lt;&gt;(); for (T input: inputs) { if (condition.test(input)) { output.add(input); } } return output; } public static void main(String[] args) { Predicate&lt;Integer&gt; isPoistive = x -&gt; x &gt; 0; System.out.println(isPoistive.test(10)); System.out.println(isPoistive.test(-10)); List&lt;Integer&gt; inputs = Arrays.asList(10, -2, 5, 0, -1); System.out.println(&quot;Postive number : &quot; + filter(inputs, isPoistive)); System.out.println(&quot;Non-positive number : &quot; + filter(inputs, isPoistive.negate())); System.out.println(&quot;positive number &gt;=0 : &quot; + filter(inputs, isPoistive.or(x -&gt; x == 0))); System.out.println(&quot;positive and even number : &quot; + filter(inputs, isPoistive.and(x -&gt; x % 2 == 0))); }} 5. Comparator Comparator&lt;T&gt; : 두 T 타입의 객체를 비교하는 함수형 인터페이스 예제 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package util;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;public class User { private int id; private String name; public User(int id, String name) { this.id = id; this.name = name; } public int getId() { return id; } public String getName() { return name; } @Override public String toString() { return &quot;User{&quot; + &quot;id=&quot; + id + &quot;, name='&quot; + name + '\\\\'' + '}'; } public static void main(String[] args) { List&lt;User&gt; users = new ArrayList&lt;&gt;(); users.add(new User(5, &quot;Charlie&quot;)); users.add(new User(3, &quot;Alice&quot;)); users.add(new User(1, &quot;Bravo&quot;)); System.out.println(users); Comparator&lt;User&gt; idComparator = (u1, u2) -&gt; u1.getId() - u2.getId(); Collections.sort(users, idComparator); System.out.println(users); Collections.sort(users, (u1, u2) -&gt; u1.getName().compareTo(u2.getName())); System.out.println(users); }} 결론각 인터페이스는 특정한 타입의 연산을 추상화하여, 개발자가 보다 효율적으로 문제를 해결할 수 있도록 도와줌","link":"/3-Functional-Interface/"},{"title":"4. Method Reference","text":"개요 Method Reference Constructor Reference Method Reference 로 더욱 심플하게 1. Method Reference 기존에 이미 선언되어있는 메서드를 지정하고 싶을때 :: 오퍼레이터를 사용 생략이 많기 때문에 사용할 메서드의 매개변수의 타입과 리턴 타입을 미리 숙지해야 함 1.1 메서드 레퍼런스의 4가지 케이스 클래스의 static method를 지정할 때 ClassName::staticMethodName 123456789101112import java.util.function.Function;public class Example { public static void main(String[] args) { int testNum = Integer.parseInt(&quot;15&quot;); System.out.println(testNum); // 15 Function&lt;String, Integer&gt; str2int = Integer::parseInt; System.out.println(str2int.apply(&quot;20&quot;)); // 20 }} 선언된 객체의 instance method를 지정할 때 objectName::instanceMethodName 12345678910111213import java.util.function.Predicate;public class Example { public static void main(String[] args) { String str = &quot;Hello&quot;; boolean testBool = str.equals(&quot;World&quot;); System.out.println(testBool); // false Predicate&lt;String&gt; equalsToHello = str::equals; System.out.println(equalsToHello.test(&quot;Hello&quot;)); // true }} 1234567891011121314151617181920212223242526272829import java.util.function.BiFunction;public class Example { public static int calculate(int x, int y, BiFunction&lt;Integer, Integer, Integer&gt; operator) { return operator.apply(x, y); } public static int multiply(int x, int y) { return x * y; } public int subtrcat(int x, int y) { return x - y; } public void myMethod() { System.out.println(calculate(10, 3, this::subtrcat)); } public static void main(String[] args) { System.out.println(calculate(8, 2, (x, y) -&gt; x + y)); // 10 System.out.println(calculate(8,2 , Example::multiply)); // 16 // ClassName::staticMethodName Example example = new Example(); System.out.println(calculate(8, 2, example::subtrcat)); // 6 // objectName::instanceMethodName example.myMethod(); // 7 }} 객체의 instance method를 지정할 때 ClassName::instanceMethodName 123456789101112131415import java.util.function.BiPredicate;import java.util.function.Function;public class Example { public static void main(String[] args) { Function&lt;String, Integer&gt; strLength = String::length; int length = strLength.apply(&quot;Hello World!&quot;); System.out.println(length); // 12 BiPredicate&lt;String, String&gt; strEquals = String::equals; boolean isEqual = strEquals.test(&quot;Hello&quot;, &quot;world!&quot;); System.out.println(isEqual); // false }} 12345678910111213141516171819202122232425262728293031323334353637383940414243public class User { private int id; private String name; public User(int id, String name) { this.id = id; this.name = name; } public int getId() { return id; } public String getName() { return name; } @Override public String toString() { return &quot;User{&quot; + &quot;id=&quot; + id + &quot;, name='&quot; + name + '\\\\'' + '}'; } public static void printUserField(List&lt;User&gt; users, Function&lt;User, Object&gt; getter) { for (User user : users) { System.out.println(getter.apply(user)); } } public static void main(String[] args) { List&lt;User&gt; users = new ArrayList&lt;&gt;(); users.add(new User(5, &quot;Charlie&quot;)); users.add(new User(3, &quot;Alice&quot;)); users.add(new User(1, &quot;Bravo&quot;)); printUserField(users, User::getName); // Charlie // Alice // Bravo }} 클래스의 constructor를 지정할 때 ClassName::new 123456789101112131415161718192021222324252627282930313233343536373839public class User { private int id; private String name; public User(int id, String name) { this.id = id; this.name = name; } public int getId() { return id; } public String getName() { return name; } @Override public String toString() { return &quot;User{&quot; + &quot;id=&quot; + id + &quot;, name='&quot; + name + '\\\\'' + '}'; }}import java.util.function.BiFunction;public class Example { public static void main(String[] args) { User user = new User(1, &quot;Alice&quot;); BiFunction&lt;Integer, String, User&gt; userCreater = User::new; User charlie = userCreater.apply(3, &quot;Charlie&quot;); System.out.println(charlie); // User{id=3, name='Charlie'} }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class Example { public static void main(String[] args) { Map&lt;String, BiFunction&lt;String, String, Car&gt;&gt; carTypeToConstructorMap = new HashMap&lt;&gt;(); carTypeToConstructorMap.put(&quot;sedan&quot;, Sedan::new); carTypeToConstructorMap.put(&quot;suv&quot;, Suv::new); carTypeToConstructorMap.put(&quot;van&quot;, Van::new); String[][] inputs = new String[][] { {&quot;sedan&quot;, &quot;Sonata&quot;, &quot;Hyundai&quot;}, {&quot;van&quot;, &quot;Carnival&quot;, &quot;Kia&quot;}, {&quot;sedan&quot;, &quot;G80&quot;, &quot;Genesis&quot;}, {&quot;suv&quot;, &quot;Sorento&quot;, &quot;Kia&quot;} }; // 기존에는 if sedan 이면 new Sedan 이런식으로 했었음... 차 종이 10개면 if, else가 10개... List&lt;Car&gt; cars = new ArrayList&lt;&gt;(); for (int i=0; i &lt; inputs.length; i++) { String[] input = inputs[i]; String carType = input[0]; String name = input[1]; String brand = input[2]; cars.add(carTypeToConstructorMap.get(carType).apply(name, brand)); } for (Car car: cars) { car.drive(); // Driving a sedan Sonata from Hyundai // Driving a van Carnival from Kia // Driving a sedan G80 from Genesis // Driving a suv Sorento from Kia } }}public abstract class Car { protected String name; protected String brand; public Car(String name, String brand) { this.name = name; this.brand = brand; } public abstract void drive();}public class Sedan extends Car { public Sedan(String name, String brand) { super(name, brand); } @Override public void drive() { System.out.println(&quot;Driving a sedan &quot; + name + &quot; from &quot; + brand); }}public class Suv extends Car { public Suv(String name, String brand) { super(name, brand); } @Override public void drive() { System.out.println(&quot;Driving a suv &quot; + name + &quot; from &quot; + brand); }}public class Van extends Car { public Van(String name, String brand) { super(name, brand); } @Override public void drive() { System.out.println(&quot;Driving a van &quot; + name + &quot; from &quot; + brand); }}","link":"/4-Method-Reference/"},{"title":"5. Stream","text":"개요Stream은 Fuctional Interface를 적극 활용해 데이터를 매우 간편하게 가공해주는 도구 Stream이 무엇이고 어떻게 만드는지 Stream Pipeline을 통해 데이터를 쉽게 가공해주는 필터, 맵 등 여러가지 Stream의 빌트인 메소드를 살펴봄 Stream이란 데이터의 흐름 컬렉션(Collection) 형태로 구성된 데이터를 lambda를 이용해 간결하고 직관적으로 프로세스하게 해줌 For, While 등을 이용하던 기존 loop를 대체 손쉽게 병렬 처리를 할 수 있게 해줌 12345678910111213141516171819202122232425import java.util.Arrays;import java.util.HashSet;import java.util.List;import java.util.Set;import java.util.stream.Collectors;import java.util.stream.Stream;public class Example { public static void main(String[] args) { Stream&lt;String&gt; nameStream = Stream.of(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;); List&lt;String&gt; names = nameStream.collect(Collectors.toList()); System.out.println(names); String[] cityArray = new String[] {&quot;Seoul&quot;, &quot;Busan&quot;, &quot;Gyeonggi&quot;}; Stream&lt;String&gt; cityStream = Arrays.stream(cityArray); List&lt;String&gt; cityList = cityStream.collect(Collectors.toList()); System.out.println(cityList); Set&lt;Integer&gt; numberSet = new HashSet&lt;&gt;(Arrays.asList(3, 5, 7)); Stream&lt;Integer&gt; numberStream = numberSet.stream(); List&lt;Integer&gt; numberList = numberStream.collect(Collectors.toList()); System.out.println(numberList); }} Filter 만족하는 데이터만 걸러내는데 사용 Predicate에 true를 반환하는 데이터만 존재하는 Stream을 리턴 1Stream&lt;T&gt; filter Predicator&lt;? super T&gt; predicate; 예제 1234567891011121314public class Example { public static void main(String[] args) { Stream&lt;Integer&gt; numberStream = Stream.of(3, -5, 7, 10, -3); Stream&lt;Integer&gt; filteredNumberStream = numberStream.filter(x -&gt; x &gt; 0); List&lt;Integer&gt; filteredNumbers = filteredNumberStream.collect(Collectors.toList()); System.out.println(filteredNumbers); // [3, 7, 10] List&lt;Integer&gt; newFilteredNumbers = Stream.of(3, -5, 7, 10, -3) .filter(x-&gt; x &gt; 0) .collect(Collectors.toList()); System.out.println(newFilteredNumbers); // [3, 7, 10] }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;import my.stduy.elasticsearch_test.test.Order.OrderStatus;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@naver.com&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@naver.com&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(true) .setEmailAddress(&quot;charlie@naver.com&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); List&lt;User&gt; verifiedUsers = users.stream() .filter(User::isVerified) .collect(Collectors.toList()); System.out.println(verifiedUsers); // [User [id=101, name=Alice, emailAddress=alice@naver.com, isVerified=true, ], User [id=103, name=Charlie, emailAddress=charlie@naver.com, isVerified=true, ]] List&lt;User&gt; unverifiedUsers = users.stream() .filter(user -&gt; !user.isVerified()) .collect(Collectors.toList()); System.out.println(unverifiedUsers); // [User [id=102, name=Bob, emailAddress=bob@naver.com, isVerified=false, ]] Order order1 = new Order() .setId(1001) .setStatus(OrderStatus.CREATED); Order order2 = new Order() .setId(1002) .setStatus(OrderStatus.ERROR); Order order3 = new Order() .setId(1003) .setStatus(OrderStatus.PROCESSED); Order order4 = new Order() .setId(1004) .setStatus(OrderStatus.ERROR); Order order5 = new Order() .setId(1005) .setStatus(OrderStatus.IN_PROGRESS); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4, order5); List&lt;Order&gt; filterErrorOrders = orders.stream() .filter(order -&gt; order.getStatus() == OrderStatus.ERROR) .collect(Collectors.toList()); System.out.println(filterErrorOrders); // [Order [id=1002, createdByUserId=0, status=ERROR, ], Order [id=1004, createdByUserId=0, status=ERROR, ]] }} Map : 데이터의 변형 데이터를 변형하는데 사용 데이터에 해당 함수가 적용된 결과물을 제공하는 stream을 리턴 1&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper); 1234567891011121314151617181920import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;import java.util.stream.Stream;public class Example { public static void main(String[] args) { List&lt;Integer&gt; numberList = Arrays.asList(3, 6, -4); List&lt;Integer&gt; numberListX2 = numberList.stream() .map(x -&gt; x * 2) .collect(Collectors.toList()); System.out.println(numberListX2); // [6, 12, -8] Stream&lt;Integer&gt; numberListStream = numberList.stream(); Stream&lt;String&gt; strStream = numberListStream.map(x -&gt; &quot;Number is &quot; + x); List&lt;String&gt; strList = strStream.collect(Collectors.toList()); System.out.println(strList); // [Number is 3, Number is 6, Number is -4] }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;import my.stduy.elasticsearch_test.test.Order.OrderStatus;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@naver.com&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@naver.com&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(true) .setEmailAddress(&quot;charlie@naver.com&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); List&lt;String&gt; emailAddresses = users.stream() .map(User::getEmailAddress) .collect(Collectors.toList()); System.out.println(emailAddresses); // [alice@naver.com, bob@naver.com, charlie@naver.com] Order order1 = new Order() .setId(1001) .setStatus(OrderStatus.CREATED) .setCreatedByUserId(101); Order order2 = new Order() .setId(1002) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(103); Order order3 = new Order() .setId(1003) .setStatus(OrderStatus.PROCESSED) .setCreatedByUserId(102); Order order4 = new Order() .setId(1004) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(104); Order order5 = new Order() .setId(1005) .setStatus(OrderStatus.IN_PROGRESS) .setCreatedByUserId(101); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4, order5); List&lt;Long&gt; mapCreatedByUserIdOrders = orders.stream() .map(Order::getCreatedByUserId) .collect(Collectors.toList()); System.out.println(mapCreatedByUserIdOrders); // [101, 103, 102, 104, 101] }} Stream의 구성요소 Source (소스) 컬렉션 배열 등 Intermediate Operations(중간 처리) 0개 이상의 filter, map 등의 중간 처리 Terminal Operation(종결 처리) Collect, reduce 여러가지 중간 처리를 이어붙이는 것이 가능 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@naver.com&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@naver.com&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(true) .setEmailAddress(&quot;charlie@naver.com&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); // 명령형 프로그래밍의 경우 List&lt;String&gt; emails = new ArrayList&lt;&gt;(); for (User user: users) { if (!user.isVerified()) { String email = user.getEmailAddress(); emails.add(email); } } System.out.println(emails); // 함수형 프로그래밍의 경우 List&lt;String&gt; emails2 = users.stream() .filter(user -&gt; !user.isVerified()) .map(User::getEmailAddress) .collect(Collectors.toList()); System.out.println(emails2); LocalDateTime now = LocalDateTime.now(ZoneId.of(&quot;Asia/Seoul&quot;)); Order order1 = new Order() .setId(1001) .setStatus(OrderStatus.CREATED) .setCreatedByUserId(101) .setCreatedAt(now.minusHours(4)); Order order2 = new Order() .setId(1002) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(103) .setCreatedAt(now.minusHours(1)); Order order3 = new Order() .setId(1003) .setStatus(OrderStatus.PROCESSED) .setCreatedByUserId(102) .setCreatedAt(now.minusHours(36)); Order order4 = new Order() .setId(1004) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(104) .setCreatedAt(now.minusHours(40)); Order order5 = new Order() .setId(1005) .setStatus(OrderStatus.IN_PROGRESS) .setCreatedByUserId(101) .setCreatedAt(now.minusHours(10)); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4, order5); List&lt;Long&gt; errorStatusCrateadByUserIds = orders.stream() .filter(order -&gt; order.getStatus() == OrderStatus.ERROR) .map(Order::getCreatedByUserId) .collect(Collectors.toList()); System.out.println(errorStatusCrateadByUserIds); // [103, 104] List&lt;Order&gt; errorOrderWithin24Hours = orders.stream() .filter(order -&gt; order.getStatus() == OrderStatus.ERROR) .filter(order -&gt; order.getCreatedAt().isAfter(now.minusHours(24))) .collect(Collectors.toList()); System.out.println(errorOrderWithin24Hours); // [Order [id=1002, createdAt=2024-02-05T12:41:46.888007400, createdByUserId=103, status=ERROR, ]] }} Sorted (데이터의 정렬) 데이터가 순서대로 정렬된 Stream을 리턴 데이터의 종류에 따라 Comparator가 필요할 수 있음 12Stream&lt;T&gt; sorted;Stream&lt;T&gt; sorted(Comparator&lt;? super T&gt; comparator); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class Example { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(3, -5, 7, 4); List&lt;Integer&gt; sortedNumbers = numbers.stream() .sorted() .collect(Collectors.toList()); System.out.println(sortedNumbers); // [-5, 3, 4, 7] }}public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Chalie&quot;) .setVerified(true) .setEmailAddress(&quot;charlie@naver.com&quot;); User user2 = new User() .setId(102) .setName(&quot;Alice&quot;) .setVerified(false) .setEmailAddress(&quot;alice@naver.com&quot;); User user3 = new User() .setId(103) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@naver.com&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); List&lt;User&gt; sortedUsers = users.stream() .sorted((u1, u2) -&gt; u1.getName().compareTo(u2.getName())) .collect(Collectors.toList()); System.out.println(sortedUsers); // [User [id=102, name=Alice, emailAddress=alice@naver.com, isVerified=false, ], User [id=103, name=Bob, emailAddress=bob@naver.com, isVerified=false, ], User [id=101, name=Chalie, emailAddress=charlie@naver.com, isVerified=true, ]] LocalDateTime now = LocalDateTime.now(ZoneId.of(&quot;Asia/Seoul&quot;)); Order order1 = new Order() .setId(1001) .setStatus(OrderStatus.CREATED) .setCreatedByUserId(101) .setCreatedAt(now.minusHours(4)); Order order2 = new Order() .setId(1002) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(103) .setCreatedAt(now.minusHours(1)); Order order3 = new Order() .setId(1003) .setStatus(OrderStatus.PROCESSED) .setCreatedByUserId(102) .setCreatedAt(now.minusHours(36)); Order order4 = new Order() .setId(1004) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(104) .setCreatedAt(now.minusHours(40)); Order order5 = new Order() .setId(1005) .setStatus(OrderStatus.IN_PROGRESS) .setCreatedByUserId(101) .setCreatedAt(now.minusHours(10)); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4, order5); List&lt;Order&gt; sortedByCreatedAtOrder = orders.stream() .sorted((o1, o2) -&gt; o1.getCreatedAt().compareTo(o2.getCreatedAt())) .collect(Collectors.toList()); System.out.println(sortedByCreatedAtOrder); // [Order [id=1004, createdAt=2024-02-03T22:08:09.948945100, createdByUserId=104, status=ERROR, ], // Order [id=1003, createdAt=2024-02-04T02:08:09.948945100, createdByUserId=102, status=PROCESSED, ], // Order [id=1005, createdAt=2024-02-05T04:08:09.948945100, createdByUserId=101, status=IN_PROGRESS, ], // Order [id=1001, createdAt=2024-02-05T10:08:09.948945100, createdByUserId=101, status=CREATED, ], // Order [id=1002, createdAt=2024-02-05T13:08:09.948945100, createdByUserId=103, status=ERROR, ]] } Distinct (데이터의 중복) 중복되는 데이터가 제거된 Stream을 리턴 1Stream&lt;T&gt; distinct(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Example { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(3, -5, 4, -5, 2, 3); List&lt;Integer&gt; distinctNumbers = numbers.stream() .distinct() .collect(Collectors.toList()); System.out.println(distinctNumbers); LocalDateTime now = LocalDateTime.now(ZoneId.of(&quot;Asia/Seoul&quot;)); Order order1 = new Order() .setId(1001) .setStatus(OrderStatus.CREATED) .setCreatedByUserId(101) .setCreatedAt(now.minusHours(4)); Order order2 = new Order() .setId(1002) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(103) .setCreatedAt(now.minusHours(1)); Order order3 = new Order() .setId(1003) .setStatus(OrderStatus.PROCESSED) .setCreatedByUserId(102) .setCreatedAt(now.minusHours(36)); Order order4 = new Order() .setId(1004) .setStatus(OrderStatus.ERROR) .setCreatedByUserId(104) .setCreatedAt(now.minusHours(40)); Order order5 = new Order() .setId(1005) .setStatus(OrderStatus.IN_PROGRESS) .setCreatedByUserId(101) .setCreatedAt(now.minusHours(10)); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4, order5); List&lt;Long&gt; sortedByCreatedAtOrder = orders.stream() .map(Order::getCreatedByUserId) .distinct() .sorted() .collect(Collectors.toList()); System.out.println(sortedByCreatedAtOrder); // [101, 102, 103, 104] }} FlatMap Map + Flatten 데이터에 함수를 적용한 후 중첩된 stream을 연결하여 하나의 stream으로 리턴 1234&lt;R&gt; Stream&lt;R&gt; flatMap ( Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper);} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import java.math.BigDecimal;import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;import java.util.stream.Stream;import my.stduy.elasticsearch_test.test.OrderLine.OrderLineType;public class Example { public static void main(String[] args) { String[][] cities = new String[][] { { &quot;Seoul&quot;, &quot;Busan&quot; }, { &quot;San Francisco&quot;, &quot;New York&quot; }, { &quot;Madrid&quot;, &quot;Barcelona&quot; } }; Stream&lt;String[]&gt; cityStream = Arrays.stream(cities); Stream&lt;Stream&lt;String&gt;&gt; cityStreamStream = cityStream.map(x -&gt; Arrays.stream(x)); List&lt;Stream&lt;String&gt;&gt; cityStreamList = cityStreamStream.collect(Collectors.toList()); System.out.println(cityStreamList); // [Seoul, Busan, San Francisco, New York, Madrid, Barcelona] Stream&lt;String[]&gt; cityStream2 = Arrays.stream(cities); Stream&lt;String&gt; flattenedCityStream = cityStream2.flatMap(x -&gt; Arrays.stream(x)); List&lt;String&gt; flattenedCityList = flattenedCityStream.collect(Collectors.toList()); System.out.println(flattenedCityList); // [Seoul, Busan, San Francisco, New York, Madrid, Barcelona] Order order1 = new Order() .setId(1001) .setOrderLines(Arrays.asList( new OrderLine() .setId(10001) .setType(OrderLineType.PURCHASE) .setAmount(BigDecimal.valueOf(5000)), new OrderLine() .setId(10002) .setType(OrderLineType.PURCHASE) .setAmount(BigDecimal.valueOf(4000)) )); Order order2 = new Order() .setId(1002) .setOrderLines(Arrays.asList( new OrderLine() .setId(10003) .setType(OrderLineType.PURCHASE) .setAmount(BigDecimal.valueOf(2000)), new OrderLine() .setId(10004) .setType(OrderLineType.DISCOUNT) .setAmount(BigDecimal.valueOf(-1000)) )); Order order3 = new Order() .setId(1003) .setOrderLines(Arrays.asList( new OrderLine() .setId(10005) .setType(OrderLineType.PURCHASE) .setAmount(BigDecimal.valueOf(2000)) )); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3); List&lt;OrderLine&gt; mergedOrderLines = orders.stream() // Stream&lt;Order&gt; .map(Order::getOrderLines) // Stream&lt;List&lt;OrderLine&gt;&gt; .flatMap(List::stream) // Stream&lt;OrderLine&gt; .collect(Collectors.toList()); System.out.println(mergedOrderLines); // [OrderLine [id=10001, type=PURCHASE, productId=0, quantity=0, amount=5000], // OrderLine [id=10002, type=PURCHASE, productId=0, quantity=0, amount=4000], // OrderLine [id=10003, type=PURCHASE, productId=0, quantity=0, amount=2000], // OrderLine [id=10004, type=DISCOUNT, productId=0, quantity=0, amount=-1000], // OrderLine [id=10005, type=PURCHASE, productId=0, quantity=0, amount=2000]] }}","link":"/5-Stream/"},{"title":"7, Stream (1)","text":"종결 처리를 통해 최종 결과물을 도출 종결 처리의 실행이 필요할 때 중간 처리들도 비로소 실행(Lazy Evaluation) 1. Max / Min / Count : Stream 안의 데이터의 최대값 / 최소값 / 개수123Optional&lt;T&gt; max(Comparator&lt;? super T&gt; comparator); Optional&lt;T&gt; min(Comparator&lt;? super T&gt; comparator);long count(); max – Stream 안의 데이터 중 최대값을 반환. Stream이 비어있다면 빈 Optional을 반환 min – Stream 안의 데이터 중 최소값을 반환. Stream이 비어있다면 빈 Optional을 반환 count – Stream 안의 데이터의 개수를 반환 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class Example { public static void main(String[] args) { Optional&lt;Integer&gt; max = Stream.of(5, 3, 6, 2, 1) .max(Integer::compareTo); System.out.println(max.get()); // 6 max = Stream.of(5, 3, 6, 2, 1) .max((x,y) -&gt; x-y); System.out.println(max.get()); // 6 User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@fastcampus.co.kr&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(false) .setEmailAddress(&quot;charlie@fastcampus.co.kr&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); User firstUser = users.stream() .min((u1, u2) -&gt; u1.getName().compareTo(u2.getName())) .get(); System.out.println(firstUser); // User [id=101, name=Alice, emailAddress=alice@fastcampus.co.kr, isVerified=true, ] long positiveIntegerCount = Stream.of(1, -4, 5, -3, 6) .filter(x -&gt; x &gt; 0) .count(); System.out.println(&quot;Positive integers: &quot; + positiveIntegerCount); // 3 LocalDateTime now = LocalDateTime.now(ZoneId.of(&quot;Asia/Seoul&quot;)); user1.setCreatedAt(now.minusDays(2)); user2.setCreatedAt(now.minusHours(10)); user3.setCreatedAt(now.minusHours(1)); User user4 = new User() .setId(104) .setName(&quot;David&quot;) .setVerified(true) .setEmailAddress(&quot;david@fastcampus.co.kr&quot;) .setCreatedAt(now.minusHours(27)); users = Arrays.asList(user1, user2, user3, user4); long unverfiedUsersIn24Hrs = users.stream() .filter(user -&gt; user.getCreatedAt().isAfter(now.minusDays(1))) .filter(user -&gt; !user.isVerified()) .count(); System.out.println(unverfiedUsersIn24Hrs); Order order1 = new Order() .setId(1001L) .setAmount(BigDecimal.valueOf(2000)) .setStatus(OrderStatus.CREATED); Order order2 = new Order() .setId(1002L) .setAmount(BigDecimal.valueOf(4000)) .setStatus(OrderStatus.ERROR); Order order3 = new Order() .setId(1003L) .setAmount(BigDecimal.valueOf(3000)) .setStatus(OrderStatus.ERROR); Order order4 = new Order() .setId(1004L) .setAmount(BigDecimal.valueOf(7000)) .setStatus(OrderStatus.PROCESSED); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4); // TODO: find order with highest amount an in ERROR status Order highestValueOrder = orders.stream() .filter(order -&gt; order.getStatus() == OrderStatus.ERROR) .max((o1, o2) -&gt; o1.getAmount().compareTo(o2.getAmount())) .get(); System.out.println(highestValueOrder); // Order [id=1002, createdByUserId=0, status=ERROR, amount=4000, ] BigDecimal maxErroredAmount = orders.stream() .filter(order -&gt; order.getStatus() == OrderStatus.ERROR) .map(Order::getAmount) .max(BigDecimal::compareTo) .orElse(BigDecimal.ZERO); System.out.println(maxErroredAmount); // 4000 }} 2. All Match / Any Match12boolean allMatch(Predicate&lt;? super T&gt; predicate);boolean anyMatch(Predicate&lt;? super T&gt; predicate); allMatch – Stream 안의 모든 데이터가 predicate을 만족하면 true anyMatch – Stream 안의 데이터 중 하나라도 predicate을 만족하면 true 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import java.util.Arrays;import java.util.List;public class Example { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(3, -4, 2, 7, 9); boolean allPostive = numbers.stream() .allMatch(number -&gt; number &gt; 0); System.out.println(&quot;Are all numbers positive: &quot; + allPostive); // Are all numbers positive: false boolean anyNegative = numbers.stream() .anyMatch(number -&gt; number &lt; 0); System.out.println(&quot;Is any number negative: &quot; + anyNegative); // Is any number negative: true }}import java.util.Arrays;import java.util.List;import util.User;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@fastcampus.co.kr&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(false) .setEmailAddress(&quot;charlie@fastcampus.co.kr&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); boolean areAllUserVerified = users.stream() .allMatch(User::isVerified); System.out.println(areAllUserVerified); // false }}import java.math.BigDecimal;import java.util.Arrays;import java.util.List;import util.Order;import util.Order.OrderStatus;public class Example { public static void main(String[] args) { Order order1 = new Order() .setId(1001L) .setAmount(BigDecimal.valueOf(2000)) .setStatus(OrderStatus.CREATED); Order order2 = new Order() .setId(1002L) .setAmount(BigDecimal.valueOf(4000)) .setStatus(OrderStatus.ERROR); Order order3 = new Order() .setId(1003L) .setAmount(BigDecimal.valueOf(3000)) .setStatus(OrderStatus.ERROR); Order order4 = new Order() .setId(1004L) .setAmount(BigDecimal.valueOf(7000)) .setStatus(OrderStatus.PROCESSED); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4); // Check if any of orders is in ERROR status boolean checkErrorStatusOrder = orders.stream() .anyMatch(order -&gt; order.getStatus() == OrderStatus.ERROR); System.out.println(checkErrorStatusOrder); // true }} 3. Find First / Find Any12Optional&lt;T&gt; findFirst();Optional&lt;T&gt; findAny(); findFirst – Stream 안의 첫번째 데이터를 반환. Stream이 비어있다면 비어있는 Optional을 반환 findAny – Stream 안의 아무 데이터나 리턴. 순서가 중요하지 않고 Parallel Stream을 사용할 때 최적화를 할 수 있다. 마찬가지로 Stream이 비어있다면 빈 Optional을 반환 1234567891011121314151617import java.util.Optional;import java.util.stream.Stream;public class Example { public static void main(String[] args) { Optional&lt;Integer&gt; anyNegativeInteger = Stream.of(3, 2, -9, -5, -7, 6) .filter(x -&gt; x &lt; 0) .findAny(); System.out.println(anyNegativeInteger.get()); // -9 Optional&lt;Integer&gt; firstPositiveInteger = Stream.of(-3, -2, -5, 6, 1) .filter(x -&gt; x &gt; 0) .findFirst(); System.out.println(firstPositiveInteger.get()); // 6 }} 4. Reduce 주어진 함수를 반복 적용해 Stream 안의 데이터를 하나의 값으로 합치는 작업 Max/Min/Count 도 reduce의 일종 12345Optional&lt;T&gt; reduce(BinaryOperator&lt;T&gt; accumulator);T reduce(T identity, BinaryOperator&lt;T&gt; accumulator);&lt;U&gt; U reduce(U identity, BiFunction&lt;U, ? super T, U&gt; accumulator, BinaryOperator&lt;U&gt; combiner); reduce 1 – 주어진 accumulator를 이용해 데이터를 합침. Stream이 비어있을 경우 빈 Optional을 반환 reduce 2 – 주어진 초기값과 accumulator를 이용. 초기값이 있기 때문에 항상 반환값이 존재 reduce 3 – 합치는 과정에서 타입이 바뀔 경우 사용 Map + reduce로 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import java.util.Arrays;import java.util.List;public class Example { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(2, 4, -2, -5, 3, 8); int sum = numbers.stream() .reduce((x, y) -&gt; x + y) .get(); System.out.println(sum); // 10 int min = numbers.stream() .reduce((x, y) -&gt; x &gt; y ? x : y) .get(); System.out.println(min); // 8 int product = numbers.stream() .reduce(1, (x, y) -&gt; x * y); System.out.println(product); // 1920 List&lt;String&gt; numberStrList = Arrays.asList(&quot;3&quot;, &quot;2&quot;, &quot;5&quot;, &quot;-4&quot;); int sumOfNumberStrList = numberStrList.stream() .map(Integer::parseInt) .reduce(0, (x, y) -&gt; x + y); System.out.println(sumOfNumberStrList); // 6 int sumOfNumberStrList2 = numberStrList.stream() .reduce(0, (number, str) -&gt; number + Integer.parseInt(str), (num1, num2) -&gt; num1 + num2); System.out.println(sumOfNumberStrList2); // 6 }}import java.util.Arrays;import java.util.List;import util.User;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setFriendUserIds(Arrays.asList(201, 202, 203, 204)); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setFriendUserIds(Arrays.asList(204, 205, 206)); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setFriendUserIds(Arrays.asList(204, 205, 207)); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); int sumOfNumberOfFriends = users.stream() .map(User::getFriendUserIds) .map(List::size) .reduce(0, (x, y) -&gt; x + y); System.out.println(sumOfNumberOfFriends); // 10 }}import java.math.BigDecimal;import java.util.Arrays;import java.util.List;import util.Order;import util.OrderLine;public class Example { public static void main(String[] args) { Order order1 = new Order() .setId(1001L) .setOrderLines(Arrays.asList( new OrderLine().setAmount(BigDecimal.valueOf(1000)), new OrderLine().setAmount(BigDecimal.valueOf(2000)))); Order order2 = new Order() .setId(1002L) .setOrderLines(Arrays.asList( new OrderLine().setAmount(BigDecimal.valueOf(2000)), new OrderLine() .setAmount(BigDecimal.valueOf(3000)))); Order order3 = new Order() .setId(1002L) .setOrderLines(Arrays.asList( new OrderLine().setAmount(BigDecimal.valueOf(1000)), new OrderLine().setAmount(BigDecimal.valueOf(2000)))); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3); // TODO: find the sum of amounts BigDecimal sumAmounts = orders.stream() .map(Order::getOrderLines) // Stream&lt;List&lt;OrderLine&gt;&gt; .flatMap(List::stream) // Stream&lt;OrderLine&gt; .map(OrderLine::getAmount) .reduce(BigDecimal.ZERO, BigDecimal::add); System.out.println(sumAmounts); // 11000 }} 5. Collectors, collect12345&lt;R, A&gt; R collect(Collector&lt;? super T, A, R&gt; collector); java.util.stream.CollectorsCollector&lt;T, ?, List&lt;T&gt;&gt; toList();Collector&lt;T, ?, Set&lt;T&gt;&gt; toSet(); collect – 주어진 collector를 이용해 Stream안의 데이터를 합침 일반적으로 특정 data structure로 데이터를 모을 때 사용 Collectors – 자주 쓰일법한 유용한 collector들을 모아놓은 util class java.util.stream 패키지에서 제공. 12345public static &lt;T, U, A, R&gt; Collector&lt;T, ?, R&gt; mapping( Function&lt;? super T, ? extends U&gt; mapper, Collector&lt;? super U, A, R&gt; downstream)public static &lt;T&gt; Collector&lt;T, ?, T&gt; reducing( T identity, BinaryOperator&lt;T&gt; op) mapping – Map과 collect를 합쳐놓은 역할을 해주는 collector. 일반적으로는 map을 한 후 collect를 해도 되지만 groupingBy 등 필요할 때가 있다. reducing – reduce를 해주는collector. 이외에도 filtering, flatMapping, counting, minBy, maxBy 등도 있다. 1234567891011121314151617181920212223242526272829import java.util.List;import java.util.Set;import java.util.stream.Collectors;import java.util.stream.Stream;public class Example { public static void main(String[] args) { List&lt;Integer&gt; numberList = Stream.of(3, 5, -3, 3, 4, 5) .collect(Collectors.toList()); System.out.println(numberList); // [3, 5, -3, 3, 4, 5] Set&lt;Integer&gt; numberSet = Stream.of(3, 5, -3, 3, 4, 5) .collect(Collectors.toSet()); System.out.println(numberSet); // [-3, 3, 4, 5] List&lt;Integer&gt; numberList2 = Stream.of(3, 5, -3, 3, 4, 5) .collect(Collectors.mapping(x -&gt; Math.abs(x), Collectors.toList())); System.out.println(numberList2); // [3, 5, 3, 3, 4, 5] Set&lt;Integer&gt; numberSet2 = Stream.of(3, 5, -3, 3, 4, 5) .collect(Collectors.mapping(x -&gt; Math.abs(x), Collectors.toSet())); System.out.println(numberSet2); // [3, 4, 5] int sum = Stream.of(3, 5, -3, 3, 4, 5) .collect(Collectors.reducing(0, (x, y) -&gt; x + y)); System.out.println(sum); // 17 }} 6. To Map123public static &lt;T, K, U&gt; Collector&lt;T, ?, Map&lt;K,U&gt;&gt; toMap( Function&lt;? super T, ? extends K&gt; keyMapper, Function&lt;? super T, ? extends U&gt; valueMapper) Stream 안의 데이터를 map의 형태로 반환해주는 collector keyMapper – 데이터를 map의 key로 변환하는 Function valueMapper – 데이터를 map의 value로 변환하는 Function 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import java.util.Map;import java.util.function.Function;import java.util.stream.Collectors;import java.util.stream.Stream;public class Example { public static void main(String[] args) { Map&lt;Integer, String&gt; numberMap = Stream.of(3, 5, -4, 2, 6) .collect(Collectors.toMap(Function.identity(), x -&gt; &quot;Number is &quot; + x)); System.out.println(numberMap.get(3)); // Number is 3 }}import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.function.Function;import java.util.stream.Collectors;import util.User;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@fastcampus.co.kr&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(false) .setEmailAddress(&quot;charlie@fastcampus.co.kr&quot;); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); Map&lt;Integer, User&gt; userIdToUserMap = users.stream() .collect(Collectors.toMap(User::getId, Function.identity())); System.out.println(userIdToUserMap); // {101=User [id=101, name=Alice, emailAddress=alice@fastcampus.co.kr, isVerified=true, ], 102=User // [id=102, name=Bob, emailAddress=bob@fastcampus.co.kr, isVerified=false, ], 103=User // [id=103, name=Charlie, emailAddress=charlie@fastcampus.co.kr, isVerified=false, ]} System.out.println(userIdToUserMap.get(103)); //User [id=103, name=Charlie, emailAddress=charlie@fastcampus.co.kr, isVerified=false, ] }}import java.math.BigDecimal;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.stream.Collectors;import util.Order;import util.Order.OrderStatus;public class Example { public static void main(String[] args) { Order order1 = new Order() .setId(1001L) .setAmount(BigDecimal.valueOf(2000)) .setStatus(OrderStatus.CREATED); Order order2 = new Order() .setId(1002L) .setAmount(BigDecimal.valueOf(4000)) .setStatus(OrderStatus.ERROR); Order order3 = new Order() .setId(1003L) .setAmount(BigDecimal.valueOf(3000)) .setStatus(OrderStatus.ERROR); Order order4 = new Order() .setId(1004L) .setAmount(BigDecimal.valueOf(7000)) .setStatus(OrderStatus.PROCESSED); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4); // TODO: Create a map from order id to order status Map&lt;Long, OrderStatus&gt; orderIdStatusMap = orders.stream() .collect(Collectors.toMap(Order::getId, Order::getStatus)); System.out.println(orderIdStatusMap.get(1002L)); // ERROR }} 7. Grouping By1public static &lt;T, K&gt; Collector&lt;T, ?, Map&lt;K, List&lt;T&gt;&gt;&gt; groupingBy(Function&lt;? super T, ? extends K&gt; classifier) Stream 안의 데이터에 classifier를 적용했을 때 결과값이 같은 값끼리 List로 모아서 Map의 형태로 반환해주는 collector 이 때 key는 classifier의 결과값, value는 그 결과값을 갖는 데이터들 예를 들어 stream에 {1, 2, 5, 7, 9, 12, 13}이 있을 때 classifier가 x → x%3 이라면 • 반환되는 map은 {0 = [9, 12], 1 = [1, 7, 13], 2 = [2, 5]} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.Set;import java.util.stream.Collectors;public class Example { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(13, 2, 101, 203, 304, 402, 305, 349, 2312, 203); Map&lt;Integer, List&lt;Integer&gt;&gt; unitDigitMap = numbers.stream() .collect(Collectors.groupingBy(number -&gt; number % 10)); System.out.println(unitDigitMap); // {1=[101], 2=[2, 402, 2312], 3=[13, 203, 203], 4=[304], 5=[305], 9=[349]} Map&lt;Integer, Set&lt;Integer&gt;&gt; unitDigitSet = numbers.stream() .collect(Collectors.groupingBy(number -&gt; number % 10, Collectors.toSet())); System.out.println(unitDigitSet); // {1=[101], 2=[2, 402, 2312], 3=[203, 13], 4=[304], 5=[305], 9=[349]} Map&lt;Integer, List&lt;String&gt;&gt; unitDigitStrMap = numbers.stream() .collect(Collectors.groupingBy(number -&gt; number % 10, Collectors.mapping(number -&gt; &quot;unit digit is &quot; + number, Collectors.toList()))); System.out.println(unitDigitStrMap.get(3)); // [unit digit is 13, unit digit is 203, unit digit is 203] }}import java.math.BigDecimal;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.stream.Collectors;import util.Order;import util.Order.OrderStatus;public class Example { public static void main(String[] args) { Order order1 = new Order() .setId(1001L) .setAmount(BigDecimal.valueOf(2000)) .setStatus(OrderStatus.CREATED); Order order2 = new Order() .setId(1002L) .setAmount(BigDecimal.valueOf(4000)) .setStatus(OrderStatus.ERROR); Order order3 = new Order() .setId(1003L) .setAmount(BigDecimal.valueOf(3000)) .setStatus(OrderStatus.ERROR); Order order4 = new Order() .setId(1004L) .setAmount(BigDecimal.valueOf(7000)) .setStatus(OrderStatus.PROCESSED); List&lt;Order&gt; orders = Arrays.asList(order1, order2, order3, order4); // TODO: create a map form order status to the list of corresponding orders Map&lt;OrderStatus, List&lt;Order&gt;&gt; orderStatusMap = orders.stream() .collect(Collectors.groupingBy(Order::getStatus)); System.out.println(orderStatusMap); // {CREATED=[Order [id=1001, createdByUserId=0, status=CREATED, amount=2000, ]], // PROCESSED=[Order [id=1004, createdByUserId=0, status=PROCESSED, amount=7000, ]], // ERROR=[Order [id=1002, createdByUserId=0, status=ERROR, amount=4000, ], Order [id=1003, createdByUserId=0, status=ERROR, amount=3000, ]]} Map&lt;OrderStatus, BigDecimal&gt; orderStatusToSumOfAmountMap = orders.stream() .collect(Collectors.groupingBy(Order::getStatus, Collectors.mapping(Order::getAmount, Collectors.reducing(BigDecimal.ZERO, BigDecimal::add)))); System.out.println(orderStatusToSumOfAmountMap); // {ERROR=7000, CREATED=2000, PROCESSED=7000} }}public static &lt;T, K, A, D&gt; Collector&lt;T, ?, Map&lt;K, D&gt;&gt; groupingBy( Function&lt;? super T, ? extends K&gt; classifier, Collector&lt;? super T, A, D&gt; downstream) 두 번째 매개변수로 downstream collector를 넘기는 것도 가능 그 경우 List 대신 collector를 적용시킨 값으로 map의 value가 만들어짐 이 때 자주 쓰이는 것이 mapping / reducing 등의 collector 8. Partitioning By12public static &lt;T&gt; Collector&lt;T, ?, Map&lt;Boolean, List&lt;T&gt;&gt;&gt; partitioningBy(Predicate&lt;? super T&gt; predicate)public static &lt;T, D, A&gt; Collector&lt;T, ?, Map&lt;Boolean, D&gt;&gt; partitioningBy(Predicate&lt;? super T&gt; predicate, Collector&lt;? super T, A, D&gt; downstream) GroupingBy와 유사하지만 Function 대신 Predicate을 받아 true와 false 두 key가 존재하는 map을 반환하는 collector 마찬가지로 downstream collector를 넘겨 List 이외의 형태로 map의 value를 만드는 것 역시 가능 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.stream.Collectors;import util.EmailService;import util.User;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;) .setFriendUserIds(Arrays.asList(201, 202, 203, 204, 211, 212, 213, 214)); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setEmailAddress(&quot;bob@fastcampus.co.kr&quot;) .setFriendUserIds(Arrays.asList(204, 205, 206)); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setEmailAddress(&quot;charlie@fastcampus.co.kr&quot;) .setFriendUserIds(Arrays.asList(204, 205, 207, 218)); List&lt;User&gt; users = Arrays.asList(user1, user2, user3); Map&lt;Boolean, List&lt;User&gt;&gt; userPartitions = users.stream() .collect(Collectors.partitioningBy(user -&gt; user.getFriendUserIds().size() &gt; 5)); EmailService emailService = new EmailService(); for (User user : userPartitions.get(true)) { emailService.sendPlayWithFriendsEmail(user); // Sending 'Play With Friends' email to alice@fastcampus.co.kr } for (User user : userPartitions.get(false)) { emailService.sendMakeMoreFriendsEmail(user); // Sending 'Make More Friends' email to bob@fastcampus.co.kr // Sending 'Make More Friends' email to charlie@fastcampus.co.kr } }} 9. For Each1void forEach(Consumer&lt;? super T&gt; action); 제공된 action을 Stream의 각 데이터에 적용해주는 종결 처리 메서드 Java의 iterable 인터페이스에도 forEach가 있기 때문에 Stream의 중간 처리가 필요없다면 iterable collection(Set, List 등)에서 바로 쓰는 것도 가능 12345678910111213141516171819import java.util.Arrays;import java.util.List;public class Example { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(3, 5, 2, 1); numbers.stream().forEach(number -&gt; System.out.println(&quot;The number is &quot; + number)); // The number is 3 // The number is 5 // The number is 2 // The number is 1 numbers.forEach(number -&gt; System.out.println(&quot;The number is &quot; + number)); // The number is 3 // The number is 5 // The number is 2 // The number is 1 }} 10. Parallel Stream : Stream을 병렬로123List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3);Stream&lt;Integer&gt; parallelStream = numbers.parallelStream();Stream&lt;Integer&gt; parallelStream2 = numbers.stream().parallel(); Sequential vs. Parallel 여러개의 스레드를 이용하여 stream의 처리 과정을 병렬화 (parallelize) 중간과정은 병렬처리되지만 순서가있는 Stream의 경우 종결처리했을 때의 결과물이 기존의 순차적 처리와 일치하도록 종결 처리과정에서 조정됨. 즉 List로 collect한다면 순서가 항상 올바르게 나온다는 것. 장점: • 굉장히간단하게병렬처리를사용할수있게해준다 • 속도가 비약적으로 빨라질 수 있다 단점: 항상 속도가 빨라지는 것은 아니다 공통으로 사용하는 리소스가 있을 경우 잘못된 결과가 나오거나 아예 오류가 날 수도 있다 (deadlock) 이를 막기 위해 mutex, semaphore등 병렬 처리 기술을 이용하면 순차 처리보다 느려질 수도 있다 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;import util.EmailService;import util.User;public class Example { public static void main(String[] args) { User user1 = new User() .setId(101) .setName(&quot;Alice&quot;) .setVerified(true) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;); User user2 = new User() .setId(102) .setName(&quot;Bob&quot;) .setVerified(false) .setEmailAddress(&quot;bob@fastcampus.co.kr&quot;); User user3 = new User() .setId(103) .setName(&quot;Charlie&quot;) .setVerified(false) .setEmailAddress(&quot;charlie@fastcampus.co.kr&quot;); User user4 = new User() .setId(104) .setName(&quot;David&quot;) .setEmailAddress(&quot;david@fastcampus.co.kr&quot;) .setVerified(true); User user5 = new User() .setId(105) .setName(&quot;Eve&quot;) .setEmailAddress(&quot;eve@fastcampus.co.kr&quot;) .setVerified(false); User user6 = new User() .setId(106) .setName(&quot;Frank&quot;) .setEmailAddress(&quot;frank@fastcampus.co.kr&quot;) .setVerified(false); List&lt;User&gt; users = Arrays.asList(user1, user2, user3, user4, user5, user6); long startTime = System.currentTimeMillis(); EmailService emailService = new EmailService(); users.stream() .filter(user -&gt; !user.isVerified()) .forEach(emailService::sendVerifyYourEmailEmail); long endTime = System.currentTimeMillis(); System.out.println(&quot;Sequential: &quot; + (endTime - startTime) + &quot;ms&quot;); // Sending 'Verify Your Email' email to bob@fastcampus.co.kr // Sending 'Verify Your Email' email to charlie@fastcampus.co.kr // Sending 'Verify Your Email' email to eve@fastcampus.co.kr // Sending 'Verify Your Email' email to frank@fastcampus.co.kr // Sequential: 12ms startTime = System.currentTimeMillis(); users.stream().parallel() .filter(user -&gt; !user.isVerified()) .forEach(emailService::sendVerifyYourEmailEmail); endTime = System.currentTimeMillis(); System.out.println(&quot;Parallel: &quot; + (endTime - startTime) + &quot;ms&quot;); // Sending 'Verify Your Email' email to eve@fastcampus.co.kr // Sending 'Verify Your Email' email to charlie@fastcampus.co.kr // Sending 'Verify Your Email' email to bob@fastcampus.co.kr // Sending 'Verify Your Email' email to frank@fastcampus.co.kr // Parallel: 5ms List&lt;User&gt; processedUsers = users.parallelStream() .map(user -&gt; { user.setName(user.getName().toUpperCase()); System.out.println(&quot;Capitalize user name &quot; + user.getName() + &quot; for user &quot; + user.getId()); return user; }) .map(user -&gt; { System.out.println(&quot;Set 'isVerified' to true for user &quot; + user.getName() + &quot; &quot; + user.getId()); user.setVerified(true); return user; }) .collect(Collectors.toList()); System.out.println(processedUsers); // Capitalize user name EVE for user 105 // Capitalize user name DAVID for user 104 // Capitalize user name CHARLIE for user 103 // Capitalize user name ALICE for user 101 // Capitalize user name BOB for user 102 // Capitalize user name FRANK for user 106 // Set 'isVerified' to true for user DAVID 104 // Set 'isVerified' to true for user BOB 102 // Set 'isVerified' to true for user EVE 105 // Set 'isVerified' to true for user ALICE 101 // Set 'isVerified' to true for user FRANK 106 // Set 'isVerified' to true for user CHARLIE 103 // [User [id=101, name=ALICE, emailAddress=alice@fastcampus.co.kr, isVerified=true, ], // User [id=102, name=BOB, emailAddress=bob@fastcampus.co.kr, isVerified=true, ], // User [id=103, name=CHARLIE, emailAddress=charlie@fastcampus.co.kr, isVerified=true, ], // User [id=104, name=DAVID, emailAddress=david@fastcampus.co.kr, isVerified=true, ], // User [id=105, name=EVE, emailAddress=eve@fastcampus.co.kr, isVerified=true, ], // User [id=106, name=FRANK, emailAddress=frank@fastcampus.co.kr, isVerified=true, ]] }} 결론 Stream의 다양한 종결 처리들 max / min / count / allMatch / anyMatch / findFirst / findAny / reduce / forEach Collector를 이용한 종결 처리들 toList / toSet / mapping / reducing / toMap / groupingBy / partitioningBy Parallel Stream의 장점과 단점","link":"/7-Stream-1/"},{"title":"6. Optional","text":"NPE(NullPointerException, 함정카드 같은 에러) Null 상태인 오브젝트를 레퍼런스할 때 발생 Runtime error 이기 때문에 실행 전까지는 발생 여부를 알기 쉽지 않음 “[Null Pointer 를 발명한 것은] 나의 10억 불 짜리 실수였다.” (I Call it my billion-dollar mistake -토니 호어(Tony Hoare, 2009)) Optional (있을 수도 있고 없을 수도 있다.) Null 일 수도, 아닐 수도 있는 오브젝트를 담은 상자 1234java.util.Optional&lt;T&gt;Optional&lt;String&gt; maybeString = Optional.of(&quot;Hello world&quot;);String string = maybeString.get(); Optional 만드는 법123public static &lt;T&gt; Optional&lt;T&gt; of(T value)public static &lt;T&gt; Optional&lt;T&gt; empty()public static &lt;T&gt; Optional&lt;T&gt; ofNullable(T value) of : Null 이 아닌 오브젝트를 이용해 Optional을 만들 때 Empty : 빈 Optional을 만들 때 ofNullable : Null인지 아닌지 알지 못하는 오브젝트로 Optional을 만들 때 값이 있는 경우 123String nullValue = null;Optional&lt;String&gt; emptyOptional = Optional.ofNullable(nullValue);emptyOptional.ifPresent(System.out::println); 값이 없는 경우 123String value = &quot;Hello, Optional!&quot;;Optional&lt;String&gt; optionalWithValue = Optional.ofNullable(value);optionalWithValue.ifPresent(System.out::println); // Hello, Optional! Optional 안에 있는 값을 확인하고 꺼내는 법123456public boolean isPresent()public T get()public T orElse(T other)public T orElseGet(Supplier&lt;? extends T&gt; supplier)public &lt;X extends Throwable&gt; T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier) throws X isPresent - 안의 오브젝트가 null인지 아닌지 체크, Null이 아닐 시 true get - Optional 안의 앖을 추출, Null 이라면 에러 ofElse - Optional이 null이 아니라면 Optional 안의 값을, null이라면 other로 공급된 값을 리턴 orElseGet – Optional이 null이 아니라면 Optional 안의 값을, null이라면 supplier로 공급되는 값을 리턴 orElseThrow – Optional이 null이 아니라면 Optional 안의 값을, null이라면 exceptionSupplier로 공급되는 exception을 던짐 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.Optional;public class Example { public static boolean userEquals(User u1, User u2) { return u1.getId() == u2.getId() &amp;&amp; u1.getName().equals(u2.getName()) &amp;&amp; u1.getEmailAddress().equals(u2.getEmailAddress()) &amp;&amp; u1.isVerified() == u2.isVerified(); } public static void main(String[] args) { User user1 = new User() .setId(1001) .setName(&quot;Alice&quot;) .setVerified(false); User user2 = new User() .setId(1001) .setName(&quot;Alice&quot;) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;) .setVerified(false); System.out.println(&quot;Same? :&quot; + userEquals(user2, user1)); // Same? :false// System.out.println(&quot;Same? :&quot; + userEquals(user1, user2)); // Exception in thread &quot;main&quot; java.lang.NullPointerException String someEmail = &quot;some@email.com&quot;; String nullEmail = null; Optional&lt;String&gt; maybeEmail = Optional.of(someEmail); Optional&lt;String&gt; maybeEmail2 = Optional.empty(); Optional&lt;String&gt; maybeEmail3 = Optional.ofNullable(someEmail); Optional&lt;String&gt; maybeEmail4 = Optional.ofNullable(nullEmail); String email = maybeEmail.get(); System.out.println(email); // some@email.com if (maybeEmail2.isPresent()) { System.out.println(maybeEmail2.get()); } String defaultEmail = &quot;default@email.com&quot;; String email3 = maybeEmail2.orElse(defaultEmail); System.out.println(email3); // default@email.com String email4 = maybeEmail2.orElseGet(() -&gt; defaultEmail); System.out.println(email4); // default@email.com // String email5 = maybeEmail2.orElseThrow(() -&gt; new RuntimeException(&quot;email not present&quot;)); // System.out.println(email5); // Exception in thread &quot;main&quot; java.lang.RuntimeException: email not present System.out.println(maybeEmail3); // Optional[some@email.com] System.out.println(maybeEmail4); // Optional.empty; }} Optional 응용을 위해 알아야할 것들1234public void ifPresent(Consumer&lt;? super T&gt; action)public &lt;U&gt; Optional&lt;U&gt; map(Function&lt;? super T, ? extends U&gt; mapper)public &lt;U&gt; Optional&lt;U&gt; flatMap( Function&lt;? super T, ? extends Optional&lt;? extends U&gt;&gt; mapper) ifPresent – Optional이 null이 아니라면 action을 실행, null이면 아무일도 일어나지 않음 1234567891011public class Example { public static void main(String[] args) { // 값이 있는 경우의 ifPresent 사용 예 Optional&lt;String&gt; optionalString = Optional.of(&quot;Hello, World!&quot;); optionalString.ifPresent(System.out::println); // Hello, World! // 값이 null인 경우의 ifPresent 사용 예 Optional&lt;String&gt; emptyOptional = Optional.empty(); emptyOptional.ifPresent(System.out::println); // 아무것도 출려되지 않음, 반환값이 void 이기 대문 } map – Optional이 null이 아니라면 mapper를 적용, null이라면 단순히 비어있는 Optional 즉 아래 예제에서는 false를 반환 1234567891011121314public class Example { public static void main(String[] args) { // Optional에 값이 있을 때의 map 사용 예 Optional&lt;String&gt; optionalString = Optional.of(&quot;test&quot;); Optional&lt;Integer&gt; length = optionalString.map(String::length); System.out.println(length.orElse(0)); // 4 // Optional이 비어있을 때의 map 사용 예 Optional&lt;String&gt; emptyOptional = Optional.empty(); Optional&lt;Integer&gt; lengthIfEmpty = emptyOptional.map(String::length); System.out.println(lengthIfEmpty.isPresent()); // false }} flatMap – mapper의 리턴 값이 또 다른 Optional이라면 한 단계의 Optional이 되도록 납작하게 해줌 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Optional;import util.User;public class Example { public static User maybeGetUser(boolean returnUser) { if (returnUser) { return new User() .setId(1001) .setName(&quot;Alice&quot;) .setEmailAddress(&quot;alice@fastcampus.co.kr&quot;) .setVerified(false); } return null; } public static void main(String[] args) { Optional&lt;User&gt; maybeUser = Optional.ofNullable(maybeGetUser(false)); maybeUser.ifPresent(user -&gt; System.out.println(user)); // 아무것도 나오지 않음 Optional&lt;User&gt; maybeUser2 = Optional.ofNullable(maybeGetUser(true)); maybeUser2.ifPresent(user -&gt; System.out.println(user)); // User [id=1001, name=Alice, emailAddress=alice@fastcampus.co.kr, isVerified=false, ] Optional&lt;Integer&gt; maybeId = Optional.ofNullable(maybeGetUser(true)) .map(user -&gt; user.getId()); maybeId.ifPresent(System.out::println); // 아무것도 나오지 않음 Optional&lt;Integer&gt; maybeId2 = Optional.ofNullable(maybeGetUser(true)) .map(user -&gt; user.getId()); maybeId2.ifPresent(System.out::println); // 1001 String userName = Optional.ofNullable(maybeGetUser(false)) .map(User::getName) .map(name -&gt; &quot;The name is &quot; + name) .orElse(&quot;Name is empty&quot;); System.out.println(userName); // Name is empty Optional&lt;String&gt; maybeEmail = Optional.ofNullable(maybeGetUser(true)) .flatMap(User::getEmailAddress); maybeEmail.ifPresent(System.out::println); // alice@fastcampus.co.kr }} 결론 Optional은 NPE 를 막을 수 있는 강력한 도구로 null일 수도, 아닐 수도 있는 오브젝트를 담을 때 사용 Optional을 생성하고 추출하고 변환하는 operator들 of / ofNullable / empty : static method를 통해 생성 get / orElse / orElseGet / orElseThrow : Optional 에서 꺼내서 씀 isPresent : Optional이 비어있는지 아닌지를 알려줌 ifPresent map / flatMap : 옵셔널이 비어있는지 체크할 필요없이 우리가 할 여러 작업을 하게 끔 해줌","link":"/6-Optional/"},{"title":"8. 함수형 프로그래밍의 응용","text":"개요 Scope, Closure &amp; Curry Lazy Evaluation Function Composition 1. cope, Closure &amp; Curry1.1 Scope Scope(스코프 / 유효범위) - 변수에 접근할 수 있는 범위 함수 안에 함수가 있을 때 내부 함수에서 외부함수에 있는 변수에 접근이 가능하다(lexical scope). 그 반대는 불가능 하다 123456789public static Supplier&lt;String&gt; getStringSupplier() { String hello = &quot;Hello&quot;; Supplier&lt;String&gt; supplier = () -&gt; { String world = &quot;World&quot;; return hello + world; }; return supplier;} 1.2 Closure 내부 함수가 존재하는 한 내부 함수가 사용한 외부 함수의 변수들 역시 계속 존재 이렇게 lexical scope를 포함하는 함수를 closure 라 함 내부 함수가 사용한 외부 함수의 변수들은 내부 함수 선언 당시로부터 변할 수 없기 때문에 final로 선언되지 않더라도 암묵적으로 final 로 취급됨 123456789public static Supplier&lt;String&gt; getStringSupplier() { String hello = &quot;Hello&quot;; Supplier&lt;String&gt; supplier = () -&gt; { String world = &quot;World&quot;; return hello + world; }; return supplier;} 1.3. Curry 여러개의 매개변수를 받는 함수를 중첩된 여러개의 함수로 쪼개어 매개변수를 한번에 받지 않고 여러 단계에 걸쳐 나눠받을 수 있게 하는 기술 123BiFunction&lt;Integer, Integer, Integer&gt; add = (x, y) -&gt; x + y;=&gt;Function&lt;Integer, Function&lt;Integer, Integer&gt;&gt; add = x -&gt; y -&gt; x + y; 1234567891011121314151617181920212223242526272829import java.util.function.BiFunction;import java.util.function.Function;import java.util.function.Supplier;public class Example { public static void main(String[] args) { Supplier&lt;String&gt; supplier = getStringSupplier(); System.out.println(supplier.get()); // Hello World BiFunction&lt;Integer, Integer, Integer&gt; add = (x, y) -&gt; x + y; Function&lt;Integer, Function&lt;Integer, Integer&gt;&gt; curriedAdd = x -&gt; y -&gt; x + y; Function&lt;Integer, Integer&gt; addThree = curriedAdd.apply(3); int result = addThree.apply(10); System.out.println(result); // 13 } public static Supplier&lt;String&gt; getStringSupplier() { String hello = &quot;Hello&quot;; Supplier&lt;String&gt; supplier = () -&gt; { String world = &quot;World&quot;; return hello + &quot; &quot; + world; }; return supplier; }} 2. Lazy Evaluation Lambda 의 계산은 그 결과값이 필요할 때가 되어서야 계산됨 이를 이용하여 불필요한 계산을 줄이거나 해당 코드의 실행 순서를 의도적으로 미룰 수 있음 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123public class Example { public static void main(String[] args) { if (true || returnFalse()) { System.out.println(&quot;true&quot;); } } public static boolean returnTrue() { System.out.println(&quot;Returning true&quot;); return true; } public static boolean returnFalse() { System.out.println(&quot;Returning false&quot;); return false; }}// falsepublic class Example { public static void main(String[] args) { if (returnTrue() || returnFalse()) { System.out.println(&quot;true&quot;); } } public static boolean returnTrue() { System.out.println(&quot;Returning true&quot;); return true; } public static boolean returnFalse() { System.out.println(&quot;Returning false&quot;); return false; }}// Returning true// truepublic class Example { public static void main(String[] args) { if (or(returnTrue(), returnFalse())) { System.out.println(&quot;true&quot;); } } public static boolean or(boolean x, boolean y) { return x || y; } public static boolean returnTrue() { System.out.println(&quot;Returning true&quot;); return true; } public static boolean returnFalse() { System.out.println(&quot;Returning false&quot;); return false; }}// 여기서는 최적화가 되지 않음 -&gt; 이를 위해 lazy evaluation을 사용// Returning true// Returning false// trueimport java.util.function.Supplier;public class Example { public static void main(String[] args) { if (lazyOr(() -&gt; returnTrue(), () -&gt; returnFalse())) { System.out.println(&quot;true&quot;); } } public static boolean or(boolean x, boolean y) { return x || y; } public static boolean lazyOr(Supplier&lt;Boolean&gt; x, Supplier&lt;Boolean&gt; y) { return x.get() || y.get(); } public static boolean returnTrue() { System.out.println(&quot;Returning true&quot;); return true; } public static boolean returnFalse() { System.out.println(&quot;Returning false&quot;); return false; }}// Returning true// trueimport java.util.List;import java.util.stream.Collectors;import java.util.stream.Stream;public class Example { public static void main(String[] args) { Stream&lt;Integer&gt; integerStream = Stream.of(3, -2, 5, 8, -3, 10) .filter(x -&gt; x &gt; 0) .peek(x -&gt; System.out.println(&quot;peeking &quot; + x)) .filter(x -&gt; x % 2 ==0); System.out.println(&quot;Before collect&quot;); List&lt;Integer&gt; integers = integerStream.collect(Collectors.toList()); System.out.println(&quot;After collect: &quot; + integers); }}// Lazy Evaluation - Stream은 collect 등의 종결 처리가 이루어지기 전까지는 모든 계싼을 미루기 때문// Before collect// peeking 3// peeking 5// peeking 8// peeking 10// After collect: [8, 10] 3. Function Composition (함수 합성) 여러 개의 함수를 합쳐 하나의 새로운 함수로 만드는 것 https://github.com/shchoice/shchoice.github.io/assets/100276387/2f930934-143c-4754-bd5f-f9d5b2304589 123java.util.function.Function&lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) 123456789101112import java.util.function.Function;public class Example { public static void main(String[] args) { Function&lt;Integer, Integer&gt; multiplyByTwo = x -&gt; 2 * x; Function&lt;Integer, Integer&gt; addTen = x -&gt; x + 10; Function&lt;Integer, Integer&gt; composedFunction = multiplyByTwo.andThen(addTen); System.out.println(composedFunction.apply(3)); // 16 }} 123456789101112131415161718192021222324252627282930313233import java.math.BigDecimal;import java.util.Arrays;import java.util.List;import java.util.function.Function;import util.Order;import util.OrderLine;import util.OrderLineAggregationPriceProcessor;import util.TaxPriceProcessor;public class Example { public static void main(String[] args) { Order unprocessedOrder = new Order() .setId(1001L) .setOrderLines(Arrays.asList( new OrderLine().setAmount(BigDecimal.valueOf(1000)), new OrderLine().setAmount(BigDecimal.valueOf(2000)))); List&lt;Function&lt;Order, Order&gt;&gt; priceProcessors = getPriceProcessors(unprocessedOrder); Function&lt;Order, Order&gt; mergedPriceProcessors = priceProcessors.stream() .reduce(Function.identity(), Function::andThen); // = .reduce(Function.identity(), priceProcessor1.andThen(priceProcessor2)) Order processedOrder = mergedPriceProcessors.apply(unprocessedOrder); System.out.println(processedOrder); // Order [id=1001, createdByUserId=0, amount=3281.25000, orderLines=[OrderLine [id=0, productId=0, quantity=0, amount=1000], OrderLine [id=0, productId=0, quantity=0, amount=2000]]] } public static List&lt;Function&lt;Order, Order&gt;&gt; getPriceProcessors(Order order) { return Arrays.asList(new OrderLineAggregationPriceProcessor(), new TaxPriceProcessor(new BigDecimal(&quot;9.375&quot;))); }} 1234567891011121314151617181920212223242526272829303132333435package util;import java.math.BigDecimal;import java.util.function.Function;public class OrderLineAggregationPriceProcessor implements Function&lt;Order, Order&gt; { @Override public Order apply(Order order) { return order.setAmount(order.getOrderLines().stream() .map(OrderLine::getAmount) .reduce(BigDecimal.ZERO, BigDecimal::add)); }}package util;import java.math.BigDecimal;import java.util.function.Function;public class TaxPriceProcessor implements Function&lt;Order, Order&gt; { private final BigDecimal taxRate; public TaxPriceProcessor(BigDecimal taxRate) { this.taxRate = taxRate; } @Override public Order apply(Order order) { return order.setAmount(order.getAmount() .multiply(taxRate.divide(new BigDecimal(100)).add(BigDecimal.ONE))); }}","link":"/8-%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EC%9D%98-%EC%9D%91%EC%9A%A9/"},{"title":"Efficient Classification of Long Documents Using Transformers","text":"업무로 KoBERT Model을 통해 Text Classification을 수행하면서 2가지의 궁금증이 있었다. BERT에서는 문서의 앞에서부터 512개의 Token들만 읽어서 처리를 수행 그렇다면 입력값이 몇 천개가 들어오면 어떻게 될까 BERT에서 Classification(Label, Target)의 갯수 몇 천개라면 input dense 와 hidden dense를 몇 개를 설정해야 적절할까. 해당 페이지는 첫번째에 대한 궁금증 해소를 위해 리서치한 결과에 대한 요약 내용이다. Efficient Classification of Long Documents Using Transformers","link":"/Efficient-Classification-of-Long-Documents-Using-Transformers/"},{"title":"3. 스프링 컨테이너와 스프링 빈","text":"스프링 컨테이너Spring 컨테이너란 무엇인가 개념 스프링 프레임워크 내에 구현된 IoC 컨테이너로 스프링 애플리케이션 내의 객체(Bean)들의 생성, 관리, 의존성 주입 등을 담당 IoC(Inversion of Control, 제어의 역전)와 DI(Dependency Injection, 의존성 주입)의 원칙을 기반으로 작동하여, 애플리케이션의 결합도를 낮추고, 코드의 재사용성 및 유지보수성을 높이는 데 중추적인 역할을 함 목적 애플리케이션의 결합도를 낮추어 유연성과 확장성을 제공 코드 재사용성과 유지보수성을 향상시킴 애플리케이션 개발 과정에서 반복되는 코드 작성을 줄여줌 애플리케이션의 설정과 구성을 중앙에서 관리하여 개발자가 비즈니스 로직에 더 집중할 수 있게 도와줌 주요 기능 Bean의 생성과 관리: 스프링 컨테이너는 애플리케이션 구성 요소를 Bean으로 등록하고, 이들의 생성, 초기화, 소멸 과정을 관리. 의존성 주입: 컨테이너는 Bean 간의 의존성을 자동으로 주입하여, 개발자가 수동으로 설정할 필요 없이 객체 간의 관계를 자동으로 설정 Bean의 생명 주기 관리: 스프링 컨테이너는 Bean의 전체 생명 주기를 관리하며, 초기화 및 소멸 시 커스텀 로직을 실행할 수 있는 콜백 인터페이스를 제공합 애플리케이션 설정의 중앙 집중화: XML, 애너테이션, 자바 기반 설정을 통해 애플리케이션의 구성 정보를 중앙에서 관리할 수 있음 예시 BeanFactory와 ApplicationContext는 스프링 컨테이너의 두 가지 주요 형 BeanFactory: 기본적인 DI 기능을 제공하며, 필요할 때 Bean을 로딩하는 지연 로딩 방식을 사용 ApplicationContext: BeanFactory의 모든 기능을 포함하며, 메시지 리소스 처리, 이벤트 발행 등 애플리케이션 개발에 필요한 추가적인 기능을 제공 스프링 컨테이너 생성1ApplicationContext applicationContext = new AnnotationConfigApplicationContext(AppConfig.class); ApplicationContext 를 스프링 컨테이너이며 인터페이스이다 스프링 컨테이너는 XML을 기반으로 할 수도 있고 annotaion 기반의 자바 설정 클래스로 스프링 컨테이너를 만들 수 있음 위의 방식은 자바 설정 클래스(AppConfig.class) 기반으로 스프링 컨테이너를 만든 것 스프링 컨테이너 생성 과정 💡 스프링은 빈을 생성하고 의존관계를 주입하는 단계가 나누어져 있음 스프링 컨테이너를 생성하고, 설정(구성) 정보를 참고해서 스프링 빈도 등록하고 의존관계도 설정 스프링 컨테이너 생성 new AnnotaionConfigApplicationContext(AppConfig.class) 스프링 컨테이너를 생성할 때는 구성 정보를 지정해주어야 한다 여기서는 AppConfig.class 를 구성 정보로 지정 스프링 빈 등록 스프링 컨테이너는 파라미터로 넘어온 설정 클래스 정보를 사용해서 스프링 빈을 등록 빈 이름은 메서드 이름을 사용하며, 직접 부여할 수도 있음(@Bean(name=”memberService2”) 주의! 빈이름은 항상 다른 이름을 부여해야 함 같은 이름이면 다른 빈이 무시되거나 기존 빈을 덮어버리거나 설정에 따라 오류가 발생 스프링 빈 의존관계 설정 - 준비 스프링 빈 의존관계 설정 완료 스프링 컨테이너는 설정 정보를 참고해서 의존관계를 주입(DI) 함 스프링 빈(Bean)스프링 빈 조회 스프링 컨테이너에 등록된 모든 빈 조회 String[] beanDefinitionNames = context.getBeanDefinitionNames(); ac.getBeanDefinitionNames() : 스프링에 등록된 모든 빈 이름을 조회 ac.getBean() : Bean 이름으로 빈 객체(인스턴스)를 조회 Object bean = context.getBean(beanDefinitionName); 123456789101112131415161718192021222324252627public class ApplicationContextInfoTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); @Test @DisplayName(&quot;모든 빈 출력하기&quot;) void findAllBean() { String[] beanDefinitionNames = context.getBeanDefinitionNames(); for (String beanDefinitionName: beanDefinitionNames) { BeanDefinition beanDefinition = context.getBeanDefinition(beanDefinitionName); Object bean = context.getBean(beanDefinitionName); System.out.println(&quot;name = &quot; + beanDefinitionName + &quot; object = &quot; + bean); /* name = org.springframework.context.annotation.internalConfigurationAnnotationProcessor object = org.springframework.context.annotation.ConfigurationClassPostProcessor@4201a617 name = org.springframework.context.annotation.internalAutowiredAnnotationProcessor object = org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor@467f77a5 name = org.springframework.context.annotation.internalCommonAnnotationProcessor object = org.springframework.context.annotation.CommonAnnotationBeanPostProcessor@1bb9aa43 name = org.springframework.context.event.internalEventListenerProcessor object = org.springframework.context.event.EventListenerMethodProcessor@420bc288 name = org.springframework.context.event.internalEventListenerFactory object = org.springframework.context.event.DefaultEventListenerFactory@df5f5c0 name = appConfig object = my.study.springcorebasic.AppConfig$$EnhancerBySpringCGLIB$$3b7649e5@308a6984 name = memberRepository object = my.study.springcorebasic.member.MemoryMemberRepository@66b72664 name = memberService object = my.study.springcorebasic.member.MemberServiceImpl@7a34b7b8 name = orderService object = my.study.springcorebasic.order.OrderServiceImpl@58cd06cb name = discountPolicy object = my.study.springcorebasic.discount.RateDiscountPolicy@3be8821f */ } }} 스프링 컨테이너에 애플리케이션 빈만 출력하기 스프링 내부에서 사용하는 빈은 getRole()로 구분이 가능 ROLE_APPLICATION : 일반적으로 사용자가 정의한 빈 ROLE_INFRASTRUCTURE : 스프링이 내부에서 사용하는 빈 1234567891011121314151617181920212223242526public class ApplicationContextInfoTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); @Test @DisplayName(&quot;애플리케이션 빈 출력하기&quot;) void findApplicationBean() { String[] beanDefinitionNames = context.getBeanDefinitionNames(); for (String beanDefinitionName: beanDefinitionNames) { BeanDefinition beanDefinition = context.getBeanDefinition(beanDefinitionName); //Role ROLE_APPLICATION: 직접 등록한 애플리케이션 빈 //Role ROLE_INFRASTRUCTURE: 스프링이 내부에서 사용하는 빈 if (beanDefinition.getRole() == BeanDefinition.ROLE_APPLICATION) { Object bean = context.getBean(beanDefinitionName); System.out.println(&quot;name = &quot; + beanDefinitionName + &quot; object = &quot; + bean); /* name = appConfig object = my.study.springcorebasic.AppConfig$$EnhancerBySpringCGLIB$$3b7649e5@308a6984 name = memberRepository object = my.study.springcorebasic.member.MemoryMemberRepository@66b72664 name = memberService object = my.study.springcorebasic.member.MemberServiceImpl@7a34b7b8 name = orderService object = my.study.springcorebasic.order.OrderServiceImpl@58cd06cb name = discountPolicy object = my.study.springcorebasic.discount.RateDiscountPolicy@3be8821f */ } } }} 스프링 빈 조회 - 기본 ac.getBean(빈이름, 타입) 1234567891011class ApplicationContextBasicBeanFindTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); @Test @DisplayName(&quot;빈 이름으로 조회&quot;) void findBeanByName() { MemberService memberService = context.getBean(&quot;memberService&quot;, MemberService.class); assertThat(memberService).isInstanceOf(MemberServiceImpl.class); }} ac.getBean(타입) 1234567891011class ApplicationContextBasicBeanFindTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); @Test @DisplayName(&quot;이름 없이 타입만으로 조회&quot;) void findBeanByType() { MemberService memberService = context.getBean(MemberService.class); assertThat(memberService).isInstanceOf(MemberServiceImpl.class); }} 1234567891011class ApplicationContextBasicBeanFindTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); @Test @DisplayName(&quot;구체 타입으로 조회&quot;) void findBeanByName2() { MemberServiceImpl memberService = context.getBean(&quot;memberService&quot;, MemberServiceImpl.class); assertThat(memberService).isInstanceOf(MemberServiceImpl.class); }} 조회 대상 Spring Bean이 없으면 예외 발생 NoSuchBeanDefinitionException: No bean named ‘xxxxx’ available` 123456789101112class ApplicationContextBasicBeanFindTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); @Test @DisplayName(&quot;빈 이름으로 조회X&quot;) void findBeanByNameX() { //MemberService memberService = context.getBean(&quot;xxxxx&quot;, MemberService.class); assertThrows(NoSuchBeanDefinitionException.class, () -&gt; context.getBean(&quot;xxxxx&quot;, MemberService.class)); }} 스프링 빈 조회 - 동일한 타입이 둘 이상 타입으로 조회 시 같은 타입의 스프링 빈이 둘 이상 있으면 오류가 발생 1234567891011121314151617181920212223class ApplicationContextSameBeanFindTest { @Configuration static class SameBeanConfig { @Bean public MemberRepository memberRepository1() { return new MemoryMemberRepository(); } @Bean public MemberRepository memberRepository2() { return new MemoryMemberRepository(); } } AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(SameBeanConfig.class); @Test @DisplayName(&quot;타입으로 조회시 같은 타입이 둘 이상 있으면, 중복 오류가 발생한다&quot;) void findBeanByTypeDuplicate() { //MemberRepository bean = ac.getBean(MemberRepository.class); assertThrows(NoUniqueBeanDefinitionException.class, () -&gt; context.getBean(MemberRepository.class)); }} 따라서 이때는 빈 이름을 지정해야 한다. 12345678910111213141516171819202122class ApplicationContextSameBeanFindTest { @Configuration static class SameBeanConfig { @Bean public MemberRepository memberRepository1() { return new MemoryMemberRepository(); } @Bean public MemberRepository memberRepository2() { return new MemoryMemberRepository(); } } AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(SameBeanConfig.class); @Test @DisplayName(&quot;타입으로 조회시 같은 타입이 둘 이상 있으면, 빈 이름을 지정하면 된다&quot;) void findBeanByName() { MemberRepository memberRepository = context.getBean(&quot;memberRepository1&quot;, MemberRepository.class); assertThat(memberRepository).isInstanceOf(MemberRepository.class); }} ac.getBeanOfType() 을 사용하면 해당 타입의 모든 빈을 조회할 수있음 1234567891011121314151617181920212223242526272829class ApplicationContextSameBeanFindTest { @Configuration static class SameBeanConfig { @Bean public MemberRepository memberRepository1() { return new MemoryMemberRepository(); } @Bean public MemberRepository memberRepository2() { return new MemoryMemberRepository(); } } AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(SameBeanConfig.class); @Test @DisplayName(&quot;특정 타입을 모두 조회하기&quot;) void findAllBeanByType() { Map&lt;String, MemberRepository&gt; beansOfType = context.getBeansOfType(MemberRepository.class); for (String key : beansOfType.keySet()) { System.out.println(&quot;key = &quot; + key + &quot; value = &quot; + beansOfType.get(key)); // key = memberRepository1 value = my.study.springcorebasic.member.MemoryMemberRepository@e72dba7 // key = memberRepository2 value = my.study.springcorebasic.member.MemoryMemberRepository@33c2bd } System.out.println(&quot;beansOfType = &quot; + beansOfType); // beansOfType = {memberRepository1=my.study.springcorebasic.member.MemoryMemberRepository@e72dba7, memberRepository2=my.study.springcorebasic.member.MemoryMemberRepository@33c2bd} assertThat(beansOfType.size()).isEqualTo(2); }} 스프링 빈 조회 - 상속 관계 부모 타입으로 죄하면, 자식 타입도 함께 조회를 수행 즉 모든 자바 객체의 최고 부모인 Object 타입으로 조회하면, 모든 스프링 빈을 조회 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class ApplicationContextExtendsFindTest { @Configuration static class TestConfig { @Bean public DiscountPolicy rateDiscountPolicy() { return new RateDiscountPolicy(); } @Bean public DiscountPolicy fixDiscountPolicy() { return new FixDiscountPolicy(); } } AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(TestConfig.class); @Test @DisplayName(&quot;부모 타입으로 조회시, 자식이 둘 이상 있으면, 중복 오류가 발생한다&quot;) void findBeanByParentTypeDuplicate() { //DiscountPolicy bean = ac.getBean(DiscountPolicy.class); assertThrows(NoUniqueBeanDefinitionException.class, () -&gt; context.getBean(DiscountPolicy.class)); } @Test @DisplayName(&quot;부모 타입으로 조회시, 자식이 둘 이상 있으면, 빈 이름을 지정하면 된다&quot;) void findBeanByParentTypeBeanName() { DiscountPolicy rateDiscountPolicy = context.getBean(&quot;rateDiscountPolicy&quot;, DiscountPolicy.class); assertThat(rateDiscountPolicy).isInstanceOf(RateDiscountPolicy.class); } @Test @DisplayName(&quot;특정 하위 타입으로 조회&quot;) void findBeanBySubType() { RateDiscountPolicy bean = context.getBean(RateDiscountPolicy.class); assertThat(bean).isInstanceOf(RateDiscountPolicy.class); } @Test @DisplayName(&quot;부모 타입으로 모두 조회하기&quot;) void findAllBeanByParentType() { Map&lt;String, DiscountPolicy&gt; beansOfType = context.getBeansOfType(DiscountPolicy.class); assertThat(beansOfType.size()).isEqualTo(2); for (String key : beansOfType.keySet()) { System.out.println(&quot;1. key = &quot; + key + &quot; value=&quot; + beansOfType.get(key)); // 1. key = rateDiscountPolicy value=my.study.springcorebasic.discount.RateDiscountPolicy@5d52e3ef // 1. key = fixDiscountPolicy value=my.study.springcorebasic.discount.FixDiscountPolicy@5298dead } } @Test @DisplayName(&quot;부모 타입으로 모두 조회하기 - Object&quot;) void findAllBeanByObjectType() { Map&lt;String, Object&gt; beansOfType = context.getBeansOfType(Object.class); for (String key : beansOfType.keySet()) { System.out.println(&quot;2. key = &quot; + key + &quot; value=&quot; + beansOfType.get(key)); // 2. key = org.springframework.context.annotation.internalConfigurationAnnotationProcessor value=org.springframework.context.annotation.ConfigurationClassPostProcessor@1dfd5f51 // 2. key = org.springframework.context.annotation.internalAutowiredAnnotationProcessor value=org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor@3c321bdb // 2. key = org.springframework.context.annotation.internalCommonAnnotationProcessor value=org.springframework.context.annotation.CommonAnnotationBeanPostProcessor@24855019 // 2. key = org.springframework.context.event.internalEventListenerProcessor value=org.springframework.context.event.EventListenerMethodProcessor@3abd581e // 2. key = org.springframework.context.event.internalEventListenerFactory value=org.springframework.context.event.DefaultEventListenerFactory@4d4d8fcf // 2. key = applicationContextExtendsFindTest.TestConfig value=my.study.springcorebasic.beanfind.ApplicationContextExtendsFindTest$TestConfig$$EnhancerBySpringCGLIB$$53523813@610db97e // 2. key = rateDiscountPolicy value=my.study.springcorebasic.discount.RateDiscountPolicy@6f0628de // 2. key = fixDiscountPolicy value=my.study.springcorebasic.discount.FixDiscountPolicy@3fabf088 // 2. key = environment value=StandardEnvironment {activeProfiles=[], defaultProfiles=[default], propertySources=[PropertiesPropertySource@507061061 {name='systemProperties', properties={gopherProxySet=false, awt.toolkit=sun.lwawt.macosx.LWCToolkit, java.specification.version=11, sun.cpu.isalist=, sun.jnu.encoding=UTF-8, org.gradle.test.worker=5, java.class.path=/Users/shchoice/.gradle/caches/7.6/workerMain/gradle-worker.jar:/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/classes/java/test:/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/classes/java/main:/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/resources/main:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/2.7.8/7596f0263544e8230a5bf5357a02fb391f036f77/spring-boot-starter-web-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/2.7.8/5751fec1eae46b9dce1ff399e25deade13b5265d/spring-boot-starter-test-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/2.7.8/fb09a04154ce2aea974755bc011a588eee2332aa/spring-boot-starter-json-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/2.7.8/df091ed288bd1dcf89fa0554a29fe96f27802efc/spring-boot-starter-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/javax.inject/javax.inject/1/6975da39a7040257bd51d21a231b76c915872d38/javax.inject-1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/2.7.8/48f875d696d55257ce380d2c399d60030508d081/spring-boot-test-autoconfigure-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/2.7.8/cb835d82d00116e907d341d11096c3476ab49721/spring-boot-autoconfigure-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/2.7.8/e90d0e2f4502d38244fbee6f5f03056abd24888b/spring-boot-test-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/2.7.8/8db5af0f1171bb402c27a85fe97d741bddaa6fee/spring-boot-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/2.7.8/439c2c6fb705c4e7338b2c7a975a84b4f0cb3724/spring-boot-starter-logging-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/2.7.8/8e7a62215cc56473c891480f722dda43dd6059d9/spring-boot-starter-tomcat-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/1.3.5/59eb84ee0d616332ff44aba065f3888cf002cd2d/jakarta.annotation-api-1.3.5.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/5.3.25/62a8258bcc4f7a58dd69af5140481b64653c90/spring-webmvc-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/5.3.25/c69815e7931cd3ce7f19cc8028fd1c36626120d6/spring-web-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/5.3.25/42a55c25a4da3bc330d8ab3ea7648cd76d0830d4/spring-test-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/5.3.25/268a70ce4f44333ce0f13304c5f8c53b3df5f5f4/spring-context-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/5.3.25/722e30759b29331726f9deed76f80b22345ee627/spring-aop-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/5.3.25/b3aeae036b4ea1abfa1f9604d452e19664efe5f6/spring-beans-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/5.3.25/d681cdb86611f03d8ef29654edde219fe5afef1d/spring-expression-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/5.3.25/85382e86321227506bf7f97ed80e2ab88bce25f0/spring-core-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.30/8fde7fe2586328ac3c68db92045e1c8759125000/snakeyaml-1.30.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.7.0/f9d7d9659f2694e61142046ff8a216c047f263e8/json-path-2.7.0.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/2.3.3/48e3b9cfc10752fba3521d6511f4165bea951801/jakarta.xml.bind-api-2.3.3.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.22.0/c300c0c6a24559f35fa0bd3a5472dc1edcd0111e/assertj-core-3.22.0.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/4.5.1/f81fb60bd69b3a6e5537ae23b883326f01632a61/mockito-junit-jupiter-4.5.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.8.2/ddeafe92fc263f895bfb73ffeca7fd56e23c2cce/junit-jupiter-params-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.8.2/c598b4328d2f397194d11df3b1648d68d7d990e3/junit-jupiter-engine-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.8.2/4c21029217adf07e4c0d0c5e192b6bf610c94bdc/junit-jupiter-api-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.8.2/b737de09f19864bd136805c84df7999a142fec29/junit-platform-engine-1.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.8.2/32c8b8617c1342376fd5af2053da6410d8866861/junit-platform-commons-1.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.8.2/5a817b1e63f1217e5c586090c45e681281f097ad/junit-jupiter-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/4.5.1/ed456e623e5afc6f4cee3ae58144e5c45f3b3bf/mockito-core-4.5.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.1/6d842d0faf4cf6725c509a5e5347d319ee0431c3/jsonassert-1.5.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.9.1/e5833662d9a1279a37da3ef6f62a1da29fcd68c4/xmlunit-core-2.9.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.11/4741689214e9d1e8408b206506cbe76d1c6a7d60/logback-classic-1.2.11.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.17.2/17dd0fae2747d9a28c67bc9534108823d2376b46/log4j-to-slf4j-2.17.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/1.7.36/ed46d81cef9c412a88caef405b58f93a678ff2ca/jul-to-slf4j-1.7.36.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/5.3.25/2e65a986dc7f98b40faed8df1d50db77c0b96c61/spring-jcl-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.13.4/e6d820112871f33cd94a1dcc54eef58874753b5/jackson-datatype-jsr310-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.13.4/858ccf6624b5fac6044813e845063edb6a62cf37/jackson-module-parameter-names-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.13.4/858c6cc78e1f08a885b1613e1d817c829df70a6e/jackson-annotations-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.13.4/cf934c681294b97ef6d80082faeefbe1edadf56/jackson-core-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.13.4/557dbba5d8dfc7b7f944c58fe084109afcb5670b/jackson-datatype-jdk8-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.13.4.2/325c06bdfeb628cfb80ebaaf1a26cc1eb558a585/jackson-databind-2.13.4.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/9.0.71/987b6460af04b08bc9914788d2762080afb09541/tomcat-embed-websocket-9.0.71.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/9.0.71/adaed61b4eaa5b52448336c0881fcd828fd51a2f/tomcat-embed-core-9.0.71.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/9.0.71/8fe43848c27ec921c8c5d6dcbd8b959076d7bf99/tomcat-embed-el-9.0.71.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.4.8/7c62f5f72ab05eb54d40e2abf0360a2fe9ea477f/json-smart-2.4.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.36/6c62681a2f655b49963a5983b8b0950a6120ae14/slf4j-api-1.7.36.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/1.2.2/99f53adba383cb1bf7c3862844488574b559621f/jakarta.activation-api-1.2.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.12.22/984e536b4f3fb668b21f15b90c1e8704292d4bdd/byte-buddy-1.12.22.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.12.22/9c4127080df12304336ca90c2ef3f8b7d72915c1/byte-buddy-agent-1.12.22.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.2/7fadf57620c8b8abdf7519533e5527367cb51f09/objenesis-3.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.2.11/a01230df5ca5c34540cdaa3ad5efb012f1f1f792/logback-core-1.2.11.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.17.2/f42d6afa111b4dec5d2aea0fe2197240749a4ea6/log4j-api-2.17.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.4.8/6e1bee5a530caba91893604d6ab41d0edcecca9a/accessors-smart-2.4.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.2.0/28c11eb91f9b6d8e200631d46e20a7f407f2a046/opentest4j-1.2.0.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.1/a99500cf6eea30535eeac6be73899d048f8d12a8/asm-9.1.jar, java.vm.vendor=AdoptOpenJDK, sun.arch.data.model=64, user.variant=, java.vendor.url=https://adoptopenjdk.net/, user.timezone=Asia/Seoul, os.name=Mac OS X, java.vm.specification.version=11, user.country=KR, sun.java.launcher=SUN_STANDARD, sun.boot.library.path=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home/lib, sun.java.command=worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 5', http.nonProxyHosts=local|*.local|169.254/16|*.169.254/16, jdk.debug=release, sun.cpu.endian=little, user.home=/Users/shchoice, user.language=en, java.specification.vendor=Oracle Corporation, org.gradle.native=false, java.version.date=2021-04-20, java.home=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home, file.separator=/, java.vm.compressedOopsMode=Zero based, line.separator= // , java.specification.name=Java Platform API Specification, java.vm.specification.vendor=Oracle Corporation, java.awt.graphicsenv=sun.awt.CGraphicsEnvironment, sun.management.compiler=HotSpot 64-Bit Tiered Compilers, ftp.nonProxyHosts=local|*.local|169.254/16|*.169.254/16, java.runtime.version=11.0.11+9, user.name=shchoice, path.separator=:, os.version=11.4, java.runtime.name=OpenJDK Runtime Environment, file.encoding=UTF-8, java.vm.name=OpenJDK 64-Bit Server VM, java.vendor.version=AdoptOpenJDK-11.0.11+9, java.vendor.url.bug=https://github.com/AdoptOpenJDK/openjdk-support/issues, java.io.tmpdir=/var/folders/f5/0f_87gv14b70d5y2pp9m1fhw0000gn/T/, java.version=11.0.11, user.dir=/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic, os.arch=x86_64, java.vm.specification.name=Java Virtual Machine Specification, java.awt.printerjob=sun.lwawt.macosx.CPrinterJob, sun.os.patch.level=unknown, org.gradle.internal.worker.tmpdir=/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/tmp/test/work, java.library.path=/Users/shchoice/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:., java.vm.info=mixed mode, java.vendor=AdoptOpenJDK, java.vm.version=11.0.11+9, sun.io.unicode.encoding=UnicodeBig, java.class.version=55.0, socksNonProxyHosts=local|*.local|169.254/16|*.169.254/16}}, SystemEnvironmentPropertySource@317960117 {name='systemEnvironment', properties={PATH=/Users/shchoice/anaconda3/bin:/Users/shchoice/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/shchoice/Library/Application Support/JetBrains/Toolbox/scripts, MANPATH=/opt/homebrew/share/man::, CONDA_DEFAULT_ENV=base, CONDA_EXE=/Users/shchoice/anaconda3/bin/conda, JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home, CONDA_PYTHON_EXE=/Users/shchoice/anaconda3/bin/python, HOMEBREW_PREFIX=/opt/homebrew, SERPAPI_API_KEY=5d568dbe5cb8465fe558b982add97b619e8f56bf9ea3969912d37e7ca4eb42c2, COMMAND_MODE=unix2003, CONDA_PREFIX=/Users/shchoice/anaconda3, _CE_M=, LOGNAME=shchoice, HOMEBREW_REPOSITORY=/opt/homebrew, XPC_SERVICE_NAME=application.com.jetbrains.intellij.21825045.21825790, CONDA_SHLVL=1, INFOPATH=/opt/homebrew/share/info:, __CFBundleIdentifier=com.jetbrains.intellij, SHELL=/bin/zsh, PAGER=less, LSCOLORS=Gxfxcxdxbxegedabagacad, HOMEBREW_CELLAR=/opt/homebrew/Cellar, OLDPWD=/, USER=shchoice, ZSH=/Users/shchoice/.oh-my-zsh, TMPDIR=/var/folders/f5/0f_87gv14b70d5y2pp9m1fhw0000gn/T/, SSH_AUTH_SOCK=/private/tmp/com.apple.launchd.ZeQ0YQuhKl/Listeners, _CE_CONDA=, XPC_FLAGS=0x0, __CF_USER_TEXT_ENCODING=0x1F5:0x0:0x0, CONDA_PROMPT_MODIFIER=(base) , LESS=-R, OPENAI_API_KEY=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, LC_CTYPE=en_US.UTF-8, LS_COLORS=di=1;36:ln=35:so=32:pi=33:ex=31:bd=34;46:cd=34;43:su=30;41:sg=30;46:tw=30;42:ow=30;43, IDEA_INITIAL_DIRECTORY=/, JAVA_MAIN_CLASS_16574=worker.org.gradle.process.internal.worker.GradleWorkerMain, HOME=/Users/shchoice}}]} // 2. key = systemProperties value={gopherProxySet=false, awt.toolkit=sun.lwawt.macosx.LWCToolkit, java.specification.version=11, sun.cpu.isalist=, sun.jnu.encoding=UTF-8, org.gradle.test.worker=5, java.class.path=/Users/shchoice/.gradle/caches/7.6/workerMain/gradle-worker.jar:/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/classes/java/test:/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/classes/java/main:/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/resources/main:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/2.7.8/7596f0263544e8230a5bf5357a02fb391f036f77/spring-boot-starter-web-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/2.7.8/5751fec1eae46b9dce1ff399e25deade13b5265d/spring-boot-starter-test-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/2.7.8/fb09a04154ce2aea974755bc011a588eee2332aa/spring-boot-starter-json-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/2.7.8/df091ed288bd1dcf89fa0554a29fe96f27802efc/spring-boot-starter-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/javax.inject/javax.inject/1/6975da39a7040257bd51d21a231b76c915872d38/javax.inject-1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/2.7.8/48f875d696d55257ce380d2c399d60030508d081/spring-boot-test-autoconfigure-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/2.7.8/cb835d82d00116e907d341d11096c3476ab49721/spring-boot-autoconfigure-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/2.7.8/e90d0e2f4502d38244fbee6f5f03056abd24888b/spring-boot-test-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/2.7.8/8db5af0f1171bb402c27a85fe97d741bddaa6fee/spring-boot-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/2.7.8/439c2c6fb705c4e7338b2c7a975a84b4f0cb3724/spring-boot-starter-logging-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/2.7.8/8e7a62215cc56473c891480f722dda43dd6059d9/spring-boot-starter-tomcat-2.7.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/1.3.5/59eb84ee0d616332ff44aba065f3888cf002cd2d/jakarta.annotation-api-1.3.5.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/5.3.25/62a8258bcc4f7a58dd69af5140481b64653c90/spring-webmvc-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/5.3.25/c69815e7931cd3ce7f19cc8028fd1c36626120d6/spring-web-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/5.3.25/42a55c25a4da3bc330d8ab3ea7648cd76d0830d4/spring-test-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/5.3.25/268a70ce4f44333ce0f13304c5f8c53b3df5f5f4/spring-context-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/5.3.25/722e30759b29331726f9deed76f80b22345ee627/spring-aop-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/5.3.25/b3aeae036b4ea1abfa1f9604d452e19664efe5f6/spring-beans-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/5.3.25/d681cdb86611f03d8ef29654edde219fe5afef1d/spring-expression-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/5.3.25/85382e86321227506bf7f97ed80e2ab88bce25f0/spring-core-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.30/8fde7fe2586328ac3c68db92045e1c8759125000/snakeyaml-1.30.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.7.0/f9d7d9659f2694e61142046ff8a216c047f263e8/json-path-2.7.0.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/2.3.3/48e3b9cfc10752fba3521d6511f4165bea951801/jakarta.xml.bind-api-2.3.3.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.22.0/c300c0c6a24559f35fa0bd3a5472dc1edcd0111e/assertj-core-3.22.0.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/4.5.1/f81fb60bd69b3a6e5537ae23b883326f01632a61/mockito-junit-jupiter-4.5.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.8.2/ddeafe92fc263f895bfb73ffeca7fd56e23c2cce/junit-jupiter-params-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.8.2/c598b4328d2f397194d11df3b1648d68d7d990e3/junit-jupiter-engine-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.8.2/4c21029217adf07e4c0d0c5e192b6bf610c94bdc/junit-jupiter-api-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.8.2/b737de09f19864bd136805c84df7999a142fec29/junit-platform-engine-1.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.8.2/32c8b8617c1342376fd5af2053da6410d8866861/junit-platform-commons-1.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.8.2/5a817b1e63f1217e5c586090c45e681281f097ad/junit-jupiter-5.8.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/4.5.1/ed456e623e5afc6f4cee3ae58144e5c45f3b3bf/mockito-core-4.5.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.1/6d842d0faf4cf6725c509a5e5347d319ee0431c3/jsonassert-1.5.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.9.1/e5833662d9a1279a37da3ef6f62a1da29fcd68c4/xmlunit-core-2.9.1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.11/4741689214e9d1e8408b206506cbe76d1c6a7d60/logback-classic-1.2.11.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.17.2/17dd0fae2747d9a28c67bc9534108823d2376b46/log4j-to-slf4j-2.17.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/1.7.36/ed46d81cef9c412a88caef405b58f93a678ff2ca/jul-to-slf4j-1.7.36.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/5.3.25/2e65a986dc7f98b40faed8df1d50db77c0b96c61/spring-jcl-5.3.25.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.13.4/e6d820112871f33cd94a1dcc54eef58874753b5/jackson-datatype-jsr310-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.13.4/858ccf6624b5fac6044813e845063edb6a62cf37/jackson-module-parameter-names-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.13.4/858c6cc78e1f08a885b1613e1d817c829df70a6e/jackson-annotations-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.13.4/cf934c681294b97ef6d80082faeefbe1edadf56/jackson-core-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.13.4/557dbba5d8dfc7b7f944c58fe084109afcb5670b/jackson-datatype-jdk8-2.13.4.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.13.4.2/325c06bdfeb628cfb80ebaaf1a26cc1eb558a585/jackson-databind-2.13.4.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/9.0.71/987b6460af04b08bc9914788d2762080afb09541/tomcat-embed-websocket-9.0.71.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/9.0.71/adaed61b4eaa5b52448336c0881fcd828fd51a2f/tomcat-embed-core-9.0.71.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/9.0.71/8fe43848c27ec921c8c5d6dcbd8b959076d7bf99/tomcat-embed-el-9.0.71.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.4.8/7c62f5f72ab05eb54d40e2abf0360a2fe9ea477f/json-smart-2.4.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.36/6c62681a2f655b49963a5983b8b0950a6120ae14/slf4j-api-1.7.36.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/1.2.2/99f53adba383cb1bf7c3862844488574b559621f/jakarta.activation-api-1.2.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.12.22/984e536b4f3fb668b21f15b90c1e8704292d4bdd/byte-buddy-1.12.22.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.12.22/9c4127080df12304336ca90c2ef3f8b7d72915c1/byte-buddy-agent-1.12.22.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.2/7fadf57620c8b8abdf7519533e5527367cb51f09/objenesis-3.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.2.11/a01230df5ca5c34540cdaa3ad5efb012f1f1f792/logback-core-1.2.11.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.17.2/f42d6afa111b4dec5d2aea0fe2197240749a4ea6/log4j-api-2.17.2.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.4.8/6e1bee5a530caba91893604d6ab41d0edcecca9a/accessors-smart-2.4.8.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.2.0/28c11eb91f9b6d8e200631d46e20a7f407f2a046/opentest4j-1.2.0.jar:/Users/shchoice/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.1/a99500cf6eea30535eeac6be73899d048f8d12a8/asm-9.1.jar, java.vm.vendor=AdoptOpenJDK, sun.arch.data.model=64, user.variant=, java.vendor.url=https://adoptopenjdk.net/, user.timezone=Asia/Seoul, os.name=Mac OS X, java.vm.specification.version=11, user.country=KR, sun.java.launcher=SUN_STANDARD, sun.boot.library.path=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home/lib, sun.java.command=worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 5', http.nonProxyHosts=local|*.local|169.254/16|*.169.254/16, jdk.debug=release, sun.cpu.endian=little, user.home=/Users/shchoice, user.language=en, java.specification.vendor=Oracle Corporation, org.gradle.native=false, java.version.date=2021-04-20, java.home=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home, file.separator=/, java.vm.compressedOopsMode=Zero based, line.separator= // , java.specification.name=Java Platform API Specification, java.vm.specification.vendor=Oracle Corporation, java.awt.graphicsenv=sun.awt.CGraphicsEnvironment, sun.management.compiler=HotSpot 64-Bit Tiered Compilers, ftp.nonProxyHosts=local|*.local|169.254/16|*.169.254/16, java.runtime.version=11.0.11+9, user.name=shchoice, path.separator=:, os.version=11.4, java.runtime.name=OpenJDK Runtime Environment, file.encoding=UTF-8, java.vm.name=OpenJDK 64-Bit Server VM, java.vendor.version=AdoptOpenJDK-11.0.11+9, java.vendor.url.bug=https://github.com/AdoptOpenJDK/openjdk-support/issues, java.io.tmpdir=/var/folders/f5/0f_87gv14b70d5y2pp9m1fhw0000gn/T/, java.version=11.0.11, user.dir=/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic, os.arch=x86_64, java.vm.specification.name=Java Virtual Machine Specification, java.awt.printerjob=sun.lwawt.macosx.CPrinterJob, sun.os.patch.level=unknown, org.gradle.internal.worker.tmpdir=/Users/shchoice/NAVER MYBOX/md/git/TIL/Spring/Lecture/Spring-Core-Basic/build/tmp/test/work, java.library.path=/Users/shchoice/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:., java.vm.info=mixed mode, java.vendor=AdoptOpenJDK, java.vm.version=11.0.11+9, sun.io.unicode.encoding=UnicodeBig, java.class.version=55.0, socksNonProxyHosts=local|*.local|169.254/16|*.169.254/16} // 2. key = systemEnvironment value={PATH=/Users/shchoice/anaconda3/bin:/Users/shchoice/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/shchoice/Library/Application Support/JetBrains/Toolbox/scripts, MANPATH=/opt/homebrew/share/man::, CONDA_DEFAULT_ENV=base, CONDA_EXE=/Users/shchoice/anaconda3/bin/conda, JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home, CONDA_PYTHON_EXE=/Users/shchoice/anaconda3/bin/python, HOMEBREW_PREFIX=/opt/homebrew, SERPAPI_API_KEY=5d568dbe5cb8465fe558b982add97b619e8f56bf9ea3969912d37e7ca4eb42c2, COMMAND_MODE=unix2003, CONDA_PREFIX=/Users/shchoice/anaconda3, _CE_M=, LOGNAME=shchoice, HOMEBREW_REPOSITORY=/opt/homebrew, XPC_SERVICE_NAME=application.com.jetbrains.intellij.21825045.21825790, CONDA_SHLVL=1, INFOPATH=/opt/homebrew/share/info:, __CFBundleIdentifier=com.jetbrains.intellij, SHELL=/bin/zsh, PAGER=less, LSCOLORS=Gxfxcxdxbxegedabagacad, HOMEBREW_CELLAR=/opt/homebrew/Cellar, OLDPWD=/, USER=shchoice, ZSH=/Users/shchoice/.oh-my-zsh, TMPDIR=/var/folders/f5/0f_87gv14b70d5y2pp9m1fhw0000gn/T/, SSH_AUTH_SOCK=/private/tmp/com.apple.launchd.ZeQ0YQuhKl/Listeners, _CE_CONDA=, XPC_FLAGS=0x0, __CF_USER_TEXT_ENCODING=0x1F5:0x0:0x0, CONDA_PROMPT_MODIFIER=(base) , LESS=-R, OPENAI_API_KEY=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, LC_CTYPE=en_US.UTF-8, LS_COLORS=di=1;36:ln=35:so=32:pi=33:ex=31:bd=34;46:cd=34;43:su=30;41:sg=30;46:tw=30;42:ow=30;43, IDEA_INITIAL_DIRECTORY=/, JAVA_MAIN_CLASS_16574=worker.org.gradle.process.internal.worker.GradleWorkerMain, HOME=/Users/shchoice} // 2. key = applicationStartup value=org.springframework.core.metrics.DefaultApplicationStartup@74d7184a // 2. key = org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry value=[] // 2. key = messageSource value=Empty MessageSource // 2. key = applicationEventMulticaster value=org.springframework.context.event.SimpleApplicationEventMulticaster@51b01960 // 2. key = lifecycleProcessor value=org.springframework.context.support.DefaultLifecycleProcessor@6831d8fd } }} BeanFactory와 ApplicationContext BeanFactory 스프링 컨테이너의 최상위 인터페이스 스프링 빈을 관리하고 조회하는 역할을 담당 getBean()을 제공 지금까지 우리가 사용한 대부분의 기능은 BeanFactory 가 제공하는 기능 ApplicationContext BeanFactory 기능을 모두 상속받아서 제공 빈을 관리하고 검색하는 기능을 BeanFactory가 제공하는데, 그러면 BeanFactory 와의 차이는? 다음과 같은 부가기능을 제공 메시지소스를 활용한 국제화 기능 예를 들면 한국에서 들어오면 한국어로, 영어권에서 들어오면 영어로 출력 환경 변수 로컬, 개발, 운영 등을 구분해서 처리 애플리케이션 이벤트 이벤트를 발행하고 구독하는 모델을 편리하게 지원 편리한 리소스 조회 파일, Class Path, 외부 등에서 리소스를 편리하게 관리 다양한 설정 형식 지원 -자바 코드, XML 스프링 컨테이너는 다양한 형식의 설정 정보를 받아들일 수 있게 유연하게 설계되어 있음 자바 코드, XML, Groovy 등 스프링 빈 설정 메타 정보 - BeanDefinition 스프링은 어떻게 Annotaion (코드)기반, XML 기반 등 다양한 설정 형식을 지원하는 것일까 → BeanDefinition 추상화 때문! 스프링 컨테이너는 자바 코드인지, XML 인지 몰라도 된다. 오직 BeanDefinition만 알면 됨 즉, 역할과 구현을 개념적으로 나눈 것 XML을 읽어서 BeanDefinition을 만들면 됨 코드를 읽어서 BeanDefinition을 만들면 됨 1234567891011121314151617181920212223public class BeanDefinitionTest { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); // GenericXmlApplicationContext ac = new GenericXmlApplicationContext(&quot;appConfig.xml&quot;); @Test @DisplayName(&quot;빈 설정 메타정보 확인&quot;) void findApplicationBean() { String[] beanDefinitionNames = context.getBeanDefinitionNames(); for (String beanDefinitionName : beanDefinitionNames) { BeanDefinition beanDefinition = context.getBeanDefinition(beanDefinitionName); if (beanDefinition.getRole() == BeanDefinition.ROLE_APPLICATION) { System.out.println(&quot;beanDefinitionName = &quot; + beanDefinitionName + &quot; beanDefinition = &quot; + beanDefinition); /* beanDefinitionName = appConfig beanDefinition = Generic bean: class [my.study.springcorebasic.AppConfig$$EnhancerBySpringCGLIB$$b7b11181]; scope=singleton; abstract=false; lazyInit=null; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null beanDefinitionName = memberRepository beanDefinition = Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=appConfig; factoryMethodName=memberRepository; initMethodName=null; destroyMethodName=(inferred); defined in my.study.springcorebasic.AppConfig beanDefinitionName = memberService beanDefinition = Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=appConfig; factoryMethodName=memberService; initMethodName=null; destroyMethodName=(inferred); defined in my.study.springcorebasic.AppConfig beanDefinitionName = orderService beanDefinition = Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=appConfig; factoryMethodName=orderService; initMethodName=null; destroyMethodName=(inferred); defined in my.study.springcorebasic.AppConfig beanDefinitionName = discountPolicy beanDefinition = Root bean: class [null]; scope=; abstract=false; lazyInit=null; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=appConfig; factoryMethodName=discountPolicy; initMethodName=null; destroyMethodName=(inferred); defined in my.study.springcorebasic.AppConfig */ } } }} BeanDefinition 정보 BeanClassName: 생성할 빈의 클래스 명(자바 설정 처럼 팩토리 역할의 빈을 사용하면 없음) factoryBeanName: 팩토리 역할의 빈을 사용할 경우 이름, 예) appConfig factoryMethodName: 빈을 생성할 팩토리 메서드 지정, 예) memberService Scope: 싱글톤(기본값) lazyInit: 스프링 컨테이너를 생성할 때 빈을 생성하는 것이 아니라, 실제 빈을 사용할 때 까지 최대한 생성을 지연 처리 하는지 여부 InitMethodName: 빈을 생성하고, 의존관계를 적용한 뒤에 호출되는 초기화 메서드 명 DestroyMethodName: 빈의 생명주기가 끝나서 제거하기 직전에 호출되는 메서드 명 Constructor arguments, Properties: 의존관계 주입에서 사용한다. (자바 설정 처럼 팩토리 역할의 빈을 사용 하면 없음)","link":"/3-%EC%8A%A4%ED%94%84%EB%A7%81-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88%EC%99%80-%EC%8A%A4%ED%94%84%EB%A7%81-%EB%B9%88/"},{"title":"9. 함수형 프로그래밍을 이용한 디자인 패턴","text":"개요 Design Pattern Builder Pattern Decorator Pattern Strategy Pattern Template Mehtod Pattern Chain of Responsibility Pattern 1. Design Pattern 반복해서 등장하는 프로그래밍 문제들에 대한 해법들을 패턴화해놓은 것 패턴들을 숙지해놓으면 비슷한 문제가 생겼을 때 패턴들이 이정표가 되어줌 by Gang of Four(GoF) Design Pattern 의 종류 생성 패턴(Creational Patterns) 오브젝트의 생성에 관련된 패턴 구조 패턴(Structure Patterns) 상속을 이용해 클래스/오브젝트를 조합하여 더 발전된 구조로 만드는 패턴 행동 패턴(Behavior Patterns) 필요한 작업을 여러 객체에 분배하여 객체 간 결합도를 줄이게 해주는 패턴 Design Pattern &amp; Functional Programming 대부분의 디자인 패턴들은 구현에 많은 인터페이스/클래스/메서드를 필요로 함 함수형 프로그래밍을 이용해 몇몇 패턴을 좀 더 간단하게 구현할 수 있음 2. Builder Pattern 대표적인 생성 패턴 객체의 생성에 대한 로직과 표현에 대한 로직을 분리해 줌 객체의 생성 과정을 유연하게 해줌 객체의 생성 과정을 정의하고 싶거나 필드가 많아 constructor 가 복잡해질 때 유용 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import java.time.LocalDateTime;import java.util.ArrayList;import java.util.List;import java.util.Optional;import java.util.function.Consumer;public class User { private int id; private String name; private String emailAddress; private boolean isVerified; private LocalDateTime createdAt; private List&lt;Integer&gt; friendUserIds; public User(Builder builder) { this.id = builder.id; this.name = builder.name; this.emailAddress = builder.emailAddress; this.isVerified = builder.isVerified; this.createdAt = builder.createdAt; this.friendUserIds = builder.friendUserIds; } public int getId() { return id; } public String getName() { return name; } public Optional&lt;String&gt; getEmailAddress() { return Optional.ofNullable(emailAddress); } public boolean isVerified() { return isVerified; } public LocalDateTime getCreatedAt() { return createdAt; } public List&lt;Integer&gt; getFriendUserIds() { return friendUserIds; } public static Builder builder(int id, String name) { return new Builder(id, name); } public static class Builder { private int id; private String name; public String emailAddress; public boolean isVerified; public LocalDateTime createdAt; public List&lt;Integer&gt; friendUserIds = new ArrayList&lt;&gt;(); private Builder(int id, String name) { this.id = id; this.name = name; } public Builder with(Consumer&lt;Builder&gt; consumer) { consumer.accept(this); return this; } public User build() { return new User(this); } } @Override public String toString() { return &quot;User [id=&quot; + id + &quot;, &quot; + (name != null ? &quot;name=&quot; + name + &quot;, &quot; : &quot;&quot;) + (emailAddress != null ? &quot;emailAddress=&quot; + emailAddress + &quot;, &quot; : &quot;&quot;) + &quot;isVerified=&quot; + isVerified + &quot;, &quot; + (createdAt != null ? &quot;createdAt=&quot; + createdAt + &quot;, &quot; : &quot;&quot;) + (friendUserIds != null ? &quot;friendUserIds=&quot; + friendUserIds : &quot;&quot;) + &quot;]&quot;; }}import util.User;public class Example { public static void main(String[] args) { User user = User.builder(1, &quot;Alice&quot;) .with(builder -&gt; { builder.emailAddress = &quot;alice@fastcampus.co.kr&quot;; builder.isVerified = true; }).build(); System.out.println(user); }} FP를 사용하지 않을 경우 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package util;import java.time.LocalDateTime;import java.util.ArrayList;import java.util.List;import java.util.Optional;import java.util.function.Consumer;public class User { private int id; private String name; private String emailAddress; private boolean isVerified; private LocalDateTime createdAt; private List&lt;Integer&gt; friendUserIds; public User(Builder builder) { this.id = builder.id; this.name = builder.name; this.emailAddress = builder.emailAddress; this.isVerified = builder.isVerified; this.createdAt = builder.createdAt; this.friendUserIds = builder.friendUserIds; } public int getId() { return id; } public String getName() { return name; } public Optional&lt;String&gt; getEmailAddress() { return Optional.ofNullable(emailAddress); } public boolean isVerified() { return isVerified; } public LocalDateTime getCreatedAt() { return createdAt; } public List&lt;Integer&gt; getFriendUserIds() { return friendUserIds; } public static Builder builder(int id, String name) { return new Builder(id, name); } public static class Builder { private int id; private String name; public String emailAddress; public boolean isVerified; public LocalDateTime createdAt; public List&lt;Integer&gt; friendUserIds = new ArrayList&lt;&gt;(); private Builder(int id, String name) { this.id = id; this.name = name; } public Builder withEmailAddress(String emailAddress) { this.emailAddress = emailAddress; return this; } public Builder withVerified(boolean isVerified) { this.isVerified = isVerified; return this; } public Builder withCreatedAt(LocalDateTime createdAt) { this.createdAt = createdAt; return this; } public Builder withFriendUserIds(List&lt;Integer&gt; friendUserIds) { this.friendUserIds = friendUserIds; return this; } public User build() { return new User(this); } } @Override public String toString() { return &quot;User [id=&quot; + id + &quot;, &quot; + (name != null ? &quot;name=&quot; + name + &quot;, &quot; : &quot;&quot;) + (emailAddress != null ? &quot;emailAddress=&quot; + emailAddress + &quot;, &quot; : &quot;&quot;) + &quot;isVerified=&quot; + isVerified + &quot;, &quot; + (createdAt != null ? &quot;createdAt=&quot; + createdAt + &quot;, &quot; : &quot;&quot;) + (friendUserIds != null ? &quot;friendUserIds=&quot; + friendUserIds : &quot;&quot;) + &quot;]&quot;; }}import util.User;public class Example { public static void main(String[] args) { User user = User.builder(1, &quot;Alice&quot;) .withEmailAddress(&quot;alice@fastcampus.co.kr&quot;) .withVerified(true) .build(); System.out.println(user); }}","link":"/9-%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8C%A8%ED%84%B4/"},{"title":"Elasticsearch와 메모리 잠금: 성능과 안정성 최적화","text":"Elasticsearch는 고성능 검색 엔진으로, 메모리 집약적인 작업을 수행된다. 이러한 특성 때문에, 메모리 관리는 Elasticsearch 성능과 안정성에 있어 핵심적인 요소가 됨. 특히, 메모리 잠금 설정은 Elasticsearch 환경에서 중요한 부분 중 하나이다. 메모리 잠금이란? 메모리 잠금은 Elasticsearch가 운영 체제에 의해 스왑 아웃되지 않도록, 즉 메모리가 디스크로 옮겨지지 않도록 보장하는 설정 스왑 아웃은 메모리 접근 속도를 현저히 저하시킬 수 있기 때문에, 이를 방지하는 것이 중요 Elasticsearch에서는 더 나은 성능을 위해 필요한 만큼의 메모리에 데이터를 잠그는 작업을 수행 이를 통해, 가비지 컬렉션(Garbage Collection)으로 인한 지연이 줄어들고 성능이 향상됨 메모리 잠금의 중요성메모리 잠금 설정은 다음과 같은 이유로 중요 성능 최적화: 메모리 접근 시간을 최소화하여 Elasticsearch의 검색과 색인 생성 성능을 향상시. 스왑 방지: 운영 체제의 스왑 메커니즘으로 인한 성능 저하를 방 안정적인 운영: 예측 가능한 메모리 퍼포먼스를 유지하여 시스템의 전반적인 안정성을 높임 설정 시 주의사항메모리 잠금을 설정할 때는 다음과 같은 사항들을 고려해야 함 시스템 메모리 용량: 충분한 물리적 메모리가 있는지 확인해야 하며, 너무 많은 메모리를 잠그는 것은 다른 프로세스에 영향을 줄 수 있 적절한 힙 사이즈 설정: 힙 사이즈는 시스템 메모리의 일정 비율을 넘지 않도록 설정해야 함. 일반적으로 사용 가능한 메모리의 절반 정도가 권장됨 컨테이너 설정 고려: Docker와 같은 컨테이너 환경에서는 컨테이너 레벨에서의 메모리 제한과 함께 메모리 잠금 설정을 조정해야 할 수 있. 결론Elasticsearch의 메모리 잠금 설정은 성능 최적화와 시스템 안정성을 위해 필수적. 하지만, 시스템의 전반적인 메모리 용량과 다른 설정들과의 균형을 맞추는 것이 중요하기에 적절한 메모리 잠금 설정을 통해 Elasticsearch는 더 빠르고 안정적인 성능을 제공할 수 있음","link":"/Elasticsearch%EC%99%80-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%9E%A0%EA%B8%88-%EC%84%B1%EB%8A%A5%EA%B3%BC-%EC%95%88%EC%A0%95%EC%84%B1-%EC%B5%9C%EC%A0%81%ED%99%94/"},{"title":"Memo","text":"TO DOJava객체지향 객체지향 생활체조 https://jamie95.tistory.com/m/99 https://blogshine.tistory.com/m/241 HttpClient https://m.blog.naver.com/zzang9ha/222031902761 GSON https://hianna.tistory.com/629 https://stackoverflow.com/questions/22761852/simple-http-post-request-in-java-with-json-post-parameters https://dejavuhyo.github.io/posts/list-to-json-convert/ https://kim-jong-hyun.tistory.com/61 https://soft.plusblog.co.kr/61 Spring Entity, DAO, … 등 코딩 ArrayList to String → [] 나오는 것, 안나오는 것!! https://codechacha.com/ko/java-convert-arraylist-to-string/ 클린코드 https://naver.github.io/hackday-conventions-java/ https://withhamit.tistory.com/411 Python알고리즘 리스트 → String https://codechacha.com/ko/python-convert-list-to-string/ Python 배열 n개로 분할하기 https://choidr.tistory.com/entry/python-배열-n-개로-분할하기 Map https://dojang.io/mod/page/view.php?id=2286 https://blockdmask.tistory.com/531 https://blockdmask.tistory.com/520 함수형 프로그래밍List Comprehension https://velog.io/@mttw2820/List-Comprehension-문법-정리 동시 프로그래밍 https://nachwon.github.io/asyncio-futures/ Decorator https://niceman.tistory.com/168?category=940952 https://pythonprogramming.altervista.org/pythons-decorators-with-arguments/?doing_wp_cron=1668494200.8291680812835693359375 https://schoolofweb.net/blog/posts/파이썬-데코레이터-decorator/ https://engineer-mole.tistory.com/181 데코레이터 2개 사용 https://hello-bryan.tistory.com/214 Logger https://heodolf.tistory.com/47 - decorator python logging 라고 검색 데이터 과학Pandas Dataframe 랜덤으로 섞기 https://snepbnt.tistory.com/384 dataframe filter https://blog.hubspot.com/website/filter-rows-pandas 데이터 프레임 index 제거 https://www.delftstack.com/ko/howto/python-pandas/pandas-remove-index/ 데이터프레임 행과 열 제거 https://blog.naver.com/PostView.nhn?blogId=rising_n_falling&amp;logNo=221629326893 Category 고유 갯수 Count https://mizykk.tistory.com/103 딥러닝기본개념 엔트로피(Entropy)와 크로스 엔트로피(Cross-Entropy) https://melonicedlatte.com/machinelearning/2019/12/20/204900.html KoBERT https://velog.io/@seolini43/KOBERT로-다중-분류-모델-만들기-파이썬Colab https://velog.io/@dev-junku/KoBERT-모델에-대해 논문 How to Fine-Tune BERT for Text Classification? TDDMockitohttps://cobbybb.tistory.com/16 RESTRESTful API 설계 가이드 https://sanghaklee.tistory.com/57 https://meetup.toast.com/posts/92 https://spoqa.github.io/2012/02/27/rest-introduction.html https://velog.io/@couchcoding/개발-초보를-위한-RESTful-API-설계-가이드 https://sabarada.tistory.com/m/28 REST API란 https://thinkforthink.tistory.com/m/326 https://hqjang.tistory.com/m/67 Routes &amp; Endpoints https://developer.wordpress.org/rest-api/extending-the-rest-api/routes-and-endpoints/#routes-vs-endpoints ElasticSearch를 통한 REST API https://www.jopenbusiness.com/mediawiki/ElasticSearch_-_REST_API https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-trained-model.html https://victorydntmd.tistory.com/313 https://esbook.kimjmin.net/04-data/4.4-_search HTTP GET POST 방식의 차이 https://brilliantdevelop.tistory.com/m/33 FastAPI https://lucky516.tistory.com/m/110?category=1060055 https://lucky516.tistory.com/m/90?category=1060055 OpsLinux https://nota.tistory.com/53 https://moonuibee.tistory.com/4 GitReset https://velog.io/@boyeon_jeong/Git-Revert와-Reset으로-커밋-삭제하기-IntelliJ-Reset-Revert-사용하기 IntelliJ Google Code 포매터 https://withhamit.tistory.com/411 기타머신러닝과 정보이론 https://www.philgineer.com/2020/10/blog-post.html https://horizon.kias.re.kr/18474/ 좋은 글 개발자에서 개발팀장이 되기까지 겪은 다섯가지 큰 실수 https://muchtrans.com/translations/techie-tech-lead-my-5-biggest-mistakes.ko.html?fbclid=IwAR1uog7j0jxg6uE9yNtZ-u7KLQK4nXtRhCWYkV_ocxDHxffG59pxBMBa-8g 전 GitHub CTO, “지난 10년간 가장 큰 아키텍처 실수는 풀 마이크로서비스로 전환한 것” (twitter.com/jasoncwarner) https://news.hada.io/topic?id=7839 블로그티스토리 스킨 https://pronist.tistory.com/5 https://github.com/tistory-projects/tistory-skin-hELLO https://github.com/google/styleguide","link":"/Memo/"},{"title":"Finetuned Language Models are Zero-Shot Learners","text":"Abstract 이 논문은 LM의 zero-shot learning 능력을 향상시키기 위해 Instruction tuning 이라는 간단한 방법을 제안함 137B의 Pretrained LLM인 FLAN 및 instruction 템플릿들로 구성된 60개 이상의 NLP 데이터셋을 사용함 FLAN은 Zero-shot 성능이 175B의 GPT-3 모델보다 성능이 좋았음 1. Introduction GPT3와 같이 큰 규모의 LM 모델들에서 few-shot learning 의 성능은 좋았으나 zero-shot learning에서는 덜 성공적이었다. 우리는 LLM 에서 zero-shot 성능을 향상시키고자 간단한 방법인 Instruction Tuning을 소개하며 다음과 같은 환경에서 논문을 작성하였음 모델 : FLAN(Fine-tuned LAnguage Net) 모델, 137B의 Pretrained LLM을및 Instruction Tuning을 사용 데이터셋 : instruction 템플릿들로 구성된 60개 이상의 NLP 데이터셋을 사용 Instruction tuning 방법은 pretrain-finetune과 prompt-based 방법들의 장점들을 모두 가지고 있으며 finetuning을 통해 언어 모델의 추론 단계에서 text 상호작용르 향상 시킴 보지못한 tasks에 대하여 Zero-shot 성능을 평가하고자 NLP 데이터셋들을 클러스터링 하였고 각 클러스터에 대해 hold-out evalutation을 수행 2. FLAN : Instruction Tuning Imporves Zero-SHot Learning2.1 Tasks &amp; Templates 62개의 Text Dataset들을 12개의 Task cluster들로 분류함 자연어 이해 Task는 파란색 (지문을 읽고 문제를 맞추는 유형 등이 있음) 자연어 생성 task는 민트색 (대표적으로 요약과 번역이 있음) 각각의 데이터셋에 대한 Task를 작성하기 위해 10개의 Template들을 수동으로 구성하였음 2.4 Training Detatils Model Architecture and pretraining 137B 파라미터의 Decoder-only Transformer LM인 LaMDA-PT 에 instruction tuning을 사용하였음 Instruction tuning 절차 3. Results NLI, Reading comprehension, closed-book QA , translation, commonsense reasoning, coreference resolution, struct-to-text 등의 Task들에 대해 FLAN을 평가하였고 다음과 같은 결과를 얻음 Instruction Tuning은 NLI, QA, translation, struct-to-text 등의 Task들에 대해서는 매우 효과적이었음 다만 commonsense reasoning, coreference resolution task 들 처럼 instruction이 매우 중복되고 언어모델링으로 형식화된 테스크에 대해서는 덜 효과적 4. Ablation Studies &amp; Further AnalysisInstruction Tuning이 모델의 Zero-shot 성능을 올렸는지 알아보기 위해 ablation을 살펴보자 4.1 Number of instruction tuning clusters Cluster 즉 task들이 많아질수록 제로샷 성능이 늘어남을 확인할 수 있음 단, 이 실험은 어떤 instruction tuning 군집이 각 평가 군집에 가장 많은 기여를 하는지 결론을 내릴 수 없으며, 감정 분석 군집으로는 성능 향상이 거의 없다는 한계가 있었음. 4.2 Scaling Laws모델의 크기가 Instriction Tuning에 어떠한 영향을 끼쳤는가 평가를 위해 모델 Size: 422M, 2B, 8B, 68B, 137B 모델들을 실험군으로 사용 100B 이상의 모델에서는 Instruction Model이 Untuned Model에 비해 상당히 성능이 향상됨 하지만 8B이하의 모델에서는 untuned 모델에 비해 성능이 떨어짐 그 이유는 instruction 이 추가된 40여개의 tasks들을 학습하는 데에 모델의 크기가 작아서 그럴 것으로 추정됨 4.3 Role of Instructions 다음의 상황들로 테스트를 수행 No Instrction : input과 output 만 넣음(Template이 없는 상태) (ex. 번역) input : “The dog runs.” output : “Le chien court Dataset 이름만 적음: input : “[Translation: WMT’14 to French] The dog runs.” FLAN의 Finetuning 방법(Instruction Tuning) input : “Please translate this sentence to French: ‘The dog runs.” 그 결과 FLAN의 Instructions가 zero-shot 성능에서 가장 우수했음 4.4 Instructions with Few-shot exemplars 이전까지는 instruction tuning에서 zero-shot setting에 대해서 알아보았고 여기서는 few-shots 상황에서 instruction이 어떻게 사용될지에 대해서 말함 few-shot instruction 예제들은 모든 task cluster에서 zero-shot instruction 보다 뛰어났음 특히 struct to text, translation closed-book QA와 같이 크고 복잡한 출력을 갖는 예제에서 성능이 더 좋았음, exemplars로 인해 모델이 더 잘 이해했기 때문 또한 표준편차가 few-shot FLAN 에서 더 낮았음 4.5 Intruction Tuning Facilitates Prompt Tuning FLAN 모델이 Soft Prompt로 추론을 할 때에도 Untuned model 보다 성능이 훨씬 높음 5. Conclusion Intruction tuning 방법을 통해 규모있는 LM에서 zero-shot task들에 대해 성능을 높였고 그 결과 FLAN은 GPT-3 보다 성능이 높았으며 언어모델이 instrcion을 따를 수 있는 잠재적인 능력을 확인하였음","link":"/Finetuned-Language-Models-are-Zero-Shot-Learners/"},{"title":"OpenSearch 란 무엇인가","text":"OpenSearch Elasticsearch 7.10.2와 Kibana 7.10.2에서 포크된 프로젝트로 OpenSearch 및 OpenSearch Dashboards를 포함 분산형 커뮤니티 기반 100% 오픈 소스 검색 및 분석 제품군으로, 실시간 애플리케이션 모니터링, 로그 분석 및 웹 사이트 검색과 같이 다양한 사용 사례에 사용됨 OpenSearch는 데이터 탐색을 쉽게 해주는 통합 시각화 도구 OpenSearch 대시보드와 함께 대량 데이터 볼륨에 빠르게 액세스하고 응답하며 뛰어난 확장성을 지닌 시스템을 제공 Apache Lucene 검색 라이브러리로 구동되며 k-nearest neighbors(KNN) 검색, SQL, Anomaly Detection, Machine Learning Commons, Trace Analytics, 전체 텍스트 검색 등 다수의 검색 및 분석 기능을 지원 OpenSearch가 만들어진 이유 2021년 1월 21일, Elastic NV는 소프트웨어 라이선스 전략을 변경하는 바, 허용적 라이선스인 Apache License Version 2.0(ALv2) 하에서 Elasticsearch 및 Kibana의 새로운 버전을 더 이상 릴리스하지 않는다고 발표 그 대신 Elastic은 Elastic License 또는 Server Side Public License(SSPL) 하에서 사용할 수 있는 소스 코드를 사용 OpenSearch 라이선스 OpenSearch 프로젝트의 모든 소프트웨어는 Apache License, 버전 2.0 (ALv2) 하에 배포 원하는 방식으로 사용, 수정, 확장, 수익화, 재판매할 수 있다는 100% 오픈 소스 제품의 이점을 누릴 수 있음 OpenSearch는 Amazon OpenSearch Service와 어떤 관련이 있나 Amazon OpenSearch Service는 인프라 관리, 모니터링 및 유지 관리에 대한 걱정이나 OpenSearch 클러스터 운영에 대한 심층적인 전문성을 쌓을 필요 없이 OpenSearch 클러스터를 실행하고 확장할 수 있는 AWS 관리형 서비스 OpenSearch에 대한 지원은 2021년 9월에 Amazon OpenSearch Service에서 버전 1.0부터 시작되었음 OpenSearch가 제공하는 기능 기능 이점 https://github.com/opensearch-project/security 암호화, 인증, 권한 부여, 감사 기능을 제공합니다. 여기에는 Active Directory, LDAP, SAML, Kerberos, JSON 웹 토큰 등과의 통합이 포함됩니다. OpenSearch는 또한 인덱스, 문서 및 필드에 대해 세분화된 역할 기반 액세스 제어를 제공합니다. https://opensearch.org/docs/latest/opensearch/ux/ 검색 경험을 사용자 지정하는 데 도움이 되는 다수의 기능을 제공합니다. 예를 들어 전체 텍스트 쿼리, 자동 완성, 스크롤 검색, 사용자 지정 가능한 점수 및 순위 지정 등이 있습니다. https://github.com/opensearch-project/sql 익숙한 SQL 쿼리 구문을 제공합니다. 데이터를 확인할 때 각종 집계, group by, where 절을 사용할 수 있습니다. 데이터를 JSON 문서 또는 CSV 테이블로 읽어 자신의 작업에 가장 어울리는 형식으로 사용할 수 있는 유연성을 확보합니다. https://opensearch.org/docs/latest/search-plugins/sql/sql-full-text/ 유사 일치, 부스팅, 구문 일치 등과 같은 풍부한 세트의 검색 기능에 액세스하는 동안 익숙한 SQL 쿼리 구문을 사용할 수 있습니다. https://opensearch.org/docs/latest/clients/data-prepper/index/ Data Prepper는 다운스트림 분석 및 시각화를 위해 데이터를 필터링, 보강, 변환, 정규화 및 집계할 수 있는 서버 측 데이터 수집기입니다. Data Prepper를 사용하면 사용자 지정 파이프라인을 구축하여 애플리케이션의 운영 보기를 개선할 수 있습니다. https://opensearch.org/docs/latest/observability-plugin/trace/index/ 추적 분석은 OpenSearch에서 https://opentelemetry.io/ 데이터를 수집하고 시각화할 수 있는 방법을 제공합니다. 이 데이터는 분산형 애플리케이션에서 성능 문제를 찾고 수정하는 데 도움이 될 수 있습니다. https://opensearch.org/docs/latest/observability-plugin/app-analytics/ 애플리케이션 분석을 사용하면 시스템의 가용성 상태를 볼 수 있는 사용자 지정 관측성 애플리케이션을 만들 수 있습니다. 여기에서 로그 이벤트와 추적 및 지표 데이터를 전체 시스템 상태를 보여주는 단일 보기로 결합할 수 있습니다. 이렇게 하면 로그, 추적 및 지표 사이를 빠르게 피벗하여 문제의 원인으로 깊이 들어갈 수 있습니다. https://github.com/opensearch-project/piped-processing-language 파이프 처리 언어는 데이터 쿼리에 파이프( https://opensearch.org/docs/latest/observability-plugin/operational-panels/ https://opensearch.org/docs/latest/observability-plugin/ppl/index(PPL)를 사용하여 생성된 관측성 시각화를 정렬하는 운영 패널을 구축합니다. https://opensearch.org/docs/latest/observability-plugin/event-analytics/ https://opensearch.org/docs/latest/observability-plugin/ppl/index(PPL) 쿼리를 사용하여 추적 로그 상관 관계를 포함한 데이터의 다양한 시각화를 대화형으로 구축하고 확인합니다. https://opensearch.org/docs/latest/ml-commons-plugin/index/ kmeans 및 이상 탐지 같은 다양한 기계 학습 알고리즘을 사용하여 모델을 훈련하고 데이터 동향을 예측합니다. ML Commons는 PPL 및 REST API와 직접 통합됩니다. https://github.com/opensearch-project/dashboards-reports 대시보드, 저장된 검색, 알림 및 시각화에서 보고서를 예약하고 내보내며 공유합니다. https://github.com/opensearch-project/anomaly-detection https://github.com/aws/random-cut-forest-by-aws에 기반한 이상 탐지 기계 학습을 활용해 데이터가 수집될 때 자동으로 이상을 탐지합니다. https://github.com/opensearch-project/alerting과 결합하여 데이터를 실시간에 가깝게 모니터링하고 자동으로 경고 알림을 전송합니다. https://github.com/opensearch-project/index-management 사용자 지정 정책을 정의해 롤오버나 삭제와 같은 인덱스 관리 태스크 루틴을 자동화하여 인덱스 및 인덱스 패턴에 적용합니다. https://opensearch.org/docs/latest/im-plugin/index-transforms/index/ 특정 필드를 중심으로 데이터의 요약 보기를 만들어 데이터를 다양한 방식으로 시각화하거나 분석하는 데 사용합니다. 예를 들어 여러 필드와 범주에 흩어져 있는 항공사 데이터가 있다고 할 때 항공사, 구간 및 요금별로 정렬된 이 데이터의 요약 보기를 만들려고 합니다. 이 경우 변환 작업을 사용하여 이 특정 범주로 정렬된 요약 인덱스를 새로 만들 수 있습니다. https://opensearch.org/docs/latest/im-plugin/index-rollups/index/ 관심 필드를 선택하고 인덱스 롤업을 사용하여 이 필드만 넓은 범위의 시간 버킷으로 집계하는 새 인덱스를 만듭니다. 이렇게 하면 동일한 쿼리 성능과 아주 저렴한 비용으로 월별 또는 연간 히스토리성 데이터를 저장할 수 있습니다. https://github.com/opensearch-project/performance-analyzer 다수의 클러스터 성능 지표 및 집계를 쿼리합니다. PerfTop 명령줄 인터페이스(CLI)를 사용해 해당 지표를 빠르게 표시하고 분석합니다. 근본 원인 분석(RCA) 프레임워크를 사용해 클러스터의 성능과 신뢰성 문제를 조사합니다. https://github.com/opensearch-project/asynchronous-search 쿼리 시간 초과를 염려하지 않고 복잡한 쿼리를 실행하며 비동기식 검색으로 검색 쿼리가 백그라운드에서 실행됩니다. 쿼리의 진척 상황을 추적하고 사용 가능한 부분적 결과를 조회합니다. https://github.com/opensearch-project/trace-analytics 배포된 애플리케이션에 대해 OpenTelemetry 데이터를 수집하고 시각화합니다. 해당 애플리케이션 간의 이벤트 흐름을 시각화하여 성능 문제를 식별합니다. https://github.com/opensearch-project/alerting 자동으로 데이터를 모니터링하고 경고 알림을 이해 관계자에게 자동으로 전송합니다. 직관적인 인터페이스와 강력한 API로 알림 설정, 관리, 모니터링이 쉬워집니다. OpenSearch의 완전한 쿼리 언어와 스크립팅 기능으로 특정 알림 조건을 섬세하게 설정합니다. https://opensearch.org/docs/latest/monitoring-plugins/alerting/monitors/#create-monitors 데이터의 그룹화된 동향에 대한 알림을 전송하는 알림 정책을 만듭니다. 예를 들어 평균 CPU가 원하는 임계값을 초과하는 각 호스트에 대한 알림을 받을 수 있습니다. https://opensearch.org/docs/latest/replication-plugin/index/ 인덱스, 매핑 및 메타데이터를 한 OpenSearch 클러스터에서 다른 클러스터로 복제하여 클러스터 간 중복성을 만들거나 보고 쿼리를 보조 클러스터로 오프로드합니다. https://github.com/opensearch-project/k-NN 기계 학습을 사용해 수 십억의 문서에서 수 천 차원에 걸친 작업도 가장 가까운 이웃 검색 알고리즘을 정규 OpenSearch 쿼리를 실행하는 것과 마찬가지로 편하게 실행합니다. 집계와 필터 조항을 사용해 유사성 검색 작업을 구체화합니다. k-NN 유사성 검색은 생산 권고 사항, 사기 탐지, 이미지 및 동영상 검색, 관련 문서 검색 등 다양한 사용 사례에 도움이 됩니다. https://github.com/opensearch-project/dashboards-notebooks 대시보드, 시각화, 텍스트 등을 결합하여 데이터 분석에 컨텍스트와 상세한 설명을 제공합니다. https://opensearch.org/docs/latest/clients/index/ OpenSearch는 Go, JavaScript, Python, Java 및 기타 다양한 언어 클라이언트를 지원합니다. 이러한 클라이언트를 사용하여 OpenSearch와 직접 통합되는 애플리케이션을 구축할 수 있습니다. OpenSearch Stack수집 및 정제 Data prepper (https://opensearch.org/docs/1.2/clients/data-prepper/index/) : Server side collector / logstash를 fork해서 나왔고 56프로 더 빠르다고 함. Beats Fluentd (https://docs.fluentd.org/output/opensearch) logstash 대시보드 그라파나 오픈서치 대시보드 참고 https://aws.amazon.com/ko/what-is/opensearch/ https://aws.amazon.com/ko/blogs/opensource/introducing-opensearch/ https://velog.io/@sosimina/OpenSearch에-대해-알아보자","link":"/OpenSearch-%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80/"},{"title":"OpenSearch 설치","text":"Docker 방식으로 OpenSearch를 설치하는 내용을 기술 1. 환경 설정리눅스 환경 Memory paging 및 swapping 성능을 비활성화 시키기 (성능 향상을 위함) 1sudo swapoff -a Memory map 의 수를 증가시키기 12345678910111213# Edit the sysctl config filesudo vi /etc/sysctl.conf# Add a line to define the desired value# or change the value if the key exists,# and then save your changes.vm.max_map_count=262144# Reload the kernel parameters using sysctlsudo sysctl -p# Verify that the change was applied by checking the valuecat /proc/sys/vm/max_map_count 윈도우 환경 Memory map 의 수를 증가시키기 12wsl -d docker-desktopsysctl -w vm.max_map_count=262144 2. OpenSearch를 Docker container로 실행시키기Docker Pull Docker Hub (추천) 1234# 24.03.05 기준 2.12.0 버전이 최신이며 해당 버전을 설치docker pull opensearchproject/opensearch:2.12.0# 가장 최신 버전 자동 설치docker pull opensearchproject/opensearch:2 1234# 24.03.05 기준 2.12.0 버전이 최신이며 해당 버전을 설치docker pull opensearchproject/opensearch-dashboards:2.12.0# 가장 최신 버전 자동 설치docker pull opensearchproject/opensearch-dashboards:2 Amazon ECR 1docker pull public.ecr.aws/opensearchproject/opensearch:2 1docker pull public.ecr.aws/opensearchproject/opensearch:2 Docker run Docker container 기동 2.12 버전 이후부터는 패스워드 입력을 해야함 ※ 패스워드 규칙이 엄격함 : 대문자 + 소문자 + 특수문자 + 숫자 등의 조합이 필요함 12docker run -d --name opensearch -p 9200:9200 -p 9600:9600 -e &quot;discovery.type=single-node&quot; -e &quot;OPENSEARCH_INITIAL_ADMIN_PASSWORD=&lt;custom-admin-password&gt;&quot; opensearchproject/opensearch:2.12.0docker run -d --name opensearch -p 9200:9200 -p 9600:9600 -e &quot;discovery.type=single-node&quot; -e &quot;OPENSEARCH_INITIAL_ADMIN_PASSWORD=Tjghks#01&gt;&quot; opensearchproject/opensearch:2.12.0 기동 확인 (Window의 경우 git bash 등을 통해서 curl 명령어를 사용) 1curl &lt;https://localhost:9200&gt; -ku 'admin:&lt;custom-admin-password&gt;' 응답 결과가 다음과 같아야 함 1234567891011121314151617{ &quot;name&quot; : &quot;a937e018cee5&quot;, &quot;cluster_name&quot; : &quot;docker-cluster&quot;, &quot;cluster_uuid&quot; : &quot;GLAjAG6bTeWErFUy_d-CLw&quot;, &quot;version&quot; : { &quot;distribution&quot; : &quot;opensearch&quot;, &quot;number&quot; : &lt;version&gt;, &quot;build_type&quot; : &lt;build-type&gt;, &quot;build_hash&quot; : &lt;build-hash&gt;, &quot;build_date&quot; : &lt;build-date&gt;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &lt;lucene-version&gt;, &quot;minimum_wire_compatibility_version&quot; : &quot;7.10.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;7.0.0&quot; }, &quot;tagline&quot; : &quot;The OpenSearch Project: &lt;https://opensearch.org/&gt;&quot;}","link":"/OpenSearch-%EC%84%A4%EC%B9%98/"},{"title":"TrainingArguments - padding과 max_length","text":"Transformers의 Tokenizer 라이브러리를 통해 학습을 할때 padding과 max_length 설정을 어떻게 해야할까? 모델 아키텍처가 어떤 것인지 확인해보기 크게 GPT 계열의 모델과 BERT 모델의 계열로 나누어서 생각해보자. GPT 계열 모델 GPT계열의 LLM은 기본적으로 pad_token을 가지고 있지 않고, eos_token을 사용하여 입력 시퀀스의 끝을 나타냄 따라서 transformers로 Tokenizer로 학습을 하려면 반드시 tokenizer.pad_token = tokenizer.eos_token 을 설정해야한다. GPT 모델의 특성 PT 계열의 대규모 언어 모델(Large Language Models, LLM)은 기본적으로 **pad_token**을 포함하지 않음 대신, 이 모델들은 **eos_token**을 사용하여 입력 시퀀스의 끝을 나타냄 이는 시퀀스 생성 과정에서 모델이 언제 문장이 끝났는지를 인식하는 데 중요한 역할을 함 Tokenizer 설정 GPT 계열 모델을 사용하여 학습을 진행할 때, Transformers 라이브러리의 **Tokenizer**를 적절히 설정해야 합니다. 이를 위해 **tokenizer.pad_token = tokenizer.eos_token**으로 설정해야 함(필수?) 이렇게 설정함으로써, 모델은 패딩된 부분을 시퀀스의 끝으로 인식하고, 해당 부분을 무시할 수 있게 됨 special_tokens_map.json 을 보면 다음 세가지 밖에 존재하지 않는다. 12345678910111213141516171819202122{ &quot;bos_token&quot;: { &quot;content&quot;: &quot;&lt;s&gt;&quot;, &quot;lstrip&quot;: false, &quot;normalized&quot;: false, &quot;rstrip&quot;: false, &quot;single_word&quot;: false }, &quot;eos_token&quot;: { &quot;content&quot;: &quot;&lt;/s&gt;&quot;, &quot;lstrip&quot;: false, &quot;normalized&quot;: false, &quot;rstrip&quot;: false, &quot;single_word&quot;: false }, &quot;unk_token&quot;: { &quot;content&quot;: &quot;&lt;unk&gt;&quot;, &quot;lstrip&quot;: false, &quot;normalized&quot;: false, &quot;rstrip&quot;: false, &quot;single_word&quot;: false } BERT 계열 모델 BERT 계열 모델의 특성 BERT 계열 모델은 일반적으로 pad_token*이 정의되어 있으며, 시퀀스의 시작(*[CLS]**)과 끝([SEP]**)을 나타내는 특별한 토큰을 사용 Tokenizer 설정 BERT 모델의 경우, **padding=&quot;max_length&quot;**로 설정하는 것이 일반적 이는 모든 시퀀스가 동일한 최대 길이를 갖도록 함 이 방식은 모델이 고정된 길이의 입력을 처리할 수 있게 하며, 특히 BERT와 같은 모델이 문장 관계를 학습하는 데 유용 padding 및 max_length 옵션에 대해 이해를 해보자 Padding 옵션 True: 가장 긴 시퀀스 길이에 맞추어 다른 모든 시퀀스를 패딩,이는 배치 처리 시 유용 False 또는 None: 패딩을 수행하지 않음. 모든 시퀀스는 원래의 길이를 유지 “max_length”: 모든 시퀀스를 max_length에 지정된 길이로 패딩합니다. 이는 고정된 길이의 입력을 처리하는 데 적합 “longest”: True와 유사하게 가장 긴 시퀀스에 맞추어 나머지 시퀀스를 패딩합니다. 하지만 이 옵션은 명시적으로 가장 긴 시퀀스를 기준으로 삼는 것을 강조 Max_length 옵션 max_length 값 설정: max_length는 모델이 처리할 수 있는 최대 시퀀스 길이를 지정, 긴 입력을 적절한 크기로 잘라내는 데 사용되며, 메모리 제한이나 모델 아키텍처의 제한을 고려할 때 중요 integer 값으로 설정을 해도 됨 추가 옵션 truncation: True로 설정하면, max_length보다 긴 시퀀스는 잘려나가서 max_length 길이에 맞춰짐 False 또는 None으로 설정하면, 입력 시퀀스가 max_length를 초과해도 잘리지 않음 return_tensors: 출력 데이터 타입을 지정합니다. 예를 들어, pt는 PyTorch 텐서, tf는 TensorFlow 텐서, np는 NumPy 배열을 반환 return_attention_mask: True로 설정하면, 모델이 어떤 부분이 실제 데이터이고 어떤 부분이 패딩인지를 구분할 수 있는 attention mask를 반환 Padding 및 max_legnth 를 설정하기 GPT 모델일 경우 개별적이거나 자연스러운 텍스트 생성을 중시하는 경우에는 padding=False가 더 합리적일 수 있음 (개인 생각: 일반적으로는 False가 가장 합리적일 것 같음) 배치 처리의 필요성이 높고, 모든 입력을 동일한 길이로 맞춰야 하는 경우에는 padding=True가 적합할 수 있음 (개인생각: **max_length**와 longest 옵션은 고정된 길이의 입력 처리에 적합하지만, GPT와 같은 생성 모델에는 일반적으로 적합하지 않을 수 있을 것 같음) 좀더 자세히 알아보자 Padding을 *False로 설정하는 경우*: 자연스러운 문장 생성: GPT 계열 모델은 주로 텍스트 생성에 사용되기에. padding=False를 설정하면, 모델은 각 입력 시퀀스의 실제 길이를 유지하며, 이는 보다 자연스러운 문장 생성에 도움이 될 수 있음 불필요한 패딩 방지: 특히 단일 입력을 처리할 때, 불필요한 패딩을 추가하는 것을 방지할 수 있습니다. 이는 모델이 더 효율적으로 작동하도록 하며, 특히 작은 데이터셋에서 유용할 수 있습니다. (개인 생각 : . GPT 모델은 연속적인 텍스트 생성에 최적화되어 있으므로, 패딩 없이 실제 데이터의 길이를 유지하는 것이 더 자연스러운 결과를 가져올 수 있을 것) Padding을 True로 설정하는 경우**:** 배치 처리에 유리: 여러 입력 시퀀스를 동시에 처리해야 하는 경우, padding=True는 모든 시퀀스가 같은 길이를 갖도록 보장, 이는 배치 처리에서 중요하며, 특히 GPU를 사용하는 경우 연산 효율성을 높일 수 있음 동적 길이의 입력 처리: 모든 입력이 동일한 길이를 가짐으로써 모델이 더 일관된 방식으로 데이터를 처리할 수 있습니다. 이는 모델의 학습과 추론 성능에 긍정적인 영향을 줄 수 있습니다. (개인 생각 : , GPT의 경우 일반적으로 생성 작업에 사용되고 배치 처리가 중요하지 않기에 False가 유리할 것 같음) Padding을 max_length로 설정하는 경우 고정된 길이의 입력 처리, Padding을 longest로 설정하는 경우 가장 긴 시퀀스 기준의 패딩으로 True와 같이 좀 더 적절하지 않아 보임 BERT 모델의 경우 배치 처리를 위해 일관된 길이가 필요하다면 max_length 또는 **longest**가 적합할 수 있고 각 시퀀스의 원래 길이를 유지하려면 **False**가 적합할 수 있음 좀더 자세히 알아보자 Padding을 1True 로 설정하는 경우: 동적 길이의 입력 처리: True로 설정하면 가장 긴 시퀀스에 맞추어 나머지 시퀀스를 패딩합니다. 이는 동적 길이의 입력을 처리하는 데 유용하며, 특히 다양한 길이의 문장들이 섞여 있는 데이터셋에 적합합니다. 배치 처리에 효율적: 여러 시퀀스를 한 번에 처리할 때, 모든 시퀀스가 같은 길이를 갖도록 하여 GPU 등의 자원을 효율적으로 사용할 수 있습니다. Padding을 1False 로 설정하는 경우: 패딩 없음: 시퀀스의 실제 길이를 유지하며, 패딩을 추가하지 않습니다. 이는 각 문장의 원래 길이를 유지하고자 할 때 유용합니다. 개별 시퀀스 처리에 적합: 단일 문장 분류나 특정 작업에서 각 문장의 실제 길이를 유지하는 것이 중요할 수 있습니다. Padding을 1max_length 로 설정하는 경우: 고정된 길이의 입력 처리: 모든 시퀀스를 max_length에 지정된 길이로 패딩합니다. 이는 특히 모델이 고정된 길이의 입력을 요구하는 경우에 적합합니다. 일관된 입력 길이: 훈련과 추론 시 일관된 길이의 입력을 제공하여 모델 성능을 향상시킬 수 있습니다. Padding을 1longest 로 설정하는 경우: 가장 긴 시퀀스 기준의 패딩: 가장 긴 시퀀스에 맞추어 나머지 시퀀스를 패딩합니다. 이는 True와 유사하지만, 가장 긴 시퀀스를 명시적으로 기준으로 삼습니다. 배치 처리에 적합: 여러 시퀀스를 한 번에 처리하는 배치 처리에 적합하며, 각 배치 내에서 가장 긴 시퀀스에 맞추어 패딩을 진행합니다. padding과 max_length 에 따른 토크나이즈 결과를 확인해보자 123456789101112131415161718192021import torchfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfigfrom peft import ( prepare_model_for_kbit_training, LoraConfig, get_peft_model)model_id = &quot;huggyllama/llama-7b&quot;bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=&quot;nf4&quot;, bnb_4bit_compute_dtype=torch.bfloat16)model = AutoModelForCausalLM.from_pretrained( model_id, quantization_config=bnb_config ) Case 01: padding=False, max_length=16 123456789101112131415161718tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_eos_token=True)tokenizer.pad_token = tokenizer.eos_tokentokenizer.padding_side = 'right'sample_sentence = [&quot;Hello, world!&quot;, &quot;Hello, Alpaca world!&quot;]context_length = 16tokenized_output = tokenizer( sample_sentence, truncation=True, max_length = context_length, return_overflowing_tokens=False, return_length=True, padding=False,)print(tokenized_output)# {'input_ids': [[1, 15043, 29892, 3186, 29991, 2], [1, 15043, 29892, 838, 29886, 11989, 3186, 29991, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'length': [6, 9]} Case 02: padding=Ture, max_length=16 123456789101112131415161718tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_eos_token=True)tokenizer.pad_token = tokenizer.eos_tokentokenizer.padding_side = 'right'sample_sentence = [&quot;Hello, world!&quot;, &quot;Hello, Alpaca world!&quot;]context_length = 16tokenized_output = tokenizer( sample_sentence, truncation=True, max_length = context_length, return_overflowing_tokens=False, return_length=True, padding=True,)print(tokenized_output)# {'input_ids': [[1, 15043, 29892, 3186, 29991, 2, 2, 2, 2], [1, 15043, 29892, 838, 29886, 11989, 3186, 29991, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'length': [9, 9]} Case 03: padding=’max_length’, max_length=16 123456789101112131415161718tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, add_eos_token=True)tokenizer.pad_token = tokenizer.eos_tokentokenizer.padding_side = 'right'sample_sentence = [&quot;Hello, world!&quot;, &quot;Hello, Alpaca world!&quot;]context_length = 16tokenized_output = tokenizer( sample_sentence, truncation=True, max_length = context_length, return_overflowing_tokens=False, return_length=True, padding='max_length',)print(tokenized_output)# {'input_ids': [[1, 15043, 29892, 3186, 29991, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 15043, 29892, 838, 29886, 11989, 3186, 29991, 2, 2, 2, 2, 2, 2, 2, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], 'length': [16, 16]}","link":"/TrainingArguments-padding%EA%B3%BC-max-length/"},{"title":"데이터 및 모델 최적화(데이터 누수, 과대적합과 일반화)","text":"데이터 분할을 실행하면서 랜덤 샘플링으로 인해 Train/Valid/Test Set 데이터로 나누는데, 이때 데이터 누수(Data Leakage)가 발생하는지 반드시 확인해야 한다. 데이터 누수(Data Leakage)실제 운영 상태 사용자가 얻을 수 없는 정보를 Train 과정에서 모델이 얻을 때 발생 → Valid/Test Set으로 검증 시, 모델 성능을 부풀림 → 하지만 실제 운영 환경에서는 훨씬 나빠질 수 있음 테스트를 수행 시 모델이 완벽에 가깝게 동작할 경우 강하게 의심을 해보아야함→ 머피의 법칙(Murphy’s law) 으로 데이터/모델 파이프로인에 데이터 누수 혹은 버그일 가능성이 높음 데이터 누수 사례 미래 이벤트를 예측하는 데이터셋 → 시계열 문제로, 시계열에 랜덤 샘플링을 하면 안됨 중복 샘플을 가진 데이터셋 1개의 샘플을 여러 번 사용한 데이터셋 대표성 있는 데이터 Train/Test Set으로 나누기 전에 반드시 데이터를 무작위로 섞는 것이 일반적 그렇지 않을 경우 0-7만 Train Set, 8-9만 Test Set에 포함 ※ 참고 : sklearn train_test_split 모듈에 startify 및 random_state 활용 sklearn train_test_split 옵션 설명 test_size: 테스트 셋 구성의 비율을 나타냅니다. train_size의 옵션과 반대 관계에 있는 옵션 값이며, 주로 test_size를 지정해 줍니다. 0.2는 전체 데이터 셋의 20%를 test (validation) 셋으로 지정하겠다는 의미입니다. default 값은 0.25 입니다. shuffle: default=True 입니다. split을 해주기 이전에 섞을건지 여부입니다. 보통은 default 값으로 놔둡니다. stratify: default=None 입니다. classification을 다룰 때 매우 중요한 옵션값입니다. stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해 줍니다. (한 쪽에 쏠려서 분배되는 것을 방지합니다) 만약 이 옵션을 지정해 주지 않고 classification 문제를 다룬다면, 성능의 차이가 많이 날 수 있습니다. random_state: 세트를 섞을 때 해당 int 값을 보고 섞으며, 하이퍼 파라미터를 튜닝시 이 값을 고정해두고 튜닝해야 매번 데이터셋이 변경되는 것을 방지할 수 있습니다. ※ 참고 : https://teddylee777.github.io/scikit-learn/train-test-split 5. 샘플 오염 * 일상적인 작업에서 감지하기 어려운 형태로 일어날 수 있음 * 데이터베이스 버전 관리 부족이 한 예 * v1 버전의 DB로 학습을 수행하고 v2 버전(미래 데이터가 추가 누적된 상태)으로 예측을 해야하는데, 관리 실수로 v2 버전으로 학습을 수행해 미래 데이터를 치팅을 해버렸다는 의미 * 학생들의 수필 점수를 예측하는 모델의 예 (높은 수준의 도메인 이해가 필요함) * 많은 학생들이 여러 개의 수필을 썼으며, 수필 데이터를 랜덤하게 Train/Test Set으로 나눌 경우 문제 * 보통은 위와 같이 작업하며, 문제가 없다고 생각하나 위의 도메인에서는 문제가됨 * 수필 데이터로 나누어 한 학생이 작성한 수필이 Train/Test Set으로 골고루 들어가, 한 학생이 작성한 수필은 정확한 예측 결과를 받을 수 있음 * 하지만 이전에 본 적 없는 학생의 점수는 잘 예측하지 못함 (단순히 Train에서 보았던 수필 작성자의 점수를 예측으로 내놓을뿐임) * 이러한 경우에는 수필단위가 아니라 학생 단위로 분할해서 데이터 누수를 해결해야함 모델 최적화훈련 성능 향상 - 과대 적합 만들기 → 일반화 성능화 개선 훈련 성능 향상 시키기 경사하강법의 핵심 파라미터 튜닝 Optimizer 선택 모델 가중치 초기값 분포 학습률 배치 크기 구조 Task에 적합한 모델을 선택 - 여러 베이스라인을 구축 및 형성 모델 용량 늘리기 층을 늘리기 층을 추가 일반화 성능 향상 시키기 데이터셋 큐레이션 훈련 데이터를 더 모음 혹은 더 나은 데이터를 모음 레이블 할당 에러 최소화 누락된 값 처리 더 나은 특성을 개발 네트워크의 용량을 감소시킴 가중치 규제를 추가 드롭아웃을 추가","link":"/%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20%E1%84%8E%E1%85%AC%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%92%E1%85%AA(%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%82%E1%85%AE%E1%84%89%E1%85%AE,%20%E1%84%80%E1%85%AA%E1%84%83%E1%85%A2%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%86%B8%E1%84%80%E1%85%AA%20%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%87%E1%85%A1%E1%86%AB%E1%84%92%E1%85%AA)/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/hello-world/"},{"title":"","text":"딥러닝 CUDA 작업 환경 구성해당 내용은 서버의 GPU CUDA 환경과 딥러닝 프로젝트의 GPU CUDA 환경이 다를 경우의 문제를 어떻게 해결해야 하는지를 위해 작성 CUDA 환경 가이드라인 서버의 기본 CUDA 버전은 최신 CUDA 버전을 사용 개발 환경으로 anaconda 또는 py env를 사용 시, 서버의 기본 CUDA 버전 보다 낮으면 CUDA의 하위호환성으로 인해 정상적으로 사용이 가능 운영 환경으로 Docker 컨테이너를 사용 시, 서버의 기본 CUDA 버전 보다 낮으면 CUDA의 하위호환성으로 인해 정상적으로 사용이 가능 예시1 서버의 CUDA 버전 : 12.3 (23.11.08 기준 최신 버전) 프로젝트 버전 ㄴ CUDA 11.8 사용 가능 CUDA 9.0 사용 가능 예시2 서버의 CUDA 버전 : 11.8 프로젝트 버전 CUDA 12.2 사용 불가 CUDA 11.6 사용 가능 개발 환경실습 환경 서버 환경 : CUDA 12.2 가상(사이드 프로젝트) 환경 : CUDA 11.8 서버 환경 - CUDA Toolkit, Nvidia-Driver, CuDNN이 설치 되어졌음을 가정 bashrc 에 다음 내용 추가 123# Use CUDA 12.2 Version export PATH=/usr/local/cuda-12.2/bin:$PATHexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.2/lib64 가상 환경 생성(anaconda) conda create -n {myenvname} cudatoolkit={cuda_version} python={python_version} 해당 명령어의 결과 가상환경이 활성화 될 때 bashrc가 다음과 같은 식으로 적용이 됨 12export PATH=/usr/local/cuda-11.8/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH ※ bashrc 파일 자체에는 변화는 없음, 하지만 echo를 날려보면 다음과 같이 적용이 되어져 있음 12345echo $LD_LIBRARY_PATH/usr/local/cuda-11.8/lib64:/usr/local/cuda-12.2/lib64echo $PATH/usr/local/cuda-12.2/bin:/home/shchoice/anaconda3/envs/csh_p311_c118/bin:/home/shchoice/anaconda3/condabin:/usr/local/cuda-11.8/bin:/usr/local/cuda-12.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 가상환경이 비활성화 될 때 12export PATH=원래의_PATHexport LD_LIBRARY_PATH=원래의_LD_LIBRARY_PATH","link":"/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20CUDA%20%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%8B%E1%85%A5%E1%86%B8%20%E1%84%92%E1%85%AA%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%20%E1%84%80%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC/"},{"title":"기본 수학","text":"Probablilistic Perspective신경망은 확률 분포 함수이다. 이전에는 신경망을 함수로 바라보았다면, 이제는 생각을 확장시켜 확률 분포로 바라보도록 하자 Before 어떠한 함수를 모사하여, 원하는 출력값을 반환하는 신경망의 파라미터를 찾자 그래서 Gradient Descent, Back-propagation, Feature Vector 등을 통해 가상의 함수를 모사하려 하였다. Deterministic target 값을 예측 After : 확률 분포 함수를 배우자 수학적으로 더 설명이 가능함 불확실성까지 학습 Neural Networks는 확률 분포 함수를 모델링할 수 있음 이를 통해 가상의 확률 분포 함수 $p(y|x)$ 를 근사(approximation)할 것 대부분의 최신 기술들은 이 관점에 기반을 두고 만들어짐 신경망을 확률 분포로 보았을 때 다음의 이론들에 대해서 이야기 할 것 Likelihood Maximum Likelihood Estimation(MLE) Maximum A Posterior Estimation(MAP) Cross Entropy KL-Divergence 확률 변수와 확률 분포확률변수 랜덤 변수(Random Variable)는 확률을 이야기할 때 랜덤하게 발생하는 어떤 사건(event) …","link":"/%EA%B8%B0%EB%B3%B8-%EC%88%98%ED%95%99/"},{"title":"자연어 처리란","text":"자연어 처리란 무엇일까 인간의 언어, 즉 자연어를 컴퓨터가 이해하고 처리할 수 있도록 하는 인공지능(AI) 및 컴퓨터 과학의 한 분야 자연어 처리의 목표는 컴퓨터가 인간처럼 자연어를 이해하고 사용하여 텍스트나 음성 데이터를 분석, 해석, 생성하고 의사소통할 수 있게 하는 것 자연어 처리의 대표적인 과제 및 응용 분야 기계 번역 (Machine Translation) 컴퓨터가 한 언어에서 다른 언어로 텍스트를 번역하는 과정 Google Translate 등의 서비스(영어 문장을 한국어 문장으로 변환) 감성 분석 (Sentiment Analysis) 텍스트에서 작성자의 감정, 의견, 태도 등을 판단하는 과정 제품 리뷰나 소셜 미디어 게시물에서 사용자들의 반응을 분석(주로 긍정적, 부정적, 중립적 등의 카테고리로 분류) 텍스트 분류 (Text Classification) 주어진 텍스트를 미리 정의된 카테고리로 분류하는 과정 뉴스 기사를 정치, 경제, 스포츠 등의 주제로 분류, 스팸 메일 필터링, 태그 지정 등 다양한 분야에서 활용 요약 (Summarization) 긴 텍스트를 짧은 텍스트로 줄이는 과정으로, 주요 내용을 유지하면서 텍스트의 길이를 줄임 요약은 추출적 요약(Extractive Summarization)과 생성적 요약(Abstractive Summarization) 두 가지 방식이 있음 질문 답변 시스템(Question Answering System) 사용자의 질문에 대해 자동으로 답변을 생성하는 시스템 질문 답변 시스템은 오픈 도메인(Open Domain)과 클로즈드 도메인(Closed Domain)으로 구분 오픈 도메인 시스템: 일반적인 주제에 대한 질문을 다룸 클로즈드 도메인 시스템: 특정 주제나 분야에 관한 질문에 초점을 맞춤 자연어 처리의 대표적인 과제 및 응용 분야 기계 번역 (Machine Translation) 컴퓨터가 한 언어에서 다른 언어로 텍스트를 번역하는 과정 Google Translate 등의 서비스(영어 문장을 한국어 문장으로 변환) 감성 분석 (Sentiment Analysis) 텍스트에서 작성자의 감정, 의견, 태도 등을 판단하는 과정 제품 리뷰나 소셜 미디어 게시물에서 사용자들의 반응을 분석(주로 긍정적, 부정적, 중립적 등의 카테고리로 분류) 텍스트 분류 (Text Classification) 주어진 텍스트를 미리 정의된 카테고리로 분류하는 과정 뉴스 기사를 정치, 경제, 스포츠 등의 주제로 분류, 스팸 메일 필터링, 태그 지정 등 다양한 분야에서 활용 요약 (Summarization) 긴 텍스트를 짧은 텍스트로 줄이는 과정으로, 주요 내용을 유지하면서 텍스트의 길이를 줄임 요약은 추출적 요약(Extractive Summarization)과 생성적 요약(Abstractive Summarization) 두 가지 방식이 있음 질문 답변 시스템(Question Answering System) 사용자의 질문에 대해 자동으로 답변을 생성하는 시스템 질문 답변 시스템은 오픈 도메인(Open Domain)과 클로즈드 도메인(Closed Domain)으로 구분 오픈 도메인 시스템: 일반적인 주제에 대한 질문을 다룸 클로즈드 도메인 시스템: 특정 주제나 분야에 관한 질문에 초점을 맞춤 자연어 처리의 발전 2010년에 RNN을 활용한 언어 모델을 시도하여 기존 n-gram 기반 언어 모델의 한계를 극복하고자 함 하지만 n-gram 언어 모델을 기계번역 분야에 적용하기에는 구조적인 한계와 비교적 높은 연산량으로 인해 한계 2013년 word2vec을 통해 단순한 구조의 신경망을 사용하여 효과적으로 단어들을 잠재공간(latent space)에 성공적으로 투사시킴으로,고차원의 공간상의 단어가 어떻게 잠재 공간에 배치되는지 알 수 있었음 → 비슷한 단어일수록 저차원의 잠재 공간에서 가깝게 위치) 2014년 CNN과 단어 임베딩 벡터를 결합하여 성능을 더 극대화 문장이란 곧 단어들의 시퀀셜 데이터이므로 RNN을 통해 해결해야 한다는 고정관념 극복 2017년 어텐션(Attention) 기법이 개발되어 기계번역에 적용되며 큰 성과를 거둠 기존의 자연어 처리 분야는 기존의 한정적인 적용 사례에만 있었는데, 주어진 정보에 기반하여 자유롭게 문장을 생성하는 자연어 생성(NLG)이 가능해짐 그에 따라 요약 챗봇 등 더 넓고 깊은 주제의 자연어 처리 문제를 적극적으로 해결하려는 시도가 이어짐 강화학습의 적용, MSE 손실 함수의 한계(손실 함수와 실제 기계번역을 위한 목적 함수와의 괴리)를 벗어나기위해 Policy Gradient 방식을 적용함으로써 큰 성공, 강화학습을 사용하여 실제 자연어 생성에서의 목적 함수로부터 보상(reward)을 받을 수 있게 되자, 훨씬 실제 사람이 사용하는 듯한 문장을 생성 자연어 처리는 단어 간의 순서 및 상호 정보가 반영된 Sequential Data라는 점이 큰 장벽이었으나, 2014년 이후 seq2seq 모델 구조가 나오며 end-to-end 신경망 기반 기계(Neural Machine, Translation, NMT)의 시대를 열었으며, Attention 메커니즘의 등장으로 인해 요원해 보이던 기계번역 분야마저 end-to-end 방식의 딥러닝에 의해 정복됨","link":"/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC%EB%9E%80/"},{"title":"데이터마이닝","text":"IntroADsP를 준비하면서 요약 정리를 위해 작성 데이터 마이닝 데이터마이닝이란 대규모 데이터베이스를 사용해서 기존에 알려지지 않은 패턴이나 규칙을 찾아내는 과정 대규모의 데이터로부터 유용한 정보를 추출 컴퓨터 과학의 패턴인식 기술, 토계 및 수학적 분석방법 등을 사요앻서 새로운 관계, 성향, 패턴 등 가치를 발견하는 일련의 과정 의미있는 패턴과 규칙을 발견하기 위해서 자동화되거나 반자동화된 도구를 이용하여 대량의 데이터를 탐색하고 분석하는 과정 Knowledge Discovery in large Databses (KDD) 통계 vs 기계학습 vs 데이터마이닝 통계(통계학) 표본 데이터를 이용하여 모집단에 대해 추론 기계학습(컴퓨터공학) 전체 데이터를 이용해 개별값 예측 기존 데이터로 모델을 학습시킨 후 새로운 데이터를 입력했을 때 예측값을 알아내기 위한 목적 데이터마이닝(경영) 통계 + 기계학습 가지고 있는 데이터에서 일정한 패터이나 특성을 발견 -&gt; 개별값 예측 분석결과를 비지니스에 응용(Business Intelligence) 데이터마이닝기법 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)의 관점 및 예측(Estimate) 과 설명(Description)의 관점으로 분류할 수 있음 지도학습과 비지도학습 지도학습 : 결과변수가 주어진 경우에 변수 간의 관계를 분석 예측 과거 데이터를 분석하여 모델을 만들고 새로운 케이스를 예측하는 방법 분류 새로운 데이터가 어느 범주에 속하는지 분류하는 방법 비지도학습 : 결과변수가 없는 경우 분석하는 방법으로 객체 간의 관계를 분석하여 결과변수를 분석함 군집 데이터 특성을 유사성으로 군집화하여 새로운 데이터의 군집을 분석 연관규칙 연관성 변수들 간의 동시발생 빈도를 분석하여 변수들 간의 관계를 파악, 마케팅 전략으로 활용 예측(Estimate)과 설명(Description) 예측기법 분류 회귀분석 시계열분석 설명기법 연관규칙 군집분석 Text Mining 기법 요약(Summarization) 데이터마이닝 분석절차","link":"/Data%20Mining/%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%86%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%82%E1%85%B5%E1%86%BC/"},{"title":"Item 1. 생성자 대신 팩터리 메서드를 고려하라","text":"아이템 1. 생성자 대신 정적 팩터리 메서드를 고려하라 핵심 정리 장점 이름을 가질 수 있음 12345678910111213141516171819202122232425public class Order { private Product product; private boolean prime; private boolean urgent; public Order() { } // 추상 팩터리 패턴 public static Order primeOrder(Product product) { Order order = new Order(); order.product = product; order.prime = true; return order; } public static Order urgentOrder(Product product) { Order order = new Order(); order.product = product; order.urgent = true; return order; }} 동일한 시그니처의 생성자는 2개 이상 가질 수 없음 12345678910111213141516// 시그니처(Product, boolean)가 동일하기 때문에 생성자를 2개 이상 생성이 불가능public class Order { private Product product; private boolean prime; private boolean urgent; public Order(Product product, boolean prime) { this.product = product; this.prime = prime; } public Order(Product product, boolean urgent) { this.product = product; this.urgent = urgent; }} 호출될 때마다 인스턴스를 새로 생성하지 않아도 된다(Boolean.valueOf) 1234567891011121314package Chap01;public class Settings { private boolean useAutoSteering; private boolean useABS; private Difficulty difficulty; private Settings() {} private static final Settings SETTINGS = new Settings(); public static Settings newInstance() { return SETTINGS; }} 1234567891011121314package Chap01;public class Product { public static void main(String[] args) { Settings settings1 = Settings.newInstance(); Settings settings2 = Settings.newInstance(); System.out.println(settings1); // Chap01.Settings@15db9742 System.out.println(settings2); // Chap01.Settings@15db9742 Boolean.valueOf(false); }} 생성자를 내부/외부에서 여러개 생성할 수 있음 1234567891011public class Settings { private boolean useAutoSteering; private boolean useABS; private Difficulty difficulty; public static void main(String[] args) { System.out.println(new Settings()); System.out.println(new Settings()); System.out.println(new Settings()); }} 반환 타입의 하위 타입 객체를 반환할 수 있는 능력이 있음(인스페이스 기반 프레임워크, 인터페이스에 정적 메소드) 입력 매개변수에 따라 매번 다른 클래스의 객체를 반환할 수 있음(EnumSet) 정적 팩터리 메서드를 작성하는 시점에는 반환할 객체의 클래스가 존재하지 않아도 됨 (서비스 제공자 프레임워크) 단점 상속을 하려면 public이나 protected 생성하기 필요하니 정적 팩터리 메서드만 제공하면 하위 클래스를 만들 수 없음 정적 팩터리 메서드는 프로그래머가 찾기 어렵다.","link":"/Effective%20Java/Item-1-%EC%83%9D%EC%84%B1%EC%9E%90-%EB%8C%80%EC%8B%A0-%ED%8C%A9%ED%84%B0%EB%A6%AC-%EB%A9%94%EC%84%9C%EB%93%9C%EB%A5%BC-%EA%B3%A0%EB%A0%A4%ED%95%98%EB%9D%BC/"},{"title":"","text":"프로세스와 쓰레드란 무엇인가요??프로세스와 쓰레드는 컴퓨터에서 실행되는 작업의 실행 단위로 모두 동시성(concurrency)을 다룹니다. 프로세스와 쓰레드의 차이는 어떻게 되나요?? 프로세스 운영체제에서 실행 중인 프로그램을 의미 독립적인 메모리 공간과 시스템 자원(CPU, 메모리 등)을 할당받음 따라서 각각의 프로세스는 완전히 분리되어 서로 영향을 미치지 않음 즉, 프로세스 간에는 메모리를 공유할 수 없음(직접적인 접근이 불가능) 만약에 하나의 프로세스에서 다른 프로세스에 접근하려면 IPC(Inter-Process Communication)를 사용 Java에서는 java.lang.Process 클래스를 통해 프로세스를 생성 및 제어 Python에서는 multiprocessing 모듈을 통해 프로세스를 생성 및 제어 쓰레드 프로세스 내에서 실행되는 작은 실행 단위 하나의 프로세스 내에서 여러 개의 쓰레드를 생성하여 실행할 수 있음 쓰레드는 각각 자신의 실행 스택을 가지고 있지만, 프로세스 내의 다른 쓰레드와 메모리 공간을 공유하며 실행(부모 프로세스의 메모리 공간과 자원을 공유) 따라서 쓰레드는 프로세스보다 가볍고 실행 속도가 빠름(자원의 효율적인 활용 가능) 하지만 여러 쓰레드가 공유된 메모리 공간을 사용하기에 데이터 불일치 문제를 고려해야함 Java에서는 동기화(synchronization)를 통해 문제를 해결 Java에서는 java.lang.Thread 클래스를 사용하여 쓰레드를 생성 및 제어 Python에서는 threading 모듈을 통해 쓰레드를 생성 및 제어 모든 Java 프로그램은 기본적으로 하나의 쓰레드(메인 쓰레드)를 갖음, main method를 실행하고, 프로그램이 종료될 때까지 실행됨 프로세스와 쓰레드의 공통점은 어떻게 되나요?? 실행 흐름 : 모두 실행 흐름을 나타냄. 즉 프로그램이 실행되어 작업을 처리하는 단위 자원 공유 : CPU, Memory, 파일, 네트워크 등의 시스템 자원을 공유 다만 Memory의 경우 프로세스는 독립된 메모리 공간을 사용하고, 쓰레드는 프로세스의 메모리 공간을 공유 스케줄링 : 모두 스케줄링의 대상이 됨, 즉 운영체제가 자원을 할당하는 대상이 됨 동시성 : 프로세스와 쓰레드는 모두 동시에 실행될 수 있으며, 이는 멀티태스킹을 구현하기 위한 필수적인 기능 컨텍스트 스위칭 : 실행중인 프로세스나 쓰레드를 일시 중단하고, 다른 프로세스나 쓰레드를 실행할 수 있음 동시성 프로그래밍(Concurrency Programming)이란 무엇인가요?? 하나의 컴퓨터 시스템에서 여러 개의 작업(task)이 동시에 실행되는 프로그래밍 기법, 즉 여러 작업을 동시에 처리해 시스템의 활용도를 향상시킬 수 있습니다. 동시성 프로그래밍에서는 동시에 실행되는 작업들이 서로 영향을 주지 않도록 관리해야 합니다. 세마포어(Semaphore), 뮤텍스(Mutex), 락(Lock) 등 동기화 기법을 사용하여 공유 데이터에 대한 접근을 제어해야 합니다. 멀티쓰레드, 멀티프로세싱, 비동기 프로그래밍 등 다양한 기법이 사용됩니다. 동시성 프로그맹과 병렬 프로그래밍은 어떠한 차이가 있나요??네. 비슷한 개념으로 생각할 수 있지만, 약간의 차이가 있습니다. 병렬 프로그래밍 여러 개의 프로세서(코어)를 사용하여 한 작업을 분할하여 동시에 실행 하나의 큰 작업을 작은 작업으로 분할하여 각각의 작은 작업을 병렬적으로 실행 병렬 프로그래밍은 대규모 컴퓨팅 자원이 필요하며, 여러 개의 CPU나 GPU를 사용하여 구현됨 동시성 프로그래밍 단일 프로세서에서 여러 작업을 동시에 실행하는 것 따라서 여러 개의 작업이 빠르게 번갈아 가며 실행되어 실제로는 작업들이 서로 영향을 주지 않도록 관리되어 동시에 실행 동시성 프로그래밍에서 멀티쓰레드, 멀티프로세싱, 비동기 프로그래밍의 차이는 무엇인가요?? 비동기 프로그래밍 작업이 완료될 때까지 기다리지 않고 다른 작업을 수행할 수 있는 기술 콜백(callback)함수나 프로미스(promise) 등을 사용하여 작업 완료 시점에 처리를 수행 입출력이 느린 작업이나 네트워크 작업 등에서 더 나은 성능을 얻을 수 있는 이점이 있음 멀티쓰레드 하나의 프로세스 내에서 여러 개의 쓰레드를 생성하여, 각 쓰레드가 병렬로 작업을 수행 쓰레드는 같은 메모리 공간을 공유하기에, 데이터를 공유할 때 동기화 문제를 고려해야함 멀티프로세싱 여러 개의 프로세스를 생성하여 각 프로세스가 병렬로 작업을 수행하는 것 각 프로세스는 독립적인 메모리 공간을 가지므로, 데이터를 공유하기 위해서는 IPC를 사용해야 함 멀티쓰레드 보다는 안정적이지만 더 많은 리소스를 사용한다는 단점이 있음 자바에서 비동기 프로그래밍은 어떻게 구현하나요?? 콜백(Callback) 기반의 비동기 프로그래밍을 주로 사용 콜반 기반의 프로그래밍은 콜백 함수를 등록하여 비동기식 작업이 완료되면 해당 콜백 함수가 호출되도록 하는 것 이를 위해 자바에서는 Future나 CompletableFuture 클래스를 사용하여 비동기식 작업을 수행1234567891011// supplyAsync() 메서드를 사용하여 비동기 작업을 수행하고, thenAccept() 메서드를 사용하여 작업이 완료된 후 호출될 콜백 함수를 등록// 이러한 방식으로 콜백 함수를 등록하여 비동기식 작업을 처리CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; { // 비동기 작업을 수행하는 코드 return &quot;result&quot;;});future.thenAccept(result -&gt; { // 비동기 작업이 완료된 후 호출될 콜백 함수 System.out.println(&quot;Result: &quot; + result);}); Stream API를 사용 Stream API는 병렬 처리를 지원하므로 멀티코어 CPU에서 더욱 효과적으로 동작1234567// parallelStream() 메서드를 사용하여 리스트의 항목을 병렬로 처리하고, forEach() 메서드를 사용하여 각 항목에 대한 비동기식 작업을 수행List&lt;String&gt; list = Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);list.parallelStream().forEach(item -&gt; { // 비동기 작업을 수행하는 코드 System.out.println(&quot;Item: &quot; + item);}); 동기식 처리와 비동기식 처리의 차이점은 무엇인가요?? 프로그램에서 작업이 실행되는 방식의 차이점에 따라 구분이 됨 동기식 처리 작업이 순차적으로 실행되어 결과가 반환되기 전까지 다음 작업이 실행되지 않는 방식 즉, 어떤 작업이 완료될 때까지 다음 작업을 기다리는 방식 예를 들면, 웹 서버에서 동기식으로 작성된 코드는 한 번에 하나의 요청만 처리할 수 있음 비동기식 처리 작업이 동시에 실행되어 결과가 반환되기를 기다리지 않고 다음 작업을 실행할 수 있는 방식 즉, 어떤 작업이 완료되기를 기다리지 않고, 다음 작업을 실행하는 방식 예를 들면 웹 서버에서 비동기식으로 작성된 코드는 여러 요청을 동시에 처리할 수 있음 또 다른 예로 웹 페이지에서 이미지를 로드되는 동안 텍스트 작업을 수행할 수 있음 비동기식 처리는 작업이 완료될 때까지 기다리지 않고 다른 작업을 처리하므로, 더 높은 처리량을 얻을 수 있으나 코드가 복잡해지고 처리 결과를 조합하는 작업이 필요할 경우 처리 방식이 복잡해질 수 있음 반면 동기식 처리는 간단하고 직관적이지만, 대규모의 작업을 처리하는 데 부적합할 수 있음 파이썬에서 비동기식 처리를 수행할 때 처리하는 원리가 어떻게 되나요?? 파이썬에서 비동기식 처리를 수행할 때, asyncio 라이브러리를 이용하여 이벤트 루프(event loop)를 생성하고, 코루틴(coroutine)을 이용하여 비동기식 처리를 수행합니다. 이벤트 루프는 이벤트 발생을 대기하다가, 이벤트가 발생하면 그에 대응하는 작업을 처리하는 루프입니다. 즉, 이벤트 루프는 코루틴을 실행하고, I/O 작업이 완료될 때까지 기다린 후, 결과를 반환합니다. 이벤트 루프는 코루틴을 실행할 때, 작업을 블로킹하지 않고 비동기적으로 실행합니다. 코루틴은 제너레이터(generator)와 비슷한 개념으로, 함수 실행 중에 일시 중지하고 다른 작업을 수행한 후, 다시 원래 작업을 재개하는 것을 가능하게 합니다. 이러한 특징을 이용하여 비동기식 처리를 수행할 수 있습니다. 비동기식 작업은 코루틴으로 구현되어, 이벤트 루프에 의해 실행되며, 작업이 완료되기 전에 다른 작업을 실행할 수 있습니다. 이러한 방식으로 비동기식 처리를 수행하면, I/O 작업이 많은 네트워크 프로그램에서 효과적으로 CPU 자원을 활용할 수 있습니다. I/O 작업이 끝날 때까지 대기하는 대신, 다른 작업을 실행할 수 있기 때문입니다. 이러한 방식은 네트워크 프로그램뿐만 아니라, 다양한 분야에서 활용할 수 있으며, 파이썬에서는 asyncio 라이브러리를 이용하여 비동기식 처리를 수행할 수 있습니다. 파이썬의 asyncio 라이브러리르 이용하여 Echo 서버를 비동기식으로 구현한 예시 코드 여러 개의 클라이언트의 요청을 동시에 처리하여 Throughput(처리량)을 향상시킬 수 있음 Echo 서버용 코드 12345678910111213141516171819202122import asyncioasync def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info('peername') print(f&quot;Received {message!r} from {addr!r}&quot;) writer.write(data) await writer.drain() print(f&quot;Send {message!r} to {addr!r}&quot;) writer.close()async def main(): server = await asyncio.start_server(handle_echo, '127.0.0.1', 8888) print(f&quot;Serving on {server.sockets[0].getsockname()}&quot;) async with server: await server.serve_forever()asyncio.run(main()) 위 코드는 asyncio 모듈의 start_server() 함수를 이용하여 Echo 서버를 구현한 코드입니다. 클라이언트가 연결되면, handle_echo() 코루틴이 실행되어 클라이언트 요청을 처리합니다. 이 코드에서는 클라이언트 요청이 처리될 때마다 새로운 코루틴을 생성하여 요청을 처리하므로, 다수의 클라이언트 요청을 동시에 처리할 수 있습니다. 클라이언트 요청을 보내는 코드 123456789101112131415import asyncioasync def tcp_echo_client(message): reader, writer = await asyncio.open_connection('127.0.0.1', 8888) print(f&quot;Send {message!r}&quot;) writer.write(message.encode()) data = await reader.read(100) print(f&quot;Received {data.decode()!r}&quot;) writer.close() await writer.wait_closed()asyncio.run(tcp_echo_client(&quot;Hello, World!&quot;)) 위 코드는 asyncio 모듈의 open_connection() 함수를 이용하여 Echo 서버에 접속하는 코드입니다. 클라이언트가 요청을 보내면, 서버에서는 새로운 코루틴을 생성하여 요청을 처리하고, 클라이언트에 응답을 보냅니다. 이러한 방식으로 비동기식 처리를 수행하면, 다수의 클라이언트 요청을 효율적으로 처리할 수 있습니다. 자바에서 쓰레드를 구현하는 방법은 어떻게 될까요?? Thread 클래스 상속 Runnable을 상속하여 만들어진 클래스이며, 클래스 객체를 생성하고 start() 메소드를 호출함으로써 Thread가 독립적으로 실행 이러한 Thread 클래스를 상속받아서 run() 메서드를 오버라이드하여 구현하는 방법 어떤 객체도 리턴하지 않음 가장 기본적인 쓰레드 구현 방법 12345class MyThread extends Thread { public void run() { // 쓰레드에서 수행할 작업 구현 }} Runnable 인터페이스 구현 Runnable 인터페이스를 구현하여 run() 메서드를 오버라이드하여 구현하는 방법 상속을 사용하지 않기 때문에 코드 재사용성이 더 높음 따라서 다른 클래스를 확장하거나 다른 인터페이스를 구현하고 있는 경우에 사용 어떤 객체도 리턴하지 않음 ExecutorService의 submit() 메소드로 작업을 실행하고 결과 값을 받을 수 있음 12345class MyRunnable implements Runnable { public void run() { // 쓰레드에서 수행할 작업 구현 }} Callable 인터페이스 구현 Callable 인터페이스를 구현하여 call() 메서드를 오버라이드하여 구현하는 방법 Runnable과 비슷하지만, 작업 결과인 특정 타입의 객체를 반환할 수 있음 ExecutorService의 submit() 메소드로 작업을 전달하고 작업이 완료되면 Future 객체를 반환해 결과값을 받을 수 있음 123456class MyCallable implements Callable&lt;Integer&gt; { public Integer call() { // 쓰레드에서 수행할 작업 구현 return 0; }} ExecutorService 를 이용한 쓰레드 풀 사용 ExecutorService 인터페이스를 사용하여 쓰레드 풀을 생성하고, 큐에 작업을 추가하고 실행 여러 쓰레드를 동시에 실행할 수 있음 주로 비동기적인 작업을 수행할 때 사용 submit() 메소드로 작업을 전달하고 작업이 완료되면 Future 객체를 반환해 결과값을 받을 수 있음 123ExecutorService executorService = Executors.newFixedThreadPool(10);Runnable myRunnable = new MyRunnable();executorService.execute(myRunnable); ExecutorService의 여러 쓰레드 풀 생성 방법 new FixedThreadPool(int) 인자 개수만큼 고정된 쓰레드풀 생성 new CachedThreadPoll() 필요할 때, 필요한 만큼 쓰레드풀 생성 이미 생성된 쓰레드를 재활용할 수 있어 성능상 이점이 있을 수 있다. new ScheduledThreadPool(int) 일정 시간 뒤 실행되는 작업이나, 주기적으로 수행되는 작업이 있을 때 고려 new SingleThreadExecutor() 쓰레드 한 개인 ExecutorService를 리턴한다. 싱글 쓰레드에서 동작해야 하는 작업을 처리할 때 사용 submit()메서드를 통해 Callable을 실행하고 Future&lt;V&gt;를 반환 받을 수 있음 실행된 Callable의 반환값을 받는 Future의 메소드 소개 get() 비동기 작업이 완료될 때까지 대기하고 결과를 가져옴 = get() 메소드 호출 시 Blocking Call이 발생하여 반환값을 가지고 올 때까지 멈춤 isDone() 비동기 작업이 완료되었는지 여부를 확인 Thread Pool에 submit되어 실행 중인 Callable 작업이 언제 끝난 지를 확인할 수 있음 cancel() 비동기 작업을 멈춤 invokeAll() 모든 작업이 완료될 때까지 대기하고 모든 작업의 결과를 가져옴 invokeAny() 작업 중 하나라도 완료되면 대기를 취소하고 완료된 작업 중 하나의 결과를 가져옴 Thread(), Runnable(), Callable(), ExecutorService() 는 어떠한 경우에 사용하는 것이 좋을까요?? 일반적으로는 Runnable을 구현한 객체를 Thread 생성자의 인자로 전달하는 방식으로 새로운 쓰레드를 생성 Callable 은 작업 수행 결과가 필요한 경우에 사용 ExecutorService는 쓰레드 풀을 생성하고 비동기 작업을 수행 파이썬에서 쓰레드를 구현하는 방법은 어떻게 될까요?? threading 모듈 사용: 파이썬에서는 threading 모듈을 사용하여 쓰레드를 구현할 수 있습니다. Thread 클래스를 상속하거나, Runnable 인터페이스와 비슷한 역할을 하는 target 매개변수를 사용하여 쓰레드를 생성할 수 있습니다. 12345678import threadingclass MyThread(threading.Thread): def run(self): # 쓰레드에서 수행할 작업 구현my_thread = MyThread()my_thread.start() 1234567import threadingdef my_function(): # 쓰레드에서 수행할 작업 구현my_thread = threading.Thread(target=my_function)my_thread.start() concurrent.futures 모듈 사용: concurrent.futures 모듈을 사용하여 ThreadPoolExecutor나 ProcessPoolExecutor 클래스를 이용하여 쓰레드 풀을 생성하고, 여러 쓰레드를 동시에 실행할 수 있습니다. ThreadPoolExecutor는 쓰레드 기반, ProcessPoolExecutor는 프로세스 기반으로 쓰레드를 실행합니다. 1234567import concurrent.futuresdef my_function(): # 쓰레드에서 수행할 작업 구현with concurrent.futures.ThreadPoolExecutor() as executor: future = executor.submit(my_function) 1234567import concurrent.futuresdef my_function(): # 쓰레드에서 수행할 작업 구현with concurrent.futures.ProcessPoolExecutor() as executor: future = executor.submit(my_function) asyncio 모듈 사용: asyncio 모듈을 사용하여 코루틴 기반의 비동기 쓰레드를 구현할 수 있습니다. 이 방법은 비동기 I/O 작업에 특화되어 있으며, 단일 스레드에서 여러 작업을 동시에 처리할 수 있습니다. 123456789import asyncioasync def my_coroutine(): # 비동기적으로 수행할 작업 구현async def main(): await asyncio.gather(my_coroutine())asyncio.run(main()) queue 모듈을 이용한 Producer-Consumer 패턴 사용: queue 모듈을 사용하여 여러 쓰레드 간에 데이터를 공유하고, Producer-Consumer 패턴을 구현할 수 있습니다. 이 방법은 여러 쓰레드가 동시에 작업을 처리해야 하는 경우에 유용합니다. 12345678910111213import queuedef producer(q): # 데이터를 생산하여 큐에 추가하는 작업 구현def consumer(q): # 큐에서 데이터를 가져와서 처리하는 작업 구현q = queue.Queue()p = threading.Thread(target=producer, args=(q,))c = threading.Thread(target=consumer, args=(q,))p.start()c.start() 쓰레드를 사용하면서 주의해야할 점은 어떻게 되나요?? Race Condition(경쟁 상태) 2개 이상의 쓰레드가 공유 데이터에 접근하여 변경하는 경우 경쟁 상태가 발생할 수 있음 자료의 일관성을 해치는 결과가 나타날 수 있기 때문 이를 해결하기 위해서는 동기화 기법을 사용 DeadLock(교착 상태) 2개 이상의 쓰레드가 서로 대기하며 무한정 기다리는 상황 교착 상태를 방지하기 위해서는 상호배제, 점유대기, 비선점, 순환 대기 등의 조건 중 하나가 만족되지 않도록 해야함 Thread.stop() 메소드 사용 금지 Thread.stop() 메소드는 쓰레드를 강제로 종료시키는 메소드이지만 안정적으로 쓰레드를 종료시키지 못하고 데이터 불일치(Data Inconsistency) 등의 문제를 발생시킬 수 있음 대신 쓰레드를 종료시키기 위해서는 플래그 변수나 interrupt() 메소드를 사용하는 것이 좋음 Thread 우선순위 지정 주의 Thread 우선순위는 시스템마다 다를 수 있음 우선순위에 따라 쓰레드가 실행되는 것을 보장하지 않음 ThreadLocal 사용 주의 쓰레드별로 독립적인 데이터를 저장하는데 사용되는 ThreadLocal을 남발하면 쓰레드 간에 데이터 불일치 문제가 발생할 수 있음 성능 문제 쓰레드를 사용하면 Context Switching과 메모리 사용량 등의 부가적인 성능 문제가 발생할 수 있음 Context Switching을 할 때는 현재 진행 중인 작업의 상태(ex. 다음에 실행해야할 위치(Program Counter)) 등의 정보를 저장하고 읽어오는 시간이 소요 따라서 필요한 만큼의 Thread만 생성하고 Thread의 생명주기를 관리하여 성능 문제를 최소화해야함 단순한 작업일 경우에는 Single Thread로 프로그래밍 하는 것이 더 효율적 그렇다면 경쟁상태를 해소하기 위해 쓰레드 동기화는 어떻게 동작하나요Critical Section(임계영역), Lock(잠금) 한 Thread가 진행 중인 작업을 다른 Thread가 간섭하지 못하도록 막는 것을 쓰레드의 동기화라 함 공유 데이터를 사용하는 코드 영역을 임계영역으로 지정해놓고, 공유 데이터(객체)가 가지고 있는 lock을 획득한 단 하나의 쓰레드만 이 영역 내의 코드를 수행할 수 있도록 함 해당 쓰레드가 임계 영역 내의 모든 코드를 수행하고 벗어나서 lock을 반납해야 비로소 다른 쓰레드가 반납된 lock을 획득하여 임계 영역의 코드를 수행할 수 있음1234567// 1. 메소드 전체를 임계 영역으로 지정public synchronized void sum() {}// 2. 특정한 영역을 임계 영역으로 지정synchronzied(객체의 참조변수) {} 자바에서의 프로세스와 쓰레드의 동시성 문제를 어떻게 해결하나요?? Lock, Semaphore 등의 동기화 기법 java.util.concurrent 패키지의 Lock, ReentrantLock, Semaphore, Condition 등을 이용 (Synchronized 키워드 보다 더욱 세밀한 동기화 제어 제공) Thread-safe한 자료구조를 이용 java.util.concurrent 패키지의 BlockingQueue, ConcurrentHashMap, CopyOnWriteArrayList 등 쓰레드 풀(Thread pool)을 이용하여 쓰레드 생성 비용을 줄이고 작업을 분배 java.util.concurrent 패키지의 ThreadPoolExecutor 비동기식 처리를 이용(I/O 작업이 많은 경우에 효과적으로 CPU 자원을 활용) java.util.concurrent 패키지의 CompletableFuture, Executor 등 Synchronized : Synchronized 키워드는 메서드나 블록 단위에서 사용할 수 있음 java.util.concurrent 패키지의 Atomic 사용 Fork/Join 프레임워크 : 병렬 처리를 위한 고수준의 도구로, RecursiveTask나 RecursiveAction 클래스를 사용하여 작업을 분할하고, 각각의 작업을 병렬로 처리한 후 결과를 합쳐서 반환 파이썬에서의 프로세스와 쓰레드의 동시성 문제를 어떻게 해결하나요?? Lock, Semaphore 등의 동기화 기법 threading 모듈의 Lock, RLock, Semaphore, Condition, Event 등의 클래스를 제공 Thread-safe한 자료구조를 이용 queue 모듈의 Queue, LifoQueue, PriorityQueue 등의 클래스를 제공 쓰레드 풀(Thread pool)을 이용하여 쓰레드 생성 비용을 줄임 concurrent.futures 모듈의 ThreadPoolExecutor, ProcessPoolExecutor 등의 클래스 비동기식 처리를 이용 (I/O 작업이 많은 경우에 효과적으로 CPU 자원을 활용) asyncio 라이브러리를 이용 코루틴 기반의 비동기 프로그래밍을 구현할 수 있음 여러 작업을 동시에 처리할 수 있으며, IO 작업이 많은 프로그램에서 성능을 향상 Queue 모듈을 사용한 작업자 스레드 패턴 Queue 모듈을 사용하여 여러 쓰레드가 공유하는 작업 큐를 생성하고, 작업자 스레드들이 이 큐에서 작업을 꺼내어 처리하도록 하는 방법 이를 사용하여 작업을 분산 처리하고, 쓰레드 간의 경쟁을 방지 multiprocessing 모듈을 사용한 프로세스 기반 병렬 처리 여러 개의 프로세스를 생성하고, 이들 간에 작업을 분산하여 병렬 처리 이를 사용하여 여러 코어를 활용하여 작업을 처리 concurrent.futures 모듈을 사용한 스레드나 프로세스 기반 병렬 처리 concurrent.futures 모듈을 사용하여 스레드나 프로세스 기반의 병렬 처리를 구현 이를 사용하여 여러 작업을 동시에 처리하고, 블로킹 작업을 효율적으로 처리할 수 있음","link":"/%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80-%EC%8A%A4%EB%A0%88%EB%93%9C-1/"},{"title":"Git 계정 변경","text":"배경 컴퓨터 1대를 통해서 개인 Git을 위한 계정 그리고 별도의 팀 프로젝트를 위한 Git 계정을 동시에 사용할 일이 생김 공용 컴퓨터에 이미 설치된 Git 환경에서 작업을 수행할 경우가 생김 작업절차 현재 git에 등록된 계정 정보를 확인 12345678# 현재 등록된 계정의 이름 확인$ git config user.name# 현재 등록된 계정의 이메일 확인$ git config user.email# (참고) 한번에 확인하려면 다음과 같이 확인 가능git config --list 변경할 계정을 등록 및 재확인 12345# 새롭게 등록할 계정의 이름 등록$ git config --global user.name 새로운_계정# 새롭게 등록할 계정의 이메일 등록$ git config --global user.email 새로운_이메일주소 이름에서 띄어쓰기 작업 시(ex. Seohwan Choi) “Seohwan Choi”으로 작성을 해야한다. 12# 새롭게 등록할 계정의 이름 등록$ git config --global user.name &quot;Seohwan Choi&quot; 만약에 “”로 감싸지 않는다면 이름에 2개 이상의 값이 있어 문제가 발생하게 된다. 추후 계정 변경 시 cannot overwrite multiple values with a single value 에러가 발생하게 된다. 해당 내용 발견 시 –replace-all 을 덧붙여 정정해야 한다. 1git config --global --replace-all user.email &quot;shchoice7140@gmail.com&quot; 윈도우 자격 증명 삭제 제어판 &gt; 사용자 계정 &gt; 자격 증명 관리 &gt; windows 자격 증명 &gt; 일반 자격 증명 git:https://github.com/ 관련한 모든 자격 증명 제거 GitHub 관련한 모든 자격 증명 제거","link":"/Git/Git%20%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%87%E1%85%A7%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC/"},{"title":"Git 사용법","text":"처음으로 깃 시작 시 이름 및 메일 주소 등록git config –global user.name “Seohwan Choi”git config –global user.email “shchoice7140@gmail.com“ 입력이 잘 되었는지 확인git config –global -l Git 저장소 확인![git 저장소](C:\\Users\\seohwan\\Desktop\\NaverCloud\\4. 컴퓨터공학 공부\\10. 마크다운\\1. GitHub\\git 저장소.png) VS Code에서 Git 사하기 vscode에서 git에서 폴더 추가하면 git init 효과를 얻는다. commit이란 스테이징 영역의 변경사항을 레포지토리에 저장하는 행위, 변경사항을 묶어주는 단위 지금까지는 staging area에 저장을 하였고 깃 저장소에 저장을 해보겠다. 팔렛트- -&gt; git add remote -&gt; origin -&gt; 원격 URL(https://github.com/shchoice/TIL.git) -&gt; 푸시 로컬의 것을 업데이트 받고 싶으면 git pull이라고 팔렛트에 입력 git bash git status git add 파일이름 git commit -m “urlretrieve example” git push origin master git remote 저장소 이름 변경 git 저장소 Settings 탭메뉴에서 Repository name을 변경합니다. git bash를 통해 해당 프로젝트 로컬 경로로 이동합니다.이때, git remote -v라는 명령어를 이용하면, 현재 프로젝트 저장소 및 url를 출력해줍니다. git remote set-url origin [변경된 url주소]명령어를 사용하여 저장소 url을 변경할 수 있습니다.ex) git remote set-url origin https://github.com/shchoice/TIL-Python-Web-Scraping 최초 설정 시에는git remote add origin http://shchoice@133.186.171.23:9000/shchoice/OJT_DQ_Solution_Edu git add에서 취소 취소git reset HEAD 파일이름 rem Git Remote 변경 git remote remove origin git remote add origin https://github.com/shchoice/TIL-Python-Advanced Git 원격 저장소 파일 받기 git fetch git pull origin 브런치명 –allow-unrelated-histories","link":"/Git/Git-%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8/"},{"title":"Git에 저장된 파일 지우기","text":"배경 업무 관련 Git Reopository에 딥러닝 모델 Checkpoint가 잘못 저장되어 있음 그로인해 프로젝트 구성을 위해 git pull 혹은 프로그램을 deploy를 수행하는데 불필요하게 시간이 많이 소요 작업절차 불필요한 파일이 model_zoo 디렉토리에 model.ckpt-50000.index 파일이 존재한다고 가정 ./model_zoo/model.ckpt-50000.index 파일 지우기 1234$ git filter-branch --tree-filter 'rm -rf ./model_zoo/model.ckpt-50000.index' HEAD# 만약에 model_zoo 디렉토리만 지우고 싶다면 다음 명령어를 입력# git filter-branch --tree-filter 'rm -rf ./model_zoo' HEAD 빈 커밋 지우기 12$ git filter-branch --prune-empty HEAD$ git push origin legacy-3.0.2.x --force","link":"/Git/Git%E1%84%8B%E1%85%A6%20%E1%84%8C%E1%85%A5%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%83%E1%85%AC%E1%86%AB%20%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%86%AF%20%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%AE%E1%84%80%E1%85%B5/"},{"title":"git 작성자(이름, 이메일)및 comment 변경","text":"","link":"/Git/git%20%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%A1(%E1%84%8B%E1%85%B5%E1%84%85%E1%85%B3%E1%86%B7,%20%E1%84%8B%E1%85%B5%E1%84%86%E1%85%A6%E1%84%8B%E1%85%B5%E1%86%AF)%20%E1%84%86%E1%85%B5%E1%86%BE%20comment%20%E1%84%87%E1%85%A7%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC/"},{"title":"FastAPI - Deploy","text":"FastAPI와 ASGI 서버인 uvicorn + WSGI 서버인 gunicorn 을 통한 웹 서비스 구현 및 배포를 하면서 궁금했던 내용들에 대한 정리한 내용입니다. FastAPI 와 uvicorn을 통해 REST API 서버를 구현하고, 현재 상황에 최적화된 Product로 배포하기 위해 자료조사를 하던 중에 gunicorn + uvicorn 을 통한 서비스를 알게 되었습니다. 그래서 각각에 대한 개념과 웹 서비스를 어떻게 최적화하여 배포를 할 수 있는지 정리하였습니다. 개요 [ASGI 서버와 WSGI 서버란](#ASGI 서버와 WSGI 서버란?) Uvicorn 과 Gunicorn 이란 FastAPI와 uvicorn과의 관계 FastAPI와 uvicorn + gunicorn과의 관계 FastAPI 배포 전략 ASGI 서버와 WSGI 서버란?ASGI 서버와 WSGI 서버가 무엇인지 알기 위해서 CGI 및 WAS와 함께 알아보겠습니다. CGI(Common Gateway Interface) 웹 서버와 외부 프로그램 간의 데이터 교환을 위한 인터페이스 규약 (일종의 웹 서버 기술로 볼 수 있음) 즉, 웹 서버와 외부 프로그램 간의 상호작용을 위해 표준화된 방식을 제공 nginx, apache 웹 서버에서 우리가 작성한 프로그램을 실행하기 위한 인터페이스 하지만 현재는 대부분 웹 프레임워크를 통해 동적인 웹 페이지를 생성(CGI는 과거에 주로 사용) 웹 페이지에 동적인 콘텐츠를 제공하기 위해 웹 서버가 클라이언트 요청에 따라 외부 프로그램을 실행하고 그 결과를 반환하는 방식으로 동작 요청 → 웹서버(nginx, apache) → CGI → CGI에 작성한 프로그램 실행(프로세스 생성) 프로세스를 생성하여 실행 즉 fork, spawn를 통해 프로세스를 생성 요청에 따라 프로세스를 생성 많은 요청이 들어오면 서버 부하가 증가하고 성능 저하(메모리 등)를 초래할 수 있음 WAS(Web Application Server) 대규모 웹 어플리케이션을 운영하는데 매우 유용 웹 어플리케이션을 실행하기 위한 미들웨어 플랫폼으로, 웹 서버와 데이터베이스 서버 사이의 중간 계층에서 동작 WAS의 기능 참고 웹 어플리케이션을 실행하기 위한 런타임 환경 제공 다양한 프로그래밍 언어와 프레임워크를 지원 세션 관리, 트랜잭션 관리, 보안 등의 기능 제공 로드 밸런싱, 클러스터링, 캐싱 등의 기능 제공 모니터링, 로깅, 성능 측정 등의 기능 제공 요청 → 웹서버(nginx, apache) → WAS(Tomcat) → WAS에 작성한 프로그램 실행(쓰레드 실행) (하지만 요즘에는 최근에는 WAS 자체가 웹 서버 기능을 포함해 단일 서버로 웹 어플리케이션을 구축하는 것이 일반적) CGI는 프로세스를 생성하여 실행하지만 WAS는 쓰레드를 통해 실행됨 Django, Flask, FastAPI는 CGI, WAS와 같은 서버가 존재해야 함 FastAPI : ASGI 서버가 필요함 Flask : WSGI 서버가 필요 Django : WSGI, ASGI 모두 지원 Deploying to Production Django, being a web framework, needs a web server in order to operate. And since most web servers don’t natively speak Python, we need an interface to make that communication happen. Django currently supports two interfaces: WSGI and ASGI. ※ https://docs.djangoproject.com/en/4.1/howto/deployment/ Flask is a WSGI application. A WSGI server is used to run the application ※ https://flask.palletsprojects.com/en/2.2.x/deploying/ The main thing you need to run a FastAPI application in a remote server machine is an ASGI server program like Uvicorn.※ https://fastapi.tiangolo.com/ko/deployment/manually/ WSGI(Web Server Gateway Interface) Python 웹 어플리케이션 개발의 표준 인터페이스 Python 웹 어플리케이션과 웹 서버 간의 데이터를 주고 받기 위한 인터페이스 규격 웹 서버와 웹 프레임워크 사이에 존재 → WAS를 대체한다고 보면 됨 CGI는 프로세스를 생성하여 우리가 작성한 코드를 실행하는 반면 WSGI는 요청에 대해 우리가 작성한 코드를 콜백하여 호출하는 역할 즉, WSGI는 웹 프레임워크의 코드를 호출해 주는 역할 단점으로는 비동기 처리가 힘들음 Websocket이나 긴 HTTP 요청을 처리하기에 적합하지 않음(단일 동기 호출 방식으로 처리되어 오랜 시간 연결을 유지) WSGI는 요청에 대한 콜백이 동기적인 형태를 가졌기 때문 ASGI(Asynchronous Server Gateway Interface) Python 비동기 웹 어플리케이션과 웹 서버 간의 인터페이스 규격 WebSocket과 같은 실시간 통신을 지원(단일 비동기 호출이 가능하도록 설계됨) 클라이언트로부터 여러 이벤트를 주고받을 수 있으며, 백그라운드 코루틴을 실행할 수 있게 됨 SGI는 WSGI에 비해 더 높은 성능과 안정성, 그리고 기능을 제공 비동기 방식으로 동작하므로 멀티 스레드 환경에서 발생할 수 있는 문제를 해결 Python의 asyncio 라이브러리를 기반으로 하여 모듈 관리가 더욱 간편 uvicorn과 gunicorn 이란 Uvicorn Python ASGI(Asynchronous Server Gateway Interface) 웹 서버 (Python 비동기 웹 애플리케이션을 처리하는 데 사용) 기존의 웹 서버와 달리 비동기 방식으로 동작하여 높은 성능을 제공 비동기 방식으로 동작하기 때문에, 블로킹 연산 없이도 빠른 응답 속도를 제공할 수 있 asyncio 라이브러리를 사용하여 비동기 방식으로 동작 cypthon 으로 구현한 uvloop 를 사용하여 이벤트 루프를 구현(비동기 서버를 구현) 네트워크 I/O 에 대한 콜백 위주로 관리 Uvicorn은 다중 프로세스와 다중 스레드를 지원하여 높은 처리량을 제공하며, HTTPS, HTTP/2, WebSocket 등 다양한 프로토콜을 지원 Python 3.6 이상의 버전에서 사용할 수 있음 Gunicorn Python 웹 어플리케이션을 위한 WSGI(Web Server Gateway Interface) 웹 서버 Gunicorn은 다중 프로세스를 사용하여 Python 웹 어플리케이션을 처리하며, 다중 코어 시스템에서 높은 성능을 제공 프로세스 관리자 역할을 수행 FastAPI와 uvicorn과의 관계 FastAPI는 ASGI 인터페이스가 구현된 starlette를 추상화하여 구현된 웹 프레임워크 [웹 프레임워크]FastAPI - Starlette - [ASGI 웹 서버]Uvicorn - uvloop - libuv FastAPI를 사용하여 Python 웹 어플리케이션을 개발하고, ASGI 서버 중 하나인 Uvicorn을 사용하여 실행 FastAPI와 uvicorn + gunicorn과의 관계 gunicorn + uvicorn 을 조합하여 사용할 수 있으며 ASGI와 WSGI의 특징으로 다음과 같은 효과를 얻을 수 있음 비동기 처리 Uvicorn에서 비동기 처리가 필요한 부분을 Gunicorn으로 연결하여 비동기 처리를 지원 안정성: Uvicorn은 비동기 처리 방식으로 동작하기 때문에, 처리량이 높아질 때도 안정적으로 동작할 수 있음 Gunicorn은 다중 프로세스 방식으로 동작하기 때문에, 안정성을 보장할 수 있음 두 웹 서버를 함께 사용하면, 안정성을 보장하면서도 높은 처리량을 제공할 수 있음 gunicorn + uvicorn 를 함께 사용했을 때의 아키텍처 ​ 사용 방법 FastAPI 배포 전략 gunicorn + uvicorn 을 사용할 경우 프로세스 수 조정을 크게 신경 쓰지 않아도 되는 간단한 앱. 클러스터가 아닌 단일 서버에서 실행하는 경우. 단일 서버에 Docker Compose로 배포하는 경우 프로메테우스와 같은 모니터링 메트릭을 가져오는 경우, 하나의 컨테이너 + 여러 프로세스 조합으로 한번에 사용하는게 간단할 수 있음. uvicorn-gunicorn-fastapi-docker 는 내부적으로 자동으로 process와 worker 수를 자동으로 조절 CPU 자원 상태를 체크하고 이에 알맞는 값을 계산 반복되는 단일 서버나 단일 컨테이너로 띄울 때 좋은 이유 하지만 현실 운영에서는 이 자동 튜닝 부분이 적합하지 않을 수 있음 uvicorn 만 사용하는 경우 Kubernetes와 같은 컨테이너 오케스트레이션을 이용하여 클러스터에 여러 개의 pod으로 띄우는 경우 각각의 컨테이너 내에서 Gunicorn + Uvicorn 으로 여러 워커 + 프로세스 매니저를 사용할 경우, 이 리소스 관리가 아주 복잡 더 자세한 내용은 공식 문서 참고 : https://jiyeonseo.github.io/2022/10/24/uvicorn-gunicorn-fastapi-docker/","link":"/FastAPI/FastAPI-Deploy/"},{"title":"FastAPI - async def","text":"파이썬은 race condition 문제가 발생하는 것을 방지하기 위해 GIL을 사용 그렇기 때문에 멀티 쓰레드 환경을 정상적으로 지원하지 못함, 하지만 비동기적인 특성을 구현하기 위해선 다수의 쓰레드를 활용해야함 → 그래서 uvicorn은 비동기를 위해 멀티 프로세싱 방식을 이용 따라서 결론적으로 uvicorn은 비동기를 위해 멀티 프로세싱 방식을 이용 프로세스를 생성하는 방식은 fork, exec, spqen이 있음 파이썬에서는 운영체제에 따라 multiprocessing에서 프로세스를 생성하는 기본방법을 가짐 GIL 이란 여러 쓰레드에서 임계영역을 접근하게 되면 문제가 발생할 수 있음 따라서 thread safe를 보장하기 위해 GIL을 통해 다수의 쓰레드가 있더라도 하나의 쓰레드만 CPU를 할당 받아 동작 GIL을 사용해도 I/O 바운드가 일어나면 GIL을 해제(release)하여 다른 쓰레드에게 CPU를 재할당 Jython, pypy 에서는 gil을 사용하지 않음 FastAPI에서 async def ,def의 차이란 def : 외부 쓰레드 풀에서 다이렉트로 실행 → 서버가 블로킹되지 않음 async def : 외부 쓰레드 풀에서 실행하지 않아 블로킹 됨","link":"/FastAPI/FastAPI-async-def/"},{"title":"01. JPA란 무엇인가","text":"","link":"/JPA/01.%20JPA%EB%9E%80%20%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80/"},{"title":"02. 영속성 관리","text":"JPA에서 가장 중요한 2가지 객체와 관계형 데이터베이스 매핑하기(Object Relational Mapping**)** 영속성 컨텍스트 엔티티 매니저 팩토리와 엔티티 매니저 영속성 컨텍스트 JPA를 이해하는데 가장 중요한 용어 엔티티를 영구 저장하는 환경 이라는 뜻 EntityManager.persist(entity); 영속성 컨텍스트 와 엔티티 매니저의 관계 영속성 컨텍스트는 논리적인 개념 눈에 보이지 않음 엔티티 매니저를 통해서 영속성 컨텍스트에 접근 엔티티의 생명주기 비영속(new/transient) 영속성 컨텍스트와 전혀 관계가 없는 새로운 상태 코드로 보기 1234// 갹채룰 생성한 상태 (비영속)Member member = new Member();member.setId(&quot;shchoice&quot;);member.setUsername(&quot;shchoi&quot;); 영속(managed) 영속성 컨텍스트에 관리되는 상태 em.persist() find() 등으로 1차 캐시에 올라간 상태 코드 12345678910// 갹채룰 생성한 상태 (비영속)Member member = new Member();member.setId(&quot;shchoice&quot;);member.setUsername(&quot;shchoi&quot;);EntityManager em = emf.createEntityManager();em.getTransaction().begin();// 객체를 저장한 상태(영속)em.persist(member); 준영속(detached) 영속성 컨텍스트에 저장되었다가 분리된 상태 코드 12// 회원 엔티티를 영속성 컨텍스트에서 분리, 준영속 상태em.detach(member); 삭제(Removed) 삭제된 상태 코드 12// 객체를 상태한 상태(삭제)em.remove(member); 영속성 컨텍스트의 이점 1차 캐시 1차 캐시에서 조회 코드 123456789Member member = new Member();member.setId(&quot;shchoice&quot;);member.setUsername(&quot;shchoi&quot;);// 1차 캐시에 저장됨em.persist (member)// 1차 캐시에서 조회Member findMember = em.find(Member.class, &quot;shchoice&quot;); 1차 캐시에 없는 데이터를 조회 시 - 데이터베이스에서 조회 Member findMember02 = em.find(Member.class, “member2”; 데이터베이스에서 캐시에 저장하는 방식은 실제 운영 상에서는 크게 도움이 되지 않음 트랜잭션을 발생하게 되면 영속성 컨텍스트를 지우기 때문에 잠깐 순간에서만 1차 캐시의 이득을 봄 동일성(identity) 보장 1차 캐시로 반복 가능한 읽기(Repeatable Read) 등급의 트랜잭션 격리 수준을 데이터베이스가 아닌 애플리케이션 차원에서 제공 1234Member findMember01 = em.find(Member.class, 10L);Member findMember02 = em.find(Member.class, 10L);System.out.println(a == b); // 동일성 비교 true 트랜잭션을 지원하는 쓰기 지연(transactional write-behind) 123456789101112131415EntityManagerFactory entityManagerFactory = Persistence.createEntityManagerFactory(&quot;hello&quot;);EntityManager em = entityManagerFactory.createEntityManager();// EntityManager는 데이터 변경 시 Transaction을 시작해야 함.EntityTransaction tx = em.getTransaction(); // 트랜잭션 시작tx.begin();Member member01 = new Member(20L, &quot;shchoi&quot;);Member member02 = new Member(21L, &quot;shchoi&quot;);em.persist(member01);em.persist(member02);// 여기까지 INSERT SQL을 데이터베이스에 보내지 않는다.// 커밋하는 순간 데이터베이스에 INSERT SQL을 보낸다transaction.commit(); // 트랜잭션 커밋 em.persist(memberA) em.persist(memberB) transaction.commit() 변경 감지(Dirty Checking) 123456789Member member = em.find(Member.class, 20L);member.setName(&quot;shchoiii&quot;);// em.persist(member); JDBC와 같이 persist를 통해 update를 수행해야할 것 같지만 JPA는 아니다.// 마치 Collection 객체의 arrayList에 담듯이 set만 해주면 된다.System.out.println(&quot;================&quot;);// DB에 쿼리가 날아가 저장되는 시점tx.commit(); 1차 캐시에는 스냅샷이 있어 최초의 상태를 스냅샷으로 남김, 변경이 이러나면 Entity와 스냅샷을 다 비교해보고 JPA가 쓰기 지연 SQL 저장소 에 만들고 DB에 반영을 하게 된다. 지연 로딩(Lazy Loading) 플러시영속성 컨텍스트의 변경내용을 데이터베이스에 반영 플러시 발생 변경 감지 수정된 엔티티 쓰기 지연 SQL 저장소에 등록 쓰기 지연 SQL 저장소의 쿼리를 데이터베이스에 전송(등록, 수정, 삭제 쿼리) 영속성 컨텍스트를 플러시하는 방법 em.flush() - 직접 호출 트랜잭션 커밋 - 플러시 자동 호출 JPQL 쿼리 실행 - 플러시 자동 호출 JPQL 쿼리 실행 시 플러시가 자동으로 호출되는 이유 1234567em.persist(memberA);em.persist(memberB);em.persist(memberC);// 중간에 JPQL 실행 - persist를 하면 DB에 안날라가니까 자동으로 플러시 호출!query = em.createQuery(&quot;select m from Member m&quot;, Member.class);List&lt;Member&gt; members = query.getResultList(); 플러시 모드 옵션 em.setFlushMode(FlushModeType.COMMIT); FlushModeType.AUTO 커밋이나 쿼리를 실행할 때 플러시(default) FlushModeType.COMMIT 커밋할 때만 플러시 플러시는 영속성 컨텍스트를 비우지 않음 영속성 컨텍스트의 변경내용을 데이터베이스에 동기화 1차 캐시는 건드리지 않음 트랜잭션이라는 작업 단위가 중요 → 커밋 직전에만 동기화하면 됨 준영속 상태 영속 → 준영속 영속 상태의 엔티티가 영속성 컨텍스트에서 분리(detached) 영속성 컨텍스트가 제공하는 기능을 사용 못함 준영속 상태로 만드는 방법 em.detach(entity) 특정 엔티티만 준영속 상태로 전환 em.clear() 영속성 컨텍스트를 완전히 초기화 em.close() 영속성 컨텍스트를 종료","link":"/JPA/02.%20%EC%98%81%EC%86%8D%EC%84%B1%20%EA%B4%80%EB%A6%AC/"},{"title":"03. 엔티티 매핑","text":"데이터베이스 스키마 자동 생성 DDL을 애플리케이션 실행 시점에 자동 생성 테이블 중심 → 객체 중심 데이터베이스 방언을 활용해서 데이터베이스에 맞는 적절한 DDL 생성 DDL 생성 기능은 DDL을 자동 생성할 때만 사용되고 JPA의 실행 로직에는 영향을 주지 않음 이렇게 생성된 DDL은 개발 장비에서만 사용 스키마 자동 생성 기능이 만든 DDL은 운영 환경에서 사용할 만큼 완벽하지는 않기 때문 생성된 DDL은 운영서버에서는 사용하지 않거나, 적절히 다듬은 후 사용 persistence.xml 12&lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot;/&gt;&lt;property name=&quot;hibernate.hbm2ddl.auto&quot; value=&quot;create&quot;/&gt; 속성 옵션 설명 create 기존 테이블을 삭제하고 새로 생성, DROP + CREATE create-drop create 속성에 추가로 애플리케이션을 종료할 때 생성한 DDL을 제거, DROP + CREATE +DROP update DB Table과 Entity Mapping 정보를 비교해서 변경 사항만 수정 (※ 참고 : 추가에 대해서는 자동으로 변경하나 지우는 것에 대해서는 update되지 않음) validate DDL 을 수정하지 않음 DB Table과 Entity Mapping 정보를 비교해서 차이가 있으면 경고를 남기고, 애ㅐ플리케이션을 실행하지 않음 none 자동 생성 기능을 사용하지 않음(hibernate.hbm2ddk.auto 속성 자체를 삭제해도 됨) HBM2DDL 주의사항 운영 서버에서는 create, create-drop, update 속성과 같이 DDL을 수정하는 옵션을 절대 사용하면 안됨 사용 전략 개발 초기 단계 : create, update 자동화된 테스트를 진행하는 개발 환경/CI 서버 : create, create-drop 테스트 서버 : update, validate 스테이징/운영 서버 : validate, none 엔티티 매핑 소개 객체와 테이블 매핑: @Entity, @Table @Entity @Entity가 붙은 클래스는 JPA가 관리, 엔티티라 한다. JPA를 사용해서 테이블과 매핑할 클래스는 @Entity 필수 주의 기본 생성자 필수(파라미터가 없는 public 또는 protected 생성자) final 클래스, enum, interface, inner 클래스 사용 불가 저장할 필드에 final 사용 불가 속성 JPA에서 사용할 엔티티 이름을 지정 기본값 : 클래스 이름을 그대로 사용 같은 클래스 이름이 없으면 가급적 기본값을 사용 @Table @Table은 엔티티와 매핑할 테이블 지정 속성 기능 기본값 name 매핑할 테이블 이름 catalog 데이터베이스 catalog 매핑 schema 데이터베이스 schema 매핑 uniqueConstraints (DDL) DDL 생성 시에 유니크 제약 조건 생성 필드와 칼럼 매핑","link":"/JPA/03.%20%EC%97%94%ED%8B%B0%ED%8B%B0%20%EB%A7%A4%ED%95%91/"},{"title":"Hexo-Icarus style 설정","text":"Profile 수정프로필 아바타 이미지 및 소개 수정 cart.styl 경로 : themes/icarus/include/style 아바타 이미지에 요소검사를 수행하면 figure 태그를 수정하면 됨을 알 수 있음, 따라서 card-content &gt; image 를 수정 제목 및 소개 글의 요소 검사를 수행하면 p태그를 수정함을 알 수 있음, card-content &gt; level-item &gt; p 의 1,2,3 번째 요소를 수정 123456789101112131415.card-content .image width: 220px height: 220px .level-item p.title.is-size-4.is-block margin-top: 22px font-weight: 600 font-size: 22px p.is-size-6.is-block margin-top: 10px font-size: 16px p.is-size-6.is-flex.justify-content-center margin-top: 7px Article 수정Title 폰트 사이즈/굵기 및 본문과 간격 변경 article.styl 경로 : themes/icarus/include/style 제목에 요소 검사를 수행하면 article tag 하위에 css class로 title이 걸려있음을 확인할 수 있음, 따라서 아래 내용처럼 수정 12345&amp;.article .title font-size: 2.5rem font-weight: bold margin-bottom: 30px 본문의 h1, h2 굵기 변경 article.styl 경로 : themes/icarus/include/style article tag 하위에 content class 밑에 있는 h1, h2 태그를 수정 12345678910111213141516171819202122232425262728&amp;.article .title font-size: 2.5rem font-weight: bold margin-bottom: 30px .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size margin-top: 20px h1 font-size: 1.75em font-weight: bold h2 font-size: 1.5em font-weight: bold styl 파일 수정 후, hexo server로 확인 시 css 파일 지우기 styl 파일을 수정 후 hexo generate만 수행해서 사용한다면 pulbic/css/*.css 파일을 지워서 사용해야 함(파일이 있을 경우 수정 내용 반영 안됨) 위를 방지하고 싶으면 hexo clean &amp;&amp; hexo generate을 하자 Google Analytics 기능 추가 Google Analytics 에 Tracking ID 찾기 (애널리틱스 계정 생성 및 속성 생성 생략) 관리 &gt; 데이터 스트림 tracking ID 복사 [상세내용] Googel Analytics Tracking ID 설정 config_icarus.yml 파일에서 google_analytics &gt; tracking_id 란에 복사한 Tracking ID 붙여넣기 123google_analytics: # Google Analytics tracking ID tracking_id: G-D7AAAAAAAA post글에 댓글 기능 추가(by disqus) Disqus 회원 가입Disqus 및 Site 생성(생략) Short Name 복사 config_icarus.yml 파일에서 comment &gt; shorname에 복사 내용 붙여넣기 1234comment: type: disqus # Disqus shortname shortname: myshortname 레퍼런스 Googel Analytics Tracking ID 설정 https://eunii.github.io/2019/04/17/tec/blog/blog10/ https://datahack.dev/2020/04/18/etc/hexo-blog/","link":"/Github%20Pages/Hexo-Icarus_style_%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC/"},{"title":"Hexo-Icarus 테마를 이용한 블로그 개설","text":"Icarus 설치Github pages 블로그 개설을 위해 icarus 테마(hexo-theme-icarus)를 사용하기로 결정 (Window 환경) ※ 참고사항 Github 페이지 생성을 위한 3가지 프레임워크로 Hexo, Jekyll, Hugo 등이 있음각 프레임워크의 간단한 설명을 확인하고 싶으면 다음 블로그를 참조* Hexo-Icarus가 원하는 블로그 디자인에 가깝고 Hexo가 한글 문서화가 잘 되어있어 선택 Node.js 설치 다음 페이지에서 Node.js 최신 LTS 버전을 설치하면 된다. Node.js 를 설치해야 하는 이유는 Hexo 가 Node.js 기반이기 때문 hexo-cli를 Global 하게 설치 1$ npm install hexo-cli -g hexo로 blog 프로젝트를 생성cf) D:\\git 디렉토리에서 작업을 수행 1$ hexo init blog icarus 테마 적용을 위한 git clone 1$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus --depth 1 _config.yml 파일 수정 - theme key에 대한 value 값만 수정하면 됨, 기본값인 landscape를 icarus로 수정 1$ theme: icarus 변경된 테마 적용 1$ hexo config theme icarus 나의 경우에 패키지들이 설치 되지 않아 오류가 발생하여 다음과 같은 추가 패키지를 설치하였음 문제 해결 1 - Error: Cannot find module ‘semver’ 1npm install semver 문제 해결 2 - Error Package [xxx] is not installed※ ERROR 내용에 현재 시점의 icarus 버전에 맞는 패키지 명과 버전을 알려주기에 복사/붙여넣기 해서 사용하면 됨 1npm install --save bulma-stylus@0.8.0 hexo-component-inferno@^1.2.0 hexo-pagination@^2.0.0 hexo-renderer-inferno@^0.1.3 inferno@^7.3.3 inferno-create-element@^7.3.3 웹 페이지 기동 1$ hexo server Git Deployment hexo-deployer-git 설치 1$ npm install hexo-deployer-git --save _config_icarus.yml 파일 수정 12345deploy: type: git repo: https://github.com/&lt;username&gt;/&lt;project&gt; # example, https://github.com/hexojs/hexojs.github.io branch: master publish용 정적 파일 생성 1hexo generate Github 배포 1$ hexo clean &amp;&amp; hexo deploy 레퍼런스 hexo 공식 사이트 hexo-theme-icarus hexo-theme-icarus document https://hexo.io/docs/index.html https://hexo.io/docs/github-pages 기타 블로그 https://techwell.wooritech.com/blog/2021/08/08/Hexo-%EB%B8%94%EB%A1%9C%EA%B7%B8/ https://garden715.github.io/2020/06/29/hexo-icarus-deploy/","link":"/Github%20Pages/Hexo-Icarus_%E1%84%90%E1%85%A6%E1%84%86%E1%85%A1%E1%84%85%E1%85%B3%E1%86%AF_%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB_%E1%84%87%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3_%E1%84%80%E1%85%A2%E1%84%89%E1%85%A5%E1%86%AF/"},{"title":"Hexo-Icarus 를 통한 환경설정 및 블로그 글쓰기","text":"환경설정 _config.yml, _config_icarus.yml config.yml Hexo 를 위한 기본 환경설정 파일로, 블로그의 큰 틀에 대한 환경설정을 담당 홈페이지 제목, 저자명, 폴더 구조, 깃 허브 레퍼지토리 연동 등 Hexo-Icarus를 위한 추가 환경설정 파일 widgets profile, category, 최신 글, 태그 등, 페이지 레이아웃에 대한 설정 article configuration, search, comment 등에 대한 설정 header, footer 등에 대한 상세 설정 URL / 게시글 파일명 관리 permalink: ‘:title’ new_post_name : ‘:title.md’ 로 설정 deploy 설정 1234deploy: type: 'git' repo: 'https://github.com/&lt;계정명&gt;/&lt;ghithub page 레퍼지토리명&gt;.git' branch: master css, img, js 관련된 파일 $icraus_home/themes/icarus/source 경로에 작업을 하면된다. logo, profile 이미지를 바꾸고 싶다면 img 파일 밑에 파일을 놓으면 됨 profile _config.icarus.yml widgets type : profile 로 있음 포스트 글 작성하기 포스팅 방법 게시글 파일 트리 관리 경로 : source/_posts 에서 *.md 파일로 게시글을 관리함 category 별로 게시글을 관리하는 것이 편하기 때문에 아래와 같이 관리 카테고리 A / 게시글 a : _source/_posts/카테고리A/a.md, 카테고리 B / 게시글 b : _source/_posts/카테고리B/b.md 게시글 layout 구성 post, draft, page 3 종류가 있음 Layout Path post source/_posts page source draft source/_drafts draft 포스트를 바로 발행하는 것이 아니라 초안 글로 작성 (로컬 서버에서만 확인할 수 있음) 1$ hexo new draft &lt;게시글명&gt; 초안된 글을 post로 변경하려면 다음과 같이 publish 를 수행 필요 1$ hexo publish &lt;게시글명&gt; post 실제 게시글의 작성 1$ hexo new post &lt;게시글명&gt; 해당 명령어를 수행하게 되면 source/_posts 경로에 게시글.md 파일이 생성이 됨 -– title: ~ (이하생략) # title, tags, date, categories 등을 설정하면 자동으로 hexo에서 카테고리 등록 태그 등록, 게시글 등록을 하게 해주므로 반드시 설정 필요 -– # 여기서 부터는 파일의 내용을 기록 12345678910111213---title: Hexo-Icarus 를 통한 환경설정 및 블로그 글쓰기tags: [hexo, icarus]date: 2022-07-25 23:43:13categories: 'Github Pages'---## 환경설정1. _config.yml, _config_icarus.yml * config.yml * Hexo 를 위한 기본 환경설정 파일로, 블로그의 큰 틀에 대한 환경설정을 담당 -– -– 에 대해서는 자동적으로 기록이 되는데 이에 대한 layout을 바꾸고 싶다면 scaffolds/posts.md 파일을 다음과 같이 수정 123456---title: {{ title }}date: {{ date }}categories:tags: [ ]--- 새로 생긴 게시글은 카테고리 폴더를 생성하여 하위 경로에 관리하여도 됨 로컬에서 글쓰기 이상 있는지 점검 수행 1$ hexo server 글쓰기를 마치면, publish용 정적 파일 생성을 수행 1$ hexo generate Github에 배포를 수행 1$ hexo clean &amp;&amp; hexo deploy Multi-Category를 구성하는 방법 1234title: KoNLPy 설치(Ubuntu)tags: [KoNLPy, Mecab]date: 2022-08-04 09:39:12categories: [인공지능, 자연어처리] 레퍼런스 https://hexo.io/docs/writing https://datahack.dev/2020/04/18/etc/hexo-blog/ https://alleyful.github.io/2019/07/09/tools/hexo/hexo-guide-02/ https://velog.io/@trollering12312/hexo-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0#1-%EB%B8%94%EB%A1%9C%EA%B7%B8-%ED%8F%AC%EC%8A%A4%ED%8A%B8-%EC%9E%91%EC%84%B1 https://study.skyksit.com/hexo/hexo-new-post-process/ https://mishka.kr/5","link":"/Github%20Pages/Hexo-Icarus%E1%84%85%E1%85%B3%E1%86%AF_%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB_%E1%84%92%E1%85%AA%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC_%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%87%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3_%E1%84%80%E1%85%B3%E1%86%AF_%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%89%E1%85%A5%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"ElasticSearch를 통해 본 REST API 구조","text":"ElasticSearchURI 기본 형태 http://localhost:9200/index/id?parameters http://localhost:9200/index/action?parameters index : DBMS에서 데이터베이스에 해당 id : Document ID action : 특정 작업을 지시 _search : 검색 작업 _bulk : Bilk 색인 작업 기타 등등 공통 parameters: pretty: 반환값이 있다면 JSON response로 표시 v : verbose, 상세 정보 표시 help: 사용 가능한 칼럼 정보 표시 h=column1, column2 : headers, 칼럼 포시 bytes=b: 1kb 대신에 1024와 같이 숫자를 표시 ※ index/type/id 를 여러개 지정할 경우 “,”를 사용하여 구분, “*”를 사용하여 모두 지정할 수 있음 레퍼런스https://www.jopenbusiness.com/mediawiki/ElasticSearch_-_REST_API https://pcloud.tistory.com/16","link":"/REST/ElasticSearch%E1%84%85%E1%85%B3%E1%86%AF-%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A2-%E1%84%87%E1%85%A9%E1%86%AB-REST-API-%E1%84%80%E1%85%AE%E1%84%8C%E1%85%A9/"},{"title":"RESTful API를 구현하기 위한 원칙","text":"REST 란 REST는 Representational state transfer 의 약자로 WWW과 같은 분산 하이퍼미디어 시스템에서 운영되는 소프트웨어 아키텍처 스타일 REST는 HTTP/1.1 스펙과 동시에 만들어졌는데, HTTP 프로토콜을 정확히 의도에 맞게 활용하여 디자인하게 유도 따라서 디자인 기준이 명확해지며 의미적인 범용성을 지니므로 중간 계층의 Component들이 서비스를 최적화하는데 도움 REST의 기본 원칙을 성실히 지킨 서비스 디자인은 RESTful 하다 라고 표현 REST는 웹 서비스 아키텍처로서 아주 중요한 역할 API는 서비스가 여러 플랫폼을 지원해야할 때 API로서 외부에 공개되어야할 때 설명을 간결하게 하며, 여러가지 문제 상황을 지혜롭게 해결하기 때문(버전, 포맷 등) 중심 규칙 URI는 정보의 자원을 표현해야 함 자원에 대한 행위는 HTTP Method(GET, POST, PUT, DELETE 등)으로 표현 1GET /users/show/1 (👎) 참고 레퍼런스 https://spoqa.github.io/2012/02/27/rest-introduction.html","link":"/REST/RESTful-API%E1%84%85%E1%85%B3%E1%86%AF-%E1%84%80%E1%85%AE%E1%84%92%E1%85%A7%E1%86%AB%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5-%E1%84%8B%E1%85%B1%E1%84%92%E1%85%A1%E1%86%AB-%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%8E%E1%85%B5%E1%86%A8/"},{"title":"KoNLPy 설치(Ubuntu)","text":"배경한국어를 위한 Text Classification을 수행하기 위해 형태소 분석기(Mecab)를 사용할 필요가 있었음이를 위해 하지만 konlpy 공식 홈페이지 의 설명대로 수행하였으나 잘 설치가 되지 않아 결국 수동으로 직접 설치 수행 서버 : Ubuntu 16.04 Python Version : 3.8.13 JDK Versuib: openjdk 1.8.0_292 Mecab은 Java로 구현된 모듈이기에 필요 KoNLPy Version : 0.60 (0.52 에서도 정상 동작 확인) 설치 날짜 : 22.08.03 설치 Install KoNLPy Package 1python3 -m pip install konlpy ubuntu dependency 설치 1sudo apt-get install g++ openjdk-8-jdk python3-dev python3-pip curl ※ openjdk-8-jdk : 필자는 기존에 JDK가 설치가 되어져 있었음, JDK가 설치 되어있지 않다면 해당 작업 외에 JDK를 bash에서 사용할 수 있도록 추가 작업 필요! mecab-ko 설치 12345678cd ~/util # util 파일 위치 지정(/tmp, /usr/local/share/util 등)sudo wget https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gzsudo tar xvf mecab-0.996-ko-0.9.2.tar.gzcd mecab-0.996-ko-0.9.2sudo ./configuresudo make checksudo make install mecab-dic 설치 123456789cd ~/utilwget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gztar zxvf mecab-ko-dic-2.1.1-20180720.tar.gzcd mecab-ko-dic-2.1.1-20180720sudo ./autogen.shsudo ./configuresudo makesudo make install ※ 아래와 같은 오류 발생 시 sudo apt-get install automake libtool 수행 12345(pytorch_p38) shchoice@dq-mls-01:~/util/mecab-ko-dic-2.1.1-20180720$ sh autogen.sh Looking in current directory for macros.autogen.sh: 11: autogen.sh: aclocal: not foundautogen.sh: 14: autogen.sh: autoconf: not foundautogen.sh: 15: autogen.sh: automake: not found mecab-python 설치 12345cd ~/utilgit clone https://bitbucket.org/eunjeon/mecab-python-0.996.gitcd mecab-python-0.996python3 setup.py buildpython3 setup.py install mecab 정상 동작 확인 123456789$ pythonPython 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; from konlpy.tag import Mecab&gt;&gt;&gt; mecab = Mecab()&gt;&gt;&gt; sentence = &quot;안녕하세요~ KoNLPy 설치 후 테스트를 위한 문장입니다.&quot;&gt;&gt;&gt; mecab.morphs(sentence)['안녕', '하', '세요', '~', 'KoNLPy', '설치', '후', '테스트', '를', '위한', '문장', '입니다', '.'] [참고] 공식 홈페이지를 통한 설치 시 오류 내용 mecab.sh 스크립트 사용 중 오류 123456789101112131415161718bash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)기존:34 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease 기존:35 http://ppa.launchpad.net/webupd8team/y-ppa-manager/ubuntu xenial InRelease 기존:36 http://mariadb.mirror.liquidtelecom.com/repo/10.4/ubuntu xenial InRelease 내려받기 72.2 k바이트, 소요시간 2초 (25.8 k바이트/초)패키지 목록을 읽는 중입니다... 완료E: 설치 방법 드라이버 /usr/lib/apt/methods/&lt;https을(를) 찾을 수 없습니다.E: 설치 방법 드라이버 /usr/lib/apt/methods/&lt;https을(를) 찾을 수 없습니다.E: 설치 방법 드라이버 /usr/lib/apt/methods/oops을(를) 찾을 수 없습니다.W: GPG 오류: http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease: 다음 서명들은 공개키가 없기 때문에 인증할 수 없습니다: NO_PUBKEY A4B469963BF863CCW: The repository 'http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease' is not signed.N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.N: See apt-secure(8) manpage for repository creation and user configuration details.E: &lt;https://dl.bintray.com/rabbitmq-erlang/debian&gt;/dists/xenial/InRelease 파일을 받는데 실패했습니다 E: &lt;https://dl.bintray.com/rabbitmq/debian&gt;/dists/xenial/InRelease 파일을 받는데 실패했습니다 E: oops://ubuntu.com/dists/foo/InRelease 파일을 받는데 실패했습니다 E: Some index files failed to download. They have been ignored, or old ones used instead. pip3 install konlpy 설치는 완료했지만 위의 스크립트 파일 실행 오류로 tagger를 읽어오지 못하는 오류 발생 1234567891011121314151617Python 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0] :: Anaconda, Inc. on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; from konlpy.tag import Mecab&gt;&gt;&gt; mecab = Mecab()Traceback (most recent call last): File &quot;/home/shchoice/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/konlpy/tag/_mecab.py&quot;, line 77, in __init__ self.tagger = Tagger('-d %s' % dicpath)NameError: name 'Tagger' is not definedDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/shchoice/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/konlpy/tag/_mecab.py&quot;, line 82, in __init__ raise Exception('Install MeCab in order to use it: http://konlpy.org/en/latest/install/')Exception: Install MeCab in order to use it: http://konlpy.org/en/latest/install/ 느낀점KoNLPy를 설치를 3번째 해보는데, 매번 오래된? 관리가 잘되지 않은? ubuntu 환경에서 사용해서인지 홈페이지 가이드 대로 설치를 수행하다 불필요하게 시간을 소모했다. 따라서 KoNLPy 설치할 때에는 그냥 처음부터 수동으로 설치를 하자","link":"/Deep%20Learning/NLP/KoNLPy%20%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5(Ubuntu)/"},{"title":"1장 Orientation - Introduction to NLG","text":"Our Objective 컴퓨터가 인간이 만들어놓은 대량의 문서를 통해 정보를 얻고(NLU) 얻어낸 정보를 사람이 이해할 수 있게 사람의 언어로 표현하는 것(NLG) Before Sequence-to-Sequence 단지 단어/문장(text)을 벡터로(numeric)으로 바꿀 뿐이었음 After Sequence-to-Sequence With Attention Beyond “numeric to text” 이제는 숫자를 넘어 text로 표현이 가능해짐, 글을 넣으면 숫자로 바뀌었던 것이 숫자를 넣으면 글로 바뀜 Era of Attention Transformer의 등장으로 인해 연구는 더더욱 가속됨 PLM의 유행으로 인해 NLG 뿐만 아니라 NLP의 다른 영역에도 큰 영향 거스를 수 없는 대세, PLM 이 수업은 PLM을 제대로 이루기 위한 Step-stone In this class, NMT(한때 자연어 처리의 꽃이었던)를 통해 자연어 생성의 근본부터 다질 수 있도록 구성 반복된 학습을 통해 Auto-regressive 특성을 몸으로 익히고, 이를 해결하기 위한 여러가지(Empirical + Mathematica, 경험/수학적) 방법들을 다룸 Sequence-to-Sequence w/ Attention뿐만 아니라, Transformer도 nano 단위로 detail하게 분해하여 이해/ 구현할 수 있도록 구성 짧은 실습을 통해 단순히 구현하는 것에 그치는 것이 아닌, 실제 업무/연구와 같이 프로젝트 설계부터 객관적인 평가방법까지 A to Z를 경험하도록 구성 이를 통해 추후 PLM을 활용한 NLP 심화 과정을 어려움 없이 받아들이도록 구성","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/1%E1%84%8C%E1%85%A1%E1%86%BC-Orientation-Introduction-to-NLG/"},{"title":"1장. Orientation - Review NLP Introduction Class","text":"Review Statistical &amp; Geometric Perspective for Deep LearningBefore this class Our Objective is 세상에 존재하는 어떤 미지의 함수를 모사하자 주어진 입력(𝒙)에 대해서 원하는 출력(𝒚)을 반환하도록, 손실함수를 최소화하는 파라미터(𝜽)를 찾자. Gradient Descent를 수행하기 위해 back-propagation을 수행하자 After this class Our Ojbective becomes 세상에 존재하는 어떤 미지의 확률 분포 함수를 모사(approximate)하자 Probablistice Perspective 확률 분포 𝑃(𝑥) 와 𝑃(𝑦|𝑥)로부터 데이터를 수집하여 해당 데이터를 가장 잘 설명하는 확률 분포 함수의 파라미터(𝜃)를 찾자: 𝑙𝑜𝑔𝑃(𝑦│𝑥;𝜃) Maximum Likelihood Estimation (해당 데이터를 잘 설명하는 score!, 확률 분포함수를 예측하고 싶음) Gradient Descent using Back-propagation (Likelihood를 maximize하기 위함) 또는 두 확률 분포를 비슷하게 만들자 Minimize Cross Entropy (or KL-Divergence) # $P(y|x) \\approx P_\\theta (y|x)$ Geometric Perspective 데이터란 저차원의 manifold에 분포되어 있으며, 여기에 약간의 노이즈가 추가되어 있는 것 노이즈란 task(x -&gt; y)에 따라서 다양하게 해석 가능할 것 따라서 해당 manifold를 배울수 있다면, 더 낮은 차원으로 효율적인 맵핑(or project)이 가능 Non-Linear dimension reduction (AutoEncoder 등) Representation Learning 낮은 차원으로의 표현을 통해, 차원의 저주(curse of dimensionality)를 벗어나 효과적인 학습이 가능 결론 딥러닝이란 확률 분포 함수인 동시에 비선형의 낮은 차원으로 차원을 축소(Gemetric 관점 + 정보이론)하는 것 DNN은 굉장히 유연한(flexible)한 함수이기 때문에 다양한 관점에서 해석이 가능 따라서 대부분의 새롭게 제시되는 방법들은 위의 관점에서 설계되고 제안된 것 위의 관점에서 딥러닝을 바라본다면, 훨씬 쉽게 이해할 수 있음 Review NLP Introduction ClassReview : AutoEncoder 인코더(encoder)와 디코더(decoder)를 통해 압축과 해제를 실행 인코더는 입력(x)의 정보를 최대한 보존하도록 손실 압축을 수행 디코더는 중간 결과물(z)의 정보를 입력(x)과 같아지도록 압축 해제(복원)를 수행 복원을 성공적으로 하기 위해, autoencoder는 특징(feature)을 추출하는 방법을 자동으로 학습 In Word2Vec objective : 주어진 단어로 주변 단어를 예측하자 y를 예측하기 위해 필요한 정보 z가 있어야 한다. 주변 단어를 잘 예측하기 위해 x를 잘 압축하자. 예제 |V| = 30000 을 256차원으로 압축 In Text Classification Using RNN &amp; CNN 요약 신경망은 $x$와 $y$ 사이의 관계를 학습하는 과정에서 feature를 자연스럽게 학습 특히 저차원으로 축소(압축)되는 과정에서 정보의 취사/선택이 이루어짐 감성분석에서 사용하는 feature와 기계번역/요약에서 사용하는 feature는 서로 다름 Word Embedding(Skip-gram) 주변 단어(y)를 예측하기 위해 필요한 정보를 현재 단어($x$)에서 추출하여 압축 Sentence Embedding(Text Classification) Label($y$ )을 예측하기 위해 필요한 정보를 단어들의 시퀀스($x$)로부터 추출하여 압축","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/1%E1%84%8C%E1%85%A1%E1%86%BC-Orientation-Review-NLP-Introduction-Class/"},{"title":"2장. Language Modeling - Auto-regressive &amp; Teacher Forcing","text":"Application Two Approaches Non-autoregressive (Non-generative) 현재 상태가 앞/뒤 상태를 통해 정해지는 경우 (sequence의 모든 timestamp를 보고 정해짐) e.g. Part of Speech(POS) Tagging, Text Classification Bidirectional RNN 사용 권장 Autoregressive (Generative) 현재 상태가 과거 상태에 의존하여 정해지는 경우 (과거에서 현재로 방향성이 있음) e.g. Natural Language Generation, Machine Translation One-to-Many case 해당 Bidirectional RNN 사용 불가!(방향성이 있으니까! 앞-&gt; 뒤 혹은 뒤-&gt; 앞 하나만 사용 가능) Auto-regressive Inference $\\hat{x}t = \\text{argmax}{x_t \\in \\chi} \\log P(x_t | \\hat{x}{&lt;t}; \\theta)$ Auto-regressive 과거 자신의 상태를 참조하여 현재 자신의 상태를 업데이트 $\\hat{x}{t=1} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t=1} | x_0; \\theta)$ $\\text{ where } x_0 =$ $\\hat{x}{t=2} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t=2} | x_0,\\hat{x}_1; \\theta)$ $\\hat{x}{t=3} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t=3} | x_0,\\hat{x}_1, \\hat{x}_2; \\theta)$ … $\\hat{x}{t} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t} | x_0,\\hat{x}_{x&lt;t}; \\theta)$ Teacher-Forcing MLE의 수식상, 정답 $x_{t-1}$을 RNN의 엽력으로 넣어줘야 함 $D={x^i }_{i=1}^N$ . // $x^i \\sim P(x)$ : $P(x)$라는 분포에서 문장을 샘플링 함($x^i$) $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log P(x_{1:n}^{i}; \\theta)$ // log-likelihood를 최대로 하는 $\\theta$를 찾음 $= \\text{argmax}{\\theta \\in \\heta} \\sum{i=1}^{N}\\sum_{j=1}^{n} \\log P(x_{j}^{i}|x_{&lt;j}^i; \\theta)$ // by chain-rule, 근데 문제는 hat이 없음 ​ $\\text{where } x_{1:n} = {x_1, …, x_n}$ Auto-regressive &amp; Teacher Forcing Inference Mode https://github.com/shchoice/shchoice.github.io/assets/100276387/8af48ef4-7112-4545-b656-8486681d78e0 학습은 이렇게 못한다. 왜냐하면 loss를 구할 수 없기 때문이다. 다음페이지와 비교! 그래서 나온 것이 Teacher forcing,즉 Auto-regressive 속성을 가진 sequential한 모델을 학습하는 방법 Training Mode 만약에 hat이 들어가면 가 들어갈 때까지의 갯수 이기에, 5단어를 넣어도 5개이상의 단어가 나올 수 있는 문제 발생 고통의 시작 : NLG is Auto-regressive Task Auto-regressive task에서는 보통 이전 time-step의 모델을 출력(x ̂_(t-1))을 다음 time-step의 입력으로 넣어줌 이전 time-step의 출력에 따라 현재 모델의 state가 바뀌게 될 것 하지만 적절한 학습을 위해서는 학습 시에는 이전 time-step의 출력 값이 아닌**, 실제 정답을 넣어줌** 따라서 학습과 추론을 위한 방법이 다르게 되어 여러가지 문제가 발생 학습을 위한 코드와 추론을 위한 코드를 따로 짜야 함 학습과 추론 방법의 괴리(discrepancy)가 발생하여 성능이 저하될 수 있음 왜냐하면 train일때는 정답 x가 time-stamp마다 들어와서 hidden state에서 넘겨주는 값보다 정답을 더 중요시 할 수 있어 이전 state를 중시 안함 하지만 inference 시에는 정답이 아니라 정답 예측값이 넘어오기에 괴리가 발생..","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Language-Modeling-Auto-regressive-Teacher-Forcing/"},{"title":"2장. Language Modeling - How to evaluate LM(Perplexity)","text":"How to evaluate LM - PerplexityHow to Evaluate Test set 나는 학교에 갑니다 나는 학교를 갑니다. Intrinsic evaluation(정성 평가) 정확함 시간과 비용이 많이 들어감 Extrinsic evaluation(정량 평가) 시간과 비용을 아낄 수 있음 Intrinsic evaluation과 비슷할수록 좋은 방법! What is Good Language Model? 실제 사용하는 언어의 분포를 가장 잘 근사한 모델 실제 사용하는 언어 → 테스트 시의 입력 문장들 분포를 잘 근사 → 문장의 likelihood가 높을 것 잘 정의된 테스트셋의 문장에 대해서 높은 확률을 반환하는 언어모델이 좋은 모델! Evaluation Perplexity (PPL) 란 테스트 문장에 대해서 언어모델을 이용하여 확률(likelihood)을 구하고 PPL 수식에 넣어 언어모델의 성능 측정 문장의 확률을 길이에 대해서 normalization (기하평균) $PPL(x_1, \\ldots, x_n; \\theta) = P(x_1, \\ldots, x_n; \\theta)^{-\\frac{1}{n}} = \\sqrt[n]{ \\frac{1}{P(x_1, \\ldots, x_n; \\theta)}}$ 단어들의 Chain Rule 이기에 문장이 길수록 확률곱으로 값이 작아짐(1보다 작은 값이기 때문) 짧은 문장에 비해 긴 문장은 확률이 작아지고 길이에 따른 기하평균을 하기에(-1/n) 길이에 상관없이 normalize할 수 있음 확률에 역수를 취했으므로 PPL은 작을수록 성능이 좋음을 의미 Chain Rule에 의해서 $PPL(x_1, \\ldots, x_n; \\theta) = P(x_1, \\ldots, x_n; \\theta)^{-\\frac{1}{n}} =\\sqrt[n]{\\frac{1}{P(x_1, \\ldots, x_n; \\theta)}} =\\sqrt[n]{\\frac{1}{\\prod_{i=1}^{n} P(x_i | x_{&lt;i}; \\theta)}}$ Markov assumption이 적용 될 경우 $PPL(x_1, \\ldots, x_n; \\theta) = P(x_1, \\ldots, x_n; \\theta)^{-\\frac{1}{n}} = \\sqrt[n]{ \\frac{1}{P(x_1, \\ldots, x_n; \\theta)}} =\\sqrt[n]{\\frac{1}{\\prod_{i=1}^{n} P(x_i | x_{&lt;i}; \\theta)}} \\approx \\sqrt[n]{\\frac{1}{\\prod_{i=1}^{n} P(x_i | x_{i-1}, \\ldots, x_{i-k}; \\theta)}}$ Perplexity 테스트 문장에 대해서 확률을 높게 반환할수록 좋은 언어모델 테스트 문장에 대한 PPL이 작을수록 좋은 언어모델 예제 주사위를 던져봅시다. 1부터 6까지의 6개의 숫자로 이루어진 수열 1부터 6까지 6개의 숫자의 출현 확률은 모두 같음 uniform distribution $D={x^i}_{i=1}^{N} \\text{, where } x_i \\sim P(x) \\text{ and } \\forall x \\in {1, 2, 3, 4, 5, 6}$ $PPL(x_1, \\ldots, x_n; \\theta) =\\sqrt[n]{\\frac{1}{P(x_1, \\ldots, x_n; \\theta)}}=\\sqrt[n]{\\frac{1}{\\prod_{i=1}^{n} P(x_i)}}$ // 독립시행이기에$=\\sqrt[n]{\\frac{1}{(\\frac{1}{6})^n}} = 6$ // 주사위 면의 갯수 Perplexity를 해석하는 방법 주사위 PPL : 매 time-step 가능한 가짓수인 6 뻗어나갈 수 있는 branch(가지)의 숫자를 의미 Time-step 별 평균 branch의 수 PPL이 낮을수록 확률 분포가 Sharp 하다. PPL이 높을수록 확률 분포가 Flat 하다. Summary 좋은 언어모델 잘 정의된 테스트셋 문장에 대해서 높은 확률(=낮은 PPL)을 갖는 모델 Perplexity(PPL) Lower is better 확률의 역수에 문장 길이로 기하 평균 매 time-step마다 평균적으로 헷갈리고(no clue) 있는 단어의 수 Perplexity &amp; EntropyPerplexity Sharp vs Flat distribution Perplexity가 높으면 flat하고(고르다) Perplexity가 낮을수록 sharp하다 Information and Entropy 정보이론에서 엔트로피는 어떤 정보의 불확실성을 나타냄 불확실성은 일어날 것 같은 사건(likely event)의 확률 자주 발생하는 (일어날 확률이 높은) 사건은 낮은 정보량을 가짐 드물게 발생하는 (일어날 확률이 낮은) 사건은 높은 정보량을 가짐 불확실성 ∝ $\\frac{𝟏}{확률}$∝ 정보량 정보량 수식 $I(\\text{x})=-\\log P(\\text{x})$ # x라는 random variable에 대한 정보 0≤𝑃(𝑥)≤1 정보량 예제 예제1 내일 아침 해는 동쪽 하늘에서 뜹니다. → 확률이 높을수록 정보량이 낮다. 내일 아침 해는 서쪽 하늘에서 뜹니다. → 확률이 낮을수록 엄청난 정보량을 가지고 있다. 예제2 올 여름 대한민국의 평균 여름 기온은 30도 입니다. → 정보량이 낮음 올 여름 대한민국의 평균 여름 기온은 10도 입니다.→ 정보량이 높음 언어모델 관점 예제 흔히 나올 수 없는 문장(확률이 낮은 문장)일수록 더 높은 정보량 Perplexity &amp; Entropy Cross Entropy $H(P, P_\\theta) = -E_{x_{1:n} \\sim P}[\\log P(x_{1:n}; \\theta)]$ // P(X)에서 어떤 문장을 sampling하여 우리 모델에 넣었을 때로그 확률값을 평균내고 마이너스를 취하는 CE $\\approx -\\frac{1}{n} \\sum_{x_{1:n} \\in X} P(x_{1:n}) \\log P(x_{1:n}; \\theta)\\text{, defined as per-word entropy}$ //P(x_{1:n})\\log{P(x_{1:n}; \\theta)}: GT x Log-likelihood $\\approx -\\frac{1}{n \\times N} \\sum_{i=1}^{N} \\log P(x_{1:n}^{i}; \\theta) \\text{, by Monte-Carlo}\\approx -\\frac{1}{n} \\log P(x_{1:n}; \\theta)\\text{, where N=1}$ // 1개의 문장으로 CE를 구함$\\approx -\\frac{1}{n} \\sum_{i=1}^{N} \\log P(x_i | x_{&lt;i}; \\theta)$ // by chain rule$=L(x_{1:n}; \\theta)$ $L(x_{1:n}; \\theta) \\approx -\\frac{1}{n} \\sum_{i=1}^{N} \\log P(x_i | x_{&lt;i}; \\theta)=-\\frac{1}{n} \\log \\prod_{i=1}^{n} P(x_i | x_{&lt;i}; \\theta)=\\log\\sqrt[n]{\\frac{1}{\\prod_{i=1}^{n} P(x_i | x_{&lt;i}; \\theta)}}=\\log{PPL(x_{1:n};\\theta)}$ 즉 PPL exp(CE)와 같다. 요약 Objective : minimize perplexity equivalent to minimize cross entropy is also same as minimizing negative log-likelihood 문장의 likelihood를 maximize하는 파라미터를 찾고 싶음 Ground-truth 확률 분포(실제 사람이 가진 언어 모델)에 언어모델을 근사(approximate)하고 싶음 GT 분포와 LM 분포 사이의 cross entropy를 구하고 minimize 문장의 perplexity를 minimize. perplexity를 통해 헷갈리는 단어들의 숫자이기에, 내 LM이 매 time-step마다 헷갈리는 단어들의 숫자가 몇 개구나라는 것을 알 수 있음.","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Language-Modeling-How-to-evaluate-LM-Perplexity/"},{"title":"2장. Language Modeling - Introduction to LM","text":"Introduction 우리의 머릿속에는 단어와 단어 사이의 확률이 우리도 모르게 학습되어 있음 대화를 하다가 정확하게 듣지 못하여도 대화에 지장이 없음 많은 문장들을 수집하여, 단어와 단어 사이의 출현 빈도를 세어 확률을 계산 궁극적인 목표는 우리가 일상 생활에서 사용하는 언어의 문장 분포를 정확하게 모델링 하는 것(그것을 통해 우리의 머릿속에 있는 확률 분포 함수를 잘 근사해야 함!) 특정 분야(domain)의 문장의 분포를 파악하기 위해서 해당 분야의 말뭉치를 수집하기도 Again, Korean is Hell 단어와 단어 사이의 확률을 계산하는데 불리하게 작용 단어의 어순이 중요하지 않기 때문 또는 생략 가능하기 때문 Example 나는 학교에 갑니다 버스를 타고. 나는 버스를 타고 학교에 갑니다. 버스를 타고 나는 학교에 갑니다. (나는) 버스를 타고 학교에 갑니다. 확률이 퍼지는 현상 ‘타고’ 다음에 나타날 수 있는 단어들은 ‘.’, ‘학교에‘ ‘나는‘ 3개이기 때문 접사를 따로 분리해주지 않으면 어휘의 수가 기하급수적으로 늘어나 희소성이 더욱 높아짐 Applicartions Natural Language Generation Task Description Speech Recognition Acoustic Model과 결합하여, 인식된 phone(음소)의 sequence에 대해서 좀 더 높은 확률을 갖는 sequence로 보완 Machine Translation 번역 모델과 결합하여, 번역된 결과 문장을 자연스럽게 만듬 Optical Character Recognition 인식된 character candidate sequence에 대해서 좀 더 높은 확률을 갖는 sequence를 선택하도록 도움 Other NLG Tasks 뉴스 기사 생성, chat-bot 등 Others 검색어 자동 완성 등 Chain Rule We can convert joint probability yo conditional probability $P(A,B,C,D)=P(D|A,B,C)P(A,B,C)=P(D|A,B,C)P(C|A,B)=P(D|A,B,C)P(C|A,B)P(B|A)P(A)$ P(,I, Love, to, play, ) = P(|,I, Love, to, play) P(,I, Love, to, play) = P(|,I, Love, to, play) P(play|,I, Love, to) P(,I, Love, to) = P(|,I, Love, to, play) P(play|,I, Love, to) P(to|,I, Love)P(,I, Love) = P(|,I, Love, to, play) P(play|,I, Love, to) P(to|,I, Love)P(Love|,I)P(,I) = P(|,I, Love, to, play) P(play|,I, Love, to) P(to|,I, Love)P(Love|,I)P(I|)P() By Chain Rule, $P(x_{1:n}) = P(x_1, \\ldots, x_n)= P(x_n | x_{1}, \\ldots, x_{n-1}) \\ldots P(x_2| x_1)P(x_1)= \\prod_{i=1}^n P(x_i | x_{&lt;i})$ // $x_i$ 이전까지의 단어가 주어졌을 때, i의 확률값의 곱 $\\log P(x_{1:n}) = \\sum_{i=1}^N \\log P(x_i | x_{&lt;i})$ 언어모델(Language Model) language modeling은 문장의 출현 확률을 모델링을 하는 동시에 단어가 주어졌을 때, 다음 단어의 출현 확률을 모델링하는 것 Chain Rule을 통해 문장의 확률을 모델링을 하게 되면, 문장 내 단어가 주어졌을 때, 다음 단어의 확률을 알 수 있게됨 언어 모델 수식 $D={x^i }_{i=1}^N$ . // $x \\sim P(x)$, $x^i$ 는 문장, 즉 대문자 N개의 문장 $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log P(x_{1:n}^{i}; \\theta)$ // log-likelihood 값을 다 더해서 최대로 하는 파라미터를 구하자 1$= \\\\text{argmax}_{\\\\theta \\\\in \\\\Theta} \\\\sum_{i=1}^{N}\\\\sum_{j=1}^{n} \\\\log P(x_{j}^{i}|x_{&lt;j}^i; \\\\theta)$ // chain rule에 의해 다음과 같이 작성할 수 있음 $\\text{where } x_{1:n} = {x_1, …, x_n}$ // 문장은 n개의 단어로 이루어짐 $L(\\theta) = -\\sum_{i=1}^{N} \\log P(x_{1:n}^{i}; \\theta)$ $\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta L(\\theta)$ 더 좋은 문장이 무엇인지 선택할 수 있음(Pick better/fluent sentece) $x^1, x^2$ (문장1과 문장2) 가 있을 때 더 높은 확률값을 고르면 된다. $P_θ(x^1) &gt; P_θ(x^2)$ 이전 단어들을 통해 다음 단어들을 예측 할 수 있음(Predict next word given previous words) $\\hat{x_t}= \\text{argmax}{x_t \\in \\Chi} \\log P(x{t}|x_{&lt;t}; \\theta)$ 요약 언어모델은 주어진 코퍼스 문장들의 likelihood를 최대화 하는 파라미터를 찾아내, 주어진 코퍼스를 기반으로 언어의 분포를 학습한다. 즉, 코퍼스 기반으로 문장들에 대한 확률 분포 함수를 근사(approximate)한다. 문장의 확률은 단어가 주어졌을 때, 다음 단어를 예측하는 확률을 차례대로 곱한 것과 같음 따라서 언어모델링은 주어진 단어가 있을 때, 다음 단어의 likelihood를 최대화하는 파라미터를 찾는 과정이라 볼 수 있다. 주어진 단어들이 있을 때, 다음 단어에 대한 확률 분포 함수를 근사하는 과정","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Language-Modeling-Introduction-to-LM/"},{"title":"2장. Language Modeling - N-gram Language Model","text":"N-gram Language Model좋은 모델이란 무엇인가 Generalization (일반화를 잘 하는 모델) Training(seen) data를 통해서 test(unseen) data에 대해 훌륭한 prediction을 할 수 있는가 만약 모든 경우의 수에 대해 학습 데이터를 모을 수 있다면, table look-up(DB)으로 모든 문제를 풀 수 있을 것 하지만 그것은 불가능하므로 generalization 능력이 중요 (사람은 이 능력이 뛰어남, 보지못한 걸 기존에 본 것으로 잘 예측 함) Count based Approximation 주어진 문장이 다음과 같다면 P(,I, Love, to, play, ) = P(|,I, Love, to, play) P(,I, Love, to, play) = P(|,I, Love, to, play) P(play|,I, Love, to) P(,I, Love, to) = P(|,I, Love, to, play) P(play|,I, Love, to) P(to|,I, Love)P(,I, Love) = P(|,I, Love, to, play) P(play|,I, Love, to) P(to|,I, Love)P(Love|,I)P(,I) = P(|,I, Love, to, play) P(play|,I, Love, to) P(to|,I, Love)P(Love|,I)P(I|)P() 우리는 Word Sequnce의 count에 기반한 조건부 확률을 통해 근사할 수 있음(We can approximate conditional probability by counting word sequence) P(,I, Love, to, play, ) $\\approx \\frac{COUNT(,I, Love, to, play, )}{COUNT(,I, Love, to, play)}$ 이를 수식으로 나타내면 $P(x_n|x_{&lt;n}) \\approx \\frac{COUNT(x_1,⋯,x_{n})}{COUNT(x_1,⋯,x_{n-1})}$ 문제점 만약에 문장의 빈도수가 0인 것이 있다면, chain rule에 따라 모두가 0이 되어서 큰 문제가 발생 심지어는 분모가 0이 될 수도 있음 Markov Assumption을 적용하면 Apporximate with counting only previous k tokens (모든 단어를 보지 않고 k개만 봄) $P(x_n|x_{&lt;n}) \\approx P(x_n|x_1,⋯,x_{n-k}) \\approx \\frac{COUNT(x_{n-k},⋯,x_{n})}{COUNT(x_{n-k},⋯,x_{n-1})}$ if k = 2 $P(x_n|x_{&lt;n}) \\approx P(x_n|x_{n-2},x_{n-1},x_{n}) \\approx \\frac{COUNT(x_{n-2},x_{n-1},x_{n})}{COUNT(x_{n-2},x_{n-1})}$ 만약에 이를 Sentence level에 까지 적용한다면 training corpus에서 보지 못한 word sequnce까지 대처할 수 있음 $\\log P(x_{1:n}) = \\sum_{i=1}^N \\log P(x_i | x_{&lt;i}) approx \\sum_{i=1}^N \\log P(x_i | x_{i-1},⋯,x_{i-k})$ N-gram n이 커질수록 오히려 확률이 정확하게 표현되는데 어려움 Markov assumption이 약하게 들어가기에, count가 증가하기 쉽지 않음 반대로 n이 너무 작으면 Markov assumption이 강하게 들어가 확률이 왜곡이 심해짐 적절한 n을 사용해야 함 보통 3-gram을 가장 많이 사용 corpus(말뭉치)의 양이 많을 때는 4-gram을 사용하기도 함 언어모델의 성능은 크게 오르지 않는데 반해 단어 조합의 경우의 수는 exponential하게 증가하므로 효율성이 없음 n = k + 1 k n-gram 명칭 0 1-gram uni-gram 1 2-gram bi-gram 2 3-gram tri-gram 3 4-gram four-gram N-gram LM을 어떻게 훈련 및 추론하는 방법 SRILM download : http://www.speech.sri.com/projects/srilm/download.html ngram-count: LM을 훈련 (그 결과 model file이 나오게 됨) vocab : lexicon file name text : training corpus file name order : n-gram count write : output countfile file name unk : mark OOV as kndiscount : Use Kneser-Ney discounting for N-grams of order n ngram : LM을 활용 ppl : calculate perplexity for test file name order : n-gram count lm : language model file name 요약 확률값을 근사하는 가장 간단한 방법은 코퍼스에서 빈도를 세는 것 하지만 복잡한 문장일수록 코퍼스에서 출현 빈도가 낮아, 부정확한 근사가 이루어질 것 따라서 Markov assumption을 도입하여 확률값을 근사하자 이제 학습 코퍼스에서 보지 못한 문장에 대해서도 확률값을 구할 수 있다. n의 크기가 중요함 n = 3~4 가 적당 하지만 Markov assumption을 도입해도 0이 나올 수가 있음 Smoothing &amp; Discounting Smoothing Markov assumption을 도입하였지만 여전히 문제는 남아있음 Training corpus에 없는 unseen word sequence의 확률은 0? Unseen word sequence에 대한 대처 Smoothing or Discounting Popular algorithm Modified Kneser-Ney Discounting Add One Smoothing To prevent count becomes zero $P(w_t|w_{&lt;t}) \\approx \\frac{C(w_{1:t})}{C(w_{1:t-1})}$ // t time-step의 시점일 때까지의 t time-step의 언어의 확률 분포(확률값) $\\approx \\frac{C(w_{1:t}) + 1}{C(w_{1:t-1}) + ,|V,|} text{where } |V| \\text{ is a size of vocabulary}$ Generalization of Add One Smoothing If we generalize this : $P(w_t|w_{&lt;t}) \\approx \\frac{C(w_{1:t})}{C(w_{1:t-1})} \\approx \\frac{C(w_{1:t}) + 1}{C(w_{1:t-1}) + ,|V,|} \\approx \\frac{C(w_{1:t}) + k}{C(w_{1:t-1}) + k \\times ,|V,|} \\approx \\frac{C(w_{1:t}) + \\frac{m}{,|V,|}}{C(w_{1:t-1}) + m} \\text{where } |V| \\text{ is a size of vocabulary}$ Take more generailzation : $P(w_t|w_{&lt;t}) \\approx \\frac{C(w_{1:t}) + m \\times P(w_t)}{C(w_{1:t-1}) + m}$ $\\text{where } P(w_t) \\text{ is unigram probability}$ Kneser-Ney Discounting In this lecture C(learning) &gt; C(laptop) Because of “deep learning”, “machine learning” 다양한 단어 뒤에 나타나는 단어일수록 unseen word sequence에 등장할 확률이 높지 않을까? 앞에 등장한 단어의 종류가 다양할수록 해당 확률이 높을 것 같음 $P_{continuation} (w) \\propto | {v : C(v, w) &gt; 0} |$ $P_{continuation}$ : 단어 $w$ 이후에 계속되는 단어의 확률을 나타냄 $C(v, w)$ : 단어 $v$와 $w$ 가 함께 등장하는 횟수를 나타냄 $\\propto$ : 비례 $| {v : C(v, w) &gt; 0} |$는 $w$와 함께 등장하는 단어 $v$의 수를 나타냄 요약 Markov Assumption Count 기반의 approximation 긴 word sequence는 학습 코퍼스에 존재하지 않을 수 있음 확률 값이 0으로 맵핑 Markov assumption을 통해 근거리의 단어만 고려 Smoothing and Discounting Markov assumption을 통해서도 여전히 확률값이 0이 될 수 있음 Smoothing 또는 discounting을 통해 현상을 완화 여전히 unseen word sequence에 대한 대처는 미흡 Interpolation &amp; Back-offInterpolation (보간법) 다른 Language Model을 linear하게 일정 비율(𝝀)로 섞는 것 general domain LM + domain specific LM = general domain에서 잘 동작하는 domain adapted LM domain specific LMExamples 의료 domain ASR, MT system 법률 domain ASR, MT system 특허 domain MT system 수식 $\\tilde{P}(w_n | w_{n-k}, \\ldots, w_{n-1}) = \\lambda P_1(w_n | w_{n-k}, \\ldots, w_{n-1}) + (1 - \\lambda) P_2(w_n | w_{n-k}, \\ldots, w_{n-1})$ $P_1(w_n | w_{n-k}, \\ldots, w_{n-1})$ : 첫번째 LM $P_2(w_n | w_{n-k}, \\ldots, w_{n-1})$ : 두번째 LM 그냥 domain specific corpus로 LM을 만들면 장땡 아닌가? 그럼 unseen word sequence가 너무 많을 수 있음 그냥 전체 corpus를 합쳐서 LM을 만들면 장땡 아닌가? Domain Specific corpus의 양이 너무 적어서 반영이 안될 수 있음 Interpolation에서 ratio(𝝀)를 조절하여 중요도(weight)를 조절 명시적(explicit)으로 섞을 수 있다. General domain test set과 Domain specific test set 모두에서 좋은 성능을 찾는 hyper-parameter 𝜆를 찾아야 한다. 예제 “준비 된 진정제 를 투여 합 시다“ General domain P(진정제 | 준비, 된) = 0.00001 P(사나이 | 준비, 된) = 0.01 Domain Specialized P(진정제 | 준비, 된) = 0.09 P(약 | 준비, 된) = 0.04 P(진정제 | 준비, 된) = 0.5 * 0.09 + (1 – 0.5) * 0.00001 = 0.045005 Back-off (tri-gram, bi-gram, uni-gram 모델들을 다 interpolation) 희소성에 대처하는 방법 Markov assumption 처럼 n을 점점 줄여가면? 조건부 확률에서 조건부 word sequence를 줄여가면, Unknown(UNK) word가 없다면 언젠가는 확률을 구할 수 있다. (uni-gram에서는 걸림!!) $\\tilde{P}(w_n | w_{n-k}, \\ldots, w_{n-1}) = \\lambda_1 P_1(w_n | w_{n-k}, \\ldots, w_{n-1}) + \\lambda_2 P_2(w_n | w_{n-k}, \\ldots, w_{n-1}) + \\ldots+\\lambda_kP_k(w_n)$ $\\text {where } \\sum_i\\lambda_i=1$ Example P(분석했다 | 비핵화, 선언과는, 거리가, 멀다고) C(비핵화, 선언과는, 거리가, 멀다고, 분석했다) &gt; 0? P(분석했다 | 거리가, 멀다고) C(거리가, 멀다고, 분석했다) &gt; 0? P(분석했다 | 멀다고) P(분석했다) // 단어가 없어서 0이면 uni-gram까지 내려감 요약 Back-off를 통해 확률값이 0이 되는 현상은 방지할 수 있음 – OOV 제외 하지만 unseen word sequence를 위해 back-off를 거치는 순간 확률값이 매우 낮아져 버림 여전히 음성인식(ASR) 등의 활용에서 어려움이 남음 전통적인 방식의 NLP 에서는 단어를 discrete symbol로 보기 때문에 문제 발생 Exact matching에 대해서만 count를 하여, 확률값을 approximation 다양한 방법을 통해 문제를 완화하려 하지만 근본적인 해결책은 아님 Markov Assumption Smoothing and Discounting Interpolation and Back-off Pros Scalable : 쉽게(large vocabulary 등의) 대형 시스템에 적용 가능 n-gram 훈련 및 추론 방식이 굉장히 쉽고 간편 Cons Poor generalization : 등장하지 않은 단어 조합에 대처 미흡(Count 기반이기에, exact matching) 단어를 discrete symbol로 취급 따라서 비슷한 단어에 대한 확률을 이용(leverage, exploit)하지 못함 Smoothing과 Back-off 방식을 통해서 단점을 보완하려 했으나, 근본적인 해결책이 아님 Poor with long dependency : 멀리있는 단어에 대해 대처 불가 n이 커질수록 용량도 커짐 실제로 어플리케이션 적용(ASR, SMT 음성인식/통계기반번역)에 있어서 큰 과제","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Language-Modeling-N-gram-Language-Model/"},{"title":"2장 Language Modeling - Wrap up","text":"Language Model 이란 실제 우리가 사용하는 (or 타깃 도메인) 언어의 분포를 확률 모델로 모델링한 것 Chain Rule에 의해서 문장의 확률을 모델링하는 것은 단어들이 주어졌을 때, 다음 단어의 확률을 모델링하는 것과 같음 $P(x_{1:n}) = P(x_1, \\ldots, x_n)$ 123$= P(x_n | x_{1}, \\\\ldots, x_{n-1}) \\\\ldots P(x_2| x_1)P(x_1)$. $= \\\\prod_{i=1}^n P(x_i | x_{&lt;i})$ $\\log P(x_{1:n}) = \\sum_{i=1}^N \\log P(x_i | x_{&lt;i})$ 언어 모델을 통해 우리는 아래의 task를 수행할 수 있음 주어진 문장들 중에서 가장 fluent한 문장을 골라낼 수 있음 단어들이 주어졌을 때, 다음 단어를 확률적으로 예측할 수 있음 Perplexity 매 time-step 마다 모델이 동등하게 헷갈리고 있는 평균 단어 수 헷갈리는 단어가 적을수록 좋은 것 == lower is better 문장의 확률의 역수에 단어 수 만큼 기하 평균을 취한 것 (10이면 10개 단어를 헷갈리는 것, 6:4 수준이 아니라 5:5 수준으로 찍기 수준으로 헷갈리는 것. 100개면 100분의 1 확률) 문장의 likelihood가 높을수록 좋은 것 == lower is better Cross Entropy에 exponential을 취한 것 (=perplexity에 log를 취한 것!) GT 분포와 모델의 분포가 비슷할수록 좋은 것 == lower is better 즉 우리는 perplexity를 minimize해야하는데, CE를 minimize하는 것과 같다!(CE가 낮으면 PPL도 낮음) n-gram and Neural Network Language Model n-gram 단어를 discrete symbol로 인식 Exact Matching에 대해서만 count 학습 코퍼스에 word sequence가 존재해야만 확률 값을 추정 가능 Markov Assumption 도입 쉽고 직관적인 구현 학습(counting) 후, 추론(table look-up) Scalable 하며, 저렴한 계산 비용ㅜ NNLM 단어를 continuous vector로 변환 unseen word sequence에 대처 가능 Generalization에 강점 비싸고 느린 연산 추론 과정 Generation task에 굉장히 강함","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Language-Modeling-Wrap-up/"},{"title":"2장. Language Modeling – RNN을 활용한 LM","text":"Language ModelingNeural Language Model Resolve Sparsity Training Set 고양이는 좋은 반려동물 입니다. Test set 강아지는 훌륭한 애완동물 입니다. (Unseen word sequence라 가정하자) Because we know (and we can approximate that) 고양이 ≈ 강아지 좋은 ≈ 훌륭한 반려동물 ≈ 애완동물 But n-gram CANNOT, because words are discrete symbols Neural Language Model Find parameter that maximize likelihood for given training corpus $D={x^i }_{i=1}^N$ . $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log P(x_{1:n}^{i}; \\theta)$ // 데이터에 대해 log-likelihood를 최대화하는 파라미터를 찾자 $= \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N}\\sum_{j=1}^{n} \\log P(x_{j}^{i}|x_{&lt;j}^i; \\theta)$// by chain-rule$\\text{where } x_{1:n} = {x_1, …, x_n}$ Take a step of gradient descent to minimize negative log-likelihood $L(\\theta) = -\\sum_{i=1}^{N}\\sum_{j=1}^{n} \\log P(x_{j}^{i},|P(x_{&lt;j}^i; \\theta)$ $\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta L(\\theta)$$\\log P(x_t | x_{&lt;t}; \\theta) = x_t^T \\cdot \\log f_\\theta(x_{t-1}, h_{t-1})$ // softmax layer를 통과하는 경우, // $x_t^T$ : GT (One-hot vector, 실제 예측 단어) // $\\log f_\\theta(x_{t-1}, h_{t-1})$ : LM(softmax layer에 log 씌운 확률값) $\\text{where } x_t \\text{ is one-hot vector, and }f_θ \\text{ is model with parameter θ.}$ 더 세부적으로 보면. $f(x_{t-1}, h_{t-1}) = \\text{softmax}(RNN(\\text{emb}(x_{t-1}), h_{t-1}) \\cdot W)$ $\\text{, where W} \\in \\mathbb{R}^{hidden-size \\times |V|}$ // W는 softmax 전에 생략된 linear layer$=\\text{softmax}(h_t \\cdot W)\\text{, where } W \\in \\mathbb{R}^{hidden-size \\times |V|}$ $=\\hat{x_t}$ $\\text{where } \\hat{x_t} \\text{ is a probability distribution that } P(\\cdot|x_{&lt;t};\\theta)$ $\\hat{𝑥_𝑡}$ : mini-batch 내 문장별 단어별 확률값, 이전단어가 주어졌을 때 세타에서의 파라미터를 갖은 확률 분포 |w| = (hs, |V|) |$h_t$|=(𝑏𝑠, ℎ𝑠) = (bs,1, hs) |$\\hat{x_t}$| = (bs,1,hs) x (hs, |V|) = (bs, |V|) softmax는 다음 time-step에 나올 단어에 대한 확률 분포를 나타냄 즉, softmax layer의 각 element는 다음 단어의 확률값(분포)(discrete multinomial distribution) Loss Function of NNLM Find theta that minimize negative log-likelihood. Find theta that minimize cross entropy with ground-truth probability distribution. softmax를 사용하면 loss function으로 CrossEntropy를 사용하고 log-softmax를 사용하면 NLL을 사용하여라(log-likelihood를 maximzie하면 NLL 사용) 요약 n-gram (previous method) 단어를 discrete symbol로 취급 exact matchin에 대해서만 count 따라서 generalization issue 발생 Markov Assumption 도입(n-gram) Smoothing &amp; Discounting Interpolation &amp; Back-off Unseen sequence에 대한 대처 미흡 빠른 연산 &amp; 쉽고 직관적 단순한 look-up table 방식 문장 fluency 비교 task에서는 괜찮음 Neural Network Language Model Word Embedding을 통해, unseen sequence에 대해 대처 가능 Generation task에서 특히 강점 연산량 많음(feed forward 연산) 해석(XAI) 난이도 증가","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Language-Modeling-%E2%80%93-RNN%E1%84%8B%E1%85%B3%E1%86%AF-%E1%84%92%E1%85%AA%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB-LM/"},{"title":"3장. Data Preparation","text":"NLP Project Workflow Preprocessing Workflow Cleaning 기계적인 노이즈 제거(무조건적으로 제거, Table에 따라) 전각문자 변환 Task에 따른 (전형적인) 노이즈 제거 음성 인식의 경우, “나는 (매우) 기분이 나쁘다♡” (매우) 날리기 하트 날리기 기계 번역의 경우, “나는 (매우) 기분이 나쁘다♡” I’m (very) upset ♡ (날릴게 없다) 하지만 이모티콘의 경우는 매우 중요 Interactive 노이즈 제거 코퍼스의 특성(크롤링 한곳마다, 외주 마다 다르다)에 따른 노이즈 제거 작업자가 상황을 확인하며 작업 수행 Therefore 전처리 과정은 Task와 언어, 도메인과 코퍼스의 특성에 따라 다르다. 시간과 품질 사이의 Trade-off 따라서 전처리 중에서도 특히 데이터 노이즈 제거의 경우, 많은 노하우가 필요 Tokenization 한국어의 경우 접사를 분리하여 희소성을 낮추고, 띄어쓰기를 통일하기 위해 tokenization을 수행 굉장히 많은 POS Tagger가 존재하는데, 전형적인 쉬운 문장(표준 문법을 따르며, 구조가 명확한 문장)의 경우, 성능이 비슷함 하지만 신조어나 고유명사를 처리하는 능력이 다름 따라서, 주어진 문제에 맞는 정책을 가진 tagger를 선택하여 사용해야함 Subword Segmentation BPE 압축 알고리즘을 통해 통계적으로 더 작은 의미 단위(subword)로 분절 수행 BPE를 통해 OoV를 없앨 수 있으며, 이는 성능상 매우 큰 이점으로 작용 한국어의 경우 띄어쓰기가 제멋대로인 경우가 많으므로, normalization 없이 바로 subword segmentation을 적용하는 것은 위험 따라서 형태소 분석기를 통한 tokenization을 진행한 이후 subword segmentation을 적용하는 것을 권장 Batchify","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Data-Preparation/"},{"title":"4장. Sequence2Sequence - AMP(Automatic Mixed Precision)","text":"Motivations GPU의 한계로 인한 학습의 비효율성을 해소할 수 있으면 좋을 것 메모리 : 더 큰 모델로 더 큰 배치 사이즈로 학습 연산 속도 : Float Point16(half-precision) Double 64 bits, Float 32 Bits (작은 실수 표현 데이터 타입을 사용하면 연산 속도가 더 빨라짐) 하지만 FP16의 경우 표현 범위의 한계가 있어, 너무 작은 값의 경우 0으로 표현됨 너무 작은 값으로 나눌 경우 NaN으로 값이 반환됨(underflow) 마찬가지로 너무 큰 값의 경우에는 inf로 표현됨(overflow) https://github.com/shchoice/shchoice.github.io/assets/100276387/28f6b73f-0b5d-4e06-a54e-0c08feb8aa05 Mixec Precision Training [Narang and Micikevicius et al., 2018] 필요에 따라 scaling을 통해 FP16에 표현 가능한 범위 내에서 연산을 수행하자 https://github.com/shchoice/shchoice.github.io/assets/100276387/9aab579f-9ed7-41a0-930d-bc13a1c3f4b9 속도는 더 빠르면서, 성능은 비슷하거나 뛰어난 효과 https://github.com/shchoice/shchoice.github.io/assets/100276387/fe094e01-7e0f-4585-ae8f-2d9e3b29a87f PyTorch에서의 AMP 참고 : https://pytorch.org/docs/stable/notes/amp_examples.html https://github.com/shchoice/shchoice.github.io/assets/100276387/4e222c6b-48b9-46f9-937c-be0aebf64e98 Summary AMP를 통해 FP16으로 연산을 수행할 수 있음 이를 통해 속도 증가와 메모리 사용량 감소를 기대할 수 있음 몇 가지 추가적인 예외 처리(e.g. NaN, Inf 처리 및 CPU 연산처리)가 필요할 수 있음","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/4%E1%84%8C%E1%85%A1%E1%86%BC-Sequence2Sequence-AMP-Automatic-Mixed-Precision/"},{"title":"4장. Sequence2Sequence - Gradient Accumlations","text":"Batch Size 큰 배치 사이즈는 epoch 내의 forward &amp; backward 횟수를 줄여주어 학습의 속도를 높여줌 한 epoch 내에서 파라미터의 update 횟수, 즉 iteration이 줄어들음 또한 배치 사이즈에 따라 모델의 성능이 바뀔 수 있음 작은 배치 사이즈는 local minima를 탈출 할 수 있다고 알려져 있으나 큰 데이터셋에서는 배치사이즈가 클수록 오히려 성능이 높아지기도 함 따라서 모델의 성능이 떨어지지 않는 한도 내에서, 배치 사이즈를 최대로 하여 학습을 빠르게 진행할 수 있음 기본적으로 SGD를 사용할 경우, LR과 배치 사이즈는 비례 관계를 갖게됨 하지만 Adam을 사용할 경우, LR에 크게 신경 쓸 필요 없음 하지만 GPU 메모리가 허락하지 않음 Gradient Accumulation Forward &amp; Backward를 할 때마다 파라미터 업데이트(optimizer.step())를 하는 대신, gradient를 누적해서 나중에 한번에 업데이트 하는 방법 마치 누적 횟수 만큼의 배치사이즈가 증가된 효과 ex) k번 accumlation = k * batch_size $\\theta \\leftarrow \\theta + \\Delta_\\theta \\sum_{i=1}^{N} y_i \\log f_\\theta (x_i)$ 속도 상의 이점은 없음 (하지만 batch_size가 커지는 것과 같은 효과) Seq2Seq에선 Adam optimizer 기준 batch_size 256이 가장 좋은 성능 iteration_per_update 파라미터로 조절(k=4 * batch_size=64 = 256 효과)","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/4%E1%84%8C%E1%85%A1%E1%86%BC-Sequence2Sequence-Gradient-Accumlations/"},{"title":"4장.  Sequence2Sequence - Machine Translation 소개","text":"Machine Translation Objective $\\hat{y} = \\text{argmax} {y \\in Y} P {x \\to y}(y|x)$ x에서 y로 변역을 하는 task 번역을 한다는 것은 어떤 x가 주어졌을 때 y가 가능한 문장의 집합 중 최대로 하는 문장을 고름을 의미 History NMT 기반 기계번역에 사용되는 이유 End-to-End 모델 SMT 방식은 여러 sub-module들이 진행될수록 error가 가중됨 Better generalization Discrete한 단어를 Continuous한 값으로 변환하여 계산 Word embedding Context embedding LSTM과 Attention의 적용 Sequence의 길이에 구애받지 않고 번역 Sequence-to-Sequence https://github.com/shchoice/shchoice.github.io/assets/100276387/605eca16-dfaf-4916-931e-0a889cddfb29 Given dataset $D={x^i, y^i}_{i=1}^{N}$ // 𝑥 ~ 𝑃(𝑥) y ~ 𝑃(𝑥) 𝑥에서 y로 즉 한글을 영어로 번역. $x^i = {x_1^i, \\ldots, x_m^i} \\text{ } y^i = {y_0^i, y_1^i, \\ldots, y_n^i}$ where $y_0=$, $y_n=$ Find parameter that maximize likelihood, $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log P(y^i|x^{i}; \\theta)$ // 데이터에 대해 log-likelihood를 최대화하는 파라미터를 찾자 $= \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N}\\sum_{j=1}^{n} \\log P(y_{j}^{i}|x^i, y_{&lt;j}^i; \\theta)$ // by chain-rule Minimize loss function by upgrading parameter with gradient descent. $L(\\theta) = -\\sum_{i=1}^{N}\\sum_{j=1}^{n} \\log P(y_{j}^{i},|x^i,y_{&lt;j}^i; \\theta)$ $\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta L(\\theta)$ $\\log P(x_t | x_{&lt;t}; \\theta) = x_t^T \\cdot \\log f_\\theta(x_{t-1}, h_{t-1})$ // softmax layer를 통과하는 경우, // $x_t^T$ : GT (One-hot vector, 실제 예측 단어) // $\\log f_\\theta(x_{t-1}, h_{t-1})$ : LM(softmax layer에 log 씌운 확률값)$\\text{where } x_t \\text{ is one- hot vector, and }f_θ \\text{ is model with parameter θ.}$ Applications Seq2Seq Applications Task(From-To) Neutal Machine Translation(NMT) 특정 언어 문장을 입력으로 받아 다른 언어의 문장으로 출력 Chatbot 사용자의 문장을 입력 받아 대답을 출력 Summarization 긴 문장을 입력으로 받아 같은 언어의 요약된 문장으로 출력 Automatic Speech Recognition(ASR) 사용자의 문장 입력을 받아 프로그래밍 코드로 출력 extract summ과 abstactive summ 2가지 형태로 있음 Lip Reading 음성을 입력으로 받아 해당 언어의 문자열(문장)으로 출력 Image Captioning 입술 움직임의 동영상을 입력으로 받아 해당 언어의 문장으로 출력 other NLP Task 변형된 seq2seq를 사용하여 이미지를 입력으로 받아 그림을 설명하는 문장을 출력 Encoder Encoder는 source 문장을 conext vector로 압축하고 decoder에게 넘겨줌 Encoder는 train/test 시에 항상 문장 전체를 받음 Encoder 자체만 놓고 보면 non-auto-regressive task 따라서 bi-directional RNN 사용 가능 수식 Given dataset $D={x^i, y^i}_{i=1}^{N}$ // 𝑥 ~ 𝑃(𝑥) y ~ 𝑃(𝑥) 𝑥에서 y로 즉 한글을 영어로 번역. $x^i = {x_1^i, \\ldots, x_m^i} \\text{ } y^i = {y_0^i, y_1^i, \\ldots, y_n^i}$ |$x^i$| = (bs, m, |$v_{soure}$|) |$y^i$|=(bs,n,|$v_{target}$|) where $y_0=$, $y_n=$ Get hidden states of encoder $h_{t}^{enc} = RNN_{enc}(emb_{enc}(x_t), h_{t-1}^{enc}) \\text{ where } h_0^{enc} = 0$ $h_{t}^{enc}$=(𝑏𝑠, 1,ℎ𝑠) |$x_t$|=(𝑏𝑠, 1, |$v_s$|) -&gt; emb layer 통과 후 (bs,1,ws(wordembedding_vector_size)) $h_{1:m}^{enc} = [h_{1}^{i}; \\ldots; h_{m}^{enc}]$ ;는 concat을 의미, $h_{1:m}^{enc}$=(bs,m,hs) $\\text{where } h_{t}^{enc} \\in \\mathbb{R}^{batchsize \\times 1 \\times hiddensize}$, $h_{1:m}^{enc} \\in \\mathbb{R}^{batchsize \\times m \\times hiddensize}$ if we use bi-directional RNN $h_{t}^{enc} \\in \\mathbb{R}^{batchsize \\times 1 \\times (2 \\times hiddensize)}$, $h_{1:m}^{enc} \\in \\mathbb{R}^{batchsize \\times m \\times (2 \\times hiddensize)}$ Decoder Decoder(One-to-Many)는 conditional language model이라고 볼 수 있음 인코더로부터 문장을 압축한 context vector를 바탕으로 문장을 생성 $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log P(y^i|x^{i}; \\theta)$ $= \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N}\\sum_{j=1}^{n} \\log P(y_{j}^{i}|x^i, y_{&lt;j}^i; \\theta)$ 이전에는 x의 문장에 대해 $\\log{P(x^i_j,x^i_{&lt;j};\\theta)}$의 𝑙𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑를 최대로 하는 것을 목표로 했지만(x에 대한 language model) y라는 sequence를 생성하는 LM으로 목표를 바꿈! $x^i, y_{&lt;j}^i$는 조건 으로 conditional language model이라고 함 Auto-regressive task에 속하므로, uni-directional RNN을 사용 수식 Given dataset $D={x^i, y^i}_{i=1}^{N}$ // 𝑥 ~ 𝑃(𝑥) y ~ 𝑃(𝑥) 𝑥에서 y로 즉 한글을 영어로 번역. $x^i = {x_1^i, \\ldots, x_m^i} \\text{ } y^i = {y_0^i, y_1^i, \\ldots, y_n^i}$ // $|y^i|=(bs,n,|v_t|)$ where $y_0=$, $y_n=$ We can get hidden state of decoder $h_{t}^{dec} = RNN_{dec}(emb_{dec}(\\hat{y}{t-1}), h{t-1}^{dec})$ //$|\\hat{y}_{t-1}|=(bs,1,|v_t|)$ emb 통과 후 (bs, 1, ws) $\\text{where } h_0^{dec} = h_m^{dec}$ $h_{1:n}^{dec} = [h_{1}^{i}; \\ldots; h_{n}^{dec}]$ // $h_{1:n}^{dec} = (bs,n,hs)$ Generator Generator는 디코더의 hidden state를 받아 현재 time-step의 출력 token에 대한 확률 분포(multinoulli distribution) 반환 단어를 선택하는 문제이므로 cross entropy loss를 통해 최적화 가능 GT 분포와 모델 분포 사이의 차이를 최소화 하기 위함 조건부 언어모델로 볼 수 있으므로, PPL로 치환 가능 수식 Given dataset $D={x^i, y^i}_{i=1}^{N}$ // 𝑥 ~ 𝑃(𝑥) y ~ 𝑃(𝑥) 𝑥에서 y로 즉 한글을 영어로 번역. $x^i = {x_1^i, \\ldots, x_m^i} \\text{ } y^i = {y_0^i, y_1^i, \\ldots, y_n^i}$ // $|y^i|=(bs,n,|v_t|)$ where $y_0=$, $y_n=$ Hidden states from decoder can be calculated like as below $h_{t}^{dec} = RNN_{dec}(emb_{dec}(\\hat{y}{t-1}), h{t-1}^{dec}), \\text{ where } h_0^{dec} = h_m^{dec}$ Generator return a probability distribution of current output token $\\hat{y}{t} = \\text{softmax}(h{t}^{dec} \\cdot W_{gen}),$ // 현재 time-step의 단어를 예측(각 단어별 확률값이 들어있음) $|\\hat{y}_t|$=(bs,1,$|v_t|$)=(bs,1,hs) $\\text{where } h_{t}^{dec} \\in \\mathbb{R}^{batchsize \\times 1 \\times hiddensize}, W_{gen} \\in \\mathbb{R}^{hiddensize \\times |V|}$ // W는 Linear Layer Loss Function $L(\\theta) = -\\sum_{i=1}^{N} \\log P(y^i|x^{i}; \\theta)$ $= -\\sum_{i=1}^{N}\\sum_{j=1}^{n} \\log P(y_{j}^{i},|x^i,y_{&lt;j}^i; \\theta)$ Log likelihood can be calculated like as below $\\log P(x_t | x_{&lt;t}; \\theta) = x_t^T \\cdot \\log {\\hat{y}_t}$ $\\text{where } x_t \\text{ is one-hot vector, and }\\hat{y}_t \\text{ is a probaility distriution from softmax}$ and Attention hidden state에 정보를 저장하지만 한계 용량이 있음 time-step이 m이고, m이 매우크다면, 많은 정보들을 encoder에 밀어넣어야 함, 이때, m=hidden state가 모든 정보를 담기 어려움 m이 커질수록 번역기, s2s의 성능이 낮아질 수 밖에 없음(capacity 한계) 또한 Encoder의 Hidden state를 Decoder의 Hidden state로 넘기는데 hidden state의 벡터가 1024차원이라고 한다면 한계 용량이 있게됨 따라서 Attention!! What is Attention? Differentiable(미분가능한) Key-Value Function 기존의 Key-Value 함수와 달리, Query와 Key의 유사도에 따라 Value를 반환 dictionary = {k:v, k:v … }, 완벽하게 key와 일치해야 v를 가져오는 것과 달리 여기서는 유사도 바탕으로 가져옴. Decoder RNN(LSTM)의 hidden state의 한계로 인해 부족한 정보를 직접 encoder에 조회하여 예측에 필요한 정보를 얻어오는 과정(즉, encoder에 Query를 잘 날리고 Query를 잘 만들어야..) 정보를 잘 얻어오기 위해 Query를 잘 만들어내는 과정을 학습 모델은 입력 데이터의 특정 부분에 “주의”를 기울이고, 입력 간의 복잡한 상호 관계를 파악하고, 중요한 정보를 효과적으로 추출할 수 있음 Attention in Seq2Seq Query 현재에 초점을 맞춘 문맥을 나타내는 벡터 현재 time-step의 decoder output Keys 다른 모든 위치와 현재 위치의 관계를 나타내는 벡터 입력 데이터 내의 각 위치를 표현하는 벡터로, Query와 얼마나 관련이 있는지를 결정하는 데 사용됨 각 time-step 별 encoder output Values 실질적인 정보를 포함하는 벡터로, Query와 Key의 매칭 정도에 따라 가중치가 적용됨 time-step별 encoder output Intuitive Explanations 처음에는 ‘나는’이 나왔다고 가정 Decoder가 ‘학교에’를 예측을 했을 때, 중학교에 인지, 고등학교에 인지, 학교에 인지 헷갈려함, 그래서 Encoder의 모든 time-step에 Query를 날려서 유사도(Similarity)를 구함 0이 아닌 부분에 대해 (여기에서는 to, school) Key(=Value)의 hidden state와 유사도를 곱함 이를 다 더하면 Context Vector가 됨 Query 와 Context Vector를 concat하여 새로운 hidden state를 만들고 softmax로 확률 값을 얻음 Linear Transformation은 Query와 Key값의 유사도를 구하기 위해 Linear Layer를 통과시켜 적절히 변화하고 유사도를 구함, 그래서 Linear Transform이 학습이 잘 되어야함 Linear Transformation Attention mechanism에서의 Linear Transformation은 기본적으로, 입력 벡터를 다른 차원의 공간으로 매핑하는 수학적 연산 주로 가중치 행렬을 입력 벡터에 곱하여 수행 Attention 메커니즘에서는 주로 세 가지 요소, 즉 Query(Q), Key(K), 그리고 Value(V)의 생성에 사용됨 Q,K,V는 모두 원본 입력 데이터에 대해 각각 다른 Linear Transformation을 수행함으로써 만들어짐 Lineaer Transformation을 통해 아래와 같은 효과를 얻음 구글 검색을 할때 어떻게 작성하면 더 좋은 정보를 얻으려면 어떻게 쿼리를 날리면 알듯이 Linear Transformation이 그 역할을 해줌 결국 Attention은 쿼리를 잘 만들기 위함 수식 With entire encoder’s hidden states and current decoder’s hidden state $w = \\text{softmax}(h_{t}^{dec} \\cdot W_{a} \\cdot h_{1:m}^{enc^T}))$ $|h_{t}^{dec}|$: (bs,1,hs) $|h_{1:m}^{enc^T}|$: (bs,m,hs$)^T$=(bs,hs,m) $|W_\\alpha|$: (hs, hs) $|w|$ : (bs,1,m), mini-batch 내 각 문장별 해당 time-step의 encoder의 각 time-step의 가중치(유사도)가 들어있음, dot-product이기에 cosine similarity와 유사하기 때문 cf ) Dot Product(내적) :$a \\cdot b = |a| |b| \\cos \\theta$ // 얼마나 같은 방향을 가지고 있는지 정보를 담으며, 벡터의 크기에도 영향을 받음 cf) Cosine Similarity : $\\text{cosine-similarity}(a, b) = \\frac{a \\cdot b}{|a| |b|}$, 방향성만 고려함, 벡터의 크기 고려x $c=w \\cdot h_{1:m}^{enc},$ $|c| = w \\times h_{1:m}^{enc} = (bs,1,m) * (bs,m,hs) =(bs,1,hs)$ $\\text{where } c \\in \\mathbb{R}^{batchsize \\times 1 \\times hiddensize}$ is a context vector, and $W_a \\in \\mathbb{R}^{batchsize \\times hiddensize}$ Redefine decoder’s hidden state, and feed into generator $\\tilde{h}t^{dec} = \\tanh([(h_t^{dec}; c)] \\cdot W{concat})$ $[(h_t^{dec}; c)]$ : $ℎ_𝑡^{𝑑𝑒𝑐}$ 과 c를 concat하면 (bs, 1, hs*2) $W_{concat}$은 hs*2를 hs로 줄여주는 layer (차원 축소) $\\hat{y}_t = \\text{softmax}(\\tilde{h}t^{dec} \\cdot W{gen})$ $|\\hat{y}_t|$=(bs,1,hs)*(hs, |V|) = (bs,1,|V|) $\\text{where } W_{concat} \\in \\mathbb{R}^{(2 \\times hiddensize) \\times hiddensize} \\text{ and } W_{gen} \\in \\mathbb{R}^{hiddensize \\times |V|}$ Evaluation 요약 Attention은 미분 가능한 Key-Value Function 이다. Attention 함수의 입력은 Query, Key, Value 정보를 잘 얻기 위한 Query를 변환하는 방법을 배우는 과정 Attention을 통해 RNN의 hidden state의 한계를 극복 가능 LSTM을 쓰더라도 context vector에 모든 정보를 담기에는 한계가 있음 더 긴 길이의 입력/출력에도 대처할 수 있게 됨 Masking on Attention Motivation 우리는 항상 미니 배치를 사용한 병렬 수행을 하게 됨(We always mini-batch parallelized operations) 그로인해 추론할 때 (상용화 수준에서) 문제를 야기할 수 있음 에도 attention 연산이 들어가기 때문 위치에는 attention weight가 들어가지 않게 해야함(엔지니어적으로 안전장치를 만들자!) 의 개수가 2개, 어떤 설정으로는 6개면, 번역결과가 마이너하게 바뀜. (사용자 입장에서consist하지 않은 결과가 나옴, 사랑해요,사랑해욧.) Solution 마스크를 사용해 −∞으로 할당하자, 그러면 softmax의 결과로 0이 나옴 수식 After dot-products, before softmax. $w = \\text{softmax}(h_{t}^{dec} \\cdot W_{a} \\cdot h_{1:m}^{enc^T}))$ $h_{t}^{dec} \\cdot W_{a}$ : Q $h_{1:m}^{enc^T}$ : $K^T$ |Q|=(bs,1,hs), |K|=(bs,hs,m), |Q∙K|=(bs,1,m) $c=w \\cdot h_{1:m}^{enc},$ $\\text{where } c \\in \\mathbb{R}^{batchsize \\times 1 \\times hiddensize}$ is a context vector, and $W_a \\in \\mathbb{R}^{batchsize \\times hiddensize}$ 요약 Mini-batch 내의 문장 구성에 따라, 가 동적으로 생성됨 의 hidden state에는 attention weight가 할당되면 안됨 따라서 Key와 Query의 dot product 이후에 (softmax 이전에), masking을 통해 위치의 값을 음의 무한대로 변경 softmax 결과 에는 0이 할당됨 이 기법은 이후 Transformer에서도 유용하게 쓰일 것 Input Feeding 이전 시점의 디코더의 출력이 다음 시점의 디코더의 입력으로 다시 들어가는 방식 이를 통해 시퀀스를 생성할 때 모델이 이전에 만든 예측을 사용하여 보다 일관성있고 연관성 높은 출력을 생성하는 데 도움이 됨 번역, 대화 생성, 요약 등과 같은 문제에서 중요한 역할 Sampling 과정에서 손실된 정보를 word embedding에 concatenate concatenate을 하는 이유 $\\hat{y}_1$이 헷갈리는지 아닌지 정보 손실방지용, 따라서 $\\tilde{h}_t$ 다음 time-step에서 concat 됨 teacher-forcing 으로 학습할 때, $y_1$만 들어가고 $\\hat{y}_1$이 들어가지 않음, 이로인해 이전 time-step의 추론값을 보고 추가적으로 정답도 보기에 정보 손실 방지 By Input Feeding, Sampling 과정에서 (=argmax 과정에서) 손실되는 정보를 최소화 Teacher Forcing으로 인한 학습/추론 사이의 괴리를 최소화 By Input Feeding, Sampling 과정에서 (=argmax 과정에서) 손실되는 정보를 최소화 Teacher Forcing으로 인한 학습/추론 사이의 괴리를 최소화 수식 $D={x^i, y^i}{i=1}^{N}$ // $y=f(x)$ $|x^i|=(bs,m,|v|)$ $|y^i|=(bs,n,|v|)=y{1:n}^i$ $x^i = {x_1^i, \\ldots, x_m^i} \\text{ } y^i = {y_0^i, y_1^i, \\ldots, y_n^i}$ // $|y^i|=(bs,n,|v_t|)$ where $y_0=$, $y_n=$ $h_{1:m}^{enc} = RNN_{enc}(emb_{enc}(x_{1:m}), h_{0}^{enc}) \\text{ where } h_0^{enc} = 0$ $h_{1:m}^{enc}$=(𝑏𝑠, m,ℎ𝑠) |$emb_{enc}(x_{1:m}), h_0^{enc}$|=(𝑏𝑠, m, ws) $h_{t}^{dec} = RNN_{dec}([emb_{dec}(\\hat{y}{t-1}); \\tilde{h}{t-1}^{dec}], h_{t-1}^{dec})$$h_{t}^{dec} = RNN_{dec}([emb_{dec}(\\hat{y}{t-1}), \\tilde{h}{t-1}^{dec}], h_{t-1}^{dec})$, $\\text{where } h_0^{dec} = h_m^{enc}$ $\\tilde{h}_{t-1}^{dec}$: input feeding으로 concat → (bs, 1, ws+hs) $h_{t-1}^{dec}$: 이전 time-steop의 decoder의 hidden state 값 = (bs,1,hs) $|y_{t-1}|=(bs,1,|v|)$ : dec $w = \\text{softmax}(h_{t}^{dec} \\cdot W_{a} \\cdot h_{1:m}^{enc^T}))$ $h_{t}^{dec} \\cdot W_{a}$ : Q $h_{1:m}^{enc^T}$ : $K^T$ |Q|=(bs,1,hs), |K|=(bs,hs,m), |Q∙K|=(bs,1,m) $c=w \\cdot h_{1:m}^{enc},$ $\\text{where } c \\in \\mathbb{R}^{batchsize \\times 1 \\times hiddensize}$ is a context vector, and $W_a \\in \\mathbb{R}^{batchsize \\times hiddensize}$ $\\tilde{h}t^{dec} = \\tanh([(h_t^{dec}; c)] \\cdot W{concat})$ $[(h_t^{dec}; c)]$ : $ℎ_𝑡^{𝑑𝑒𝑐}$ 과 c를 concat하면 (bs, 1, hs*2) $\\hat{y}_t = \\text{softmax}(\\tilde{h}t^{dec} \\cdot W{gen})$ $|\\hat{y}_t|$=(bs,1,hs)*(hs, |V|) = (bs,1,|V|) $\\text{where } W_{concat} \\in \\mathbb{R}^{(2 \\times hiddensize) \\times hiddensize} \\text{ and } W_{gen} \\in \\mathbb{R}^{hiddensize \\times |V|}$ $L(\\theta) = -\\sum_{i=1}^{N}\\sum_{t=1}^{n} \\log P(y_{j}^{i},|x^i,y_{&lt;t}^i; \\theta)$$\\log P(x_t | x_{&lt;t}; \\theta) = y_t^{iT} \\cdot \\log{\\hat{y}{t}^i})$ // softmax layer를 통과하는 경우,// $|y_t^{iT}|$$y_t^{iT}$ : (bs,1,|v|)// $|\\log{\\hat{y}{t}^i}|$$\\log{\\hat{y}_{t}^i}$ : (bs,|v|,1)$\\text{where } x_t \\text{ is one- hot vector, and }f_θ \\text{ is model with parameter θ.}$$\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta L(\\theta)$ Wrap-up Encoder 문장을 받아 context vector로 압축 Bi-directional RNN을 통해 구현 Decoder &amp; Generator Conditional Language Model Encoder로부터 정보를 받아 문장을 생성 Cross Entropy(PPL)을 통해 최적화 Attention Key-Value 함수 디코더의 hidden state를 인코더의 각 hidden state에 유사도 비교 좋은 query를 만들어내는 과정을 학습 decoder의 hidden state가 query가 됨 Input Feeding Sampling 과정에서 손실된 정보를 word embedding에 concatenate Teacher ForcingAuto-regressive Inference $\\hat{x}t = \\text{argmax}{x_t \\in \\chi} \\log P(x_t | \\hat{x}{&lt;t}; \\theta)$ Auto-regressive 과거 자신의 상태를 참조하여 현재 자신의 상태를 업데이트 $\\hat{x}{t=1} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t=1} | x_0; \\theta)$ $\\text{ where } x_0 =$ $\\hat{x}{t=2} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t=2} | x_0,\\hat{x}_1; \\theta)$ $\\hat{x}{t=3} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t=3} | x_0,\\hat{x}_1, \\hat{x}_2; \\theta)$ … $\\hat{x}{t} = \\text{argmax}{x_t \\in \\chi} \\log P(x_{t} | x_0,\\hat{x}_{x&lt;t}; \\theta)$ Training vs Inference Summary Auto-regressive task를 feed-forward할 때는 보통 이전 time-step의 출력이 현재 time-step의 입력이 됨 Teacher Forcing을 통해 Auto-regressive task에 대한 sequential modeling을 할 수 있음 하지만 training mode와 inference mode의 괴리(discrepancy)가 생김 RL을 통해 이러한 괴리를 없애고 성능을 높일 수 있음 이외에도 다양한 방법(e.g. professor forcing)들이 제안됨","link":"/Deep%20Learning/%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/4%E1%84%8C%E1%85%A1%E1%86%BC-Sequence2Sequence-Machine-Translation-%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2/"},{"title":"1장. Representation Learning - AutoEncoders","text":"AutoEncoders Overview 인코더(encoder)와 디코더(decoder)를 통해 압축과 해제를 실행 인코더는 입력(𝑥)의 정보를 최대한 보존하도록 손실 압축을 수행 디코더는 중간 결과물(𝑧)의 정보를 입력(𝑥)과 같아지도록 압축 해제(복원)를 수행 복원을 성공적으로 하기 위해, autoencoder는 특징(feature)을 추출하는 방법을 자동으로 학습 Encoder 복원에 필요한 정보를 중심으로 손실 압축을 수행 필요없는 정보(뻔한 특징)는 버릴 수도 있음 예시1) 일반적인 사람의 얼굴을 학습할 때: 사람의 얼굴에서 눈은 2개이다 등 예시2) Bottleneck(𝒛) 입력(𝒙)에 비해 작은 차원으로 구성 따라서 정보의 선택과 압축이 발생, 차원에 따라 압축의 정도를 결정함 집에 불이 나서 탈출할 때, 무엇을 들고 나갈 것인가? 그러므로 𝒛 는 입력(𝒙)에 대한 feature vector라고 할 수 있다. 압축의 효율이 높아야 하므로, 입력에 비해 dense vector일 것 Decoder 압축된 중간 결과물(𝒙)을 바탕으로 최대한 입력(𝒛)과 비슷하게 복원 : $\\hat{x}$ 보통 MSELoss 를 통해 최적화 수행 ($MSE=|\\hat{x}-x|^2_2$) 뻔한 정보는 주어지지 않더라도 어차피 알 수 있기에 복원 가능 Hidden RepresentationMotivation 인코더로부터 나온 중간 결과물(𝒛)은 입력(𝒙)에 대한 feature vector이다. feature vector의 각 차원은 어떤 의미를 내포하고 있을까? 인코더의 결과물 𝒛를 plot 하였을 때, 비슷한 샘플들은 비슷한 곳에 위치함을 확인 이 plot이 뿌려진 공간을 hidden(latent) space라고 부름(잠재공간, feature vector가 위치하는 곳) Input space의 MNIST 샘플이 latent space에 embedding 된 것 Mapping to Hidden(Latent) Space 각 레이어의 결과물을 hidden vector 라고 부름 모두 feature vector라고 볼 수 있음 Hidden(Latent) Representation Tabular data의 feature vector와 달리, hidden vector는 해석이 어려움 해석하고자 하는 연구(XAI. Explainable AI)들이 이어지고 있으나, 아직 갈 길이 멀다. 하지만, 비슷한 특징을 가진 샘플은 비슷한 hidden vector를 가질 것 만약 각 차원이 명확하게 하나의 의미를 지닌다면 각 차원의 숫자를 조절하여 원하는 이미지를 합성해 낼 수 있을 것 요약 오토인코더(AE)는 압축과 해제를 반복하며 특징 추출을 자동으로 학습 필요한 정보와 필요없는 정보를 구분할 수 있게되는 것 인코더로부터 나온 중간 결과물(𝒛)은 입력(𝑥)에 대한 feature vector이다. a.k.a Embedding vector 인코더에 통과시키는 것은 feature vector에 대한 embedding 과정이라고 볼 수 있음 Hidden layer의 결과값들을 hidden vectors라 부르며, 이들은 샘플의 feature를 담고 있음 여러개의 hidden vector들을 feature vector라 부를 수 있음 딥러닝에서는 label을 classify하기 위한 feature 를 자동으로 추출하여 학습(전통적인 머신러닝과의 차이점) 신경망(또는 레이어)을 통과시키는 것은 입력 공간(input space)에서 잠재 공간(latent space)로의 맵핑 과정 고차원 공간(high-dimensional space) → 저차원 공간(lower-dimensional space) Hidden representaion을 해석하는 것은 매우 어려움 하지만 비슷한 샘플은 비슷한 hidden representation을 지닐 것!","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/1%E1%84%8C%E1%85%A1%E1%86%BC-Representation-Learning-AutoEncoders/"},{"title":"1장. Representation Learning - Feature Vector","text":"Representation Learning (표현 학습) 머신러닝의 하위 분야로서, 데이터의 숨겨진 구조를 학습하여 복잡한 데이터를 효율적이고 쉽게 이해할 수 있는 표현 형태로 변환하는 방법을 연구하는 분야 이 변환된 형태를 통해 원본 데이터의 중요한 특성이나 패턴을 찾아내고, 이를 사용해 효과적으로 학습 및 예측을 수행 원본 데이터의 복잡성을 줄이고, 노이즈를 제거하며, 데이터의 중요한 특성을 보다 명확하게 강조하는 역할, 이를 통해 머신러닝 모델의 성능을 향상시키고, 학습 과정을 단순화 오토인코더, 딥 비지도 학습, 임베딩 학습 등은 표현 학습의 대표적인 예시 원본 데이터에서 중요한 특성을 추출하고 차원 공간에서 표현하는 방법을 학습하며 결과적으로 데이터의 가장 핵심적인 특성만을 잘 보존하는 표현을 찾아냄 Feature(특징)Feature란? 샘플을 잘 설명하는 특징 사람을 설명할 때 좋은 특징 Continuous: 나이, 키, 몸무게, 소득 Categorical: 성별, 직업, 거주지, 출신 학교/학과 나쁜 특징 (생물 분류학적) 종 모두가 호모 사피엔스이므로 구분이 불가능 주민등록번호, 이름 전 국민의 수 만큼 momory가 필요할 것 주민등록번호만으로는 (비록 일부 특징이 유사하여도) 두 사람의 유사도를 알 수 없음 Categorical value로 볼 수 있음 특징을 통해 우리는 특정 샘플을 수치화 할 수 있음 현실에서의 대표적인 Feature의 예 - 몽타주(Montage) 범인의 얼굴을 특정하기 위해서, 목격자들에게 물어서 나온 특징들을 합쳐 만든 것 좋은 단서 뺨에 큰 붉은 반점 쳐진 눈 긴 생머리 나쁜 단서 눈이 2개 귀가 2개 Feature in Machine Learning MNIST Classifcation 특정 위치에 곧은(휘어진) 선이 얼마나 있는가? 특정 위치에 선이 얼마나 굵은가? 특정 위치에 선이 얼마나 기울어져 있는가? https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de No Need of Hand-crafted Feature in Deep Learning Traditional Machine Learning 사람이 데이터를 면밀히 분석 후, 가정을 세움 가정에 따라 전처리를 하여 feature를 추출 추출된 feature를 model에 넣어 학습 장점 : 사람이 해석하기 쉬움 단점 : 사람이 미처 생각하지 못한 특징의 존재 가능성 Current Deep Learning Raw 데이터에 최소한의 전처리(e.g. scale)를 수행 데이터를 model에 넣어 학습 장점 : 구현이 용이함, 미처 발견하지 못한 특징도 활용 단점 : 사람이 해석하기 어려움 Feature Vector 각 특징들을 모아서 하나의 vector로 만든 것 Tabular Dataset의 각 row도 이에 해당 각 차원(dimension)은 어떤 속성에 대한 level을 나타냄 각 속성에 대한 level이 비슷할수록 비슷한 샘플이라고 볼 수 있음 우리는 feature vector를 통해 샘플 사이의 거리(유사도)를 계산할 수 있음 키 몸무게 나이 월 소득 출신 로버트 174cm 78kg 42 100 미국 매사추세츠 캡틴아메리카 183cm 88kg 58 1000 미국 뉴욕","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/1%E1%84%8C%E1%85%A1%E1%86%BC-Representation-Learning-Feature-Vector/"},{"title":"1장. Representation Learning - One-hot Encoding 및 Embedding Vector","text":"One-hot EncodingCategorical Value vs Continuous Value Categorical Value 보통은 discrete value 단어, 클래스 Continuous Value 키, 몸무게 Categorical Value와 Continuous Value의 가장 결정적인 차이점 Continous value는 비슷한 값은 비슷한 의미를 지니지만 Categorical value는 비슷한 값일지라도 상관없는 의미를 지닌다. One-hot Encoding의 필요성 One-hot Encoding의 필요성을 느끼기 위해 아래의 단어를 사전 순으로 index에 mapping 해보자 0 1 2 3 4 5 6 공책 딱풀 볼펜 샤프 연필 자 필기장 상식적으로는 distance(연필, 볼펜) &lt; distance(연필, 자) distance(공책, 필기장) &lt; distance(공책, 딱풀) 하지만 이 테이블에서는 아래와 같은 결과가 나온다. |연필 - 볼펜| = 2 &gt; 1 = |연필 - 자| |공책 - 필기장| = 6 &gt; 1 = |공책 - 딱풀| 즉, 범주형 데이터를 임의의 숫자로 표현하게 되면, 텍스트 간에 원래 존재하지 않았던 ‘크기’나 ‘순서’의 개념이 부여됨. 이로 인해 범주형 데이터 간에 실제로는 없는 거리 혹은 차이가 존재하는 것처럼 해석되어, 불필요하거나 잘못된 정보를 학습하는 결과를 초래할 수 있음 One-hot Encoding 이란 One-hot 인코딩은 범주형 데이터를 컴퓨터가 이해할 수 있는 형태로 변환하는 방법 각각의 범주를 벡터의 형태로 표현하며, 각 범주의 위치에 해당하는 인덱스만 1로 표시하고 나머지는 0으로 표시 크기가 의미를 갖는 integer 값 대신, 1개의 1과 n-1개의 0으로 이루어진 n차원의 벡터 index 공책 딱풀 볼펜 샤프 연필 자 딱풀 1 0 1 0 0 0 0 연필 4 0 0 0 0 1 0 자 5 0 0 0 0 0 1 n개의 항목 → n차원 즉, 6개의 항목 → 6차원 Vector의 대부분의 element가 0인 경우 Sparse Vector라고 부름 반대 개념 : Dense Vector ↔ Sparse Vector One-hot Encoding의 장점 범주형 데이터를 숫자로 변환 범주 간의 순서 없음 특성 간의 독립성 보장 One-hot Encoding의 단점 서로 다른 두 벡터는 항상 직교(orthogonal)한다. (element-wise 곱 = 0) Cosine Similarity가 0, 따라서 두 샘플 사이의 유사도(거리)를 구할 수 없음 Embedding VectorsMotivation of Embedding Vectors NLP에서 단어는 categorical value &amp; discrete value의 속성을 갖음 따라서 one-hot representation으로 표현 하지만 이는 실제 존재하는 단어 사이의 유사도를 표현할 수 없음 따라서 실제적으로는 좀더 고급화된 Embedding 기법을 사용 다른 Embedding Vectors 표현 기법 Contextual Word Embedding (문맥적 단어 임베딩) 기존의 단어 임베딩 방법은 단어의 의미가 문맥에 따라 달라질 수 있다는 점을 고려하지 못했는데 이를 해결하기 위해 등장 단어를 임베딩할 때 주변 문맥을 고려해 동일한 단어라도 다른 문맥에서는 다른 임베딩 벡터를 갖게함 BERT, ELMO 등이 해당됨 코드로 구현하기 123456789101112131415161718from transformers import BertTokenizer, BertModelimport torch# BERT 모델과 토크나이저 초기화tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')model = BertModel.from_pretrained('bert-base-uncased')# 입력 문장sentences = [&quot;I went to the store&quot;, &quot;I went to the school&quot;]# 토큰화 및 임베딩inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=&quot;pt&quot;)with torch.no_grad(): outputs = model(**inputs)# 각 문장의 [CLS] 토큰에 대한 임베딩 가져오기embeddings = outputs.last_hidden_state[:, 0, :]print(embeddings) Subword Embedding (서브워드 임베딩) 특정 언어들은 단어를 더 작은 의미 단위로 분리할 수 있음 이런 경우, 서브워드 임베딩을 사용하여 작은 단위들을 학습할 수 있음 BPE(Byte Pair Encoding), SentencePiece 등이 해당됨 Document Embedding (문서 임베딩) 문서 전체를 하나의 벡터로 표현하는 방법 문서 임베딩은 문서의 전체적인 의미를 이해하는데 도움이 됨 Doc2Vec, FastText 등이 해당 Word Embedding (단어 임베딩) 고차원의 One-hot 벡터를 저차원의 실수 벡터로 변환하는 기법 비슷한 의미를 가진 단어들이 벡터 공간에서 가까이 위치하도록 학습 Word2Vec, GloVe 등이 해당됨 개인적인 경험으로는 Contextual Word Embedding + Subword Embedding을 결합해서 가장 많이 사용하는 것 같으며, Word Embedding은 김기현 님의 말씀에 의하면 Embedding에 적합하지 않다고 말씀해주신 것으로 알고 있다. 요약 Categorical Value는 One-hot Encoding을 통해 벡터로 표현됨 Sparse Vector는 벡터 간 유사도 계산이 어려움 → One-hot의 단점 따라서 Dense Vector로 표현할 필요가 있음 → Contextual Embedding 등을 사용!","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/1%E1%84%8C%E1%85%A1%E1%86%BC-Representation-Learning-One-hot-Encoding-%E1%84%86%E1%85%B5%E1%86%BE-Embedding-Vector/"},{"title":"2장. Probabilistic Perspective - Basic Statistics","text":"기본 통계학(Basic Statistics)Random Variable &amp; Probability Distribution 확률 변수(Random Variable)는 사건의 시행의 결과(확률)를 하나의 수치로 대응시킬 때의 확률 값, 일반적으로 대문자 X를 사용 어떤 변수(Random Variable) x가 𝒙 라는 값을 가질 확률 $𝑃(\\text{x}=𝑥) =𝑃(𝑥)$ 확률 분포 (함수) 입력 : 확률 변수 x 출력 : x가 각 값에 해당될 때에 대한 확률값 이산 확률 변수 확률변수가 취할 수 있는 값의 수가 유한한 변수 ex) 동전 던지기, 주사위 던지기(X={0, 1, 2, 3}) 연속 확률 변수 확률변수가 취할 수 있는 값의 수가 무한한 변수 ex) 키, 몸무게(P(50 ≤ X ≤ 60)) 등 Function? or Value? 확률값 $𝑃(𝑥) = P(\\text{x}=x)$ 확률 분포 함수 $P(\\text{x})$ $P(y|x)$ $P(y|x) = P(\\text{y}=y|\\text{x}=x)$ Random Variable x가 𝑥 라는 값을 가졌을 때, Random Variable y가 𝑦일 확률 값 $P(\\text{y}|x)$ $P(\\text{y}|x) = P(\\text{y}|\\text{x}=x)$ Random Variable x가 𝑥 라는 값을 가졌을 때, Random Variable y의 분포 $P(y|\\text{x})$ $P(y|\\text{x})=f(\\text{x})=P(\\text{y}=y|\\text{x})$ 어떤 Random Variable이 입력으로 주어졌을 때, Random Variable y가 𝑦일 확률 값을 뱉어내는 함수 확률 분포(Probability Distribution)확률 분포는 수치로 대응된 확률 변수의 개별 값들이 가지는 확률값의 분포 이산 확률 분포(Discrete Probability Distribution) : 확률 변수가 취할 수 있는 값의 수가 유한한 확률 분포 확률 질량함수(Probability Mass Function) : 확률 변수에서 특정 값에 대한 확률을 나타내는 함수 확률값의 총 합은 1 $\\sum_{x} P(\\text{x}=x) = 1, \\text{ where } 0 \\leq P(\\text{x}=x) \\leq 1, \\forall x \\in \\chi$ 주사위가 예시가 될 수 있음(이산 확률 분포) 연속 확률 분포(Continuous Probability Distribution) : 확률 변수가 취할 수 있는 값의 수가 무한한 확률 분포 확률 밀도 함수(robability Density Function , PDF) : 확률 변수의 분포를 나타내는 함수 면적의 합이 1 $\\int P(x) , dx = 1, \\text{ where } P(x) \\geq 0, \\forall x \\in \\mathbb{R}$ 함수값이 1보다 클 수 있음 ※ 주사위는 확률질량함수(이산), 확률밀도함수는 연속함수 연속 확률 변수의 경우, 어떤 샘플이 주어졌을 때, 확률값을 알 수 없다. (높이는 단지 확률 밀도) 결합 확률 분포(Joint Probability Distribution) 두 개 이상의 Random Variable이 결합되었을 때의 확률 분포 두 이벤트 A와 B가 동시에 발생할 확률을 의미합니다. 이를 기호로 나타내면 P(A ∩ B) 또는 P(A, B)로 표현 P(A, B) = P(A|B) * P(B) = P(B|A) * P(A) P(A, B) : 사건 A와 사건 B가 동시에 발생할 확률 P(A|B) : 사건 B가 발생한 조건 하에서 사건 A가 발생할 확률 P(B) : 사건 B가 발생할 확률 조건부 확률(Conditional Probability) 어떤 사건 A가 일어났을 때 사건 B가 일어날 확률을 의미. 즉, 사건 A가 주어졌을 때의 사건 B의 확률 조건부 확률 분포 랜덤 변수의 분포가 다른 랜덤 변수의 값에 의해 어떻게 영향을 받는지를 설명하는 확률 분포 즉, 하나의 랜덤 변수 X의 값이 주어졌을 때, 다른 랜덤 변수 Y의 분포를 의미 $P(\\text{y}|\\text{x}) = \\frac{P(\\text{x}, \\text{y})}{P(\\text{x})}$ (x가 조건으로 주어졌을 때 y의 확률값) P y given x 라고 읽음 조금더 친해져야할 형태(결합확률분포 형태) $P(\\text{x}, \\text{y}) = P(\\text{y} ,|, \\text{x})P(\\text{x})$ 예를 들어, 랜덤 변수 X가 ‘오늘 비가 올 확률’을 나타낸다고 할 때, 랜덤 변수 Y는 ‘비가 올 때 우산을 가지고 갈 확률’을 나타낼 수 있음. 이 경우, ‘우산을 가지고 갈 확률’은 ‘비가 올 확률’에 의해 영향을 받음. 이런 상황을 모델링한 것이 조건부 확률 분포이다. 베이즈 정리(Bayes Theorm) 베이즈 정리는 확률 이론과 통계학에서 굉장히 중요한 개념으로, 조건부 확률에 관한 정리 조건부 확률의 개념을 확장하고 이를 뒤집는 데 사용 주어진 사건 B가 발생했을 때, 사건 A가 발생할 확률을 알아내는 방법을 제공. 즉, 이미 알고 있는 정보에 기반하여 새로운 사건의 확률을 추정하는 방법 $P(A ,|, B) = \\frac{P(B,|,A)P(A)}{P(B)}$, (A|B)는 사건 B가 발생했을 때 사건 A가 발생할 조건부 확률을 의미하며, 이를 사후 확률(Posterior probability)이라고 함 P(B|A)는 사건 A가 발생했을 때 사건 B가 발생할 조건부 확률을 의미하며, 이를 가능도(Likelihood)라고 함 P(A)는 사건 A가 발생할 확률을 의미하며, 이를 사전 확률(Prior probability)라고 함 P(B)는 사건 B가 발생할 확률을 의미하며, 이를 증거(Evidence)라고 함 A를 “오늘 비가 올 것”이라는 사건, B를 “오늘 하늘이 흐림”이라는 사건이라고 가정합시다. 이 경우, 베이즈 정리는 다음과 같이 표현될 수 있습니다. 사후 확률(P(A|B)), 즉 “하늘이 흐릴 때 비가 올 확률”은 알고 싶은 최종적인 확률입니다. 이는 우리가 이미 알고 있는 “하늘이 흐림”이라는 정보를 바탕으로 “오늘 비가 올 것”이라는 새로운 사건에 대한 확률을 업데이트하는 것을 의미합니다. 가능도(P(B|A)), 즉 “비가 올 때 하늘이 흐릴 확률”은 우리가 이미 알고 있는 “비가 올 것”이라는 사건이 주어졌을 때 “하늘이 흐림”이라는 사건의 확률입니다. 이는 사건 A가 주어졌을 때 사건 B가 발생할 확률을 나타냅니다. $P(h ,|, D) = \\frac{P(D,|,h)P(h)}{P(D)}$ $P(h ,|, D)P(D) = P(D,|,h)P(h) =P(h,D)$ 가설이 있을 때 데이터가 있을 확률을 뒤집어서 알 수 있게 해준다. 조건부 확률과 베이즈 정리의 차이를 다음의 예시로 알아보자예시 : “비가 오는 날에 우산을 들고 나가는 확률” 조건부 확률 ‘비가 오는 날’이라는 사건이 이미 일어났을 때, 그 조건 하에서 ‘우산을 들고 나가는’ 사건이 일어날 확률을 계산하는 것이 조건부 확률 베이즈 정리 ”우산을 들고 나가는 사람 중에서 비가 오는 경우” 주변 분포(Marginal Distribution) 여러 변수의 결합 확률 분포에서 한 개 또는 일부 변수의 분포를 나타냄, 이는 ‘주변화(marginalization)’라는 과정을 통해 얻어짐 주변화는 결합 확률 분포에서 관심 있는 변수를 제외한 나머지 변수들을 모두 합산(이산)하거나 적분(연속)하는 과정을 의미 주변 확률 분포를 통해, 특정 변수가 다른 변수들로부터 독립적으로 어떻게 분포하는지를 파악할 수 있음, 이를 통해 그 변수의 분포를 단독으로 볼 수 있게 됨 주변 분포는 확률론의 핵심 개념인 결합 확률, 조건부 확률, 그리고 이들의 관계를 연결해 보여주며, 주어진 데이터에서 특정 변수의 확률 분포를 독립적으로 파악하는 데 유용 Marginal Distribution의 공식 $P(x) = \\int P(x,z) dz$ ( P(x, z)는 x와 z의 결합 확률 분포를 나타내고, 이를 z에 대해 적분하면 x의 주변 확률 분포 x를 얻게 됨) $= \\int P(x|z)P(z) dz$ (결합 확률분포를 조건부 확률 P(x | z)와 z의 주변 확률 P(z)의 곱으로 분해하고 이를 z에 대해 적분하여 x의 주변확률 분포 P(x)를 얻게됨) $= \\int P(z|x)P(x) dz = P(x)\\int P(z|x) dz = P(x)$ $(∵\\int P(z|x) dz=1)$ (조건부 확률 $P(z | x)$가 z에 대해 적분하면 1이 됨, 왜냐하면 조건부 확률 P(z|x)는 z의 확률 분포를 나타내기 때문) Expectation and Sampling 기대값(expectation)은 확률 분포와 그에 대응하는 함수의 가중평균을 의미 기대값의 수식 (앞으로 자주 보게될 수식) $\\mathbb{E} {\\text{x} \\sim P(\\text{x})}[f(x)] = \\sum {x \\in \\chi} P(x) \\cdot f(x)$ $\\chi$는 모든 가능한 $x$의 집합, $f(x)$는 $x$에 대한 함수, $P(x)$는 $x$의 확률 분포 f(x)라는 어떤 함수가 있는데, P(x)에서 샘플링한 x를 함수에 넣어봄 주사위의 기대값은? $\\mathbb{E}{x \\sim P(x)}[f(x)] = \\sum{x \\in {1,2,3,4,5,6}} P(x = x) \\cdot f(x)$ $=\\frac{1}{6}×(f(1)+f(2)+f(3)+f(4)+f(5)+f(6))$ $= \\frac{1}{6}×(1+2+3+4+5+6)=3.5 \\text{, where } f(x)=x$ cf) 모든 기대값이 같으면 uniform distribution 따라서 시그마 1/n *f(x) $P(x) = \\int P(x,z) dz$ $= \\int P(x|z)P(z) dz$ $= \\mathbb{E}_{\\text{z} \\sim P(\\text{z})}[P(x|\\text{z})]$ (P(x)를 z에 대한 P(x|z)의 기대값으로 표현) Monte-Carlo 방법 무작위 표본을 사용하여 수학적 문제를 해결하는 통계적인 방법 주로 다음과 같은 상황에서 사용됨 강화 학습에서는 Monte Carlo 방법을 사용하여 에이전트의 정책(policy)을 평가하거나 업데이트 딥러닝에서 복잡한 확률 모델에서 샘플을 추출하기 위해 Monte Carlo 샘플링 기법을 사용 복잡한 수학적 문제 해결 복잡한 수학적 문제를 직접 해결하는 것이 어려울 때, Monte Carlo 방법을 사용하여 근사적인 해답을 찾을 수 있음 확률 분포로부터 샘플링을 통해 𝒇의 가중 평균을 구해보자 sample의 크기가 커질수록 보다 정확하게 구할 수 있음 $\\mathbb{E}{x \\sim P(x)}[f(x)] \\approx \\frac{1}{n} \\sum{i=1}^{n} f(x_i) \\text{, where } x_i \\sim P(x)$","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Basic-Statistics/"},{"title":"2장. Probabilistic Perspective - Information &amp; Entropy","text":"Information &amp; EntropyInformation(정보량) 본디 통신이나 압축을 위해 주로 다루어지던 분야 Representation Learning에 관해서 다루다 보니 자연스럽게 연결됨 Representation Learning에서는 정보 이론의 개념들을 이용하여 불확실성을 줄이고 정보를 최적으로 표현하려는 노력을 함(AutoEncoder의 특성 압축 등) 불확실성(Uncertainty)을 나타내는 값 정보가 높으면 불확실하다, 정보가 낮으면 정확하다. 정보량 수식 $I(\\text{x})=-\\log P(\\text{x})$ # x라는 random variable에 대한 정보 0≤𝑃(𝑥)≤1 정보량 예제 예제1 내일 아침 해는 동쪽 하늘에서 뜹니다. → 확률이 높을수록 정보량이 낮다. 내일 아침 해는 서쪽 하늘에서 뜹니다. → 확률이 낮을수록 엄청난 정보량을 가지고 있다. 예제2 올 여름 대한민국의 평균 여름 기온은 30도 입니다. → 정보량이 낮음 올 여름 대한민국의 평균 여름 기온은 10도 입니다.→ 정보량이 높음 Entropy 정보량의 기대값(평균) 분포의 평균적인 uncertainty를 나타내는 값 분포의 형태를 예측해볼 수 있음 수식 $H(P)=-\\mathbb{E}_{x \\sim P(x)} [\\log P(x)]$ # H(P) : P라고 하는 분포의 엔트로피 P1: 엔트로피가 클수록 flat한 분포를 갖는다고 예측할 수 있음 P3: 엔트로피가 낮아질수록 sharp한 분포를 갖는다고 예측할 수 있음 Cross Entropy 분포 P의 관점에서 본 분포 Q의 정보량의 평균 두 분포가 비슷할수록 작은 값을 갖음 이런 성질 때문에 DNN을 최적화(Optimize)하는 데 사용함 Ground Truth P를 모사하고 싶은데, DNN이라는 weight parameter를 가진 확률분포 함수(Q)를 P와 가까이 하고 싶음. 가까이 하기 위해 Cross Entropy로 측정하고 이를 줄이기 위해 Gradient Descent를 수행 KL Divergence와 비슷 범위 : 0~무한 비슷하면 0에 가까고 다르면 무한대 수식 $H(P, Q)=-\\mathbb{E}{x \\sim P(x)} [\\log Q(x)]$ # 샘플링을 반복해 평균을 내겠음 $= \\int P(x) \\log Q(x) dx$ $\\approx -\\frac{1}{n} \\sum{i=1}^{n} \\log Q(x_i)$ DNN Optimization using Cross Entropy Classification 문제에서 Cross Entropy Loss를 사용하여 최소화(두 분포를 가까이 하기위함) $CE(y_{1:N}, \\hat{y}{1:N}) = -\\frac{1}{N} \\sum{i=1}^{N} y_i^T \\cdot \\log \\hat{y}i$ $= -\\frac{1}{N} \\cdot \\sum{i=1}^{N} \\sum_{j=1}^{d} y_{i,j} \\times \\log \\hat{y}{i,j}$ $= -\\frac{1}{N} \\cdot \\sum{i=1}^{N} \\log P_{\\theta} (y_i | x_i)$ $\\text{where,} y_{1:N} \\in \\mathbb{R}^{N \\times d} \\text{ and } \\hat{y}_{1:N} \\in \\mathbb{R}^{N \\times d}$ $\\mathcal{L}(\\theta)=-\\mathbb{E}{x \\sim P(x)} [\\mathbb{E}{y \\sim P(y|x)} [\\log P(y|x;\\theta)]]$ $\\approx -\\frac{1}{N\\cdot k} \\sum_{i=1}^{N}\\sum_{j=1}^{k}\\log P(y_{i,j}|x_i;\\theta)$ $\\approx -\\frac{1}{N} \\sum_{i=1}^{N}\\log P(y_i|x_i;\\theta), \\text{ if } k=1$ $= -\\frac{1}{N} \\sum_{i=1}^{N} y_i^T \\cdot \\log \\hat{y}i$ // $\\mathbb{E}{x \\sim P(x)}$: Ground Truth P에 대해서 x를 샘플링 // $\\mathbb{E}_{y \\sim P(y|x)}$: 샘플링한 x를 넣은 ground truth P(y|x)에서 를 샘플링 함 // $P(y|x;\\theta)$: x와 y를 DNN 확률 분포 함수에 넣음 KL-Divergence &amp; Cross Entropy KL-Divergence와 Corss Entropy를 𝜽로 미분하면 같음 $KL(p||p_{\\theta})=-\\mathbb{E}{x \\sim p(x)} [\\log \\frac{p{\\theta}(x)}{p(x)}]$ $=-\\int_{1} p(x)\\log \\frac{p_{\\theta}(x)}{p(x)} dx$ $=-\\int_{1} p(x)\\log p_{\\theta} (x) dx+ \\int_{1} p(x) \\log p(x) dx$ // corss enropy + entropy $= H(p,p_{\\theta})-H(p)$ // croeeEntropy - entropy $\\theta$ 로 미분하면 $\\nabla_{\\theta} KL(p||p_{\\theta})=\\nabla_{\\theta} H(p,p_{\\theta})-\\nabla_{\\theta} H(p)$ // H(p)는 𝜽로 미분 시 날아감 요약 Objective 확률 분포 𝑃(𝑥)로부터 수집한 데이터셋 D를 통해, 확률 분포 함수 𝑃(𝑦|𝑥)를 근사하고 싶다. 확률 분포 함수 신경망 $P_\\theta(y|x)$를 통해 이를 수행하자 Cross Entropy(KL-Divergence)가 최소가 되도록 gradient descent 수행","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Information-Entropy/"},{"title":"2장. Probabilistic Perspective - Introduction","text":"IntroductionAgain, our objective is 가상의 함수를 모사하여, 원하는 출력값을 반환하는 신경망의 파라미터를 찾자. 그래서 우리는 Deep Neural Networks를 이야기할 때, Gradient Descent Back-Propagation Feature Vector and blah blah.. 이제는 우리의 생각을 확장시켜야 할 때! → Probabilistic Perspective! Probabilistic Perspective 이 세상은 확률에 기반 아래의 그림에 대해서 모두가 같은 대답을 하지는 않을 것 https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702 우리의 새로운 목표: 확률 분포를 학습하는 것 Before vs After Before 함수를 배우자(모사하자) After 확률 분포 함수를 배우자 수학적으로 더 설명이 가능해짐 불확실성까지 학습 https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a 요약 Neural Networks는 확률 분포 함수를 모델링할 수 있음 이를 통해 가상의 확률 분포 함수 𝑃(𝑦 | 𝑥)를 근사(approximation)할 것 대부분의 최신 기술들은 이 관점에 기반을 두고 만들어짐 DNN을 확률 분포로 보았을 때, 가능한 이론들에 대해서 앞으로 이야기 할 것 Likelihood Maximum Likelihood Estimation(MLE) Maximum A Posterior(MAP) Estimation Cross Entropy &amp; KL-Divergence","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Introduction/"},{"title":"2장. Probabilistic Perspective - Kullback-Leibler Divergence","text":"Kullback-Leibler Divergence Kullback-Leibler Divergence 란 KL Divergence(Kullback-Leibler Divergence, = 상대 엔트로피(Relative Entropy))는 두 확률분포 P와 Q 간의 차이를 측정하는 방법 Q와 P를 근사하는 모델로 보았을 때, 모델이 실제 확률분포를 얼마나 잘 근사하고 있는지를 나타내는 지표로 활용 KL Divergence 수식 $D_{KL}(p || q) = \\mathbb{E}_{x \\sim p(x)} [\\log \\frac{p(x)}{q(x)}] = \\int p(x)\\log \\frac{p(x)}{q(x)} dx$ $D_{KL}(p || q) \\neq D_{KL}(q || p)$ p(x)는 실제 확률분포에서 x라는 사건이 일어날 확률, q(x)는 모델이 추정한 x라는 사건이 일어날 확률 KL Divergence는 항상 양수아며, p와 q가 비슷할수록 작은 값을 같게 된다.(완전히 같으면 최저가 0이 됨) 사용 사례 딥러닝에서는 종종 KL Divergence를 손실함수(loss function)로 사용하여 모델이 데이터의 실제 확률분포를 얼마나 잘 근사하는지를 측정 생성 모델(Generative Model)인 GAN(Generative Adversarial Networks)이나 VAE(Variational AutoEncoder)에서는 KL Divergence를 사용해 생성된 데이터의 확률분포와 실제 데이터의 확률분포가 얼마나 가까운지를 측정하는데 사용 KL-Divergence를 사용한 DNN 최적화(Optimization) $\\mathcal{L}(\\theta)=-\\mathbb{E}{x \\sim p(x)} [\\mathbb{E}{y \\sim p(y|x)} [\\log \\frac{p_{\\theta} (y|x)}{p(y|x)}]]$ // $\\mathbb{E}{y \\sim p(y|x)} [\\log \\frac{p{\\theta} (y|x)}{p(y|x)}]$ = $D_{KL}(p(y|x)||p_\\theta(y|x))$, ground truth인 확률분포 $p(y|x)$를 신경망의 확률 분포인 $p_{\\theta}(y|x)$로 모사하기 위해 이 둘의 차이가 0이 되면 잘 최적화했음을 알 수 있음 $\\mathcal{L}(\\theta) \\approx -\\frac{1}{N\\cdot k} \\sum_{i=1}^{N}\\sum_{j=1}^{k} \\log \\frac{p_{\\theta} (y_{i,j}| x_i)}{p(y_{i,j}| x_i)}$ (Monte-Carlo 방법에 의함) ​ $\\approx -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{p_{\\theta} (y_i| x_i)}{p(y_i| x_i)}$, if k=1 # k=1 : 즉 x는 여러개 뽑는 반면 y는 1개만 뽑음 ​ $\\hat{\\theta}=\\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}(\\theta)$ ​ $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} L(\\theta)$","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Kullback-Leibler-Divergence/"},{"title":"2장. Probabilistic Perspective - MAP","text":"MAP(Maximum A Posterior) Estimation Data가 주어졌을 때 가설 h를 maximize 해야한다. 이를 위해 Posterior을 maximize할 수도 있고 likelihood로 maximize할 수도 있음 likelihood를 maximize 한다면 MLE 이 시간에는 poeteriror 를 maximize해서 h를 찾도록 하겠음 → MAP MAP를 이해하기 위한 예제 절도 사건의 범인은 발자국을 남겼습니다. 신발 사이즈 240일 때, 범인은 남자일까? 여자일까? $P(\\text{y}│\\text{x} =240)$ 지인 중에 신발 사이즈가 240이었던 사람들을 떠올려보자 여자 중에 많을까? 남자 중에 많을까? 여자 중에서 신발 사이즈가 240인 사람 이라고 생각하면 likelihood(가정이 들어간 것 $P(\\text{x}=240|\\text{y})$ 그런데 범행 장소가 군부대라면? $P(\\text{y}=\\text{male}) &gt; P(\\text{y}=\\text{female})$ // 99% &gt; 1% Likelihood는 여자일 가능성이 높지만, Prior를 고려하였을 때 범인은 남자일 가능성이 높음 $P(\\text{y} | \\text{x} = 240) = \\frac{P(\\text{x} = 240 | \\text{y})P(\\text{y})}{P(\\text{x} = 240)}$ $\\frac{P(\\text{x} = 240 | \\text{y=male})P(\\text{y=male})}{P(\\text{x} = 240)} &gt; \\frac{P(\\text{x} = 240 | \\text{y=female})P(\\text{y=female})}{P(\\text{x} = 240)}$ MAP Estimation Find $\\hat{h}$, which maximizes posterior $\\hat{h} = \\text{argmax}_{h \\in H} P(h|D)$ $= \\text{argmax}_{h \\in H} \\frac{P(D|h)P(h)}{P(D)}$ // 밑변은 h와 상관 없어 날려버림 $= \\text{argmax}_{h \\in H}P(D|h)P(h)$ // 𝑃(𝐷|ℎ) : likelihood, 𝑃(ℎ) : prior MAP Estimation (딥러닝이니 h를 𝜽로 바꿈) Bayesian 관점 $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} P(\\theta|D) = \\text{argmax}{\\theta \\in \\Theta} \\frac{P(D|\\theta)P(\\theta)}{P(D)} = \\text{argmax}_{\\theta \\in \\Theta}P(D|\\theta)P(\\theta)$ // Data가 주어졌을 때 𝜃가 궁금함 파라미터 또한 Random Variable로 간주하며, 사전 정보(prior)를 통해 파라미터의 사전 분포를 설정 (prior에 대한 가정이 필요) 이후 관찰된 데이터를 통해 사후(posterior) 분포를 계산하며, 사후 분포가 최대가 되는 파라미터를 찾는 MAP 방법을 사용 미래의 uncertainty 까지 고려 likelihood를 maximize하는 동시에 prior까지 maximize 함 Frequentist 관점 $\\hat{\\theta} = \\text{argmax}_{\\theta \\in \\Theta}P(D;\\theta)$ //# 𝑃(𝜃,𝐷), likelihood를 maximize 파라미터는 고정된 값이며 최적화의 대상으로 봄 관찰된 데이터를 기반으로 MLE 방법을 사용하여 파라미터를 추정 과적합(Overfitting)에 취약함 Summary MAP를 통해 우리는 posterior를 최대화 하는 hypothesis를 찾을 수 있음 마찬가지로 주어진 데이터셋에 대한 posterior를 최대화 하는 파라미터(𝜃)를 찾을 수 있음 Bayesian 관점에서는 파라미터들을 랜덤 변수로 보고 prior에 대한 가정을 통해, 앞으로의 uncertainty 까지 고려 이를 통해 overfitting 등의 문제도 해결할 수 있음 Bayesian Deep Learning에 대한 다양한 시도들도 이어지고 있음 베이지안 방법론은 계산량이 많이 필요하고 복잡한 딥러닝 모델에 적용하는 데에는 여전히 여러 도전적인 문제들이 존재","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-MAP/"},{"title":"2장. Probabilistic Perspective - Maximum Likelihood Estimation","text":"Maximum Likelihood Estimation(MLE)Maximum Likelihood Estimation 란 다음의 예시를 통해 왜 MLE가 필요한지 알아보자 다음과 같이 대한민국 키의 평균을 얻기 위해 아래와 같은 샘플을 얻었다고 가정해보자. 이의 평균과 표준편차를 가장 잘 얻게 하려면 어떻게 해야할까? 다음과 같이 3개의 가우시안 분포(Gaussian Distribution)를 살펴보자. 어떤 가우시안 분포가 가장 우리의 샘플을 잘 설명했다고 할 수 있을까? 점선의 길이(Line’s length = probabilty density)가 가장 긴 것이 제일 잘 표현한 것 같은데!(Approxmation 3) 따라서 우리의 샘플을 가장 잘 표현하는 가우시안 분포를 그리기 위한 𝜇와 𝜎를 찾자 Likelihood 를 최대화 하는 MLE(Maximum Likelihood Estimation)을 구하자 Likelihood Function 입력으로 주어진 확률 분포(파라미터(𝜇,𝜎))가 데이터를 얼마나 잘 설명하는지 나타내는 점수(Likelihood)를 출력 하는 함수 입력 : 확률 분포를 표현하는 파라미터(𝜇,𝜎) 출력 : 데이터를 설명하는 정도 (Probability Density가 높아야) 데이터를 잘 설명하는지 알 수 있는 방법 데이터가 해당 확률 분포에서 높은 확률 값을 가질 것 수식 $L(\\theta | x) = P(x | \\theta)$ $x$ : 관찰된 데이터 $\\theta$ : 모델의 파라미터 $P(x|\\theta) : \\theta$로 주어진 $x$의 확률 $L : \\theta$ 에 대한 likjelihood function(우도 함수) $L(\\theta | x_1, x_2, …, x_n) = \\prod_{i=1}^{n} P(x_i | \\theta)$ (주어진 데이터셋이 독립적이고 동일하게 분포(i.i.d)한 경우, 전체 데이터의 likelhood function은 각각의 데이터 포인트에 대한 확률의 곱으로 표현될 수 있음) 하지만 실제 계산에서는 이렇게 곱셈 형태의 우도 함수는 계산이 어렵고, underflow 문제를 일으키기 때문에, 대부분의 경우 로그 우도 함수(log-likelihood function)을 사용 (중요) 로그를 취하면 곱셈이 덧셈으로 바뀌어 계산이 쉬워짐 $\\log L(\\theta | x_1, x_2, …, x_n) = \\sum_{i=1}^{n} \\log P(x_i | \\theta)$ 다음의 예제를 통해 MLE를 더 잘 이해해보자 주사위의 확률 분포를 알고 싶다. 점수(likelihood)가 가장 높은 것은 $\\theta_3$이다. 따라서 위 3개 중에서는 데이터를 잘 설명하는 파라미터라고 할 수 있음 Log-Likelihood 확률 분포의 가능성을 측정 앞에서 소개했다 싶이, Likelihood는 확률 곱으로 표현이 됨 underflow의 가능성이 있음 따라서 Log를 취하여 곱셈을 덧셈으로 바꾸고, Log Likelihood로 문제를 해결 덧셈이 곱셈보다 연산도 빠름 $\\prod_{i=1}^{n} P_{\\theta}(\\text{x} = x_{i})$ → $\\sum_{i=1}^{n} \\log P_{\\theta}(\\text{x} = x_{i})$ MLE via Gradient Ascent 랜덤 생성 대신, Gradient Ascent를 통해 likelihood 값을 최대로 만드는 파라미터(𝜽)를 찾자 $\\theta \\leftarrow \\theta + \\alpha \\cdot \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta}$ 예제 n = 100번 던졌을 때, k=27번 앞면(True)이 나오는 동전이 있다. 이 동전의 확률 분포(파라미터 𝜽)를 추정하자. Binomial Distribution $\\mathcal{L}(\\theta) = \\frac{n!}{k!(n-k)!} \\theta^k (1-\\theta)^{n-k}$ 요약 우리는 가상의 확률 분포를 모사하는 확률 분포의 파라미터(𝜽)를 찾고 싶다 목표 확률 분포로부터 데이터를 수집한 후, 데이터를 잘 설명하는 파라미터를 찾자 Likelihood라는 값을 통해 얼마나 잘 설명하는지 알 수 있다 Likelihood function은 𝜃 을 입력으로 받아, 데이터들의 𝜃 에 대한 확률 값의 곱을 출력 Likelihood를 최대화 하는 파라미터를 찾으면, 주어진 데이터를 가장 잘 설명한다. Gradient Ascent를 통해서 찾자. $\\theta \\leftarrow \\theta + \\alpha \\cdot \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta}$","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Maximum-Likelihood-Estimation/"},{"title":"2장. Probabilistic Perspective - 신경망과 MLE","text":"Again, our object is 데이터를 넣었을 때 출력을 반환하는 가상의 함수를 모사하는 것 확률 분포로부터 샘플링하여 데이터를 넣었을 때, 확률 분포를 반환하는 가상의 함수를 모사하는 것 출력 분포에서 샘플링하면 원하는 출력 값을 얻을 수 있다. Example: 손 글씨가 주어졌을 때, 글씨의 클래스의 확률 분포 $P(c|x) \\text{, where } x \\sim P(x)$ Before we start, 모두 같은 표현 (𝜃 를 𝑅𝑎𝑑𝑜𝑚 𝑉𝑎𝑟𝑖𝑎𝑏𝑙𝑒로 취급) $P_{\\theta}(x) = P(x; \\theta) = P(x|\\theta)$ 𝜃 라는 파라미터를 갖는 확률분포에서의 𝑥의 확률값 $P_{\\theta}(y|x) = P(y|x; \\theta) = P(y|x, \\theta)$ 𝜃 라는 파라미터를 갖는 확률분포에서의 𝑥가 주어졌을 때 𝑦의 확률값( Parameters for Probability Distribution Bernoulli Distribution 𝜃={𝑝} (동전던지기의 확률 같은) Gaussian Distribution 𝜃={𝜇,𝜎} (대한민국 키(신장)의 분포를 알고 싶어요) 딥러닝에서는 파라미터가 W와 b 가 되는 것이다!! 이 W와 b를 통해 확률 분포 함수를 정의할 수 있음 Parameters for Deep Neural Networks $\\theta = {W_1, b_1, W_2, b_2, \\ldots, W_l, b_l}$ 마찬가지로 Gradient Ascent를 통해 Likelihood를 최대로 하는 파라미터(𝜽)를 찾을 수 있다. $\\sum_{i=1}^{n} \\log P(x_i | \\theta)$ 를 Maximize 하는 𝜃를 찾을 수 있음 Negative Log Likelihood(NLL) 하지만 대부분의 딥러닝 프레임워크들은 Gradient Descent만 지원 $\\theta \\leftarrow \\theta - \\alpha \\cdot \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta}$ 따라서 maximization 문제에서 minimization 문제로 접근 (-1만 곱하면 됨) Deep Neural Networks with MLE MLE는 관측된 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 방법으로, 모델의 손실 함수를 설계하는 데 사용되며, 일반척으로 cross-entropy loss function 및 NLL이 MLE를 기반으로 함. 이 손실 함수를 최소화 함으로써 모델은 데이터를 가장 잘 설명하는 파라미터 값을 찾아가는 과정을 수행 MLE는 특정 데이터를 가장 잘 설명하는 확률 모델의 매개변수를 추정하는 방법 관찰된 데이터에 대한 모델의 적합성을 측정 분포 $P(\\text{x})$로부터 샘플링한 데이터 𝒙가 주어졌을 때, 파라미터 𝜃를 갖는 DNN은 조건부 확률 분포를 나타냄 $P(\\text{y}|x;\\theta) \\text{, where } x \\sim P(\\text{x})$ 이때, 우리는 Gradient Descent를 통해 NLL을 최소화하는 𝜽를 찾을 수 있음 $\\hat{\\theta} = \\text{argmin}{\\theta \\in \\Theta} \\sum{i=1}^{N} -\\log P(y_i | x_i; \\theta)$ Summary MLE를 통해 수집한 데이터셋을 잘 설명하는 확률 분포의 파라미터를 찾을 수 있음 하지만 비선형 함수에서는 베르누이나 가우시안 확률분포로는 설명할 수 없음 분류 작업의 경우에는 네트워크의 출력을 softmax 함수에 통과시켜 각 클래스에 속할 확률을 나타내는 확률 분포를 얻을 수 있음 이진 분류 모델에서는 시그모이드 함수를 사용할 수 있음(출력 범위는 0~1) Neural Networks 또한 확률 분포 함수이므로, MLE를 통해 파라미터(𝝁, 𝝈)를 찾을 것 최대한 대신 최소화를 위해 Negative Log-Likelihood(NLL)을 Gradient Descent Gradient Descent를 수행하기 위해선, 파라미터에 대한 미분이 필요함 이를 효율적으로 수행하기 위해 back-propagation을 활용 딥러닝이란 확률 분포 함수인 동시에 낮은 차원으로 차원을 축소하자 MLE Equation $D={(x_i, y_i)}_{i=1}^{N} \\text{, where } x_i: \\sim P(\\text{x}) \\text{, } y_i= \\sim P(\\text{y} ,|x_i)$ $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log P(y_i | x_i; \\theta)$$= \\text{argmin}{\\theta \\in \\Theta} -\\sum{i=1}^{N} \\log P(y_i | x_i; \\theta)$ $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)$ Connection to Deep Neural Networks $D={(x_i, y_i)}_{i=1}^{N}$ $\\hat{\\theta} = \\text{argmin}{\\theta \\in \\Theta} -\\sum{i=1}^{N} \\log P(y_i | x_i; \\theta)$ → $\\hat{y_i}= f_\\theta(x_i)$ $-\\sum_{i=1}^{N} \\log P(y_i | x_i; \\theta)=-\\sum_{i=1}^{N} y_i^T \\cdot \\log\\hat{y_i}$ MLE(NLL) &amp; Cross Entropy Loss Minimizing Negative Log-Likelihood is equal to Minimizing Cross Entropy $CE(y_{1:N},\\hat{y}{1:N}) = -\\frac{1}{N} \\sum{i=1}^{N} y_i^T \\cdot \\log \\hat{y}_i$ // (1/𝑁은 나중에 𝜃로 미분되어서 사라지기에 결국 MLE와 같음) $= -\\frac{1}{N} \\cdot \\sum_{i=1}^{N} \\sum_{j=1}^{d} y_{i,j} \\times \\log \\hat{y}_{i,j}$ $= -\\frac{1}{N} \\cdot \\sum_{i=1}^{N} \\log P_\\theta (y_i|x_i)$ $\\text{where } y_{1:N} \\in \\mathbb{R}^{N \\times d} \\text{ and } \\hat{y}_{1:N} \\in \\mathbb{R}^{N \\times d}$ 분류문제에서 Cross Entropy를 사용하고 CE Loss를 사용하여 최적화를 수행했는데, 이 모든 것은 결국 MLE를 수행하기 위한 것이다.(𝜽를 잘 찾는 과정(내가 수집한 데이터를 가장 잘 나타내기 위한 과정)을 하는 것) Cross-Entropy 손실 함수는 두 확률 분포 사이의 차이를 측정 딥러닝에서는 일반적으로 하나의 분포는 실제 레이블을 나타내고, 다른 하나는 모델의 예측을 나타냄 이 경우 Cross-Entropy 손실 함수는 모델의 예측이 실제 레이블을 얼마나 잘 반영하는지를 측정하는 데 사용 Negative Log-Likelihood (NLL)와 Cross-Entropy 손실 함수는 비슷한 개념을 포함하고 있지만, 엄밀히 말해서 동일한 것은 아님 이 둘의 차이점은 NLL이 단일 확률 분포에 대한 측정을 제공하는 반면, Cross-Entropy는 두 확률 분포 사이의 차이를 측정한다는 점입니다. 이 차이 때문에, 이 두 손실 함수는 다른 문맥에서 사용되며, 그 결과도 약간 다를 수 있음 그러나 분류 문제에서는 두 손실 함수가 동일한 결과를 제공하는 경우가 많습니다. 이는 각 클래스에 대한 실제 레이블 분포가 원-핫 인코딩 형식이기 때문 따라서 NLL과 Cross-Entropy 손실 함수는 종종 상호 교환적으로 사용","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%E1%84%80%E1%85%AA-MLE/"},{"title":"3장. Geometric Perspective - Dimension Reduction","text":"Dimension ReductionLinear Dimension Reduction: PCA(Principal Component Analysis) n 차원의 공간에 샘플들의 분포가 주어져 있을 때, 분포를 잘 설명하기 위한 새로운 axis를 찾아내는 과정 새로운 axis는 두 가지 조건을 만족해야 함 새롭게 찾아낸 axis에 샘플들을 투사(projection)하면 차원 축소가 가능 차원 축소(Dimension reduction)는 왜 필요할까? Binary Classifcaion in 2D 차원 축소의 한계 Binary Classification in 2D","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Geometric-Perspective-Dimension-Reduction/"},{"title":"2장. Probabilistic Perspective – MSE with Probabilistic Perspective","text":"MSE with Probabilistic Perspective 그 전에는 분류의 관점에서 봤지만 이번엔 MSE를 사용한 회귀의 문제로 봄 분류에서는 Multi-Nomial Distribtion을 사용해 Cross Entropy를 사용했지만, Gaussian Distribution을 가정하고 작성 Gaussian PDF (가우시안 확률밀도함수) 수식 $p(x; \\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$ $\\log{p(x;\\mu,\\sigma)} = -\\log{\\sigma\\sqrt{2\\pi}}-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2$ $-\\log{p(x;\\mu,\\sigma)} = \\log{\\sigma\\sqrt{2\\pi}}-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2$ MLE with Gradient Descent 수식 $D={(x_i, y_i)}_{i=1}^N$ $\\hat{\\theta} = \\text{argmax}{\\theta \\in \\Theta} \\sum{i=1}^{N} \\log p(y_i | x_i; \\theta)$ // $\\log p(y_i | x_i; \\theta)$ : log likelihood를 최대화 $L(\\theta) = -\\sum_{i=1}^{N} \\log p(y_i | x_i; \\theta)$ // NLL로 바꾸어 loss function으로 삼고 minimize함, 그로 인해 gradient descent를 함 $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} L(\\theta)$ Get Gradient of NLL $\\log{p(y_i,x_i;\\phi,\\psi)} = \\log{\\sigma_{\\psi}(x_i)\\sqrt{2\\pi}} +\\frac{1}{2}(\\frac{y_i-\\mu_\\phi(x_i)}{\\sigma_{\\psi}(x_i)})^2 \\text{, where } \\theta={\\phi, \\psi}$","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-%E2%80%93-MSE-with-Probabilistic-Perspective/"},{"title":"3장. Geometric Perspective - Manifold Hypothesis","text":"Manifold 가설데이터의 분포 MNIST는 784(28x28) 차원의 벡터로 나타내어 진다. 784 차원의 공간에 존재하는 데이터 샘플들이 uniform 하게 분포되어 있을까? Not uniform, 한 군데에 몰려있을 확률이 높음 Manifold Hypothesis (아직 증명되지 않음) 실생활에서의 Manifold 우리는 3차원의 좌표계에 살지만, 2차원 좌표계로 세상을 인식 (Mapping to Lower Dimensional Space) 고차원 공간의 샘플들이 다양체(manifold)의 형태로 분포해 있다는 가정 따라서 다양체를 해당 차원의 공간에 mapping 할 수 있음 고차원 공간에서의 두 점 사이의 거리는 저차원 공간으로의 맵핑 후 거리와 다름 MNIST 예제 796 차원의 샘플들이 2D 공간 안에 맵핑이 됨 Non-linear Dimension Reduction 예제(Binary Classification ind 2-D) ] 코드로 확인해보기 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import numpy as npimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.optim as optimfrom utils import load_mnistfrom trainer import Trainerdef show_image(x): if x.dim() == 1: x = x.view(int(x.size(0) ** .5), -1) plt.imshow(x, cmap='gray') plt.show()from argparse import Namespaceconfig = { 'train_ratio': .8, 'batch_size': 256, 'n_epochs': 50, 'verbose': 1, 'btl_size': 2}config = Namespace(**config)print(config)train_x, train_y = load_mnist(flatten=True)test_x, test_y = load_mnist(is_train=False, flatten=True)train_cnt = int(train_x.size(0) * config.train_ratio)valid_cnt = train_x.size(0) - train_cnt# Shuffle dataset to split into train/valid set.indices = torch.randperm(train_x.size(0))train_x, valid_x = torch.index_select( train_x, dim=0, index=indices).split([train_cnt, valid_cnt], dim=0)train_y, valid_y = torch.index_select( train_y, dim=0, index=indices).split([train_cnt, valid_cnt], dim=0)print(&quot;Train:&quot;, train_x.shape, train_y.shape)print(&quot;Valid:&quot;, valid_x.shape, valid_y.shape)print(&quot;Test:&quot;, test_x.shape, test_y.shape)from model import Autoencodermodel = Autoencoder(btl_size=config.btl_size)optimizer = optim.Adam(model.parameters())crit = nn.MSELoss()trainer = Trainer(model, optimizer, crit)trainer.train((train_x, train_x), (valid_x, valid_x), config)with torch.no_grad(): import random index1 = int(random.random() * test_x.size(0)) index2 = int(random.random() * test_x.size(0)) z1 = model.encoder(test_x[index1].view(1, -1)) z2 = model.encoder(test_x[index2].view(1, -1)) recon = model.decoder((z1 + z2) / 2).squeeze() show_image(test_x[index1]) show_image(test_x[index2]) show_image((test_x[index1] + test_x[index2]) / 2) show_image(recon)","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Geometric-Perspective-Manifold-Hypothesis/"},{"title":"3장. Geometric Perspective - 차원의 저주(Curse of Dimensionality)","text":"Sparseness in High Dimensional Space d차원의 공간의 구(sphere) 안에 임의로 n개의 점을 흩뿌려보자 이 때, 구 테두리(회색 영역)와 안쪽에 위치한 점의 갯수를 살펴보자. 차원이 증가함에 따른 각 영역 별 점의 갯수의 차이는 어떻게 될까? Curse of Dimensionality 차원이 높을수록 데이터는 희소하게 분포하게 되어 학습이 어려워짐 모든 점들을 학습하기 위해서, 모든 구역들을 살펴봐야함 같은 구역 내의 점들은 구별할 수 없음 같은 정보의 데이터를 표현할 때, 차원이 높아질수록 희소성(sparseness)이 증가 희소성이 높을수록 modeling의 난이도가 높아짐 Gaussian Mixture를 fitting하고자 할 때. K-Means 클러스터링을 수행하고자 할 때. 따라서 데이터의 특징(feature)을 더럽히지 않으면서 낮은 차원에서 표현해야 함","link":"/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Geometric-Perspective-%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%8B%E1%85%B4-%E1%84%8C%E1%85%A5%E1%84%8C%E1%85%AE-Curse-of-Dimensionality/"},{"title":"Docker Multi Stage 빌드","text":"팀장님의 지시로 한 팀원이 Docker Multi Stage 빌드 기법을 통해 Docker Image를 새로 빌드했던 적이 있다. 그 결과 약 650MB 크기의 이미지를 590MB 의 이미지로 축소한 결과를 보았다.블로그의 어떤 글쓴이는 3GB 의 크기가 몇 백 MB 로 축소된 결과를 보았는데, 그 결과에 비하면 빌드에 필요한 내용이 그렇게 많지 않아 상대적으로 적게 경량화를 한 것으로 보이지만, 충분히 좋은 시도였다고 판단되어 따로 직접 공부한 내용에 대한 기록을 남긴다. Docker Multi Stage 빌드란 Docker 이미지를 빌드할 때, 중간에 생성되는 임시 파일들이나 빌드 도구 등이 최종 이미지에 포함되지 않도록 Dockerfile을 작성하는 방법 즉 최종 컨테이너 이미지에는 필요 없는 환경을 제거하여 이미지를 경량화 시키는 방법 Docker 17.05 버전 이후부터 사용 가능 Multi Stage 빌드 방식이 나온 배경 Docker Image의 크기를 작게 만들어 이미지의 빌드 및 컨테이너를 통한 배포 시간을 줄이려고 함 Builder Pattern을 활용 빌드와 실행을 분리하는 디자인 패턴으로, Docker에서는 빌드 이미지와 런타임 이미지를 분리하는 방식으로 적용됨 렇게 하면 빌드에만 필요한 도구나 라이브러리를 제외하고, 필요한 바이너리만을 포함하는 가벼운 Docker 이미지를 생성할 수 있음 하지만 이 방식을 사용하려면 빌드 이미지와 런타임 이미지, 두 개의 Dockerfile이 필요, 이미지 관리에 번거로움 이로 인해 하나의 Dockerfile로 빌드 이미지와 실행 이미지를 분리할 수 있게 되는 Multi-stage build 기능이 Docker 17.05 버전 이후 사용 가능해짐 Simple Python app을 통한 Multi Stage 빌드 사례123456789101112# Builder stageFROM python:3.7 AS buildWORKDIR /appCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY . .# Package stageFROM python:3.7-slimCOPY --from=build /app /appWORKDIR /appCMD [ &quot;python&quot;, &quot;./your-app.py&quot; ] pip install 명령어를 통해 파이썬 패키지를 설치할 때를 예로 들 수 있음 일부 파이썬 패키지는 C/C++ 라이브러리에 의존하는 경우가 많은데, 이 경우 해당 라이브러리들을 빌드하기 위해 gcc와 같은 컴파일 도구가 필요하게 되어짐 그로인해 header 파일, 정적 라이브러리, 스크립트, 문서 등도 함께 설치됨 header 파일, 정적 라이브러리, 스크립트, 문서의 내용은 빌드할 때만 사용되고, 실제 애플리케이션을 실행하는데는 필요하지 않음 Docker Multi Stage 빌드의 장점 이미지 크기 최소화 Docker 이미지 크기를 상당히 줄일 수 있음 얼마나 줄일 수 있는지는 설치하는 패키지와 의존성에 따라 다르지만, 수백 MB 정도를 줄일 수도 있음 그로 인해 배포 시간 및 빌드 시간을 줄일 수 있으며, 저장 공간을 절약 구성 관리 용이성 각 단계별로 필요한 도구와 설정을 분리하여 관리할 수 있음 예를 들어, 빌드 도구는 빌드 단계에서만 사용하고, 런타임에는 사용하지 않는 라이브러리는 별도의 단계에서 관리 보안 향상 빌드 도구와 그 외 라이브러리를 최종 실행 이미지에서 제거함으로써, 이를 통한 공격 경로를 줄일 수 있음 단계별로 필요한 권한을 최소화하여 설정할 수 있어, 악의적인 사용자가 컨테이너 내에서 원치 않는 작업을 수행하는 것을 방지 Docker Image의 크기를 줄이려면 다른 방법은 어떠한 것이 있을까? Docker Layer 최적화 RUN, COPY, ADD 등의 Dockerfile 명령은 각각 새로운 레이어를 생성 따라서 가능한 한 적은 수의 레이어가 생성되도록 Dockerfile을 작성하고, 캐시를 잘 활용하도록 함 필요하지 않은 파일 삭제 apt-get install 등의 명령을 실행한 후에는 rm -rf /var/lib/apt/lists/* 등의 명령을 사용해 필요 없는 파일들을 삭제 Dockerignore 파일 사용 Docker 빌드 컨텍스트에 포함되지 않아야 하는 불필요한 파일들을 .dockerignore 파일에 명시하여 이미지 크기를 줄임 최소한의 베이스 이미지 사용","link":"/Container%20Orchestration/Docker/Docker-Multi-Staging/"},{"title":"Docker Orchestration Tools","text":"Docker Orchestration ToolsDocker orchestration tool 이란 Docker 컨테이너를 관리하고, 자동화하는 기능을 제공하는 도구들을 의미함 대규모 시스템에서 수십, 수백, 수천 개의 컨테이너를 효과적으로 관리하는 데 도움이 됨 컨테이너의 배포 및 스케일링 서비스 간의 네트워킹과 통신 로드 밸런싱 컨테이너 간의 종속성 관리 장애 발생 시 복구 리소스 할당 Docker Orchestration tool 종류 Kubernetes(k8s) 컨테이너화된 애플리케이션을 배포, 확장, 관리하기 위한 오픈소스 플랫폼 Google에서 개발한 이 플랫폼은 현재 Cloud Native Computing Foundation에서 관리 주요 기능 및 컴포넌트 컨테이너 배포 및 스케일링 k8s는 컨테이너 그룹 (Pods라고 부름)에 대한 선언적 업데이트와 롤백을 제공 사용자는 특정 수의 컨테이너가 실행 중인지 확인할 수 있으며, 필요에 따라 수동 또는 자동으로 스케일링을 조정할 수 있음 서비스 디스커버리와 로드 밸런싱 DNS 이름이나 자체 IP 주소를 사용하여 컨테이너를 찾을 수 있음 컨테이너에 트래픽이 많을 경우, Kubernetes는 로드 밸런싱과 분산을 통해 트래픽을 관리 스토리지 오케스트레이션 로컬 스토리지나 공용 클라우드 제공자 (GCP, AWS 등)의 스토리지, 네트워크 스토리지 시스템 (iSCSI, NFS, Gluster, Ceph 등) 등 다양한 스토리지 시스템을 자동으로 마운트할 수 있음 자동화된 롤아웃과 롤백 애플리케이션의 원하는 상태를 기술하고, 현재 상태를 원하는 상태로 변경하는 속도를 제어할 수 있음 애플리케이션의 새 버전을 배포하거나, 현재 버전을 이전 버전으로 롤백하거나, 동시에 모든 인스턴스를 업데이트하지 않아도 됨 Docker Compose Docker Compose는 여러 컨테이너로 구성된 애플리케이션을 정의하고 실행하는 도구 여러 컨테이너로 구성된 도커 애플리케이션을 관리 주로 단일 호스트를 대상으로 yaml 파일 형식의 compose 파일을 사용하여 서비스, 네트워크, 볼륨 등을 정의 docker-compose up 명령을 사용해 모든 컨테이너를 한번에 시작할 수 있음 Docker Swarm 여러 Docker 호스트를 하나의 가상 호스트처럼 작동하게 하는 클러스터링 및 스케줄링 도구 주로 멀티 호스트를 대상으로 함 서비스 디스커버리, 로드밸런싱, 서비스 복제(replica) 등을 통해 고가용성과 확장성을 제공 Docker Service 스웜에서 클러스터 안의 서비스(컨테이너 하나 이상의 집합)를 관리 Docker Stack 스웜에서 여러 개의 서비스를 합한 애플리케이션을 관리","link":"/Container%20Orchestration/Docker/Docker-Orchestration-Tools/"},{"title":"Docker Persistance Data","text":"Docker Persistance DataPersistence Data란 컨테이너가 생성하는 데이터를 저장하고 관리하기 위한 기능 Docker 컨테이너는 기본적으로 휘발성(volatile), 즉 컨테이너가 종료되면 컨테이너에서 생성된 모든 데이터가 삭제됨 이를 위해 Data를 영구적으로 보존하도록 하는 기능 애플리케이션을 종료하고 다시 실행하더라도 이전에 저장한 데이터를 다시 불러올 수 있는 기술 Docker는 데이터 지속성을 제공하는 몇 가지 메커니즘을 제공 Docker Volumes 바인드 마운트(Bind mounts) tmpfs 마운트 (tmpfs mounts) Volume Container Data Volume Docker에 의해 직접 관리되며, 컨테이너 간에 쉽게 데이터를 공유할 수 있음 Docker의 볼륨은 호스트 시스템에서 관리되는 공간으로, 컨테이너에 마운트할 수 있음 즉, 호스트와 컨테이너 사이의 디렉터리 공유 및 재사용 기능을 제공 Docker CLI 또는 Docker Compose를 사용하여 생성하고 관리할 수 있음 Docker Volume 드라이버를 사용하면 클라우드 기반 스토리지 서비스 등을 이용하여 볼륨을 호스팅할 수 있음 볼륨의 백업과 복원을 수행할 수 있음 컨테이너를 파기해도 디스크에 그대로 남아 stateful한 애플리케이션 실행에 적합함 -v 또는 --volume 옵션 뒤에 오는 값이 VOLUME_NAME:CONTAINER_PATH 형태라면, 이는 볼륨을 사용한다는 의미 (bind mounts와 햇갈리지 말기) VOLUME_NAME : Docker에 의해 관리되는 볼륨의 이름 CONTAINER_PATH는 컨테이너 내에서 해당 볼륨이 마운트될 위치 1234567# mysql_data라는 이름의 볼륨을 생성하고, 이를 컨테이너의 /var/lib/mysql 디렉토리에 마운트docker volume create mysql_datadocker run -d \\ --name mysql \\ -v mysql_data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ mysql:tag Bind mounts 바인드 마운트는 호스트 시스템의 특정 디렉토리를 컨테이너에 직접 마운트하는 방법 이 방법은 컨테이너에서 생성된 데이터를 호스트에 직접 저장하게 함 -v 또는 --volume 옵션 뒤에 오는 값이 HOST_PATH:CONTAINER_PATH 형태라면, 이는 바인드 마운트를 사용한다는 의미 HOST_PATH는 호스트 시스템의 파일 또는 디렉토리 경로 CONTAINER_PATH는 컨테이너 내에서 해당 경로가 마운트될 위치 1234# 호스트 시스템의 ~/myapp 디렉토리를 컨테이너의 /app 디렉토리에 마운트docker run -d \\ -v ~/myapp:/app \\ my_docker_image tmpfs mounts Docker 컨테이너 내부에서 일시적인 파일 저장소로 사용되는 메모리 기반 파일 시스템 컨테이너가 실행되는 동안에만 데이터를 저장하며, 컨테이너가 종료되면 모든 데이터가 사라짐 디스크에 데이터를 저장할 때 발생하는 I/O 오버헤드 없이 빠른 데이터 액세스를 필요로 하는 경우에 유용 ex. 웹사이트에서 세션 정보나 캐시 정보와 같은 일시적인 데이터를 tmpfs 마운트에 저장할 수 있음 이런 정보들은 빠르게 접근해야 하지만, 서버가 재시작되는 경우 이전 세션 정보나 캐시 정보가 사라져도 큰 문제가 없는 데이터로 tmpfs 마운트를 사용하면 디스크 I/O의 부하를 줄이면서 빠른 데이터 액세스를 가능하게 할 수 있음 임시 데이터를 저장하기에만 적합하며, 영구적인 데이터 저장에는 적합하지 않음 Why! 컨테이너가 중지되거나 재시작되면 메모리에 저장된 모든 데이터가 사라지기 때문 tmpfs 마운트는 Linux 기반 시스템에서만 작동하며, 다른 운영 체제에서는 사용할 수 없음 1234# 컨테이너에서 /app 디렉토리를 tmpfs로 마운트docker run -d \\ --tmpfs /app \\ my_docker_image Data Volume Container Docker의 초기 버전에서 주로 사용되던 데이터 지속성 방법 중 하나였음. 그러나 Docker Volume과 Bind Mounts, tmpfs 등의 개념이 소개되면서 최근에는 잘 사용 되지 않음 특정 컨테이너에 볼륨을 생성하고, 다른 컨테이너들이 해당 컨테이너를 통해 볼륨에 접근하는 방식으로 작동 볼륨 관리에 대한 미세 조절이 필요하거나 컨테이너간에 데이터를 공유하는 데 있어서 제한적인 면이 있음 현재 Docker에서는 Docker Volumes를 사용하여 데이터를 지속적으로 저장하는 것이 표준화 컨테이너 간에 디렉터리를 공유 데이터 볼륨 컨테이너가 공유한 디렉터리 역시 호스트 머신의 스토리지에 저장됨 예시 1234567891011# dbdata라는 볼륨을 가진 dbstore라는 이름의 컨테이너를 만듬# 이 컨테이너는 실행되지 않고 볼륨을 제공하는 역할만 함# docker create -v /dbdata --name dbstore ubuntu 명령을 실행하면, Docker는 자동으로 /dbdata라는 이름의 볼륨을 생성, 이 볼륨은 호스트 시스템에 저장됨# 일반적으로 Linux 시스템에서는 /var/lib/docker/volumes/ 디렉터리 아래에 위치# 따라서, /dbdata 볼륨에 데이터를 저장하면, 실제로는 호스트의 /var/lib/docker/volumes/&lt;volume-identifier&gt;/_data 디렉터리에 저장docker create -v /dbdata --name dbstore ubuntu# dbstore 볼륨 컨테이너의 볼륨을 공유하는 새 컨테이너를 만듬# --volumes-from 옵션은 dbstore 컨테이너의 볼륨을 db1 컨테이너와 공유docker run -d --volumes-from dbstore --name db1 ubuntu ※ 참고 : docker inspect 을 통해 볼륨의 세부 정보를 확인할 수 있음","link":"/Container%20Orchestration/Docker/Docker-Persistance-Data/"},{"title":"Docker-compose를 이용한 이미지 생성 및 컨테이너 배포 방법(Java+Tomcat+MariaDB)","text":"개요On-Premise 환경에서 사용하던 프로그램을 Docker-compose를 활용해 도커 컨테이너로 전환을 수행 이를 통해 각 컨테이너가 독립적인 환경에서 운영됨으로써, 서비스 간의 의존성을 크게 줄이고, 서비스의 안정성을 높임 AS-IS다음 요소들을 On-Preimise 환경에서 개별적으로 설치 및 운영 비즈니스 요구사항을 처리하는 Java 어플리케이션, 사용자에게 웹 인터페이스를 제공하는 JSP를 통한 HTML/CSS 프론트엔드 웹 애플리케이션 서버인 Tomcat 데이터 저장 및 관리를 담당하는 MariaDB TO-BEDocker-compose를 활용하여, 각 서비스 구성요소를 도커 컨테이너화 전환 전체 시스템은 my-app-image라는 도커 이미지를 중심으로 구성 my-app-image : Docker-compose를 활용하여 2개의 컨테이너로 이루어진 애플리케이션을 정의하고 실행 My app Ubuntu 기반 이미지 위에 MYAPP 애플리케이션 및 웹 애플리케이션 서버인 Tomcat을 통합하여 설치한 컨테이너 my-app-mariadb MariaDB 데이터베이스를 담당하는 컨테이너로, 데이터의 안정적인 저장 및 관리를 담당 ※ Docker-compose 를 사용한 배경 : my-app과 MariaDB를 각각의 컨테이너로 올리지만 하나의 쌍으로 pairing 하여 관리하여 운영 및 관리의 편의성을 제고 ※ {my-app, tomcat, mariaDB} 로 구성하지 않고 {my-app+tomcat, mariaDB} 로 구성한 이유는 tomcat의 server.xml이 my-app의 경로(context-path)와 mapping이 되어야 하기 때문 절차다음과 같이 3가지 절차를 거치게 된다. my- app 어플리케이션 이미지 생성 my- app애플리케이션과 Tomcat을 설치하는 데는 두 가지 방법이 고려됐지만 1번을 선택 Ubuntu Base 이미지 위에 my- app애플리케이션과 Tomcat을 설치 Tomcat Base 이미지 위에 my- app애플리케이션을 설치 my- app구성에 필요한 추가 설치와 설정이 많아 1번이 더 많은 유연성을 제공하고, 설정을 쉽게 관리할 수 있었기 때문, 다만, Ubuntu Base 이미지를 사용하기 때문에 이미지 크기가 상대적으로 더 커지게 됨 my- app에서 딥러닝을 실행할 때 컨테이너 내에서 docker의 사용이 필요함 이를 위해 DooD(Docker-out-if-Docker) 방식을 사용 DooD는 호스트의 Docker 데몬을 컨테이너와 공유하는 방식으로, docker.sock을 마운트하여 사용※ dockerfile을 작성할 때 docker-ce-client 설치가 필요 1docker run -v /var/run/docker.sock:/var/run/docker.sock -it my-container Docker를 Docker 안에 설치하는 Docker-in-Docker (DinD) 방식은 여러가지 문제점을 갖고 있음 MairaDB 이미지 생성 Docker-compose my- app어플리케이션 이미지와 MariaDB 이미지를 하나의 stack으로 운영 및 관리 1. my-app 어플리케이션 이미지 생성 Ubuntu base 이미지 구성 : OpenJDK 1.8 및 docker-client, mysql-client를 설치 Dockerfile-ubuntu-base 123456789101112131415161718192021222324252627282930FROM ubuntu:18.04RUN sed -i 's/archive.ubuntu.com/mirror.kakao.com/g' /etc/apt/sources.list \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y tzdata locales language-pack-ko \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; localedef -i ko_KR -f UTF-8 ko_KR.UTF-8ENV TZ=Asia/Seoul \\ LANGUAGE=ko_KR.UTF-8 \\ LANG=ko_KR.UTF-8 \\ LC_ALL=ko_KR.UTF-8 \\ LC_CTYPE=ko_KR.UTF-8 \\ LC_MESSAGES=ko_KR.UTF-8 \\ DEBIAN_FRONTEND=noninteractiveRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone \\ &amp;&amp; echo $(date +%r) &amp;&amp; \\ apt-get update -y &amp;&amp; apt-get install -y git nano curl vim sudo wget rename gnupg lsb-release software-properties-common &amp;&amp; \\ apt-get update &amp;&amp; apt-get install --no-install-recommends -y build-essential &amp;&amp; rm -rf /var/lib/apt/lists/* &amp;&amp; \\ apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk &amp;&amp; \\ apt-get update &amp;&amp; apt-get install -y gettext-base &amp;&amp; \\ apt-get update &amp;&amp; apt-get install -y mysql-client &amp;&amp; \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp; \\ add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; &amp;&amp; \\ apt-get update -y &amp;&amp; apt-get install -y docker-ce-cli &amp;&amp; \\ apt-get cleanENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/ docker-build-ubuntu-base.bat 1234567@echo offREM ----SET DOCKER_IMAGE_NAME=my-app-ubuntu-baseSET TAG=0.0.0-basedocker build --file Dockerfile-ubuntu-base --no-cache -t %DOCKER_IMAGE_NAME%:%TAG% . Dokcerfile-my-app myapp.tar.tz : myapp의 소스코드 압축본 tomcat9.tar.gz : Apache tomcat9 이며, 여기서 중요한 것은 server.xml에 미리 myapp의 context path를 명시 12345678910111213141516171819202122232425262728293031FROM my-app-ubuntu-base:0.0.0-baseENV MY_APP_VERSION=&quot;3.1.0&quot;ENV CATALINA_HOME=/root/tomcat9ENV MY_APP_HOME=/root/myappENV DB_HOST=localhostENV DB_PORT=3306ENV DB_SCHEMA=MY_APPENV DB_USER=rootENV DB_USER_PASSWORD=1111ENV MY_APP_NETWORK_PORT=10001ENV MY_APP_SERVER_PORT=20002ENV MY_APP_JVM_SIZE=2096ENV MY_APP_LOG_PERIOD=30WORKDIR /root/COPY ./apps/myapp.tar.gz /root/myapp.tar.gzCOPY ./apps/tomcat9.tar.gz /root/tomcat9.tar.gzCOPY ./apps/start.sh /root/RUN tar -xzf /root/tomcat9.tar.gz -C /root &amp;&amp; rm /root/tomcat9.tar.gzRUN tar -xzf /root/myapp.tar.gz -C /root &amp;&amp; rm /root/myapp.tar.gzRUN chmod +x /root/start.shRUN echo &quot;export CATALINA_HOME=${CATALINA_HOME}&quot; &gt;&gt; /root/.bashrc &amp;&amp; \\ echo &quot;export MY_APP_HOME=${MY_APP_HOME}&quot; &gt;&gt; /root/.bashrc &amp;&amp; CMD [&quot;bash&quot;, &quot;-c&quot;, &quot;/root/start.sh&quot;] Dockerfile에서 ENV는 MY_APP 의 환경변수 및 설정값에 대한 입력값을 자유롭게 설정할 수 있음 솔루션 환경변수 및 입력값 설정 .bashrc : 환경 변수 및 alias 등록 (Dockerfile로 인해 자동으로 수행됨) 12export CATALINA_HOME=/root/tomcat9export MY_APP_HOME=/root/myapp xml 파일에 ENV 적용법 12345&lt;?xml version=&quot;1.0&quot; encoding=&quot;euc-kr&quot; standalone=&quot;yes&quot;?&gt;&lt;myAppSetting&gt; &lt;serverName value=&quot;myAdminServer&quot;/&gt; &lt;myAdminServer ip=&quot;localhost&quot; port=&quot;${MY_APP_SERVER_PORT}&quot;/&gt;&lt;/myAppSetting&gt; jsp 파일에 ENV 적용법 12static String masterServerIp = &quot;localhost&quot;;static String masterServerPort = System.getenv(&quot;MY_APP_SERVER_PORT&quot;); apps/start.sh 12345678910111213141516#!/bin/bashenvsubst &lt; /root/myapp/myAppSetting.xml &gt; /root/myapp/myAppSetting.xml_subs &amp;&amp; mv /root/myapp/myAppSetting.xml_subs /root/myapp/myAppSetting.xmlecho &quot;Tomcat starts!&quot;sh /root/tomcat9/bin/startup.sh | tee /var/log/tomcat.logecho &quot;MaraiDB Check : ${DB_HOST}/${DB_PORT}&quot;while ! mysql -h ${DB_HOST} -P ${DB_PORT} -u ${DB_USER} -p${DB_USER_PASSWORD} -e &quot;SELECT 1&quot; &gt; /dev/null 2&gt;&amp;1; do echo &quot;Waiting for MariaDB to start&quot; sleep 2doneecho &quot;myapp starts!&quot;sh /root/myapp/myAppStart.sh | tee /var/log/myapp.logtail -f /dev/null docker-build-my-app 12345678910@echo offREM Create Base Docker imageREM ----SET DOCKER_IMAGE_NAME=my-appSET TAG=3.1.0docker build --file Dockerfile-my-app ^--no-cache ^-t %DOCKER_IMAGE_NAME%:%TAG% . docker volume setup 12345678910@ECHO OFFREM ----SET DOCKER_VOLUME_NAME=my_app_volumedocker volume create %DOCKER_VOLUME_NAME%docker volume lsREM ---- docker network setup 12345678@ECHO OFFREM ----SET DOCKER_NETWORK_NAME=my_app_bridgedocker network create $DOCKER_NETWORK_NAMEdocker network lsREM ---- 2. MairaDB 이미지 생성 Dockerfile-mariadb 1FROM mariadb:10.5 docker-build-mariadb.bat 1docker build -f Dockerfile-mariadb -t mariadb:10.5 . docker-run-mariadb.bat 12345678docker run --name my-app-mariadb ^-e MYSQL_ROOT_PASSWORD=1111 ^-e MYSQL_DATABASE=MY-APP ^-e MYSQL_USER=super ^-e MYSQL_PASSWORD=1111 ^-p 13306:3306 ^-v my_app_volume:/var/lib/mysql ^-d mariadb:10.5 3. Docker-compose MY-APP 1개 구성 DooD(Docker-out-if-Docker) 방식을 사용 123456789101112131415161718192021222324252627282930313233343536373839404142version: '3'services: myapp: build: context: . dockerfile: Dockerfile-myapp args: DOCKER_IMAGE_NAME: my-app TAG: 1.0.0 environment: DB_HOST: 172.0.0.1 DB_PORT: 13306 DB_SCHEMA: MY_APP DB_USER: root DB_USER_PASSWORD: 1111 MY_APP_NETWORK_PORT: 10001 MY_APP_SERVER_PORT: 20002 MY_APP_JVM_SIZE: 2096m MY_APP_LOG_PERIOD: 30 volumes: - my_app_volume:/root/volumes - /var/run/docker.sock:/var/run/docker.sock ports: - &quot;8092:8080&quot; - &quot;10001-10005:10001-10005&quot; depends_on: - my-app-mariadb my-app-mariadb: image: mariadb:10.5 environment: MYSQL_ROOT_PASSWORD: 1111 MYSQL_DATABASE: MY_APP MYSQL_USER: root MYSQL_PASSWORD: 1111 volumes: - my_app_volume:/var/lib/mysql ports: - &quot;13306:3306&quot;volumes: my_app_volume: MY-APP 3개 구성 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758version: '3'services: myapp-1: build: context: . dockerfile: Dockerfile-my-app args: MY_APP_IMAGE_NAME: my_app TAG: 1.0.0 environment: volumes: - my_app_volume:/root/volumes ports: depends_on: - my_app-mariadb myapp-2: build: context: . dockerfile: Dockerfile-my-app args: MY_APP_IMAGE_NAME: my_app TAG: 1.0.0 environment: volumes: - my_app_volume:/root/volumes ports: depends_on: - my_app-mariadb myapp-3: build: context: . dockerfile: Dockerfile-my-app args: MY_APP_IMAGE_NAME: my_app TAG: 1.0.0 environment: volumes: - my_app_volume:/root/volumes ports: depends_on: - my_app-mariadb my-app-mariadb: image: mariadb:10.5 environment: MYSQL_ROOT_PASSWORD: 1111 MYSQL_DATABASE: MY_APP MYSQL_USER: root MYSQL_PASSWORD: 1111 volumes: - my_app_volume:/var/lib/mysql ports: - &quot;13306:3306&quot;volumes: my_app_volume: 부록 - docker-compose가 아닌 docker-run으로 수행하기 my-app 어플리케이션 이미지 my-app-run.bat 123456789101112131415161718192021222324252627282930313233343536373839@ECHO OFFREM ----IF &quot;%1&quot;==&quot;&quot; ( ECHO Error : Please Input CONTAINER_NAME parameter GOTO :EOF)IF &quot;%2&quot;==&quot;&quot; ( ECHO Error : Please Input HOST_PORT parameter GOTO :EOF)SET CONTAINER_NAME=%1SET HOST_PORT=%2SET DOCKER_IMAGE_NAME=my-appSET TAG=1.0.0SET DOCKER_NETWORK_NAME=my_app_bridgeSET DOCKER_VOLUME_NAME=my_app_volumeSET DOCKER_CONTAINER_NAME=%DOCKER_IMAGE_NAME%-%TAG%-%CONTAINER_NAME%docker rm -f %DOCKER_CONTAINER_NAME%docker run --name %DOCKER_CONTAINER_NAME% ^-e DB_HOST=172.0.0.1 ^-e DB_PORT=13306 ^-e DB_SCHEMA=MYAPP ^-e DB_USER=root ^-e DB_USER_PASSWORD=1111 ^-e MY_APP_NETWORK_PORT=10001 ^-e MY_APP_SERVER_PORT=20002 ^-e MY_APP_JVM_SIZE=2096m ^-e MY_APP_LOG_PERIOD=30 ^-v /var/run/docker.sock:/var/run/docker.sock ^--network %DOCKER_NETWORK_NAME% ^--volume %DOCKER_VOLUME_NAME%:/root/volumes ^--publish %HOST_PORT%:8080 ^--publish 10001-10005:10001-10005-itd %DOCKER_IMAGE_NAME%:%TAG%timeout /t 10 MairaDB image docker-build-mariadb.bat 1docker build -f Dockerfile-mariadb -t mariadb:10.5 . docker-run-mariadb.bat 12345678docker run --name my-app-mariadb ^-e MYSQL_ROOT_PASSWORD=1111 ^-e MYSQL_DATABASE=MYAPP ^-e MYSQL_USER=super ^-e MYSQL_PASSWORD=1111 ^-p 13306:3306 ^-v dqcat3_volume:/var/lib/mysql ^-d mariadb:10.5 느낀점 Docker-compose를 활용 경험 각 서비스 구성요소를 도커 컨테이너화 해봄으로써 서비스 구성요소들의 설치와 관리가 간소화되어 서비스의 배포 및 확장이 매우 용이해졌음을 느꼈음 kubernetes에서 활용할 때는 kubenetes에서 docker-compose.yaml을 자동으로 변환해주는 명령어가 있기에 그것을 확인해보자 Docker-out-of-Docker (DooD) 컨테이너 내에서 도커 명령어가 필요할 경우 Host의 Docker와 연동시켜서 여러가지 발생 가능한 문제점을 막자 환경변수의 사용 dockerfile에서 ENV DB_HOST=localhost을 기술하고 내가 원하는 파일(xml, 소스코드 등)에 선택적으로 입력이 가능 12345&lt;?xml version=&quot;1.0&quot; encoding=&quot;euc-kr&quot; standalone=&quot;yes&quot;?&gt;&lt;myAppSetting&gt; &lt;serverName value=&quot;myAdminServer&quot;/&gt; &lt;myAdminServer ip=&quot;localhost&quot; port=&quot;${MY_APP_SERVER_PORT}&quot;/&gt;&lt;/myAppSetting&gt;","link":"/Container%20Orchestration/Docker/Docker-compose%E1%84%85%E1%85%B3%E1%86%AF-%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB-%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5-%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC-%E1%84%86%E1%85%B5%E1%86%BE-%E1%84%8F%E1%85%A5%E1%86%AB%E1%84%90%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5-%E1%84%87%E1%85%A2%E1%84%91%E1%85%A9-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8(Java-Tomcat-MariaDB)/"},{"title":"Docker 이미지 생성 및 컨테이너 배포 방법(FastAPI RESTful 서버)","text":"Docker 이미지 생성 및 컨테이너 배포 방법(FastAPI RESTful 서버)개요 FastAPI 기반의 RESTful API 서버로 구성된 딥러닝 Python 어플리케이션을 Docker 컨테이너화하는 절차를 설명 Dockerfile과 requirements.txt 파일을 이용해 Docker Image를 빌드 Docker Image를 tar 파일로 압축 (필요에 따라 생략 가능) 이렇게 생성된 tar 파일은 Docker 플랫폼이 설치된 어떤 환경(다른 서버 포함)에서든지 활용이 가능하기 때문 docker-load 스크립트를 이용하여 tar파일을 docker image에 등록 docker-run 스크립트를 이용하여 생성한 image를 docker container에 등록함으로써, 독립적인 container를 운영 이 방법을 통해 다양한 환경에서 일관된 어플리케이션 실행을 보장할 수 있음 구성 파일 트리 구성 Dockerfile-my-app : 응용 프로그램을 설치 및 구성을 하기 위한 dockerfile (OS 이미지를 사전에 필요로 함) ※ src 경로가 필요하기 때문에 프로젝트 최상위에 존재 docker-build-my-app.sh : docker image를 빌드하기 위한 스크립트※ src 경로가 필요하기 때문에 프로젝트 최상위에 존재 src/ : 어플리케이션 소스가 들어있음 docker/ base-image/ : 운영체제를 설치하기 위한 기본 이미지 Dockerfile-base : OS 이미지를 구축하기 위한 구성 내용 작성 Docker-build-base.sh : docker image를 빌드하기 위한 스크립트 image docker-save-my-app-tar.sh my_app-1.0.0.0-image.tar docker-load-my-app-tar.sh setup docker-volume-create.sh docker-network-create.sh containers docker-run-my-app.sh docker-copy.sh 절차1. Docker Image 생성다음과 같은 2가지의 Dockerfile을 구성 운영체제 이미지를 설정하는 dockerfile : Python 3.8 환경으로 구성 응용 프로그램을 설치 및 구성하는 dockerfile : 1번의 도커 이미지를 뼈대로 Python 패키지 설치 및 FastAPI 기동 운영체제 이미지를 설정하는 dockerfile 경로 : docker/base-image/ dockerfile-base 작성 12345678910111213141516171819202122232425FROM nvidia/cuda:10.1-base-ubuntu18.04RUN sed -i 's/archive.ubuntu.com/mirror.kakao.com/g' /etc/apt/sources.list \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y tzdata locales language-pack-ko \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; localedef -i ko_KR -f UTF-8 ko_KR.UTF-8ENV TZ=Asia/Seoul \\ LANGUAGE=ko_KR.UTF-8 \\ LANG=ko_KR.UTF-8 \\ LC_ALL=ko_KR.UTF-8 \\ LC_CTYPE=ko_KR.UTF-8 \\ LC_MESSAGES=ko_KR.UTF-8RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone \\ &amp;&amp; echo $(date +%r) &amp;&amp; \\ apt-get update -y &amp;&amp; apt-get install -y \\ git nano curl vim sudo wget rename &amp;&amp; \\ apt-get update &amp;&amp; apt-get install --no-install-recommends -y build-essential python3.8 python3-pip python3.8-venv \\ python3.8-dev &amp;&amp; rm -rf /var/lib/apt/lists/* &amp;&amp; \\ ln -s /usr/bin/python3.8 /usr/bin/python &amp;&amp; ln -s /usr/bin/pip3 /usr/bin/pip &amp;&amp;\\ python -m pip install --upgrade --no-cache-dir pip wheel setuptoolsRUN apt-get clean docker-build-base.sh 123456#!/bin/bashDOCKER_IMAGE_NAME=&quot;my-app-base&quot;TAG=&quot;1.0.0.0&quot;docker build --file dockerfile-base --no-cache -t &quot;${DOCKER_IMAGE_NAME}:${TAG}&quot; . 앞서 설정한 운영 체제 이미지 위에 응용 프로그램을 추가 설치/구성 경로 : project-root-path Dockerfile-my-app 1234567891011121314151617181920212223242526272829# BuilderFROM dqcat3_dl_kobert:0.0.0-base AS builderCOPY apps/ /root/apps/WORKDIR /root/apps/KoBERT/scriptRUN sh compile.sh# RunnerFROM dqcat3_dl_kobert:0.0.0-baseCOPY --from=builder /root/apps /root/appsCOPY requirements.txt /root/apps/KoBERT/requirements.txtWORKDIR /root/apps/KoBERTRUN python -m pip install --no-cache-dir -r requirements.txt --no-deps &amp;&amp; \\ apt-get -y --purge autoremove vim git nano curl vim wget rename &amp;&amp;\\ apt-get cleanRUN mkdir -p /root/volumes/logs &amp;&amp; \\ mkdir -p /root/volumes/data &amp;&amp; \\ mkdir -p /root/volumes/resultsWORKDIR ~/.cache/pipRUN rm -rf *WORKDIR /root/apps/KoBERT/srcCMD [&quot;uvicorn&quot;, &quot;main:app_kobert&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;80&quot;] requirements.txt 작성 1234567fastapi==0.85.0torch==1.7.1+cu101-f https://download.pytorch.org/whl/torch_stable.htmltransformers==4.8.1filelock==3.10.7huggingface-hub==0.0.12... docker-build-my-app.sh 1234567#!/bin/bashDOCKER_IMAGE_NAME=&quot;dqcat3_dl&quot;TAG=&quot;3.1.0&quot;docker image rm &quot;${DOCKER_IMAGE_NAME}:${TAG}&quot;docker build --file Dockerfile-kobert --no-cache -t %DOCKER_IMAGE_NAME%:%TAG% . 2. Docker Image tar 압축이미지를 tar로 뽑아 파일 시스템에서 관리, 해당 tar 파일을 통해 다른 도커 시스템에서 1번의 image build 과정 없이 사용할 수 있음 경로 : docker/image docker-save-my-app-tar.sh.sh 123456#!/bin/bashDOCKER_IMAGE_NAME=&quot;my_app&quot;TAG=&quot;1.0.0.0&quot;docker save --output &quot;${DOCKER_IMAGE_NAME}-${TAG}-image.tar&quot; &quot;${DOCKER_IMAGE_NAME}:${TAG}&quot; 스크립트의 결과로 docker/image/my-app-1.0.0.0-image.tar 파일이 생성되게됨 3. Docker Volume 및 Network 생성 경로 : docker/setup docker-volume-create.sh 123456789#!/bin/bashDOCKER_VOLUME_NAME=&quot;new_volume&quot;docker volume create $DOCKER_VOLUME_NAMEdocker volume ls## Remove Docker environment# docker volume rm $DOCKER_VOLUME_NAME docker-network-create.sh 123456789#!/bin/bashDOCKER_NETWORK_NAME=&quot;new_bridge&quot;docker network create $DOCKER_NETWORK_NAMEdocker network ls## Remove Docker environment# docker network rm $DOCKER_NETWORK_NAME ※ Bridge network로 구성 시, Docker Container에 IP를 부여하는 방법 12345678910111213141516171819202122232425262728293031$ docker network inspect new_bridge[ { &quot;Name&quot;: &quot;new_bridge&quot;, &quot;Id&quot;: &quot;a096fc37e0bb009e3f40bf92580b5a4b241dcd22ea9909014766fa3df6ec5147&quot;, &quot;Created&quot;: &quot;2023-04-18T03:25:43.9595696Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: { &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: {}, &quot;Config&quot;: [ { &quot;Subnet&quot;: &quot;172.18.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot; } ] }, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: { &quot;Network&quot;: &quot;&quot; }, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: {}, &quot;Options&quot;: {}, &quot;Labels&quot;: {} }] 172.18.0.0/16이라는 서브넷은 다음과 같이 이해할 수 있음 할당 가능한 IP 주소 범위는 172.18.0.2에서 172.18.255.254까지 172.18.0.0은 네트워크 주소, 172.18.255.255는 브로드캐스트 주소로 예약 172.18.0.1는 이 네트워크의 게이트웨이 주소로 설정 /16은 이 서브넷에서 처음 16비트(xxx.xxx.)가 네트워크를, 나머지 16비트(..xxx.xxx)가 호스트를 나타낸다는 것을 의미마지막 16비트를 변경하여 총 65536개(2^16)의 IP 주소를 생성할 수 있음 참고 : /8(2^24), /16(2^16), /24(2^8), /30(2^2), /31(2^1), /32(2^0) 만약에 docker 명령어에 ip 옵션을 사용하지 않는다면 172.18.0.2부터 시작하여 컨테이너가 추가될때마다 이어서 할당 4. Docker Image Load현재 시나리오에서는 작업을 안해도 되지만 tar파일만 있으면 앞으로는 다른 도커 환경에서도 1,2번 작업을 생략하고 작업할 수 있음 docker-load-my-app.sh: Docker image 설치 123456#!/bin/bashDOCKER_IMAGE_NAME=&quot;my_app&quot;TAG=&quot;1.0.0.0&quot;docker load --input ./&quot;${DOCKER_IMAGE_NAME}-${TAG}-image.tar&quot; 5. Docker Container Run docker-run-my-app.sh 최초 1번만 수행하면 됨, 여러 번 사용 시 컨테이너를 지우고 다시 만들게 됨 CONTAINER_NAME, HOST_IP, HOST_PORT를 사용자에게 입력을 받아 실행을 함 실행 결과 HOST_IP:HOST_PORT/docs 를 통해 정상적으로 동작함을 확인할 수 있음 만약 localhost:HOST_PORT/docs로 하고 싶다면 HOST_IP 관련 부분을 빼면 됨 1234567891011121314151617181920212223242526272829303132333435#!/bin/bashif [ -z &quot;$1&quot; ]then echo &quot;Error : Please Input CONTAINER_NAME parameter&quot; exitfiif [ -z &quot;$2&quot; ]then echo &quot;Error : Please Input HOST_IP parameter&quot; exitfiif [ -z &quot;$3&quot; ]then echo &quot;Error : Please Input HOST_PORT parameter&quot; exitfiCONTAINER_NAME=$1HOST_IP=$2HOST_PORT=$3DOCKER_IMAGE_NAME=my_appTAG=1.0.0.0DOCKER_NETWORK_NAME=my_app_bridgeDOCKER_VOLUME_NAME=my_app_volumeDOCKER_CONTAINER_NAME=&quot;${DOCKER_IMAGE_NAME}-${TAG}-${CONTAINER_NAME}&quot;docker rm -f ${DOCKER_CONTAINER_NAME}# GPU Environment - Run Docker containernvidia-docker run -itd --network ${DOCKER_NETWORK_NAME} --volume ${DOCKER_VOLUME_NAME}:/root/volumes --publish ${HOST_IP}${HOST_PORT}:80 --name ${DOCKER_CONTAINER_NAME} ${DOCKER_IMAGE_NAME}:${TAG}sleep 10s 기타 - 편의를 위한 스크립트 파일1. Docker Copy 운영 중일 경우 docker volume과 연결이 된 데이터 송수신 전용 컨테이너를 추가 생성해 데이터를 주고 받는 것이 권장된다고 생각을 하나, 개발 편의상 docker copy 스크립트를 작성 123456789101112131415161718192021222324252627282930#!/bin/bashif [ -z &quot;$1&quot; ]then echo &quot;Error : Please Input CONTAINER_NAME parameter&quot; exit 1fiif [ -z &quot;$2&quot; ]then echo &quot;Error : Please Input HOST_DATA_PATH parameter&quot; exit 1fiif [ -z &quot;$3&quot; ]then echo &quot;Error : Please Input CONTAINER_DATA_PATH parameter&quot; exit 1fiCONTAINER_NAME=&quot;$1&quot;HOST_FILE_PATH=&quot;$2&quot;CONTAINER_FILE_PATH=&quot;$3&quot;DOCKER_IMAGE_NAME=my_appTAG=1.0.0.0DOCKER_CONTAINER_NAME=&quot;${DOCKER_IMAGE_NAME}-${TAG}-${COLLECTION_NAME}&quot;docker cp ${HOST_FILE_PATH}/. ${DOCKER_CONTAINER_NAME}:${CONTAINER_FILE_PATH}/raw/``` 2. Docker Terminal 접속 Running 상태의 docker container에 bash shell을 통해 접속할 때 사용 1234567891011121314#!/bin/bashif [ -z &quot;$1&quot; ] then echo &quot;Error : Please Input CONTAINER_NAME parameter&quot; exit 1fiCOLLECTION_NAME=$1DOCKER_IMAGE_NAME=my_appTAG=1.0.0.0DOCKER_CONTAINER_NAME=${DOCKER_IMAGE_NAME}-${TAG}-${COLLECTION_NAME}docker exec -it ${DOCKER_CONTAINER_NAME} /bin/bash","link":"/Container%20Orchestration/Docker/Docker-%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5-%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC-%E1%84%86%E1%85%B5%E1%86%BE-%E1%84%8F%E1%85%A5%E1%86%AB%E1%84%90%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%82%E1%85%A5-%E1%84%87%E1%85%A2%E1%84%91%E1%85%A9-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8((FastAPI%20RESTful%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5))/"},{"title":"Docker 주요 명령어","text":"주요 도커 명령어 이미지 생성 및 관리 docker build : Dockerfile을 사용해 이미지를 빌드 docker pull : Docker 이미지를 레지스트리로부터 가져옴 docker push : Docker 이미지를 레지스트리에 푸시 docker images : 로컬에 저장된 모든 Docker 이미지를 나열 docker rmi : 하나 이상의 이미지를 제거 docker inspect : Docker 객체에 대한 상세한 정보를 JSON 형식으로 출력 docker save : Docker 이미지를 tar 파일로 저장 docker load : Docker 이미지를 tar 파일로부터 이미지를 로드 docker commit : 컨테이너의 현재 상태를 새 Docker 이미지로 저장 docker tag: 이미지에 태그를 추가하거나 변경 docker history: 이미지의 변경 이력을 확인 컨테이너 생성 및 관리 docker run : 새 Docker 컨테이너를 실행 docker start : 정지된 Docker 컨테이너를 시작 docker stop : 실행 중인 Docker 컨테이너를 중지 docker restart : Docker 컨테이너를 재시작 docker pause: 컨테이너를 일시 중지 docker unpause: 일시 중지된 컨테이너를 다시 시작 docker wait: 컨테이너가 종료될 때까지 대기하고, 종료 상태 코드를 출력 docker ps : 현재 실행 중인 Docker 컨테이너를 나열 docker rm : 하나 이상의 컨테이너를 제거 docker exec : 실행 중인 Docker 컨테이너에서 명령을 실행 docker logs : Docker 컨테이너의 로그를 출력 docker cp : Docker 컨테이너와 호스트 시스템 간에 파일이나 디렉토리를 복사 docker diff : 컨테이너에서 변경된 파일과 디렉토리를 출력 docker stats : 실행 중인 컨테이너의 리소스 사용량을 실시간으로 표시 docker top : 컨테이너에서 실행되는 프로세스를 보여줌 Docker 시스템 docker info : Docker 시스템에 대한 정보를 출력 docker version : Docker의 버전 정보를 출력 docker system df : Docker의 디스크 사용량을 출력 docker system prune : 사용하지 않는 Docker 리소스를 제거 docker system events: 실시간 시스템 이벤트를 출력 네트워킹 docker network create : 새로운 네트워크를 생성 docker network ls : 모든 네트워크를 나열 docker network rm : 하나 이상의 네트워크를 제거 docker port : 특정 컨테이너의 네트워크 포트 매핑을 확인 docker network connect: 컨테이너를 네트워크에 연결합니다. docker network disconnect: 컨테이너를 네트워크에서 연결 해제 docker network inspect: 네트워크 상세 정보를 출력 볼륨 관리 docker volume create : 새로운 볼륨을 생성 docker volume ls : 모든 볼륨을 나열 docker volume rm : 하나 이상의 볼륨을 제거 docker volume inspect: 볼륨 상세 정보를 출력 docker volume prune: 사용하지 않는 볼륨을 제거","link":"/Container%20Orchestration/Docker/Docker-%EC%A3%BC%EC%9A%94-%EB%AA%85%EB%A0%B9%EC%96%B4/"},{"title":"Text Summarization 이란","text":"Text Summarization 이란원문을 이해하기 쉬우면서도 가치있는 정보로 변환하는 작업을 말함 인간은 길이가 길거나 여러 문서로 나눠져있는 텍스트 정보를 한 눈에 파악하기 어려워함 때로는 알지 못하는 전문 용어가 많이 사용되어 있을 수도 있음 이러한 텍스트를 원문을 잘 반영하면서도 간결하여 이해하기 쉬운 형태로 바꿔주는 작업은 상당히 가치있는 일 Text summarization is the process of distilling the most important information from a text to produce an abridged version for a particular task and user 텍스트 요약은 특정 작업 및 사용자에 대한 요약 버전을 생성하기 위해 텍스트에서 가장 중요한 정보를 추출하는 프로세스이다.Berry, Dumais, &amp; O’Brien (1995) Text Summarization 은 요약의 대상(source)이 text 로 한정이 됨 Text를 image로 본다면 image captioning, video로 본다면 video summarization이 됨 Text, image, video 등 다양한 형태의 source를 함께 요약하는 방식을 multimodal summarization이라고 함 Task CategoriesText summarization의 task는 크게 요약문을 생성하는 방식에 따라 extractive summarization과 abstractive summarization으로 나눌 수 있음 추출적 요약(Extractive Summarization) 원문에서 중요한 문장이나 구문을 선택하여 요약문을 만드는 방법 원문의 문장을 그대로 사용하며, 요약문은 원문에 있는 문장들의 조합으로 이루어짐 원문의 정보를 그대로 유지하는 것이 중요한 경우에 적합 주요 알고리즘: TextRank, LSA(Latent Semantic Analysis), Luhn 알고리즘 등 생성적 요약(Abstractive Summarization) 원문의 의미를 파악한 후, 새로운 문장을 생성하여 요약문을 만드는 방법 원문에 없는 표현이나 단어도 요약문에 포함될 수 있음 원문의 정보를 보다 잘 요약하고, 자연스러운 문장으로 표현할 수 있는 장점이 있음 요약문이 자연스럽고 독립적인 정보를 제공해야 하는 경우에 적합 주요 기술 : Sequence-to-Sequence, Attention, Transformer 등 ※ 이외에도 다음과 같은 관점으로 Task를 구분할 수 있음 keyword/sentence summarization : 생성해내는 Text 형태에 따라 구분 knowledge-poor/rich summarization : 요약 과정에서 원문 외 외부 정보를 얼마나 사용하는지에 따라 single/multi document summarization : 원문의 개수에 따라 구분 (G. Sizov(2010). Extraction-Based Automatic Summarization: Theoretical and Empirical Investigation of Summarization Techniques)Text Summarization 이란 Text Summarization 기초 개념 Summarization 기본 용어 Original text = Source text generated summary = 모델이 생성해낸 요약문을 의미 reference summary = 반면 우리가 정답으로 간주하는(보통은 사람이 직접 원문을 보고 생성한) 요약문, 또는 gold summary라고 부름 보통은 두 용어를 크게 구분없이 쓰는듯 하나, generated summary는 평가하기 위한 기준이 되는 요약문이라는 면을 강조할 때, gold summary는 우리가 찾는 진짜 요약문이라는 점을 강조할 때 주로 사용되는 듯 함 Metric: Rouge, BLEU, Perplexity(PPL) 등 참고 https://github.com/uoneway/Text-Summarization-Repo [글] icoxfog417. awesome-text-summarization [PPT] Sang-Houn Choi. Text summarization","link":"/Deep%20Learning/Text%20Summarization/Text-Summarization-%E1%84%8B%E1%85%B5%E1%84%85%E1%85%A1%E1%86%AB/"},{"title":"MLflow 개요 및 설치법","text":"배경새로 들어오는 DL 프로젝트에 대해 매번 실험을 수기(PPT, 스프레드시트)로 수행하는 데에 한계를 느낌 복수의 모델 및 여러 하이퍼 파라미터 의 값을 조정해 가며 테스트를 수행해 최적화된 모델을 산출하고 싶었음 Epoch, Layer Depth, Regularization(Dropout, Batch Norm, max_len_size, num_update_token 등) 정확도 평가 지표를 여러개 사용하고 그 결과를 한눈에 보기를 희망 분류 : Accuracy, Recall, Precision, F1-Score, AUROC 등 예측 : MAE, RMSE, MPE, R2 Score 등 산출한 모델의 버전 관리가 어려움 ex) model_loss .pkl 등 설치 환경 OS : Ubuntu 16.04 Python : 3.8 (python 3.6 이상이면 문제 없음) MLflow 개요 MLflow란 MLflow는 실험(experimentation), 재현성(reproducibility), 배포(deployment) 및 Model registry 등의 ML life-cycle을 관리할 수 있는 오픈소스 플랫폼 CLI 및 웹 기반 GUI 를 지원 Tensorflow, PyTorch, Keras, sklearn, fast.ai 등 다양한 라이브러리와 통합하여 사용할 수 있음 부분 유료인 WanDB 등 다른 툴도 존재하지만, 무료로 사용할 수 있음 주요기능(크게 4가지 Component로 구성) MLflow Tracking ML Code를 수행할 때 설정한 parameter, code version, metris, loss 의 변화, 시각화 자료 등의 log를 API 및 UI로 제공 Tensorflow &amp; Keras, pytorch, FastAI, Scikit-learn, Spark, XGBoost 등의 라이브러리에 대해서는 자동으로 로그를 기록함(Automatic Logging) MLflow Projects ML code를 재사용, 재구현 가능한 형태로 패키징하여 다른 데이터 과학자들과 공유하거나 프로덕션으로 변환 기능 제공 MLflow Models REST API 기반으로 실시간 서빙 및 추론 플랫폼 제공 Model Registry Centeralized model store의 역할을 수행함 Model의 생성부터 모델의 버전 및 스테이지를 충돌없이 관리할 수 있음따라서 Model Registry에 update가 발생할 경우, 핵심 정보를 로그로 남길 수 있도록 이벤트를 설계 REST API 및 CLI 를 통해 모든 기능에 엑세스할 수 있음 API는 Pyhthon, R, Java 등 존재 MLflow 설치 및 기동 MLflow 설치 및 확인 12345# 22.08.10 기준으로 1.27.0 버전이 설치되었음$ pip install mlflow# 설치 확인$ mlflow --version# mlflow, version 1.27.0 MLflow 디렉토리 생성 간단한 구성을 위해 계정 home path 에 mlflow 디렉토리를 구성하고 해당 디렉토리에 mlflow의 artifact 및 data 파일을 저장 12$ mkdir ~/mlflow$ cd ~/mlflow MLflow tracking server 기동 mlflow ui 로 기동하는 방법, mlflow server 로 기동하는 방법 총 2가지 방법이 있음 mlflow ui는 localhost에서만 기동 가능 작업 연속성을 위해서는 기동은 반드시 같은 디렉토리에서 수행해야함 1234567# 해당 작업의 결과 ~/mlflow 경로에 mlruns 디렉토리가 생김을 확인할 수 있음# mlruns 디렉토리를 통해 entity와 artifacts 관련 내용을 관리$ mlflow ui -h &lt;IP&gt;[2022-08-12 15:57:21 +0900] [5691] [INFO] Starting gunicorn 20.1.0[2022-08-12 15:57:21 +0900] [5691] [INFO] Listening at: http://&lt;IP&gt;:5000 (5691)[2022-08-12 15:57:21 +0900] [5691] [INFO] Using worker: sync[2022-08-12 15:57:21 +0900] [5693] [INFO] Booting worker with pid: 5693 mlflow server는 production 용도로 사용 worker를 여러개 띄울 수 있고 prometheus가 metrics를 가져갈 수 있는 엔드포인트를 제공하는 추가적 기능 존재 123456789101112131415161718192021222324252627282930313233343536# 해당 작업 결과 ~/mlflow 경로에 artifacts 디렉토리와 mlflow.db 파일이 생김을 확인할 수 있음# mlflow.db를 통해 entity를 관리하며 artifacts 디렉터리에서 artifacts 관련 내용을 관리$ mlflow server -h &lt;IP&gt; --backend-store-uri sqlite:///mlflow.db --default-artifact-root $(pwd)/artifacts2022/08/16 11:06:58 INFO mlflow.store.db.utils: Creating initial MLflow database tables...2022/08/16 11:06:58 INFO mlflow.store.db.utils: Updating database tablesINFO [alembic.runtime.migration] Context impl SQLiteImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.runtime.migration] Running upgrade -&gt; 451aebb31d03, add metric stepINFO [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tagsINFO [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric valuesINFO [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags TableINFO [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limitINFO [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics tableINFO [89d4b8295536_create_latest_metrics_table_py] Migration complete!INFO [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to dbINFO [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.INFO [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!INFO [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killedINFO [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraintINFO [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags tableINFO [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags tableINFO [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_versionINFO [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_idINFO [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessaryINFO [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysqlINFO [alembic.runtime.migration] Running upgrade c48cb773bb87 -&gt; bd07f7e963c5, create index on run_uuidINFO [alembic.runtime.migration] Context impl SQLiteImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.[2022-08-16 11:06:59 +0900] [26693] [INFO] Starting gunicorn 20.1.0[2022-08-16 11:06:59 +0900] [26693] [INFO] Listening at: http://&lt;IP&gt;:5000 (26693)[2022-08-16 11:06:59 +0900] [26693] [INFO] Using worker: sync[2022-08-16 11:06:59 +0900] [26695] [INFO] Booting worker with pid: 26695[2022-08-16 11:06:59 +0900] [26704] [INFO] Booting worker with pid: 26704[2022-08-16 11:06:59 +0900] [26776] [INFO] Booting worker with pid: 26776[2022-08-16 11:06:59 +0900] [26777] [INFO] Booting worker with pid: 26777 옵션 설명 1$ mlflow server -h &lt;ip&gt; -p &lt;port&gt; --backend-store-uri &lt;path&gt; -- default-artifact-root &lt;path&gt; -h : 접속이 가능한 네트워크 주소 -p : 접속에 사용할 포트번호 –backend-store-uri MLflow 엔티티가 기록되는 저장소 (experiments 이름, params, metrics 등) DB나 파일시스템으로 저장이 되며 아무 설정 없을시 ./mlruns에 데이터가 로깅이 된다. DB : sqlite:///path/to/file.db 1234567# MLflow 의 Entitiy가 SQL에 저장이 됨을 확인$ sqlite3 mlflow.dbsqlite&gt; .tablealembic_version metrics registered_model_tagsexperiment_tags model_version_tags registered_models experiments model_versions runs latest_metrics params tags 파일시스템 : file:///absolute/path/to/directory 12345678910111213141516171819202122232425262728293031323334353637383940# MLflow 의 Entitiy가 file system에 저장됨을 확인# DB와 다르게 artigact 와 함께 있음을 확인할 수 있음$ tree mlruns/mlruns/└── 0 ├── 05a7b8d5a9f842be85de459e3bc758f9 │ ├── artifacts │ │ ├── feature_importance_weight.json │ │ ├── feature_importance_weight.png │ │ └── model │ │ ├── MLmodel │ │ ├── conda.yaml │ │ ├── model.xgb │ │ ├── python_env.yaml │ │ └── requirements.txt │ ├── meta.yaml │ ├── metrics │ │ ├── accuracy │ │ ├── log_loss │ │ └── train-mlogloss │ ├── params │ │ ├── colsample_bytree │ │ ├── early_stopping_rounds │ │ ├── eval_metric │ │ ├── learning_rate │ │ ├── maximize │ │ ├── num_boost_round │ │ ├── num_class │ │ ├── objective │ │ ├── seed │ │ ├── subsample │ │ └── verbose_eval │ └── tags │ ├── mlflow.log-model.history │ ├── mlflow.source.name │ ├── mlflow.source.type │ └── mlflow.user └── meta.yaml –default-artifact-root ML experiment에 대한 artifacts가 저장이 됨 backend-store-uri를 db로 설정할시 artifact만 다음 파일시스템에 저장됨 1234567891011$ tree artifacts/artifacts/└── 0 └── 365c26af022e4850bd17e01834b30933 └── artifacts └── model ├── MLmodel ├── conda.yaml ├── model.pkl ├── python_env.yaml └── requirements.txt backend-store-uri를 filesystem 으로 설정할 시 artifact 가 entity와 함께 저장됨 파일시스템으로 저장이 되며 아무설정 없을시 ./mlruns에 데이터가 로깅이 된다. 추가 옵션을 사용하고 싶을시 mlflow server –help 로 상세 확인 MLflow tracking server UI 확인 http://&lt;ip&gt;:&lt;port&gt; 로 접속 느낀점MLflow를 설치 시, backend-store-uri 는 sqlite, mariadb 등의 RDBMS 에 등록을 해야 추후 관리하기에 용이한 것 같다. 참고 https://mlflow.org/ https://zzsza.github.io/mlops/2019/01/16/mlflow-basic/ https://myeonghak.github.io/mlops/MLOps-MLFlow%EB%A1%9C-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/ https://velog.io/@ifelifelse/MLflow https://dailyheumsi.tistory.com/260","link":"/MLOps/MLflow/MLflow%20%E1%84%80%E1%85%A2%E1%84%8B%E1%85%AD%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5%E1%84%87%E1%85%A5%E1%86%B8/"},{"title":"Java FP","text":"자바인액션1장 자바 8,9,10,11 무슨 일이 일어나고 있는가?자바의 역사 JDK(1.0) : 1996년 Java7 : 2011년 Java8 : 2014년 3월 Java9 :2017년 9월 Java10 : 2018년 3월 Java11 : 2018년 9월 Java 8 이전에는 어떠한 패러다임이었는가Java 8 부터는 어떠한 패러다임에 응하고 있는가 Java가 거듭 변화하는 이유 컴퓨팅 환경의 변화 멀티코어 CPU 대중화와 같은 하드웨어적 변화 Java에 부여되는 시대적 변화 요구 Java 8, Java 9의 새로운 핵심 기능 소개 프로그래밍 언어 생태계에서 자바의 위치 초창기 많은 유용한 라이브러리를 포함하는 잘 설계된 객체지향 언어로 출발 스레드와 락을 이용한 동시성 지원 인터넷 애플릿 프로그램의 주요 언어가 됨 코드를 JVM 바이트 코드로 컴파일하는 특징 때문( 모든 브라우저에서 가상 머신 코드를 지원) 캡슐화 덕분에 C에 비해 소프트웨어 엔지니어링적인 문제가 훨씬 적었음 하지만 프로그래밍 언어 생태계에 변화의 바람 빅데이터(테라바이트 이상의 데이터셋) 라는 도전에 직면하면서 멀티코어 컴퓨터나 컴퓨팅 클러스터를 이용해서 빅데이터를 효과적으로 처리할 필요성이 커짐 병렬 프로세싱을 활용해야 하는데7까지의 자바로는 충분히 대응하기 어려웠음 하지만 Java8을 통해 효과적으로 대응 병렬성을 활용하는 코드, 간결한 코드를 구현할 수 있도록 Java 8 에서 제공하는 기능인 3 가지 프로그래밍 개념을 제시 Java 8 설계의 밑바탕을 이루는 3가지 프로그래밍 개념값을 변화시키는 데 집중했던 고전적인 객체지향에서 벗어나 함수형 프로그래밍으로 다가선 것이 Java8의 가장 큰 변화 1. 스트림 처리 (Stream processing) 스트림이란 기존에는 한 번에 한 항목을 처리했지만, 우리가 하려는 작업증능 고수준으로 추상화해서 일련의 스트림으로 만들어 처리 효과 : 스트림 파이프라인을 이용해서 입력 부분을 여러 CPU 코어에 쉽게 할당할 수 있음, 즉 스레드라는 복잡한 작업을 사용하지 않으면서도 공짜로 병렬성을 얻을 수 있음 예제: cat data.csv | sort | head -n 5 2. 동작 파라미터화(Behavior parameterization)로 메소드에 코드 전달Java 8에서는 메소드를 다른 메소드의 인수로 넘겨주는 기능을 제공(기존에는 메소드를 다른 메소드로 전달할 방법이 없었음) 3. 병렬성과 공유 가변 데이터순수 함수를 통해 다른 코드와 동시에 실행하더라도 안전하게 실행할 수 있음 (기존 synchronized를 이용해서 공유된 가변 데이터를 보호하는 규칙을 만들 수는 있지만 일반적으로 synchronized는 시스템 성능에 악영향을 미침) ※ 공유되지 않은 가변 데이터(no shared mutable data), 메소드, 함수 코드를 다른 메소드로 전달하는 기능은 함수형 패러다임의 핵심적인 사항 (반면 명령형 프로그래밍(imperative programming) 패러다임은 일련의 가변 상태로 프로그램을 정의), 즉 이 요구사항은 수학에서의 함수처럼 함수가 정해진 기능만 수행하여 다른 부작용은 일으키지 않음을 의미 자바가 진화해야 하는 이유변화된 하드웨어나 프로그래머의 기대에 부응하기 위함, 기존 값을 변화시키는 데 집중했던 고전적인 객체지향에서 벗어나 함수형 프로그래밍으로 다가섰던 것이 자바8의 가장 큰 변화","link":"/Java/Java8/Java-FP/"},{"title":"인터페이스","text":"인터페이스(Interface)default 메서드 자바 8에서부터 인터페이스에 default 메서드를 허용 기존의 인터페이스에서 메서드를 추가하면, 그 인터페이스를 구현하는 모든 클래스에서 새로운 메서드를 구현해야 했음 → 그렇지 않으면 컴파일 에러가 발생 큰 시스템에서는 비용이 많이드는 작업이며, 때로는 수정이 불가능 하기도 했음 이러한 문제를 해결하기 위해 default 메소드를 도입 default 메서드는 인터페이스에 메서드를 추가하더라도 기존의 클래스를 변경하지 않아도 되게 하는 기능 default 메서드는 인터페이스 내에서 구현이 가능하므로, 이를 구현하는 클래스에서는 선택적으로 해당 메서드를 오바라이드 할 수 있음 이러한 원리로 인터페이스가 바뀌어도 기존 코드를 깨뜨리지 않으면서 인터페이스를 확장할 수 있음 default 메서드를 이용하면 인터페이스를 구현하는 클래스가 중복 코드를 줄일 수 있게 됨 이전에는 상속을 통해 중복 코드를 줄이는 것이 주요 방법이었지만, default 메서드를 이요하면 인터페이스에 중복되는 메서드를 정의하고, 이를 공유할 수 있음 코드의 재사용성을 증가시키고 유지보수를 용이하게 함 코드 예시 default 메소드 사용 이전 1234567891011121314151617public interface Vehicle { void honk();}public class Car implements Vehicle { @Override public void honk() { System.out.println(&quot;Beep Beep!&quot;); }}public class Main { public static void main(String[] args) { Vehicle car = new Car(); car.honk(); // 출력: Beep Beep! }} default 메소드 사용 12345678910111213141516public interface Vehicle { default void honk() { System.out.println(&quot;Beep Beep!&quot;); }}public class Car implements Vehicle { // Car 클래스에서는 honk 메소드를 구현하지 않아도 됩니다.}public class Main { public static void main(String[] args) { Vehicle car = new Car(); car.honk(); // 출력: Beep Beep! }} static 메서드 자바 8에서부터 인터페이스에 static 메서드를 허용 인터페이스가 단일 구현 클래스 없이도 유틸리티 메서드나 헬퍼 메서드를 제공할 수 있게 하기 위함 기존의 문제점 유틸리티 메서드 들은 해당 클래스의 인스턴스가 필요하지 않으며, 클래스 자체는 상태를 유지하지 않음 이러한 메서드들은 종종 특정 인터페이스와 밀접한 관련이 있는데, 이 메서드들이 해당 인터페이스와 같은 클래스에 있지 않아서 메서드를 찾기 어렵다는 문제 예시 Collections 클래스의 sort 메소드 Arrays 클래스의 asList 메서드 등 인터페이스에 static 메서드를 추가함으로써 이러한 문제를 해결 별도의 유틸리티 클래스를 만들 필요가 없어지므로 코드의 구조가 더 단순해짐 static 메서드는 인터페이스에서 바로 호출할 수 있으므로, 특정 인터페이스와 관련된 유틸리티 메서드들을 쉽게 찾을 수 있음 코드 예시 static 메소드 사용 이전 123456789101112131415public class ListUtils { public static &lt;T&gt; List&lt;T&gt; reverse(List&lt;T&gt; list) { List&lt;T&gt; copy = new ArrayList&lt;&gt;(list); Collections.reverse(copy); return copy; }}public class Main { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4, 5); List&lt;Integer&gt; reversed = ListUtils.reverse(numbers); System.out.println(reversed); // 출력: [5, 4, 3, 2, 1] }} static 메소드 사용 123456789101112131415public interface ListUtils { static &lt;T&gt; List&lt;T&gt; reverse(List&lt;T&gt; list) { List&lt;T&gt; copy = new ArrayList&lt;&gt;(list); Collections.reverse(copy); return copy; }}public class Main { public static void main(String[] args) { List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4, 5); List&lt;Integer&gt; reversed = ListUtils.reverse(numbers); System.out.println(reversed); // 출력: [5, 4, 3, 2, 1] }}","link":"/Java/Java8/%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"},{"title":"함수형 인터페이스와 람다 표현식","text":"함수형 인터페이스와 람다 표현식함수형 인터페이스(Functional Interface) 자바 8에서 도입된 개념 정확히 하나의 추상 메소드를 가진 인터페이스를 의미 함수형 인터페이스는 람다 표현식(Lambda Expression)의 대상 타입이 될 수 있음 자바8 이전에는 인터페이스를 구현하는데 클래스를 작성하고, 그 클래스의 인스턴스를 생성해야 해야하는 번거로운 과정이 있었지만, 람다 표현식과 함수형 인터페이스의 도입으로 이 과정을 훨씬 간단하게 만들 수 있게됨 ex) 자바에서는 Runnable, Callable, Comparator 등의 함수형 인터페이스가 이미 존재하며, 이러한 인터페이스는 하나의 메소드만을 정의하므로 람다 표현식으로 간단히 표현할 수 있음 123456// 예시 1 - RunnableRunnable r = () -&gt; System.out.println(&quot;Hello, World!&quot;);new Thread9r).start();// 예시 2 - ComparatorComparator&lt;String&gt; c = (s1, s2) -&gt; s1.compareTo(s2); Java 8 이전 12345678910111213141516// 예시 1 - RunnableRunnable r = new Runnable() { @Override public void run() { System.out.println(&quot;Hello, World!&quot;); }};new Thread(r).start();// 예시 2 - ComparatorComparator&lt;String&gt; c = new Comparator&lt;String&gt;() { @Override public int compare(String s1, String s2) { return s1.compareTo(s2); }}; SAM(Single Abstract Method) 인터페이스 @FunctionaInterface 애노테이션을 가지고 있는 인터페이스 람다 표현식(Lambda Expressions) 함수를 하나의 식으로 표현하게 해주는 프로그래밍 기법 이름없는 함수(익명 함수)를 생성할 수 있게 해주고, 이를 통해 간결하고 유연한 프로그래밍이 가능해짐 람다 표현식은 함수를 일급 객체로 취급하게 해주며, 이를 통해 고차 함수(Higher-Order Function)를 지원하게 해주는 등 함수형 프로그래밍의 요소가 됨 함수형 인터페이스와 인스턴스를 만드는 방법으로 사용됨 코드를 줄일 수 있음 메소드 매개변수, 리턴 타입, 변수로 만들어 사용할 수 있음 자바에서의 함수형 프로그래밍 순수 함수(Pure function) 동일한 입력에 대해 항상 동일한 출력을 반환 주어진 입력에 따라 결과를 계산하며, 이때 외부 상태를 참조하거나 변경하지 않음 즉 동일한 인자로 순수 함수를 호출하면 항상 동일한 결과를 반환한다는 말임 두 수를 더하는 함수는 순수 함수 부수 효과(side effect)가 없음 부수 효과란 함수가 외부의 상태를 변경하거나, 함수 외부에서 관찰 가능한 다른 방식으로 외부 세계와 상호작용하는 것을 의미 함수 내에서 전역 변수를 변경하거나 파일에 쓰기를 수행하거나, 화면에 출력하는 등의 I/O작업은 모두 부수 효과를 일으킴 순수 함수는 이러한 부수효과가 없어 함수의 독립성과 예측 가능성을 높임 예시 123456// 순수 함수 예시public static int add(int a, int b) { return a + b;}// 이 함수는 외부 상태에 의존하지 않고, 외부 상태를 변경하지도 않습니다.// 같은 입력 값 (a, b)에 대해 항상 동일한 결과를 반환합니다. 일급 함수 함수를 값처럼 다룰 수 있는 함수를 의미 함수를 변수에 할당할 수 있으며, 함수의 인자로 다른 함수를 전달하거나, 함수의 결과로 함수를 반환할 수 있음 일급 함수를 지원하는 언어에서는 함수를 일급 객체(First-class object) 또는 일급 시민(First-class citizen) 이라고 표현 예시 123456// 함수를 변수에 저장Function&lt;Integer, Integer&gt; square = x -&gt; x * x;// 함수를 다른 함수의 인자로 전달List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4);numbers.stream().map(square).collect(Collectors.toList()); // 결과: [1, 4, 9, 16] 순수 함수와 일급 함수의 차이 순수 함수는 함수의 동작에 대한 제약을, 일급 함수는 함수를 어떻게 사용할 수 있는지에 대한 가능성을 정의 고차 함수(Higher-Order Function) 하나 이상의 다른 함수를 인자로 받거나, 함수를 결과로 반환하는 함수 즉 함수의 인수나 반환 값으로 다른 함수를 사용하는 함수 예시 1234567891011121314// 고차 함수 예시: 'compose' 함수// 이 함수는 두 함수 f와 g를 매개변수로 받아, 새로운 함수를 반환합니다.public static &lt;A, B, C&gt; Function&lt;A, C&gt; compose(Function&lt;B, C&gt; f, Function&lt;A, B&gt; g) { return x -&gt; f.apply(g.apply(x));}// 사용 예Function&lt;Integer, Integer&gt; square = x -&gt; x * x;Function&lt;Integer, Integer&gt; increment = x -&gt; x + 1;// compose 함수를 이용하여 새로운 함수 생성Function&lt;Integer, Integer&gt; squareThenIncrement = compose(increment, square);System.out.println(squareThenIncrement.apply(4)); // 출력: 17 (4*4 + 1) 함수형 프로그래밍(Functional Programming) 함수형 프로그래밍은 프로그래밍 패러다임으로 순수 함수를 이용해 상태 변경이나 가변 데이터를 피하는 것을 강조 일반적으로 일급 함수와 고차 함수를 지원 병렬 처리와 같은 멀티코어/분산 처리에 강점이 있음","link":"/Java/Java8/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4%EC%99%80-%EB%9E%8C%EB%8B%A4-%ED%91%9C%ED%98%84%EC%8B%9D/"},{"title":"함께 자라기","text":"이 포스트는 함께 자라기 - 애자일로 가는길책의 1장 자라기 를 읽으면서 느꼈던 점을 나눕니다. 우리는 올바르게 학습을 하고 있는가? 무엇을 배워야 보다 유리해질수 있을까 어떻게 배워야 하는가 올바른 학습이 우리에게 주는 진정한 효과 1. 우리는 올바르게 학습을 하고 있는가?필드에서 일하는 우리에게 학습의 모습은 어떠해야할까?? 어떠한 프로젝트 일감이 들어왔는데, 그 프로젝트는 내가 기존에 사용해본 적이 없는 심지어는 단순한/단편적인 지식만 있다고 하자. 이 때 어떻게 반응을 하는가? 나의 경우의 학습의 문제점(반성할 점 😰)나의 경우에는 많은 경우 아래와 같은 습관이 있다. 습관 주제에 맞는 책을 구매하고 A-Z까지 정독을 하고, 책에 있는 예제를 따라하며 정리한다. 해당 내용을 동료들과 공유하기 위해 PPT를 작성() 배운 것들을 바탕으로 구현을 시작하나 얼마가지 않아 문제점에 봉착함 업무를 하다보면 책에 없는 내용들이 많음 문제점을 해결하기 위해 블로그/오픈채팅방 및 또다른 전문서적 등을 찾아보며 점진적인 구현 단점 학습에 너무 많은 시간을 사용하다보니 구현에 많은 시간을 쏟지를 못함 과할 경우, 코드를 치는 것보다 읽고 정리하는 시간이 더 길음 최악의 경우, 읽은 책이 실제 코드에 활용되는 부분이 별로 없음 위와 같이 반응하는 이유는 책에서 말하는 것처럼 학교 학습에서 효과적이었던 방법들을 그대로 야생 학습에 가져와서 적용하려 하기 때문이다. 학교에서의 습관과 전략을 여전히 필드에서 사용하기에 거기에서부터 문제가 발생한다. 야생학습과 학교학습 야생학습(😀) 학교학습(😰) 대부분 협력적 대부분 개별적 대부분 비순차적 공부 순서가 정해져있음 대부분 자료에 한정이 없다 대부분 교과서, 교재, 시험 범위 등 정해져 있음 명확한 평가가 없다 대부분 시험이라는 명확한 평가 기준이 있음 대부분 정답이 없음 무엇이 정답인지 명확 대부분 목표가 불분명하고 바뀜 대부분 합격, 자격증 같은 목표가 분명 우리가 속한 필드는 거의 대부분이 야생학습의 환경이다. 그래서 우리가 속한 곳은 야생학습이라는 패러다임을 갖고 나아가야 한다. 2. 무엇을 배워야 보다 유리해질수 있을까인공지능 시대에 대비하려면 배우기 힘든 것에 집중하는 것이 더 좋다. 왜냐하면 정형화할 수 있거나 일정 수준의 규칙이 있는 것에 대해서는 인공지능의 능력이 사람의 능력보다 뛰어나기에, 우리는 인공지능이 배우기 힘든 것에 대해서 배워야 보다 미래에 유리해질 것이다. 학습에 불리한 조건(인공지능이 잘하는 것)(😀) 학습에 유리한 조건(인공지능에 대체대기 어려운 것) 목표가 분명/객관적/정적 목표가 모호/주관적/동적 매 순간 선택할 수 있는 행동/선택의 종류가 유한함 매 순간 선택할 수 있는 행동/선택의 종류가 불확실 매 순간 자신이 목표에 얼마나 근접했는지 알 수 있음(선택의 피드백을 빨리 받을 수 있음) 매 순간 자신의 목표에 얼마나 근접했는지 알기 어렵다(선택의 피드백을 빨리 받기 어려움) 닫힌 시스템(즉, 예상 못한 외부 요소가 갑자기 들어오지 않는) 속에서 일함 열린 시스템(예상 못한 외붕 요소가 갑자기 들어오는 경우가 흔한) 속에서 일함 과거의 선택과 결과에 대한 구조화된 데이터가 많음 선택과 결과에 대한 구조화된 데이터가 별로 없음 즉 암묵지, 직관으로 작동하는 회색 영역, 다시말해 자신이 왜 이런 선택을 했는지 쉽게 설명할 수 없는 것들을 선택하는 것이 보다 좋을 것이다. 3. 어떻게 배워야 하는가진정한 1만 시간의 법칙고등학생 때 말콤 글래드웰의 아웃라이어에서 1만시간의 법칙을 읽고 감명을 받았던 적이 있다. 하지만 시간이 지나고 나서 이제 어느덧 30대 중반을 향해 나가는 내 나이에서 1만시간 이상은 컴퓨터 관련 업무를 한것 같은데, 여전히 나는 업계에서 상위 Tier의 개발자들을 우러러 보고 있다. 무엇이 문제일까 → 학습 방법을 바꿔야함!우리는 **의도적 수련(deliberate practice)**이 필요하다. 즉 단순한, 나이브한 경험을 통한 수련이 아니라 자신의 기량을 매우 의도/의지적으로 향상시킬 목적의 수련이 필요하다. 이와 같은 노력이 있어야 1만 시간의 경험이 쌓여야 특정 분야의 전문가가 될 수 있을 것이다. 한 22년 경력의 스타크래프트 이야기 가끔 유튜브로 전 프로게이머의 스타크래프트 대전 방송을 본다. 한 대전자는 전적은 1만 판이 넘었다. 채팅을 통해 확인한 결과는 22년동안 이 게임만 한 것이다. 엄청나게 고수라고 생각을 하며 기대하며 보았지만, 생각보다 너무 기대 이하의 실력을 보였다. 1만판이면.. 최소 10만 시간은 사용했을텐데.. 경력이 보다 훨씬 짧은 프로게이머에게 처참하게 무너졌다. 그 이유가 와닿는가?? 자신의 약점을 개선하려고 애쓰는 의도적 훈련 - 애자일 철학을 활용해 해결!의도적 수련의 필수 조건 동기 : 실력을 개선시켜야겠다는 강한 의지 피드백 내가 한 행동에 대한 피드백을 10분후, 1시간, 1일, 1주일 후 등 여러 주기를 통해 지속적으로 얻어야 함 (그렇지 못한다면) 행동/결정한 문제에 대한 피드백을 몇 달후(테스트 단계)에 받음 - 결정 내렸던 근거들에 깊이감이 없어지며며 심지어는 결정의 이유조차 가물해짐 저지른 실수는 바로 다음 주기에서 교정 상사, 고객 혹은 내가 개발하는 프로그램에서 직접 피드백을 적극적으로 구해야함 타당성 변수를 제한하고 실험을 하면서 규칙성과 인과관계를 찾으려는 노력을 해야함 적절한 난이도 나의 실력과 작업의 난이도가 비슷해야 함 난이도는 어떻게 설계해 최대의 보람을 성취할 수 있을까우선 미하이 칙센트미하이의 몰입이론 에 대해 알아보자 A 영역 (지루함 😒) 실력 &gt; 작업 난이도 → 지금 당장은 조금 편할지라도 곧 지루함을 느끼게 됨 B 영역 (불안함/두려움 😱) 작업 난이도 &gt; 실력 → 불안함, 두려움을 느끼게 됨 인지 부하 이론(Congnitive Load Theory) : 학습 시 불필요하게 인지적인 부담을 주면 어떤 것도 제대로 학습하기 어려움(독일어로 미적분을 배우면, 독일어라는 엉뚱한 것에 두뇌 에너지를 빼앗겨 미적분 자체를 온전히 배우기 힘듬) C 영역 (몰입 🤩) 실력 ≒ 작업 난이도 → 최고 수준의 집중력, 퍼포먼스 및 학습 능력 최대치, 최고 수준의 행복감 크라센의 입력 가설(Input Theory) : i+1 이론이라고 하며, 학습자의 수준을 i라고 할 때 딱 한 단계 높은 i+1 수준의 입력이 주어질 때에만 언어 능력이 유의미하게 진전한다는 이론 몰입 단계로 들어가기 - 제자리걸음에서 벗어나기 지루함을 느끼는 경우 난이도 높이기(a2) - 요구사항에 대해 좀 더 심층적으로 요구사항을 해결함 100 rps면 되는 시스템 요구사항을 1000 rps 기준으로 만들기 익숙치 않은 코딩 방식으로 짜보기 함수형 프로그래밍 패턴 적용해보기 새로운 언어로 작성해보기 등 자동화 테스트를 수행 리팩토링 등 실력 낮추기(a1) 달리기 연습을 할때 모래주머니를 달고 운동하는 것과 유사함 디버깅, 컴파일을 하는 횟수를 줄여서 긴장감을 높일수 있음 불안함을 느끼는 경우 실력 높이기(b2) 전문서적 읽기 스터디/교육 참석 나보다 뛰어난 전문가의 도움을 얻음 (온/오프라인) 튜토리얼 문서 오픈소스 라이브러리 난이도 낮추기(b1) 아기 버전(0.0.1 버전)을 첫번째 목표로! WTSTTCPW(What’s The Simplest Thing That Could Possibly Work?) 4. 올바른 학습이 우리에게 주는 진정한 효과자기 계발은 복리로 돌아옴 자신이 습득한 지식이나 능력은 복리(더하기가 아닌 곱하기!)로 이자가 붙기 때문에 우리의 자기 계발은 축적이 되면 엄청난 차이를 만듬 빨리 자라고 싶다면? 어떻게 이율을 높일 것인가 자신이 갖고 있는 것들을 잘 활용하자 새로운 지식을 받아들이는데만 집중하면 새로 들어온 것들이 이미 있는 것들을 덮을 수 있음 내가 갖고 있는 지식을 얼마나 어떻게 활용하는지 반성해야한다. 이미 갖고 있는 지식들을 하이퍼링크로 서로 촘촘히 연결 새로운 지식을 받아들이면 지금 가지고 있는 지식들과 충돌을 시도하기 외부 물질을 체화하라 계속 내부 순환만 하다가는 일정 수준에 수렴할 수 있음. 따라서 주기적인 외부 자극을 받으면 좋음 외부 물질을 받아들이면 소화해서 자신의 일부로 체화해야함 피드백을 자주 받아들여라 자신의 능력을 높여주는 도구와 환경을 점진적으로 만들어라 자신을 개선하는 프로세스에 대해 생각해 보기 어떻게 이자 적용 주기를 짧게 만들 것인가.","link":"/Programming/Agile/%E1%84%92%E1%85%A1%E1%86%B7%E1%84%81%E1%85%A6-%E1%84%8C%E1%85%A1%E1%84%85%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"Clean Code","text":"Chap01. 깨끗한 코드코드는 앞으로도 여전히 중요하다인공지능 등의 기술의 발전으로 인해 코드를 자동으로 생성하는 시대가 오기 때문에, 프로그래머는 필요가 없어질까?? 이 책에서는 그것이 불가능하다고 한다. Why⁉️ 코드는 요구사항을 상세히 표현하는 수단이기 때문! 어느정도 수준에 도달하면 코드의 도움 없이는 요구사항을 상세히 표현하기란 불가능 추상화가 불가능하기 때문 나쁜 코드나쁜 코드란 무엇인가 WTF가 분당 빈도수가 많을수록 나쁜 코드(=법규형을 얼마나 많이 찾느냐) 알아보기 힘든 코드는 어쩔 수 없이 나올 수 밖에 없다. 하지만 나쁜 코드란 “WTF”이라고 나오는 빈도가 얼마나 많이 나오느냐이다 나쁜 코드를 짜는 이유 일정이 촉박 일정 안에 새로운 기능을 완성시켜야 함 하지만 나쁜 코드는 **팀 생산성을 저하시키기 때문에 오히려 일정을 지연**시키게 만든다. 우리 모두는 자신이 처음에 나이브하게 짠 코드를 보면서 나중에 손보겠다고 생각하지만 **나중은 결코 오지 않는다**.(르블랑의 법칙) 영향 범위가 넓어서 생각보다 영향 범위가 넓어서 건드렸다가 다른 부분에 버그가 발생할까봐. 하지만 기술 부채는 부메랑처럼 우리에게로 돌아옴 Chap01 깨끗한 코드 클린코드와 그 첫걸음 네이밍 나쁜 코드 란 무엇인가 알아보기 힘든 코드는 어쩔 수 없이 나온다. 나쁜 코드란 와 이게 뭐지 라고 나오는 빈도가 얼마나 많이 나오느냐라고 나올 수 밖에 없다. 성능이 나쁜 코드 불필요한 연산이 들어가서 개선의 여지가 있는 코드 의미가 모호한 코드 이해하기 어려운 코드 네이밍과 그 내용이 다른 코드 중복된 코드 비슷한 내용인데 중복되는 코드들은 버그를 생성하고 유지보수를 어렵게 만듬 나쁜 코드가 나쁜 이유 나쁜 코드는 깨진 유리창처럼 계속 나쁜 코드가 만들어지도록 함. 생산성 저하 나쁜 코드는 기술 부채를 만들어 요구사항에 대한 수정을 더 어렵게 만들며 그 결과 팀 생산성을 저하시킴 새로운 시스템을 만들어야 함 현 시스템을 유지보수하며 대체할 새로운 시스템 개발은 현실적으로 매우 어렵다. 나쁜 코드를 짜는 이유 일정이 촉박 일정 안에 새로운 기능을 완성시켜야 함 하지만 나쁜 코드는 생산성을 저하시키기 때문에 오히려 일정을 지연시키게 만든다. 영향 범위가 넓어서 생각보다 영향 범위가 넓어서 건드렸다가 다른 부분에 버그가 발생할까봐. 하지만 기술 부채는 부메랑처럼 우리에게로 돌아온다. 그렇다면 클린 코드 란 무엇인가 나는 우아하고 효율적인 코드를 좋아한다. 논리가 간단해야 버그가 숨어들지 못한다. 의존ㄴ성을 최대한 줄여야 유지보수가 쉬워진다. 오류는 명백한 전략에 의거해 철저히 처리한다. 성능을 최적으로 유지해야 사람들이 원칙 없는 최적화로 코드를 망치려는 유혹에 빠지지 않는다. 깨끗한 코드는 한 가지를 제대로 한다 &lt;비야네 스트롭 c++ 창시자&gt; 그레디 부지(객체지향 대가) 깨끗한 코드는 단순하고 직접적이다. 깨끗한 코드는 잘 쓴 문장처럼 읽힌다. 깨끗한 코드는 결코 설계자의 의도를 숨기지 않는다. 오히려 명백한 추상화와 단순한 제어문으로 가득하다. 보이스카웃 룰 “전보다 더 깨끗한 코드로 만든다.” 클린코드 성능이 좋은 코드 의미가 명확한 코드 = 가독성이 좋은 코드 중복이 제거된 코드","link":"/Programming/Clean%20Code/Chap01.%20%EA%B9%A8%EB%81%97%ED%95%9C%20%EC%BD%94%EB%93%9C/"},{"title":"Clean Code","text":"Chap01 깨끗한 코드코드는 앞으로도 여전히 중요하다인공지능 등의 기술의 발전으로 인해 코드를 자동으로 생성하는 시대가 오기 때문에, 프로그래머는 필요가 없어질까?? 이 책에서는 그것이 불가능하다고 한다. Why⁉️ 코드는 요구사항을 상세히 표현하는 수단 이기 때문! 어느정도 수준에 도달하면 코드의 도움 없이는 요구사항을 상세히 표현하기란 불가능 추상화가 불가능하기 때문 나쁜 코드나쁜 코드란 무엇인가 WTF가 분당 빈도수가 많을수록 나쁜 코드(=법규형을 얼마나 많이 찾느냐) 알아보기 힘든 코드는 어쩔 수 없이 나올 수 밖에 없다. 하지만 나쁜 코드란 “WTF”이라고 나오는 빈도가 얼마나 많이 나오느냐이다 나쁜 코드를 짜는 이유 일정이 촉박 일정 안에 새로운 기능을 완성시켜야 함 하지만 나쁜 코드는 **팀 생산성을 저하시키기 때문에 오히려 일정을 지연**시키게 만든다. 우리 모두는 자신이 처음에 나이브하게 짠 코드를 보면서 나중에 손보겠다고 생각하지만 **나중은 결코 오지 않는다**.(르블랑의 법칙) 영향 범위가 넓어서 생각보다 영향 범위가 넓어서 건드렸다가 다른 부분에 버그가 발생할까봐. 하지만 기술 부채는 부메랑처럼 우리에게로 돌아옴 Chap01 깨끗한 코드 &amp; Chap02 의미있는 이름 클린코드와 그 첫걸음 네이밍 나쁜 코드 란 무엇인가 알아보기 힘든 코드는 어쩔 수 없이 나온다. 나쁜 코드란 와 이게 뭐지 라고 나오는 빈도가 얼마나 많이 나오느냐라고 나올 수 밖에 없다. 성능이 나쁜 코드 불필요한 연산이 들어가서 개선의 여지가 있는 코드 의미가 모호한 코드 이해하기 어려운 코드 네이밍과 그 내용이 다른 코드 중복된 코드 비슷한 내용인데 중복되는 코드들은 버그를 생성하고 유지보수를 어렵게 만듬 나쁜 코드가 나쁜 이유 나쁜 코드는 깨진 유리창처럼 계속 나쁜 코드가 만들어지도록 함. 생산성 저하 나쁜 코드는 기술 부채를 만들어 요구사항에 대한 수정을 더 어렵게 만들며 그 결과 팀 생산성을 저하시킴 새로운 시스템을 만들어야 함 현 시스템을 유지보수하며 대체할 새로운 시스템 개발은 현실적으로 매우 어렵다. 나쁜 코드를 짜는 이유 일정이 촉박 일정 안에 새로운 기능을 완성시켜야 함 하지만 나쁜 코드는 생산성을 저하시키기 때문에 오히려 일정을 지연시키게 만든다. 영향 범위가 넓어서 생각보다 영향 범위가 넓어서 건드렸다가 다른 부분에 버그가 발생할까봐. 하지만 기술 부채는 부메랑처럼 우리에게로 돌아온다. 그렇다면 클린 코드 란 무엇인가 나는 우아하고 효율적인 코드를 좋아한다. 논리가 간단해야 버그가 숨어들지 못한다. 의존ㄴ성을 최대한 줄여야 유지보수가 쉬워진다. 오류는 명백한 전략에 의거해 철저히 처리한다. 성능을 최적으로 유지해야 사람들이 원칙 없는 최적화로 코드를 망치려는 유혹에 빠지지 않는다. 깨끗한 코드는 한 가지를 제대로 한다 &lt;비야네 스트롭 c++ 창시자&gt; 그레디 부지(객체지향 대가) 깨끗한 코드는 단순하고 직접적이다. 깨끗한 코드는 잘 쓴 문장처럼 읽힌다. 깨끗한 코드는 결코 설계자의 의도를 숨기지 않는다. 오히려 명백한 추상화와 단순한 제어문으로 가득하다. 보이스카웃 룰 “전보다 더 깨끗한 코드로 만든다.” 클린코드 성능이 좋은 코드 의미가 명확한 코드 = 가독성이 좋은 코드 중복이 제거된 코드 Chap02. 의미 있는 이름 의미 있는 이름 짓기 1234567int a;String b;System.out.println(&quot;User Requested %s. count = %d&quot;, a, b);int itemCount;String itemName; 루프 속 i, j, k 사용하지 않기 배열을 순회할 때 index를 의미하는 i를 사용하지 않고 advanced for문으로 대체할 수 있음 123for(int i = 0 ; i &lt; message.size(); i++) { // ...} advanced for문 으로 대체 123for (String message: messages) { // ...} lamda 로 대체 123messages.stream().forEach( message -&gt; // ...} 최대한 의미를 찾을 수 있게 하자 i, j, k 대신 맥락에 맞는 이름이 있음 i, j → row, col /width, height i, j, k → row, col, depth 통일성 있는 단어 사용하기 Member / Customer / User : Name 팀 단위에서 통일할 필요 Service / Manager Repository / DAO 변수명에 타입 넣지 않기 123456789String nameString() -&gt; nameInt itemPriceAmount -&gt; itemPriceAccount[] accountArray() -&gt; accountsList&lt;Account&gt; accountList(괜찮음) -&gt; accounts, accountListMap&lt;Account&gt; accountMap(괜찮음)public interface IShapeFactory -&gt; ShapeFactorypublic class ShapeFactoryImpl() -&gt; CircleFactory Google Java Naming Guide Package Naming Guide All lower case, no underscores 123com.example.deeplearning(굳)com.example.deepLearning(bad)com.example.deep_space(bad) Class Naming Guide UpperCamelCase(대문자로 시작) 12345678// 클래스는 명사, 명사구Character, ImmutableList// 인터페이스는 명사, 명사구 (형용사)List, Readable// 테스트 클래스는 Test로 끝나기HashTest, HashIntegerationTest Method Naming Guide LowerCamelCase (소문자로 시작) 123456// 메소드는 동사, 동사구sendMessage, stop// JUnit 테스트에 underscore 사용되기도 함// &lt;methodUnderTest&gt;_&lt;state&gt; 패턴pop_emptyStack Chap03 함수를 안전하고 간결하게 작성하기1. SOLID 원칙 객체지향 설계의 5가지 원칙 SRP(단일 책임 원칙) : 한 클래스는 하나의 책임만 가져야 함 클래스는 하나의 기능만 가지며, 어떤 변화에 의해 클래스를 변경해야하는 이유는 오직 하나뿐이어야 한다. SRP 책임이 분명해지기 때문에, 변경에 의한 연쇄작용에서 자유로워질 수 있음. 가독성 향상과 유지보수가 용이해진다. 실전에서는 쉽지 않지만 늘 상기해야 한다. OCP(개방-폐쇄 원칙) : 소프트웨어 요소는 확장에는 열려 있으나 변경에는 닫혀 있어야 함 변경을 위한 비용은 가능한 줄이고, 확장을 위한 비용은 가능한 극대화 해야함 요구사항의 변경이나 추가사항이 발생하더라도, 기존 구성요소에는 수정이 일어나지 않고, 기존 구성 요소를 쉽게 확장해서 재사용함 객체지향의 추상화와 다형성을 활용 LSP(리스코프 치환 원칙) : 서브 타입은 언제나 기반 타입으로 교체할 수 있어야 함 서브타입은 기반 타입이 약속한 규약(접근제한자, 예외 포함)을 지켜야함 클래스 상속, 인터페이스 상속을 이용해 확장성을 획득 다형성과 확장성을 극대화하기 위해 인터페이스를 사용하는 것이 더 좋음 합성(composition)을 이요할 수도 있음 ISP(인터페이스 분리 원칙) - 자신이 사용하지 않는 인터페이스는 구현하지 말아야 함 가능한 최소한의 인터페이스만 구현 만약 어떤 클래스를 이용하는 클라이언트가 여러 개고이들이 클래스의 특정 부분만 이용한다면, 여러 인터페이스로 분류하여 클라이언트가 필요한 기능만 전달 SRP가 클래스의 단일 책임이라면ㄴ, ISP는 인터페이스의 단일 책임 DIP(의존성 역전 원칙) - 상위 모델은 하위 모델에 의존하면 안됨. 둘다 추상화에 의존해야 함. 추상화는 세부사항에 의존해서는 안됨, 세부사항은 추상화에 따라 달라짐 하위 모델의 변경이 상위 모듈의 변경을 요구하는 위계관계를 끊는다. 실제 사용관계는 그대로이지만, 추상화를 매개로 메세지를 주고 받으면서 관계를 느슨하게 만듬 2. 간결한 함수 작성하기 작게 쪼갠다. 함수 내 추상화 수준을 동일하게 맞춘다. 계산과 타입관리를 분리 타입에 대한처리는 최대한 Factory에서만 함수 인수 인수의 갯수는 0-2개가 적당하다. 3개 이상인 경우에는? 1234567//객체를 인자로 넘기기 (굳)Circle makeCirlce(double x, double y, double radius); 배드Circle makeCircle(Point center, double radius); 굳// 가변 인자를 넘기기 0&gt; 특별한 경우가 아니면 잘..String.format(String format, Object... args);//객체를 통해 인자를 넘기는 것이 가장 많이 사용됨 3. 안전한 함수 작성하기 부수효과(Side Effect) 없는 함수 부수효과? 값을 반환하는 함수가 외부 상태를 변경하는 경우 함수와 관계없는 외부 상태를 변경시킴 4. 함수 리팩터링 3단계 기능을 구현하는 서투른 함수를 작성한다. 길고, 복잡하고, 중복도 있다 테스트 코드를 작성한다 함수 내부의 분기와 엣지값마다 빠짐없이 테스트하는 코드를 짠다 리팩터링을 한다 코드를 다듬고, 함수를 쪼개고 이름을 바꾸고, 중복을 제거한다. Chap04 주석 - 코드를 보조하는 주석1. 3가지 종류의 주석12345678910 // This is a single linie comment/* * This is a regular multi-line comment *//** * This is a documentation comment and in general its called doc comment * The JDK javadoc tool uses doc comments when preparing automatically generated documentation. */ 2. 주석을 최대한 쓰지말자 주석은 나쁜 코드를 보완하지 못한다. 코드에 주석을 추가하는 일반적인 이유는 코드 품질이 나쁘기 때문 자신이 저지른 난장판을 주석으로 설명하지 말고 개선하는데 시간을 보내야 한다 코드로도 의도를 표현할 수 있다. 12345// 직원에게 복지 혜택을 받을 자격이 있는지 검사한다.if ((employee.flags &amp; Hourly_Flag) &amp;&amp; employee.age &gt; 65))// 의미있는 이름을 지으면 해결된다.if (employee.isEligibleForFullBenefits()) 주석은 방치된다 코드의 변화를 따라가지 못하고, 주석은 방치됨 코드는 컴파일되어 호출되지만, 주석은 그저 주석이기 때문에 그 자리에 방치되고 결국 의미없는 텍스트가 되어버림 3. 좋은 주석 구현에 대한 정보를 제공 1Pattern timeFormat = Patern.compile(&quot;\\\\\\\\d*:\\\\\\\\d:\\\\\\\\d* \\\\\\\\w*, \\\\\\\\w* \\\\\\\\d* \\\\\\\\d*&quot;); 의도와 중요성을 설명 1234567// 스레드를 많이 생성하여 시스템에 영향을 끼쳐 테스트를 만들도록 함for(int i = 0; i &lt; 25000; i++) { SomeThread someThread = ThreadBuilder.builder().build();}// 유저로부터 입력받을 값을 지정할 때 trim으로 공백제거 필요String userName = userNameInput.trim(); TODO, FIXME 주석 TODO : 앞으로 할 일, 지금은 해결하지 않지만 나중에 해야할 일을 미리 적어둘 때 FIXME : 문제가 있지만, 당장 수정할 필요는 없을 때, 가능하면 빨리 수정하는게 좋음 4. 주석보다 annotation annotation = 코드에 대한 메타 데이터 java.lang.annotation 코드의 실행 흐름에 간섭을 주기도 하고 주석처럼 코드에 대한 정보를 줄 수 있음 @Deprecated: 컴파일러가 warning을 발생시킴, IDE에서 사용 시 표시됨 @NotThreadSafe: Thread Safe하지 않음을 나타냄 5. JavaDoc JAVA 코드에서 API 문서를 HTML 형식으로 생성해주는 도구 Class Level 123456789101112131415161718192021/*** &lt;h1&gt;Hello, World!&lt;/h1&gt;* The HelloWorld program implements an application that* simply displays &quot;Hello World!&quot; to the standard output.* Please See the {@link com.baeldung.javadoc.Person class for true identity}* &lt;p&gt;* Giving proper comments in your program makes it more* user friendly and it is assumed as a high quality code.* ** @author Zara Ali* @version 1.0* @since 2014-03-31 */public class HelloWorld extends World { public static void main(String[] args) { // Prints Hello, World! on standard output. System.out.println(&quot;Hello World!&quot;); }} Field Level 1234/** * The unique ID of the A Universiry student */private String Student_ID Method Level 12345678910111213141516171819202122232425262728public class AddNum { /** * This method is used to add two integers. This is * a the simplest form of a class method, just to * show the usage of various javadoc Tags. * @param numA This is the first paramter to addNum method * @param numB This is the second parameter to addNum method * @return int This returns sum of numA and numB. */ public int addNum(int numA, int numB) { return numA + numB; } /** * This is the main method which makes use of addNum method. * @param args Unused. * @return Nothing. * @exception IOException On input error. * @see IOException */ public static void main(String args[]) throws IOException { AddNum obj = new AddNum(); int sum = obj.addNum(10, 20); System.out.println(&quot;Sum of 10 and 20 is :&quot; + sum); }} Java Doc으로 볼 수 있으며 IDE Reader Mode로 설명을 볼 수 있음 Chap05 형식 맞추기코드의 가독성에 필수적인 포맷팅 포맷팅이 중요한 이유 코드를 수월하게 읽어나갈 수 있는 가독성에 필수적, 의무적인 요소 오늘 작성한 코드가 리팩토링을 하면서 혹은 다음 버전에서 바뀔 확률은 아주 높음 다시 코드를 볼때 가독성이 좋아야 시간 및 정신 건강에 이롭다 포맷팅으로 인해 코드를 잘못해석해 버그를 발생할 수 있는 위험을 줄임 포맷팅 규칙 적절한 행 길이를 유지하라 작은 파일이 큰 파일보다 읽기가 편하다. 500줄을 넘기지 않고 200줄 정도의 파일을 작성해도 충분히 대규모 시스템을 구축할 수 있음 코드 길이를 200줄 정도로 제한하는 것은 반드시 지킬 엄격한 규칙은 아니지만, 일반적으로 큰 파일보다는 작은 파일이 이해하기 쉽다. 코드 길이가 200라인을 넘긴다면 SRP 원칙을 위배하고 있을 가능성이 있다 클래스가 여러 개의 일을 하고 있을 수 있기 때문 현업에서도 대부분의 코드들은 200라인 내로 유지함","link":"/Programming/Clean%20Code/Clean-Code/"},{"title":"Chap02. 의미 있는 이름","text":"Chap02. 의미 있는 이름 의미 있는 이름 짓기 1234567int a;String b;System.out.println(&quot;User Requested %s. count = %d&quot;, a, b);int itemCount;String itemName; 루프 속 i, j, k 사용하지 않기 배열을 순회할 때 index를 의미하는 i를 사용하지 않고 advanced for문으로 대체할 수 있음 123for(int i = 0 ; i &lt; message.size(); i++) { // ...} advanced for문 으로 대체 123for (String message: messages) { // ...} lamda 로 대체 123messages.stream().forEach( message -&gt; // ...} 최대한 의미를 찾을 수 있게 하자 i, j, k 대신 맥락에 맞는 이름이 있음 i, j → row, col /width, height i, j, k → row, col, depth 통일성 있는 단어 사용하기 Member / Customer / User : Name 팀 단위에서 통일할 필요 Service / Manager Repository / DAO 변수명에 타입 넣지 않기 123456789String nameString() -&gt; nameInt itemPriceAmount -&gt; itemPriceAccount[] accountArray() -&gt; accountsList&lt;Account&gt; accountList(괜찮음) -&gt; accounts, accountListMap&lt;Account&gt; accountMap(괜찮음)public interface IShapeFactory -&gt; ShapeFactorypublic class ShapeFactoryImpl() -&gt; CircleFactory Google Java Naming Guide Package Naming Guide All lower case, no underscores 123com.example.deeplearning(굳)com.example.deepLearning(bad)com.example.deep_space(bad) Class Naming Guide UpperCamelCase(대문자로 시작) 12345678// 클래스는 명사, 명사구Character, ImmutableList// 인터페이스는 명사, 명사구 (형용사)List, Readable// 테스트 클래스는 Test로 끝나기HashTest, HashIntegerationTest Method Naming Guide LowerCamelCase (소문자로 시작) 123456// 메소드는 동사, 동사구sendMessage, stop// JUnit 테스트에 underscore 사용되기도 함// &lt;methodUnderTest&gt;_&lt;state&gt; 패턴pop_emptyStack","link":"/Programming/Clean%20Code/Chap02%20%E1%84%8B%E1%85%B4%E1%84%86%E1%85%B5%E1%84%8B%E1%85%B5%E1%86%BB%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%8B%E1%85%B5%E1%84%85%E1%85%B3%E1%86%B7/"},{"title":"Python Class &amp; Method","text":"순서 Class &amp; Method Data Model Sequence First-class Functions Object Reference Concurrency 클래스 어떤 문제를 해결하기 위한 데이터를 만들기 위해 OOP 원칙에 따라 집단(현실 세계)에 속하는 속성과 행위(methods)를 변수와 메소드로 정의한 것 하단 부의 User 클래스 예시와 같이 User 라는 실제 존재를 id, name, age 등의 속성 등으로 모델링하여 purchase, refund 등의 어떠한 행위를 하는 것으로 나타낼 수 있음 클래스를 사용하기 전코드 생성/조회/수정/삭제에 코드를 구현하기 힘들다 가정 쇼핑몰을 운영하고 있으며 쇼핑몰을 이용하기 위해서는 아이디, 패스워드, 고객이름, 고객나이, 쇼핑 리스트가 필요하며 쇼핑몰을 이용하는 고객이 10,000 명이라고 하자 절차지향적 프로그래밍 → 코드 반복으로 생성/조회/수정/삭제 모두 너무 힘들다 12345678910111213141516171819202122232425262728293031# User 01user_01_id = 'id_01'user_01_name = 'name_01'user_01_passwd = 'passwd_01'user_01_age = 10user_01_shopping_list = ['mac', 'magic_mouse']# User 02user_02_id = 'id_02'user_02_name = 'name_02'user_02_passwd = 'passwd_02'user_02_age = 20user_02_shopping_list = ['imac']# User 03user_03_id = 'id_03'user_03_name = 'name_03'user_03_passwd = 'passwd_03'user_03_age = 30user_03_shopping_list = ['ipad', 'apple_pencil']...# User 10,000user_10000_id = 'id_10000'user_10000_name = 'name_10000'user_10000_passwd = 'passwd_10000'user_10000_age = 30user_10000_shopping_list = ['iphone'] 그렇다면 List 자료구조? → 수정, 삭제 여전히 어려움 123456789user_id_list = ['id_01', 'id_02', 'id_03']user_name_list = ['name_01', 'name_02', 'name_03']user_passwd_list = ['passwd_01', 'passwd_02', 'passwd_03']user_age_list = [10, 20, 30]user_shopping_list = [ ['mac', 'magic_mouse'], ['imac'], ['ipad', 'apple_pencil']] 123456789101112# user_02 수정이 필요할 경우, list의 index를 정확하게 매핑해야함user_id_list[1] = 'id_04'user_name_list[1] = 'name_04'user_passwd_list[1] = 'passwd_04'user_age_list[1] = 40user_shopping_list[1] = ['로지텍 마우스']print(user_id_list) # ['id_01', 'id_04', 'id_03']print(user_name_list) # ['name_01', 'name_04', 'name_03']print(user_passwd_list) # ['passwd_01', 'passwd_04', 'passwd_03']print(user_age_list) # [10, 40, 30]print(user_shopping_list) # [['mac', 'magic_mouse'], ['로지텍 마우스'], ['ipad', 'apple_pencil']] 123456789101112# user_02 삭제가 있을 경우, list의 index를 정확하게 매핑해야함del user_id_list[1]del user_name_list[1]del user_passwd_list[1]del user_age_list[1]del user_shopping_list[1]print(user_id_list) # ['id_01', 'id_03']print(user_name_list) # ['name_01', 'name_03']print(user_passwd_list) # ['passwd_01', 'passwd_03']print(user_age_list) # [10, 30]print(user_shopping_list) # [['mac', 'magic_mouse'], ['ipad', 'apple_pencil']] List + Dictionary 자료구조? → 코드 반복으로 중첩 문제 발생 1234567891011user_dict = [ {'user_id': 'user01', 'user_name': 'name_01'}, {'user_id': 'user02', 'user_name': 'name_02'}, {'user_id': 'user03', 'user_name': 'name_03'}]print(user_dict)# [{'user_id': 'user01', 'user_name': 'name_01'}, {'user_id': 'user02', 'user_name': 'name_02'}, {'user_id': 'user03', 'user_name': 'name_03'}]del user_dict[1]print(user_dict)# [{'user_id': 'user01', 'user_name': 'name_01'}, {'user_id': 'user03', 'user_name': 'name_03'}] 클래스를 사용 시 코드의 재사용성이 좋아짐 같은 클래스를 여러번 사용하여 객체를 생성할 수 있음 -&gt; 코드 생산성 높임 코드의 가독성이 좋아짐 클래스 내부에 포함된 데이터(멤버 변수)와 행동(메소드)을 클래스 이름을 통해 더 쉽게 이해할 수 있음, 즉 코드를 보다 직관적으로 표현 유지보수를 쉽게할 수 있음 코드를 생성/조회/수정/삭제 하기가 쉬워짐 -&gt; 코드 생산성 높임 모듈화 클래스를 사용하면 데이터와 메소드를 하나의 객체로 묶어서 사용할 수 있음 12345678910111213141516171819202122232425262728293031323334353637class User: def __init__(self, id: str, name: str, passwd: str, age: int, shopping_list:List[str]): self.__id = id self.__name = name self.__passwd = passwd self.__age = age self.__shopping_list = shopping_list def __str__(self): return f'str : {self.__id} - {self.__name}' def __repr__(self): return f'repr : {self.__id} - {self.__name}' def purchase(self, __id): # assume some codes implemented pass def refund(self, __id): # assume some codes implemented passuser_01 = User('id_01', 'name_01', 'passwd_01', 10, ['mac', 'magic_mouse'])user_02 = User('id_02', 'name_02', 'passwd_02', 20, ['imac'])user_03 = User('id_03', 'name_03', 'passwd_03', 30, ['ipad', 'apple_pencil'])print(user_01) # str : id_01 - name_01print(user_02) # str : id_02 - name_02print(user_03) # str : id_03 - name_03print(user_01.__dict__)# {'_User__id': 'id_01', '_User__name': 'name_01', '_User__passwd': 'passwd_01', '_User__age': 10, '_User__shopping_list': ['mac', 'magic_mouse']}print(user_02.__dict__)# {'_User__id': 'id_02', '_User__name': 'name_02', '_User__passwd': 'passwd_02', '_User__age': 20, '_User__shopping_list': ['imac']}print(user_03.__dict__)# {'_User__id': 'id_03', '_User__name': 'name_03', '_User__passwd': 'passwd_03', '_User__age': 30, '_User__shopping_list': ['ipad', 'apple_pencil']} 12345678910111213141516171819user_list = []user_list.append(user_01)user_list.append(user_02)user_list.append(user_03)print()print()print(user_list) # [repr : id_01 - name_01, repr : id_02 - name_02, repr : id_03 - name_03]for user in user_list: print(repr(user)) print(user)# repr : id_01 - name_01# str : id_01 - name_01# repr : id_02 - name_02# str : id_02 - name_02# repr : id_03 - name_03# str : id_03 - name_03","link":"/Python/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/Python-Class-Method/"},{"title":"Asterisk(*)를 통한 Packing과 Unpacking","text":"패킹 기능/함수의 가변 인수로써의 별표(*)별표(aterisk)는 매개변수의 변수명 앞에 한 개 또는 두 개(**)를 붙여 여러 개의 변수가 함수에 한번에 들어갈 수 있도록 처리, 이것이 가능한 이유는 별표에는 컨테이너(container, 일종의 데이터를 담는 그릇으로 여러 개의 변수를 한꺼번에 넣는 기능을 함)로서의 속성이 있기 때문이다. 가변 인수 입력받은 여러 변수들에 대해 dictionary 형태로 packing을 해줌 12345678def asterisk_args(num, *args): print(num, args) print(type(args)) asterisk_args(1, 2, 3.14, 'a', &quot;가&quot;, 5, ['도', '레','미'], {'key': 'value'})# 1 (2, 3.14, 'a', '가', 5, ['도', '레', '미'], {'key': 'value'})# &lt;class 'tuple'&gt; 키워드 가변인수 입력받은 여러 변수들에 대해 dictionary 형태로 packing을 해줌 12345678def asterisk_kargs(num, **kargs): print(num, kargs) print(type(kargs)) asterisk_kargs(1, a=1, b=2, c=3.14, d='a', e=&quot;가&quot;, f=['도', '레', '미'], g={'key':'value'})# 1 {'a': 1, 'b': 2, 'c': 3.14, 'd': 'a', 'e': '가', 'f': ['도', '레', '미'], 'g': {'key': 'value'}}# &lt;class 'dict'&gt; 혼합으로 사용 12345678910111213141516def agrs_test(name, *contents, point=None, **attrs): return '&lt;agrs_test&gt; -&gt; ({}) ({}) ({}) ({})'.format(name, contents, point, attrs)print('Ex1 -', agrs_test('test1'))print('Ex2 -', agrs_test('test1', 'test2'))print('Ex3 -', agrs_test('test1', 'test2', 'test3'))print('Ex4 -', agrs_test('test1', 'test2', 'test3', id='admin'))print('Ex5 -', agrs_test('test1', 'test2', 'test3', id='admin', point=7))print('Ex6 -', agrs_test('test1', 'test2', 'test3', id='admin', password='1234', point=7))# Ex1 - &lt;agrs_test&gt; -&gt; (test1) (()) (None) ({})# Ex2 - &lt;agrs_test&gt; -&gt; (test1) (('test2',)) (None) ({})# Ex3 - &lt;agrs_test&gt; -&gt; (test1) (('test2', 'test3')) (None) ({})# Ex4 - &lt;agrs_test&gt; -&gt; (test1) (('test2', 'test3')) (None) ({'id': 'admin'})# Ex5 - &lt;agrs_test&gt; -&gt; (test1) (('test2', 'test3')) (7) ({'id': 'admin'})# Ex6 - &lt;agrs_test&gt; -&gt; (test1) (('test2', 'test3')) (7) ({'id': 'admin', 'password': '1234'}) 언패킹 기능으로써의 별표(*)여러 개의 데이터를 담는 리스트, 튜플, 딕셔너리와 같은 자료형에서는 해당 데이터를 언패킹(unpacking)하는 기능을 함 튜플에 대한 언패킹 12345678def asterisk_tuple_unpacking(num, args): print(num, *args) print(type(args)) asterisk_tuple_unpacking(1, (2, 3.14, 'a', '가', 5, ['도', '레', '미'], {'key': 'value'}))#1 2 3.14 a 가 5 ['도', '레', '미'] {'key': 'value'}#&lt;class 'tuple'&gt; 123456789data = ([1, 2], [3, 4], [5, 6])print(*data)# [1, 2] [3, 4] [5, 6]a, b, c = ([1, 2], [3, 4], [5, 6])print(a, b, c)# [1, 2] [3, 4] [5, 6] 딕셔너리에 대한 언패킹 1234567def asterisk_dict_unpacking(a, b, c, d, e, f, g): print(a, b, c, d, e, f, g) data = {'b':2, 'c':3.14, 'd':'a', 'e':&quot;가&quot;, 'f': -5, 'g': 3}asterisk_dict_unpacking(10, **data)# 10 2 3.14 a 가 -5 3 zip() 함수와 함께 많이 사용됨 2개의 for문을 1개의 for문 만으로 원하는 결과를 얻을 수 있기 때문 12345678for data in zip(*[[1, 2], [3, 4], [5, 6]]): print(data) print(type(data)) # (1, 3, 5)# &lt;class 'tuple'&gt;# (2, 4, 6)# &lt;class 'tuple'&gt;","link":"/Python/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/Asterisk%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%20Packing%E1%84%80%E1%85%AA%20Unpacking/"},{"title":"Context Manager","text":"Context Manager 파일이나 소켓 등의 리소스 관리에 사용됨 리소스를 안전하게 열고 닫을 수 있도록 돕는 기능을 제공 ```with 123456789101112131415161718192021222324252627282930313233343536373839404142 키워드를 사용 - `with` 블록을 벗어나면 자동으로 리소스가 닫히게 되어 있음 - 따라서 예외 발생 시에도 리소스가 제대로 닫히도록 보장Context Manager는 `__enter__` 와 `__exit__` 메소드를 구현하여 사용- enter 메소드 - with 블록을 실행할 때 호출됨 - 리소스를 열고 리소스 객체를 반환- exit 메소드 - with 블록을 빠져나갈 때 호출됨 - 리소스를 안전하게 닫는 역할- 예시 - with 블록에서 FileHandler 클래스의 인스턴스를 생성 - 파일 객체를 얻어온 후, 파일에 문자열을 write 수행 - with 블록을 벗어나면 자동으로 파일이 닫힘```pythonclass FileHandler: def __init__(self, filename): self.filename = filename def __enter__(self): self.file = open(self.filename, 'w') return self.file def __exit__(self, exc_type, exc_val, exc_tb): self.file.close()with FileHandler('example.txt') as file: file.write('Hello, Python!')","link":"/Python/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/Context-Manager/"},{"title":"Python 개념","text":"파이썬의 핵심 프레임워크인 시퀀스(Sequence), 반복(Iterator), 함수(Functions), 클래스(Class)에 대해 자세히 공부를 해보도록 하자 Sequence Iterator : Generator로 파생이 되며 병행처리까지 Functions Class 클래스 Instance Variable, Class Variable self의 의미 Instance Method, Class Method, Static Mehtod 접근제한자 매직 메소드 _str_() _repr_() _del_() 데이터 모델 NampedTuple NampedTuple attrs NampedTuple method List Comprehension 구조화된 모델 설명 Sequence 1급 함수 (First-class Functions) 일급 함수 파이썬 함수 특징 런타임 초기화 가능 함수를 변수 등에 할당 가능 → 데코레이터, 클로저 사용 가능 함수를 함수의 인수로 전달 가능 sorted(keys=len) 함수 안에 함수를 전달 가능 함수를 함수 결과로 반환 가능 return funcs 함수 객체 속성 확인 Map, Filter, Reduce Map() 연속 데이터를 저장하는 Sequence 자료형에서 요소마다 같은 기능을 적용할 때 사용 list나 tuple map(함수이름, list 데이터) 의 구조를 띄움 1234list_data = [1, 2, 3, 4, 5]power_func = lambda x:x ** 2print(list(map(power_func, list_data))) 한 개 이상의 Sequence 자료형 데이터의 처리 여러 개의 seqeunce 자료형 데이터를 입력값으로 사용할 수 있음 12345list_data01 = [1, 2, 3, 4, 5]list_data02 = [1, 2, 3, 4, 5]add_func = lambda x,y: x + yprint(list(map(add_func, list_data01, list_data02))) Generator의 사용 python 3.x에서는 반드시 list(map(func, list_data))처럼 list를 붙여야 리스트로 반환 generator라는 개념이 강화되면서 생긴 코드 generator는 Sequence 자료형의 데이터를 처리할 때, 실행 시점의 값을 생성하여 효율적으로 메모리를 관리할 수 있다는 장점이 있음 python 2.x에서는 map(func, list_data) 라고만 입력해도 가능했었음 최근에는 lambda() 나 map() 함수를 프로그램 개발에 사용하는 것을 권장하지 않음 list comprehension으로 얼마든지 같은 효과를 낼 수 있기 때문 12list_data = [1, 2, 3, 4, 5][x ** 2 for x in list_data] reduce() Sequence 자료형에 차례대로 함수를 적용하여 모든 값을 통합하는 함수 123from functools import reduceprint(reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])) # 15 12345x=0for y in [1, 2, 3, 4, 5]: x += y print(x) # 15 익명함수 lambda 파이썬 3.x 버전부터는 람다 함수의 사용을 권장하지 않음 자바 같은 언어에서 람다 함수를 차용하고 스칼라와 같은 빅데이터 관련 언어에서 대중적으로 사용하지만 파이썬의 몇몇 개발자들은 람다를 직관적이지 않는다는 이유로 권장하지 않음 하지만 아직도 람다를 사용하는 개발자가 매우 많아 다양한 코드에서 확인할 수 있음 따라서 다른 함수들과 함께 많이 사용하는 기법이기에 일단한 잘 알아두자 가급적 주석 작성 가급적 함수 사용 → 일반 함수 형태로 리팩토링 권장","link":"/Python/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/Python-%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/"},{"title":"Python 개념","text":"파이썬 데이터 모델파이썬 데이터 모델이란 무엇인가 파이썬 객체가 가지는 내부 동작 및 상호작용을 정의하는 일련의 프로토콜과 규칙 객체 간 상호작용을 관리하며, 파이썬이 지원하는 많은 기능과 특징을 가능하게 함 일종의 프레임워크 파이썬 데이터 모델은 다음과 같은 프로토콜로 구성됨 시퀀스(sequence) 프로토콜 : 시퀀스 형식의 객체(리스트, 튜플, 문자열) 에 대한 연산을 정의 len() : 시퀀스의 길이를 반환 getitem(): 시퀀스의 인덱스를 기반으로 요소(index)를 반환 12345678910111213class MySequence: def __init__(self, items): self.items = items def __len__(self): return len(self.items) def __getitem__(self, index): return self.items[index]seq = MySequence([1, 2, 3, 4, 5])print(len(seq)) # 5, 만약에 __len__ 메서드가 없다면 TypeError 발생print(seq[2]) # 3, 만약에 __getitem__ 메서드가 없다면 TypeError 발생 매핑(mapping) 프로토콜: 매핑 형식의 객체(딕셔너리) 등에 대한 연산을 정의 getitem() : 매핑 객체의 키를 기반으로 값을 반환 setitem() : 매핑 객체의 키를 기반으로 값을 설정 delitem() : 매핑 객체의 키를 기반으로 값을 삭제 len() : 매핑 객체의 길이를 반환 keys, values, items 메서드는 각각 매핑 객체의 키, 값, 키-값 쌍에 대한 리스트를 반환 1234567891011121314151617181920212223242526272829class MyMapping: def __init__(self, data): self.data = data def __getitem__(self, key): return self.data[key] def __setitem__(self, key, value): self.data[key] = value def __delitem__(self, key): del self.data[key] def __len__(self): return len(self.data) def keys(self): return self.data.keys() def values(self): return self.data.values() def items(self): return self.data.items()mapping = MyMapping({'a': 1, 'b': 2, 'c': 3})print(mapping['b']) # 2mapping['d'] = 4print(mapping.items()) # dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4)]) 반복(순회) 가능한(iterable) 프로토콜 : 객체를 반복할 수 있는(iterable) 객체에 대한 연산을 정의 iter() : iterator 객체인 iter(self.items)를 반환 12345678910class MyIterable: def __init__(self, items): self.items = items def __iter__(self): return iter(self.items)iterable = MyIterable([1, 2, 3, 4, 5])for item in iterable: print(item, end=' ') # 1 2 3 4 5 반복자(nextable) 프로토콜 : 객체를 반복(iteration)할 수 있는 객체에 대한 연산을 정의 next() : 요소를 하나씩 반환하다가, 모든 요소를 반환하면 StopEteration 예외를 발생시켜 반복을 멈춤 123456789101112131415161718class MyIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index &gt;= len(self.data): raise StopIteration item = self.data[self.index] self.index += 1 return itemiterator = MyIterator([1, 2, 3, 4, 5])for item in iterator: print(item, end=' ') # 1 2 3 4 5 호출 가능한(callable) 프로토콜 : 객체를 함수처럼 호출(callable) 할 수 있는 객체에 대한 연산을 정의 call() : 객체를 함수처럼 호출할 때 실행됨 1234567class MyCallable: def __call__(self, x): return x * 2my_callable = MyCallable()result = my_callable(10)print(result) # 20 속성(attribute) 프로토콜 : 객체의 속성(attribute)을 접근할 수 있는 객체에 대한 연산을 정의 getattr() : 객체의 속성을 가져오려고 할 때 실행됨 1234567891011121314class MyClass: def __init__(self, x): self.x = x def __getattr__(self, name): if name == 'y': return self.x * 2 else: raise AttributeError(f&quot;'{self.__class__.__name__}' object has no attribute '{name}'&quot;)my_object = MyClass(10)print(my_object.x) # 10print(my_object.y) # 20print(my_object.z) # AttributeError: 'MyClass' object has no attribute 'z' 컨텍스트 관리(Context management protocol) 프로토콜 : with 문과 함께 사용되는 콘텍스트 관리 객체에 대한 연산을 정의 enter() : 컨텍스트 매니저 객체가 생성될 때 호출되어 컨텍스트 진입 시 처리를 수행하는 메서드 exit () 컨텍스트에서 빠져나올 때 호출되어 컨텍스트 종료 시 처리를 수행하는 메서드 컨텍스트 내에서 예외가 발생하거나, 정상적으로 종료되었을 때 모두 호출됨 exit ()는 세개의 인자를 받음 exc_type : 발생한 예외의 타입, 예외가 발생하지 않은 경우 None이 전달됨 exc_value : 발생한 예외의 인스턴스, 예외가 발생하지 않은 경우 None이 전달됨 traceback : 발생한 예외의 추적 정보, 예외가 발생하지 않은 경우 None이 전달됨 예외를 처리하고 예외를 처리한 경우 True를 반환, 예외를 처리하지 않은 경우 False를 반환, 반환 값은 예외 처리 이후의 실행 흐름을 결정 12345678910class MyContextManager: def __enter__(self): print(&quot;Enter context&quot;) return self def __exit__(self, exc_type, exc_val, exc_tb): print(&quot;Exit context&quot;)with MyContextManager(): print(&quot;Do something in the context&quot;) 문자열 표현(String representation) 프로토콜 : 객체를 문자열로 표현하는 방법을 정의 str() : str함수로 호출되고, 일반적으로 객체의 간단한 문자열을 반환 repr() : repr함수로 표출되고, 공식적인 문자열을 표현을 반환 123456789101112131415class Person: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return f&quot;{self.name} ({self.age})&quot; def __repr__(self): return f&quot;Person(name='{self.name}', age={self.age})&quot;person = Person(&quot;Alice&quot;, 30)print(str(person)) # 출력: Alice (30)print(repr(person)) # 출력: Person(name='Alice', age=30) 복제 가능한(copyable) 프로토콜 : 객체를 복제할 수 있는 메서드를 정의 copy() 얕은 복사(shallow copy)를 수행하는 메서드 객체의 참조값을 복사하므로, 원본 객체와 복사본 객체는 동일한 객체를 참조 따라서 원본 객체의 속성 값이 변경되면 복사본 객체의 속성 값도 변경됨 1234567891011import copya = [[1,2],[3,4]]b = copy.copy(a)a[1].append(5)print(a) # [[1, 2], [3, 4, 5]]print(b) # [[1, 2], [3, 4, 5]]print(id(a)) # 140272569851328print(id(b)) # 140272569851008 deepcopy() 깊은 복사(deep copy)를 수행하는 메서드 객체의 값을 복사하므로, 원본 객체와 복사본 객체는 서로 독립된 객체 따라서 원본 객체의 속성 값이 변경되어도 복사본 객체의 속성 값은 변경되지 않음 1234567import copya = [[1,2],[3,4]]b = copy.deepcopy(a)a[1].append(5)print(a) # [[1, 2], [3, 4, 5]]print(b) # [[1, 2], [3, 4]] 1234567891011121314151617181920212223242526import copyclass Person: def __init__(self, name, age): self.name = name self.age = age def __copy__(self): return Person(self.name, self.age) def __deepcopy__(self, memo): return Person(copy.deepcopy(self.name, memo), copy.deepcopy(self.age, memo)) def __str__(self): return f&quot;{self.name}, {self.age}&quot;person1 = Person(&quot;Alice&quot;, 30)person2 = copy.copy(person1) # 얕은 복사person3 = copy.deepcopy(person1) # 깊은 복사print(id(person1)) # 140555491712496print(person1) # Alice, 30print(id(person2)) # 140555491712880print(person2) # Alice, 30print(id(person3)) # 140555491712976print(person3) # Alice, 30 비교 가능한(comparable) 프로토콜 : 객체를 비교할 수 있는 메서드 eq(self**,** other) : self == other 를 구현, 같은 값인 경우 True를 반환 ne(self**,** other) : self != other 를 구현, 같은 값이 아닌 경우 True를 반환 lt(self**,** other) : self &lt; other 를 구현, self가 other보다 작은 경우 True를 반환 le(self**,** other) self &lt;= other 를 구현, self가 other보다 작거나 같은 경우 True를 반환 gt(self**,** other) : self &gt; other 를 구현, self가 other보다 큰 경우 True를 반환 ge(self**,** other) : self &gt;= other 를 구현, self가 other보다 크거나 같은 경우 True를 반환 123456789101112131415161718class Person: def __init__(self, name, age): self.name = name self.age = age def __eq__(self, other): return self.age == other.age def __lt__(self, other): return self.age &lt; other.agep1 = Person(&quot;John&quot;, 30)p2 = Person(&quot;Mike&quot;, 25)p3 = Person(&quot;Mary&quot;, 30)print(p1 == p2) # Falseprint(p1 == p3) # Trueprint(p2 &lt; p1) # True 해시 가능한(hashable) 프로토콜 객체를 해시 테이블의 키로 사용할 수 있는지 여부를 판단하는 프로토콜 이 프로토콜을 구현하려면 객체의 해시값(hash value)을 반환하는 hash() 메서드와 객체의 동등성(equality)을 비교하는 eq() 메서드를 구현해야함 해시 가능한 객체는 불변성(immutable)을 갖으며 같은 값의 객체는 항상 같은 해시값을 갖도록 구현되어야 함 해시 가능한 객체는 dictionary와 set에서 키로 사용될 수 있음 hash() : 객체의 해시값(hash value)을 반환 eq() : 메서드와 객체의 동등성(equality)을 비교 1234567891011121314151617181920212223class Person: def __init__(self, name, age): self.name = name self.age = age def __eq__(self, other): if not isinstance(other, Person): return False return self.name == other.name and self.age == other.age def __hash__(self): return hash((self.name, self.age))person1 = Person(&quot;Alice&quot;, 30)person2 = Person(&quot;Bob&quot;, 25)person3 = Person(&quot;Alice&quot;, 30)d = {person1: &quot;Alice is 30&quot;, person2: &quot;Bob is 25&quot;}print(d[person1]) # &quot;Alice is 30&quot;print(d[person3]) # &quot;Alice is 30&quot;# person1과 person3은 이름과 나이가 동일하므로, 동일한 해시값을 반환# 따라서 d[person1]과 d[person3]는 동일한 값인 &quot;Alice is 30&quot;을 반환 해시 가능한(hashable) 프로토콜 시퀀스 프로토콜과 같은 기능을 수행하지만, 인덱스를 통해 객체에 접근하는 방식을 강제 getitem() : 인덱스로 객체에 접근할 수 있도록 함 123456789class IndexableExample: def __init__(self, data): self.data = data def __getitem__(self, index): return self.data[index]example = IndexableExample([1, 2, 3, 4, 5])print(example[2]) # 3 출력 슬라이싱 가능한(sliceable) 프로토콜 : 객체를 슬라이싱(slicing)할 수 있는 메서드(시퀀스 객체의 일부를 추출할 때 사용됨) getitem () 이 메소드는 슬라이싱 연산자에 의해 호출됨 시작 인덱스(start index), 끝 인덱스(end index), 그리고 간격(step)을 인자로 받아 시퀀스 객체의 일부를 추출하여 반환 1234567891011121314151617class MyList: def __init__(self, items): self.items = items def __getitem__(self, index): if isinstance(index, slice): start, stop, step = index.indices(len(self.items)) return [self.items[i] for i in range(start, stop, step)] else: return self.items[index] def __len__(self): return len(self.items)lst = MyList([1, 2, 3, 4, 5, 6])print(lst[1:4]) # [2, 3, 4]print(lst[:4:2]) # [1, 3] empty 프로토콜 : 객체가 비어 있는지 여부를 알 수 있음 bool() 또는 len() 메소드를 구현해야함 bool() : 객체가 비어 있으면 False를 반환하고, 그렇지 않으면 True를 반환 12345678910111213141516class MyList: def __init__(self, data): self.data = data def __bool__(self): return bool(self.data)# 빈 MyList 객체 생성my_list_empty = MyList([])# MyList 객체 생성my_list = MyList([1, 2, 3])# MyList 객체가 참(True)인지 아니면 거짓(False)인지 출력print(bool(my_list_empty)) # Falseprint(bool(my_list)) # True len() : 객체의 길이(크기)를 반환, 객체가 비어 있으면 0을 반환 , 그렇지 않으면 객체의 길이를 반환 123456789class MyList: def __init__(self, data): self.data = data def __len__(self): return len(self.data)my_list = MyList([1, 2, 3])print(len(my_list)) # 출력결과: 3 길이(length) 프로토콜 : 객체의 길이를 반환 len() : 리스트의 길이를 반환 123456789class MyList: def __init__(self, data): self.data = data def __len__(self): return len(self.data)my_list = MyList([1, 2, 3])print(len(my_list)) # 출력결과: 3 존재성(test for membership) 프로토콜 : in 연산자를 사용하여 객체 내에 특정 값이 존재하는지 확인하는 데 사용됨 contains() : 인자로 전달된 값이 객체 내에 존재하는지 여부를 불리언 값으로 반환 12345678910class MyList: def __init__(self, data): self.data = data def __contains__(self, value): return value in self.datalst = MyList([1, 2, 3, 4, 5])print(3 in lst) # Trueprint(6 in lst) # False 산술 연산 프로토콜 : 산술 연산(덧셈, 뺄셈, 곱셈, 나눗셈 등)할 수 있도록 정의하는 프로토콜 add() sub() mul() truediv() 1234567891011121314151617181920212223class Vector: def __init__(self, x, y): self.x = x self.y = y def __add__(self, other): return Vector(self.x + other.x, self.y + other.y) def __sub__(self, other): return Vector(self.x - other.x, self.y - other.y) def __mul__(self, scalar): return Vector(self.x * scalar, self.y * scalar) def __truediv__(self, scalar): # 스칼라 값으로 Vector 객체를 나눈 결과 return Vector(self.x / scalar, self.y / scalar)v1 = Vector(1, 2)v2 = Vector(3, 4)v3 = v1 + v2print(v3.x, v3.y) # 출력: 4 6 비트 연산 프로토콜 : 객체 간 비트 연산을 정의하는 프로토콜 비트 연산자(&amp;, |, ^, ~, &lt;&lt;, &gt;&gt;)를 사용하여 비트 연산을 수행 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class Bitwise: def __init__(self, x): self.x = x def __and__(self, other): return Bitwise(self.x &amp; other.x) def __or__(self, other): return Bitwise(self.x | other.x) def __xor__(self, other): return Bitwise(self.x ^ other.x) def __invert__(self): return Bitwise(~self.x) def __lshift__(self, other): return Bitwise(self.x &lt;&lt; other) def __rshift__(self, other): return Bitwise(self.x &gt;&gt; other) def __str__(self): return str(self.x) a = Bitwise(0b0101)b = Bitwise(0b1010)print(a &amp; b) # 0b0000print(a | b) # 0b1111print(a ^ b) # 0b1111print(~a) # -0b0110print(a &lt;&lt; 2) # 0b010100print(b &gt;&gt; 1) # 0b0101class BitArray: def __init__(self, bits): self.bits = bits def __and__(self, other): result = [a &amp; b for a, b in zip(self.bits, other.bits)] return BitArray(result) def __or__(self, other): result = [a | b for a, b in zip(self.bits, other.bits)] return BitArray(result) def __xor__(self, other): result = [a ^ b for a, b in zip(self.bits, other.bits)] return BitArray(result) def __invert__(self): result = [~a for a in self.bits] return BitArray(result) def __lshift__(self, n): result = self.bits[n:] + [0] * n return BitArray(result) def __rshift__(self, n): result = [0] * n + self.bits[:-n] return BitArray(result)a = BitArray([1, 0, 1, 0])b = BitArray([1, 1, 0, 0])print(a &amp; b) # BitArray([1, 0, 0, 0])print(a | b) # BitArray([1, 1, 1, 0])print(a ^ b) # BitArray([0, 1, 1, 0])print(~a) # BitArray([-2, -1, -6, -1])print(a &lt;&lt; 2) # BitArray([1, 0, 1, 0])print(a &gt;&gt; 2) # BitArray([0, 0, 1, 0]) 매직 메서드매직 메서드란 파이썬 데이터 모델에서 프로토콜을 구현하기 위한 특별한 메서드 앞서 보았던 프로토콜로 포함이 되는 __로 시작하고 __로 끝나는 이름을 갖음(ex, len) 객체 간의 상호작용을 정의하고 객체를 기능적으로 확장하는 데 사용됨 매직 메서드는 어떻게 사용하나 my_object.len() 으로 직접 사용하지 않고 len(my_object) 형태로 호출 list, str, bytearray 등과 같은 내장 자료형의 경우 파이썬 인터프리터는 손 쉬운 방법을 선택 CPython의 경우 len() 메서드는 메모리에 있는 모든 가변 크기 내장 객체를 나타내는 PyVarObject C 구조체의 ob_size 필드의 값을 반환(이 방법이 메서드를 호출하는 방법보다 빠름) 일반적으로 사용자 코드에서 특별 메서드를 직접 호출하는 경우는 그리 많지 않음 사용자 코드에서 특별 메서드를 자주 호출하는 경우는 init() 메서드가 유일 특별 메서드를 호출해야 하는 경우에는 일반적으로 len(), iter(), str() 등 관련된 내장 함수를 호출하는 것이 좋음 →내장 데이터형의 경우 특별 메서드를 호출하지 않는 경우도 있으며 메서드 호출보다 빠름 매직 메서드의 장점 파이썬에서 매우 유용하게 사용되며 객체 지향 프로그래밍의 일관성을 유지하는 데 도움이 됨 내장 함수나 연산자가 사용되는 상황에서 클래스 객체에 대한 사용자 정의 동작을 정의할 수 있음 123456789101112131415# __add__ 매직 메서드를 사용하여 두 객체를 더하는 동작을 사용자 정의class MyNumber: def __init__(self, number): self.number = number def __add__(self, other): return MyNumber(self.number + other.number) def __str__(self): return str(self.number)num1 = MyNumber(5)num2 = MyNumber(10)print(num1 + num2) # 출력: 15 매직 메서드를 구현하면 이를 활용한 기능 구현을 라이브러리 형태로 공유할 수 있음 len() : len for 루프 등 : iter, next 파이썬 표준 라이브러리에서 제공하는 풍부한 기능을 별도로 구현할 필요 없이 바로 사용 random의 choice(), reversed(), sorted() 함수 등 리스트와 같은 시퀀스 자료형 1234567891011121314151617181920212223242526272829303132333435# __len__ 매직 메서드를 구현하여 len() 함수를 사용할 수 있음my_list = [1, 2, 3]print(len(my_list)) # 출력: 3# __getitem__ 매직 메서드를 구현하여 인덱싱을 사용할 수 있음print(my_list[0]) # 출력: 1# __setitem__ 매직 메서드를 구현하여 인덱스를 이용한 값 변경이 가능my_list[0] = 4print(my_list) # 출력: [4, 2, 3]# __delitem__ 매직 메서드를 구현하여 인덱스를 이용한 요소 삭제가 가능del my_list[0]print(my_list) # 출력: [2, 3]# __iter__ 매직 메서드를 구현하여 반복문을 사용할 수 있음for item in my_list: print(item)# __contains__ 매직 메서드를 구현하여 in 연산자를 사용할 수 있음print(2 in my_list) # 출력: True# sort() 함수를 사용하여 리스트를 정렬할 수 있음my_list.sort()print(my_list) # 출력: [2, 3]# reverse() 함수를 사용하여 리스트를 뒤집을 수 있음my_list.reverse()print(my_list) # 출력: [3, 2]# count() 함수를 사용하여 리스트에서 특정 값의 개수를 세어줄 수 있음print(my_list.count(2)) # 출력: 1# index() 함수를 사용하여 리스트에서 특정 값의 인덱스를 찾을 수 있음print(my_list.index(2)) # 출력: 1 내장 함수(Built-in function) 파이썬에서 이미 정의되어 있는 함수로, 별도의 정의 없이 바로 사용 가능 print(), len(), type(), range(), sum(), min(), max() 등이 내장 함수 1234567891011121314151617181920212223# print()print(&quot;Hello, world!&quot;) # Hello, world!# len()my_list = [1, 2, 3, 4, 5]print(len(my_list)) # 5# type()my_var = 10print(type(my_var)) # &lt;class 'int'&gt;# range()my_range = range(10)for i in my_range: print(i) # 0 1 2 3 4 5 6 7 8 9# max()my_numbers = [4, 8, 2, 6, 10]print(max(my_numbers)) # 10# sum()my_numbers = [1, 2, 3, 4, 5]print(sum(my_numbers)) # 15 매직 메서드와 다름 매직 메소드와 내장 함수의 다른점 매직 메소드는 클래스에서 정의된 특별한 이름을 가진 메서드fh 해당 클래스의 객체에 대해 호출될 때 자동으로 호출되어 객체의 동작을 정의 매직 메소드는 클래스의 일부이며, 해당 클래스에서 정의된 메서드이기에 내장 함수와는 별개의 개념 결론 파이썬 데이터 모델은 프로토콜을 구현함으로써 객체 간의 상호 작용을 쉽게 구현할 수 있게 함 프로토콜은 인터페이스를 정의하는데 사용되는 개념이며, 매직 메서드는 이러한 인터페이스를 구현하는 데 사용되는 메서드 리스트와 튜플은 시퀀스 프로토콜을 구현하므로, 인덱싱, 슬라이싱, 정렬 등과 같은 연산을 쉽게 수행할 수 있음 객체의 속성을 직접 접근할 수 있는 속성 접근 프로토콜을 구현함으로써, 객체 간의 데이터 공유 및 상호작용을 구현할 수 있음 파이썬 데이터 모델 일종의 프레임워크로써, 파이썬을 설명하는 것 시퀀스, 반복자, 함수, 클래스, 콘텍스트 관리자 등 언어 자체의 구성단위에 대한 인터페이스를 공식적으로 정의 파이썬 데이터 모델이 제공하는 API를 이용해서 커스텀 객체를 정의하면 대부분의 파이썬 상용구를 적용할 수 있음 매직 메소드는 custom 객체를 구현할 때 다음과 같은 기본적인 언어 구조체를 구현하고 지원하고 함께 사용할 수 있게 해줌 반복 컬렉션 속성 접근 연산자 오버로딩 함수 및 메소드 호출 객체 생성 및 제거 문자열 표현 및 포맷 블록 등 콘텍스트 관리 스페셜 메소드의 장점 클래스 장체에서 구현한 임의 메서드 명을 암기할 필요가 없음 len() 등 자바에서는 length(), size() 등 헷갈리.. 파이썬 표준 라이브러리에서 제공하는 풍부한 기능을 별도로 구현할 필요 없이 바로 사용 파이썬의 핵심 프레임워크인 시퀀스(Sequence), 반복(Iterator), 함수(Functions), 클래스(Class)에 대해 자세히 공부를 해보도록 하자 Sequence Iterator : Generator로 파생이 되며 병행처리까지 Functions Class 클래스 Instance Variable, Class Variable self의 의미 Instance Method, Class Method, Static Mehtod 접근제한자 매직 메소드 _str_() _repr_() _del_() 데이터 모델 NampedTuple NampedTuple attrs NampedTuple method List Comprehension 구조화된 모델 설명 Sequence 1급 함수 (First-class Functions) 파이썬 데이터 모델 데이터 모델 시퀀스 반복자 함수 클래스 콘텍스트 관리자 일급 함수 파이썬 함수 특징 런타임 초기화 가능 함수를 변수 등에 할당 가능 → 데코레이터, 클로저 사용 가능 함수를 함수의 인수로 전달 가능 sorted(keys=len) 함수 안에 함수를 전달 가능 함수를 함수 결과로 반환 가능 return funcs 함수 객체 속성 확인 Map, Filter, Reduce Map() 연속 데이터를 저장하는 Sequence 자료형에서 요소마다 같은 기능을 적용할 때 사용 list나 tuple map(함수이름, list 데이터) 의 구조를 띄움 1234list_data = [1, 2, 3, 4, 5]power_func = lambda x:x ** 2print(list(map(power_func, list_data))) 한 개 이상의 Sequence 자료형 데이터의 처리 여러 개의 seqeunce 자료형 데이터를 입력값으로 사용할 수 있음 12345list_data01 = [1, 2, 3, 4, 5]list_data02 = [1, 2, 3, 4, 5]add_func = lambda x,y: x + yprint(list(map(add_func, list_data01, list_data02))) Generator의 사용 python 3.x에서는 반드시 list(map(func, list_data))처럼 list를 붙여야 리스트로 반환 generator라는 개념이 강화되면서 생긴 코드 generator는 Sequence 자료형의 데이터를 처리할 때, 실행 시점의 값을 생성하여 효율적으로 메모리를 관리할 수 있다는 장점이 있음 python 2.x에서는 map(func, list_data) 라고만 입력해도 가능했었음 최근에는 lambda() 나 map() 함수를 프로그램 개발에 사용하는 것을 권장하지 않음 list comprehension으로 얼마든지 같은 효과를 낼 수 있기 때문 12list_data = [1, 2, 3, 4, 5][x ** 2 for x in list_data] reduce() Sequence 자료형에 차례대로 함수를 적용하여 모든 값을 통합하는 함수 123from functools import reduceprint(reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])) # 15 12345x=0for y in [1, 2, 3, 4, 5]: x += y print(x) # 15 익명함수 lambda 파이썬 3.x 버전부터는 람다 함수의 사용을 권장하지 않음 자바 같은 언어에서 람다 함수를 차용하고 스칼라와 같은 빅데이터 관련 언어에서 대중적으로 사용하지만 파이썬의 몇몇 개발자들은 람다를 직관적이지 않는다는 이유로 권장하지 않음 하지만 아직도 람다를 사용하는 개발자가 매우 많아 다양한 코드에서 확인할 수 있음 따라서 다른 함수들과 함께 많이 사용하는 기법이기에 일단한 잘 알아두자 가급적 주석 작성 가급적 함수 사용 → 일반 함수 형태로 리팩토링 권장 클로저 Generator 패킹 기능/함수의 가변 인수로써의 별표(*)별표(aterisk)는 매개변수의 변수명 앞에 한 개 또는 두 개(**)를 붙여 여러 개의 변수가 함수에 한번에 들어갈 수 있도록 처리, 이것이 가능한 이유는 별표에는 컨테이너(container, 일종의 데이터를 담는 그릇으로 여러 개의 변수를 한꺼번에 넣는 기능을 함)로서의 속성이 있기 때문이다. 가변 인수 입력받은 여러 변수들에 대해 dictionary 형태로 packing을 해줌 12345678def asterisk_args(num, *args): print(num, args) print(type(args)) asterisk_args(1, 2, 3.14, 'a', &quot;가&quot;, 5, ['도', '레','미'], {'key': 'value'})# 1 (2, 3.14, 'a', '가', 5, ['도', '레', '미'], {'key': 'value'})# &lt;class 'tuple'&gt; 키워드 가변인수 입력받은 여러 변수들에 대해 dictionary 형태로 packing을 해줌 12345678def asterisk_kargs(num, **kargs): print(num, kargs) print(type(kargs)) asterisk_kargs(1, a=1, b=2, c=3.14, d='a', e=&quot;가&quot;, f=['도', '레', '미'], g={'key':'value'})# 1 {'a': 1, 'b': 2, 'c': 3.14, 'd': 'a', 'e': '가', 'f': ['도', '레', '미'], 'g': {'key': 'value'}}# &lt;class 'dict'&gt; 혼합으로 사용","link":"/Python/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8A%E1%85%A5%E1%86%AB%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF/"},{"title":"UML &amp; Class Diagram","text":"UML 이란 UML(Unified Modeling Language)은 시스템의 구조와 설계를 시각화하는 데 사용되는 표준화된 모델링 언어 추상적인 개념을 시각적 형태로 표현함으로써 복잡한 시스템의 설계와 분석을 용이하게 하는 중요한 도구 다음과 같은 종류가 있음 클래스 다이어그램(Class Diagrams): 시스템 내의 클래스들과 그들 사이의 관계를 보여줌 사용 사례 다이어그램(Use Case Diagrams): 시스템이 사용자와 상호 작용하는 방식을 설명 시퀀스 다이어그램(Sequence Diagrams): 객체 간의 상호 작용과 시간에 따른 이들의 관계를 나타냄 상태 다이어그램(State Diagrams): 객체의 상태 변화를 보여줌 활동 다이어그램(Activity Diagrams): 워크플로우나 비즈니스 프로세스의 흐름을 나타냄 구성 다이어그램(Component Diagrams): 시스템의 물리적 구성 요소를 나타냄 배치 다이어그램(Deployment Diagrams): 시스템의 물리적 배치와 인프라를 나타냄 UML 다이어그램은 구조 다이어그램과 행위 다이어그램으로 나뉨 구조 다이어그램(Structure Diagram) 시스템의 정적인 측면, 즉 시스템의 물리적 구성 요소와 그들 간의 관계를 나타냄 시스템의 구조를 보여주며, 주요 구성 요소들이 어떻게 서로 연결되어 있는지를 설명 종류 클래스 다이어그램(Class Diagram): 시스템의 클래스들과 그들 사이의 관계를 보여줌 객체 다이어그램(Object Diagram): 특정 시점에서 클래스 다이어그램의 인스턴스를 보여줌 패키지 다이어그램(Package Diagram): 시스템의 패키지들과 그들 간의 의존성을 나타냄 구성 다이어그램(Component Diagram): 시스템의 소프트웨어 구성 요소들을 보여줌. 배치 다이어그램(Deployment Diagram): 시스템의 물리적 구조와 소프트웨어 구성 요소들이 배치되는 방법을 나타냄 행위 다이어그램(Behavior Diagram) 시스템의 동적인 측면, 시스템이 어떻게 동작하고, 구성 요소들이 시간에 따라 어떻게 상호 작용하는지를 보여줌 시스템의 행동, 프로세스, 알고리즘 등을 시각화 종류 사용 사례 다이어그램(Use Case Diagram): 시스템의 기능과 외부 요소(사용자, 외부 시스템 등)와의 상호 작용을 나타냄 시퀀스 다이어그램(Sequence Diagram): 객체 간의 상호 작용과 이들이 메시지를 교환하는 순서를 보여줌 활동 다이어그램(Activity Diagram): 시스템의 프로세스 흐름이나 워크플로우를 나타냄 상태 다이어그램(State Diagram): 객체의 상태 변화와 그에 따른 이벤트 처리를 나타냄. 커뮤니케이션 다이어그램(Communication Diagram): 시퀀스 다이어그램과 유사하지만, 객체 간의 연결과 메시지 흐름에 더 집중 왜 UML을 알아야 하나요?🤔 **다른 사람들과의 협업에 필요(**의미가 명확하고 설계에 대한 논의가 명확해짐) 다양한 다이어그램을 통해 개발자들이 소프트웨어의 구조를 더 잘 이해할 수 있음 클라이언트나 이해관계자들에게 시스템의 작동 방식을 명확하게 전달할 수 있음 전체 시스템의 구조와 클래스의 의존성을 파악 다이어그램을 분석하면, 시스템의 구조는 물론 클래스 간의 의존성도 파악이 쉬워짐 유지보수를 위한 백엔드 문서로 사용 시스템 구조가 복잡할 경우 UML을 통해 시스템의 설계 및 구조를 복기 및 참고할 수 있음 구조적으로 효율적이지 않거나, 모듈화 또는 구체화 해야하는 작업이 필요할 경우 UML을 먼저 참고 및 작성해보고 구조를 수정할 수 있음 Class Diagram 클래스 다이어그램은 시스템 내의 클래스들, 그들 사이의 관계 그리고 각 클래스가 가지는 속성과 메서드를 시각적으로 표현 클래스 다이어그램의 구성 요소 클래스 클래스는 속성(attributes)과 메서드(methods)를 가진 사각형으로 표시됨 상단에는 클래스 이름이, 중간에는 속성이, 하단에는 메서드가 위치 클래스의 세부 사항은 필드와 메서드의 1접근 제한자(Access Modifier) , 1속성(메서드)명 , 1데이터 타입 , 1매개변수(Parameter) , 1리턴 타입 등을 나타낼 수 있음 접근 제한자 (Access modifier) + : public - : private # : protected 속성 클래스의 특성이나 상태를 나타냄 예를 들어, ‘Customer’ 클래스에는 id, name, age등의 속성이 있을 수 있음 메서드 클래스가 수행할 수 있는 동작이나 기능을 나타냄 예를 들어, ‘Customer’ 클래스는 order, addToCart등의 메서드를 가질 수 있음 https://github.com/shchoice/shchoice.github.io/assets/100276387/3505725c-f003-4952-8fa6-dd76f6487bd6 12345678910111213141516import lombok.Data;@Datapublic class Customer { private int id; private String name; private int age; public void order() { // 주문 관련 로직 } public Boolean addToCart() { // 장바구니 추가 관련 로직 }} 관계 클래스 간의 다양한 관계를 표현 일반화(Generalization) 한 클래스(자식 클래스)가 다른 클래스(부모 클래스)의 모든 특성을 상속받는 관계 부모 클래스는 자식 클래스를 일반화(Generalize)한 것이고, 자식 클래스는 부모 클래스를 구체화(Specialize)한 것 클래스 간의 is-a 관계를 표현 빈 삼각형 화살표로 표시되며, 상위 클래스 쪽으로 화살표가 향함 https://github.com/shchoice/shchoice.github.io/assets/100276387/aa7fa828-cb86-4bda-aba3-3a2a155d121d 123456789101112131415161718192021222324252627public class Person{ private String id; private String name; private int age; public void changePassward(String password) { // 패스워드 변경 로직 } public void changeName(String name) { // 계정 이름 변경 }}public class Customer extends Person { public void order() { // 주문 관련 로직 } public Boolean addToCart(Item item) // 장바구니 추가 관련 로직 }public class Manager extends Person { public void manageAccount() { // 계정 관리 로직 }} 실현 관계 (Realization) 주로 인터페이스와 그 인터페이스를 구현하는 클래스 사이의 관계를 나타냄 클래스가 인터페이스의 모든 추상 메소드를 구현해야함 점선과 빈 삼각형 화살표로 표시되며, 인터페이스 쪽으로 화살표가 향함 https://github.com/shchoice/shchoice.github.io/assets/100276387/ad999ae5-e731-4f91-a267-94df97444102 12345678910public interface Vehicle { void drive();}public class Car implements Vehicle { @Override public void drive() { System.out.println(&quot;The car is driving.&quot;); }} 의존 관계 (Dependency) 한 클래스가 다른 클래스를 참조(의존)하는 관계를 나타냄 메서드 내 대상 클래스 객체 생성 메서드 내 대상 클래스 객체 사용 메서드 내 대상 클래스 메서드 호출 메서드 내 대상 클래스 객체 리턴 메서드에서 대상 클래스 객체를 매개변수로 받는 것 객체 참조는 계속 유지되지 않고 메서드의 호출이 끝나면 의존 관계의 클래스와 관계가 끝남 점선 화살표로 표시되며, 의존 받는 클래스 쪽으로 화살표가 발생 스트레오 타입으로 어떤 목적의 Dependency인지 의미를 명시할 수 있음 https://github.com/shchoice/shchoice.github.io/assets/100276387/f2bf3bba-b692-42a5-ad5a-2fb7c84a7fa7 1234567891011121314151617public class Document { private String content; public Document(String content) { this.content = content; } public String getContent() { return content; }}public class Printer { public void print(Document doc) { System.out.println(doc.getContent()); }} 1234567891011public class Pen { public void write(String text) { }}public class Writer { public void writeSomething() { Pen pen = new Pen(); pen.write(&quot;Hello, World!&quot;); }} 연관 관계(Association) 두 클래스 간의 일반적인 관계를 나타냄 양방향이거나, 단방향 일수 있음 선으로 표현되면, 종종 화살표로 방향성을 나타냄 일반적인 Association 실선 하나로 클래스를 연결 방향이 없으므로 Engine 클래스가 Car 클래스를 참조할 수도 Car가 Engine을 참조할 수도, 둘 다일 수도 있음 Directed Association 클래스를 실선으로 연결 후 끝에 화살표를 추가 화살표가 향하는 쪽으로 참조하는 것을 의미함 다중성(multiplicity)을 표시하여 관계의 성격(예: 1:1, 1:*, n:m)을 명확히 할 수 있음 https://github.com/shchoice/shchoice.github.io/assets/100276387/96e5c294-df54-4c77-ab97-b58568f3b1a6 123456789101112131415public class Engine { public void start() { }}public class Car { private Engine engine; public Car() { engine = new Engine(); } public void startCar() { engine.start(); } 집합 관계(Aggregation) Association을 조금더 특수하게 나타낸 것으로 한 클래스가 다른 클래스의 집합을 의미하며, ‘전체-부분’ 관계를 나타냄 Collection 이나 Array를 이용한 집합 관계 하나의 클래스가 다른 클래스의 컬렉션을 포함하지만, 생명주기가 독립적일 때 사용됨 빈 다이아몬드로 표시되는 선으로 나타내며, 전체 클래스 쪽에 빈 다이아몬드가 위치 https://github.com/shchoice/shchoice.github.io/assets/100276387/e68303ba-dd15-482d-9da9-a2feda818519 123456789101112131415161718192021@AllArgsConstructor@Getter@Setterpublic class Employee { private String name;}@Getter@Setterpublic class Company { private List&lt;Employee&gt; employees; public Company() { this.employees = new ArrayList&lt;&gt;(); } public void addEmployee(Employee employee) { // 집합 관계에서는 외부에서 생성된 Employee 객체를 추가 employees.add(employee); }} 합성 관계(Composition): 강한 형태의 집합 관계로, ‘전체-부분’ 관계 중에서도 부분이 전체에 의존하는 경우를 나타냄 부분 클래스의 생명주기가 전체 클래스에 종속적 whole 인스턴스가 part 인스턴스를 생성 whole 인스턴스가 소멸하면 part 인스턴스도 함께 소멸 whole 인스턴스가 복사되면 part 인스턴스도 함께 복사 Composition을 사용할 때 개발자가 구현해야할 내용 part 인스턴스의 공유 방지를 위한 Deep Copy 를 구현. whole 인스턴스가 part 인스턴스의 수명을 책임져야 하므로 whole 클래스의 생성자 또는 기타 메소드 내에서 part 인스턴스를 생성 외부에서 part 객체를 생성하지 못하도록 whole 클래스에는 part 인스턴스에 대한 setter를 삭제 자바는 Garbage Collector 가 객체 소멸을 담당하므로 part 인스턴스의 소멸은 신경쓰지 않아도 됨 채워진 다이아몬드로 표시되는 선으로 나타내며, 전체 클래스 쪽에 채워진 다이아몬드가 위치 https://github.com/shchoice/shchoice.github.io/assets/100276387/c18323b3-ef95-4721-b5be-bd418de22d40 1234567891011121314151617181920@AllArgsConstructor@Getter@Setterpublic class Employee { private String name;}@Getterpublic class Company { private List&lt;Employee&gt; employees; public Company(String name) { this.name = name; this.employees = new ArrayList&lt;&gt;(); } public void addEmployee(Employee employee) { employees.add(employee); }} Streo Type UML에서 streo type은 클래스 다이어그램을 비롯한 UML 다이어그램에서 사용되는 확장 메커니즘의 한 형태 기존 UML 요소에 추가적인 의미를 부여하거나 새로운 속성을 제공하여, 모델의 정확성과 표현력을 향상시키는 데 사용됨 abstract, enumerate, interface 등의 키워드에 적용이 가능하며, controller, service 등 표현력을 강화하기 위해 사용이 되기도 함 길러멧 기호 («») 사이에 작성을 함 1234567891011+-------------------+ +---------------------+ +--------------------------------+| &lt;&lt;interface&gt;&gt; | | &lt;&lt;abstract&gt;&gt; | | &lt;&lt;service&gt;&gt; || CustomerActions | | AbstractCustomer | | CustomerService ||-------------------| |---------------------| |--------------------------------|| + order(): void | | # id: int | | - customerDAO: DAO || + addToCart(): void | | # name: String | |--------------------------------|+-------------------+ | # age: int | | + getCustomer(): Customer | |---------------------| | + saveCustomer(Customer): void | | + order(): void | +--------------------------------+ | + addToCart(): void | +---------------------+ 궁금증 Association vs Composition 연관관계 예시 한 대학에 여러 교수가 있을 수 있으며, 이 교수들은 대학과 연관되어 있지만, 교수가 대학을 떠날 때 대학이 사라지지는 않음 123456789101112131415161718192021222324class University { private String name; public University(String name) { this.name = name; }}class Professor { private String name; private University university; // 연관 관계 public Professor(String name, University university) { this.name = name; this.university = university; }}public class Main { public static void main(String[] args) { University uni = new University(&quot;A University&quot;); Professor prof = new Professor(&quot;RAON CHOI&quot;, uni); }} 합성 관계 예시 자동차와 엔진의 관계를 들 수 있습니다. 자동차가 없어지면, 그 자동차의 엔진도 더 이상 존재하지 않게됨 123456789101112131415161718192021222324class Engine { public void start() { System.out.println(&quot;Engine starts.&quot;); }}class Car { private Engine engine; // 합성 관계 public Car() { this.engine = new Engine(); } public void startCar() { engine.start(); }}public class Main { public static void main(String[] args) { Car car = new Car(); car.startCar(); }} 특징 비교 연관 관계 : 연관 관계는 두 클래스가 서로 독립적인 생명주기를 가지며, 느슨하게 연결되어 있을 때 사용 독립적인 생명주기: 연관된 객체들이 서로 독립적으로 생성되고 소멸할 수 있을 때 사용, 예를 들어, 여러 Controller 클래스가 하나의 Service 클래스의 인스턴스를 공유하는 경우에 적합 재사용성과 공유: 하나의 객체가 여러 곳에서 사용되거나 공유되어야 할 필요가 있을 때 연관 관계가 유용. 이는 코드의 재사용성을 높이고 중복을 줄이는 데 도움이 됨 변경의 유연성: 연관된 객체 중 하나를 변경하거나 교체해야 할 경우, 연관 관계는 이러한 변경을 용이. 객체 간의 결합도가 낮기 때문에, 한 객체의 변경이 다른 객체에 미치는 영향이 적음 합성 관계 : 한 클래스가 다른 클래스의 객체를 포함하며, 포함된 객체의 생명주기가 포함하는 객체에 의존적일 때 사용 강한 소유권과 생명주기의 의존성: 합성 관계에서는 포함하는 객체가 생성될 때 포함된 객체도 함께 생성되고, 포함하는 객체가 소멸될 때 포함된 객체도 함께 소멸. 이는 “부품과 전체” 관계에 적합 단일 책임 원칙: 포함된 객체가 특정 클래스에 대해 단일 책임을 갖고 있을 때 사용됨. 예를 들어, 특정 클래스가 자신의 로직을 수행하는 데 필요한 여러 하위 컴포넌트를 포함하고 있을 때 적합. 독점적 사용: 포함된 객체가 포함하는 객체에 의해서만 사용되고, 다른 객체와 공유되지 않을 때 합성 관계가 적합. Composition vs Aggregation","link":"/Programming/UML/UML-Class-Diagram/"},{"title":"1만 시간의 법칙","text":"강의 ✏️프로그래밍 Java Python 파이썬 웹 개발 패키지 - 패스트캠퍼스 딥러닝 처음부터 시작하는 딥러닝 유치원 - 패스트캠퍼스 김기현의 딥러닝을 활용한 자연어처리 입문 - 패스트캠퍼스 논문 📝 Transformer *Efficient Classification of Long Documents Using Transformers [Park et.al., 2022]* Flow 🏃‍♂️[22.11.16.(금) 1h] Programming map, lambda 운동 턱걸이 - 65kg 보조 12회 4세트 바벨로우 - 25kg 12회 3회 컨벤셔널 데드리프트 - 45kg 10회 3세트 [22.11.16.(목) 1h] DL Teacher-forcing Auto-Regressive Programming Python Decorator [22.11.16.(수) 1h] Programming 강의(*더 자바, 코드를 조작하는 다양한 방법) -* 바이트 코드 DL Language Model - perplexity &amp; Cross Entropy 운동 하체 1.0h 스쿼트 - 45kg 3set, 35kg 2set 런지 - 맨몸 3set 유산소 0.5h [22.11.15.(화)] Programming 강의(더 자바, 코드를 조작하는 다양한 방법) - JVM 구조, 클래스로더 [22.11.14.(월)] Programming 클린코드 1장 - Page 1-20 운동 가슴 1.0h 벤치프레스 - 45kg 3set 35kg 2set 덤벨프레스 - 12kg 3set 윤산소 0.2h","link":"/Programming/Agile/1%E1%84%86%E1%85%A1%E1%86%AB-%E1%84%89%E1%85%B5%E1%84%80%E1%85%A1%E1%86%AB%E1%84%8B%E1%85%B4-%E1%84%87%E1%85%A5%E1%86%B8%E1%84%8E%E1%85%B5%E1%86%A8/"},{"title":"객체지향 프로그래밍","text":"Object Oriented Programming(OOP) 데이터를 추상화시켜 상태(속성)와 행위(methods)를 가진 객체(object)로 만들고 그 객체들 간의 유기적인 상호작용을 통해 로직(흐름)을 구성하는 프로그래밍 방법 프로그램을 실제 세상에 가깝게 모델링하는 기법 Class 어떤 문제를 해결하기 위한 데이터를 만들기 위해 OOP 원칙에 따라 집단(현실 세계)에 속하는 속성과 행위(methods)를 변수와 메소드로 정의한 것 로봇 설계도 Instacne(Object) class에서 정의한 것(설계도)를 토대로 실제 메모리상에 할당된 것(실제 사물, object)으로 실제 프로그램에서 사용되는 데이터 하나의 class로 만들어진 여러 instance(object)는 각각 독립적이다. 실제 로봇 OOP 의 4가지 특징 캡슐화(Encapsulation) 객체(Obejct)의 데이터와 해당 데이터를 조작하는 메소드(속성과 행위)를 하나로 묶고, 구현된 일부를 외부에 감추어 은닉 클래스 내부에서 데이터의 상태를 보호하기 위해, 데이터에 직접 접근하지 못하도록 제한하는 기능을 제공 코드의 안정성과 보안성을 높임 Java 에서는 다음과 같은 방법들로 캡슐화를 구현 접근 제한자(access modifier)를 제공 public : 어디에서든 접근 가능 protected : 같은 패키지 또는 상속받은 클래스에서만 접근 가능 default : 같은 페키지에서만 접근 가능 private : 같은 클래스 내에서만 접근 가능 Getter와 Setter 메소드를 사용 클래스의 멤버 변수(속성)를 private으로 지정하고, Getter/Setter 를 통해 접근 final 키워드 사용 final 키워드를 사용하여 클래스, 변수, 메소드를 final로 선언하면, 해당 요소들은 더 이상 변경할 수 없음 interface 사용 interface를 사용하여 해당 interface를 구현한 클래스에서만 접근할 수 있는 메소드를 선언할 수 있음 내부 클래스(Inner Class) 사용 내부 클래스를 사용하여 클래스 내부에서 다른 클래스를 선언하고, 해당 클래스의 변수와 메서드를 private으로 지정하여 캡슐화를 구현 123456789101112131415161718public class Outer { private int data; public void printData() { Inner inner = new Inner(); System.out.println(inner.getData()); } private class Inner { public int getData() { return data; } }}// Inner 클래스는 private으로 선언되어 외부에서 직접 접근할 수 없음// Inner 클래스에서는 Outer 클래스의 data 멤버 변수에 접근할 수 있음// printData() 메서드에서 Inner 클래스를 생성하여 Inner 클래스의 getData() 메서드를 호출// 이를 통해 data 멤버 변수에 대한 접근을 Outer 클래스 내부에서만 가능하도록 캡슐화를 구현 Python 에서는 다음과 같은 방법으로 캡슐화를 구현 Python 에소는 캡슐화를 언어적으로 강제할 수 있는 방법은 제공하지 않지만 언더스코어(_)를 활용하여 데이터를 보호하고 클래스 외부에서 접근하지 못하도록 함으로써 캡슐화를 구현할 수 있음 언더스코어 하나(_) : Protected 멤버 해당 클래스 내부 또는 하위 클래스에서만 접근 가능 언더스코어 두개(__) : Private 멤버 클래스 내부에서만 접근 가능 12345678910111213141516171819class Example: def __init__(self): self._protected_data = 10 self.__private_data = 20 def get_protected_data(self): return self._protected_data def set_protected_data(self, data): self._protected_data = data def __get_private_data(self): return self.__private_data def __set_private_data(self, data): self.__private_data = data def access_private_data(self): return self.__get_private_data() 추상화(Abstaction) 복잡한 시스템에서 핵심적인 개념이나 기능을 간추려 내는 과정 핵심적인 개념과 기능에 집중하여 코드를 구현하고, 코드의 가독성과 유지보수성을 높임 대개 인터페이스(interface)나 추상 클래스(abstact class)를 정의하는 것으로 이루어짐 구현 세부 사항이나 로직을 제외하고 해당 기능의 목적과 기능에 대한 설명만 포함됨 불필요한 정보는 숨기고 중요한(필요한) 정보만을 표현함으로써 공통의 속성이나 행위(methods)를 하나로 묶어 이름을 붙임 추상 자료형 = class class를 실제로 구현한 것 = instance class 내의 data = member variable class 내의 operation (function) = method Java 에서는 다음과 같은 방법으로 추상화를 구현 인터페이스를 사용 123456789101112131415161718192021222324252627public interface Animal { public void move();} public class Cat implements Animal { @Override public void move() { System.out.println(&quot;고양이가 걷습니다.&quot;); } } public class Fish implements Animal { @Override public void move() { System.out.println(&quot;물고기가 헤엄칩니다.&quot;); } } public class Example { public static void main(String[] args) { Animal cat = new Cat(); Animal fish = new Fish(); cat.move(); // 고양이가 걷습니다. fish.move(); // 물고기가 헤엄칩니다. } } 인터페이스(interface)란인터페이스는 메서드 시그니처(메서드 이름, 매개변수, 반환 타입)만 정의하고, 구현하지 않은 추상 메서드(abstract method)를 가지는 클래스인터페이스를 구현하는 클래스에서는 인터페이스에 정의된 추상 메서드를 반드시 구현해야함 추상 클래스(abstract class)를 사용 ** 추상 클래스(abstact class)란추상 클래스는 인스턴스를 생성할 수 없는 클래스로, 추상 메소드를 포함할 수 있음추상 클래스를 상속받은 클래스에서는 추상 메소드를 반드시 구현해야 함 123456789101112131415161718192021222324252627public abstract class Animal { public abstract void move();}public class Cat extends Animal { @Override public void move() { System.out.println(&quot;고양이가 걷습니다.&quot;); }}public class Fish extends Animal { @Override public void move() { System.out.println(&quot;물고기가 헤엄칩니다.&quot;); }}public class Example { public static void main(String[] args) { Animal cat = new Cat(); Animal fish = new Fish(); cat.move(); // 고양이가 걷습니다. fish.move(); // 물고기가 헤엄칩니다. }} Python 에서는 다음과 같은 방법으로 캡슐화를 구현 추상 클래스 1234567891011121314151617181920from abc import ABC, abstractmethod class Animal(ABC): @abstractmethod def move(self): pass class Cat(Animal): def move(self): print(&quot;고양이가 걷습니다.&quot;) class Fish(Animal): def move(self): print(&quot;물고기가 헤엄칩니다.&quot;) cat = Cat()fish = Fish() cat.move() # 고양이가 걷습니다.fish.move() # 물고기가 헤엄칩니다. 인터페이스 1234567891011121314151617181920from abc import ABCMeta, abstractmethodclass Animal(metaclass=ABCMeta): @abstractmethod def move(self): passclass Cat(Animal): def move(self): print(&quot;고양이가 걷습니다.&quot;)class Fish(Animal): def move(self): print(&quot;물고기가 헤엄칩니다.&quot;)cat = Cat()fish = Fish()cat.move() # 고양이가 걷습니다.fish.move() # 물고기가 헤엄칩니다. 상속(Inheritance) 부모 class의 속성과 행위(methods)을 그대로 상속 받고 행위(methods)의 일부분을 수정해야 할 경우, 상속받은 자식 class에서 해당 행위(methods)만 다시 수정하여 사용할 수 있음 class 에서 추가적으로 속성이나 행위(methods)를 정의할 수 있음 Java 에서 상속을 구현하는 방법 1234567891011121314151617181920212223242526272829303132333435363738// 부모 클래스public class Animal { protected String name; public Animal(String name) { this.name = name; } public void eat() { System.out.println(name + &quot; is eating.&quot;); }}// 자식 클래스public class Dog extends Animal { public Dog(String name) { super(name); } public void bark() { System.out.println(name + &quot; is barking.&quot;); } // 오버라이딩 @Override public void eat() { System.out.println(name + &quot; is eating dog food.&quot;); }}// 사용 예시public class Main { public static void main(String[] args) { Dog dog = new Dog(&quot;Max&quot;); dog.bark(); // &quot;Max is barking.&quot; dog.eat(); // &quot;Max is eating dog food.&quot; }} Python 에서 상속을 구현하는 방법 12345678910111213141516171819202122232425# 부모 클래스class Animal: def __init__(self, name): self.name = name def eat(self): print(f&quot;{self.name} is eating.&quot;)# 자식 클래스class Dog(Animal): def __init__(self, name): super().__init__(name) def bark(self): print(f&quot;{self.name} is barking.&quot;) # 오버라이딩 def eat(self): print(f&quot;{self.name} is eating dog food.&quot;)# 사용 예시if __name__ == &quot;__main__&quot;: dog = Dog(&quot;Max&quot;) dog.bark() # &quot;Max is barking.&quot; dog.eat() # &quot;Max is eating dog food.&quot; 다형성(Polymorphism) 같은 이름의 메소드나 연산자가 다양한 형태의 객체에서 다른 방식으로 작동할 수 있는 능력 상속 계층 구조에서 부모 클래스의 메서드를 오버라이딩하거나, 자식 클래스에서 오버로딩하여 다양한 객체에 대해 같은 인터페이스를 제공 오버로딩 같은 이름의 메서드나 연산자를 매개변수의 개수나 타입에 따라 다른 형태로 구현하는 것을 말함 메서드 이름의 재사용성을 높일 수 있음 java 에서 오버라이딩을 사용할 경우 1234567891011121314151617181920212223public class Calculator { public int add(int x, int y) { return x + y; } public double add(double x, double y) { return x + y; } public int add(int x, int y, int z) { return x + y + z; }} // 사용 예시public class Main { public static void main(String[] args) { Calculator calculator = new Calculator(); System.out.println(calculator.add(1, 2)); // 3 System.out.println(calculator.add(2.5, 3.5)); // 6.0 System.out.println(calculator.add(1, 2, 3)); // 6 }} Python 에서는 오버로딩을 정식적으로 지원하지 않음 동적 타이핑(Dynamic Typing) 언어이기 때문에, 매개변수의 타입을 체크하는 과정이 불필요, 즉 파이썬에서는 매개변수의 타입을 미리 지정하지 않음 함수 호출 시 객체 타입을 런타임에 체크하므로, 매개변수의 타입을 미리 지정하는 것이 필요하지 않음 다만, 오버로딩 대신 매개변수의 타입 체크를 직접 수행하는 방식으로 오버로딩을 구현할 수 있음 123456789101112def add(x, y): if isinstance(x, int) and isinstance(y, int): return x + y elif isinstance(x, str) and isinstance(y, str): return x + &quot; &quot; + y else: return None# 사용 예시print(add(1, 2)) # 3print(add(&quot;hello&quot;, &quot;world&quot;)) # &quot;hello world&quot;print(add(1, &quot;hello&quot;)) # None 오버라이딩 부모 클래스의 메서드를 자식 클래스에서 재정의하여 자식 클래스에서 특화된 구현을 제공 즉, 같은 이름의 메서드를 자식 클래스에서 재정의하여 자식 클래스에서 부모 클래스의 메서드를 덮어쓰는 것 Java 에서 오버라이딩을 사용할 경우 1234567891011121314151617181920212223public class Animal { public void makeSound() { System.out.println(&quot;Animal is making a sound.&quot;); }}public class Dog extends Animal { @Override public void makeSound() { System.out.println(&quot;Dog is barking.&quot;); }}// 사용 예시public class Main { public static void main(String[] args) { Animal animal = new Animal(); animal.makeSound(); // &quot;Animal is making a sound.&quot; Dog dog = new Dog(); dog.makeSound(); // &quot;Dog is barking.&quot; }} Python 에서 오버라이딩을 사용할 경우 123456789101112131415class Animal: def make_sound(self): print(&quot;Animal is making a sound.&quot;)class Dog(Animal): def make_sound(self): print(&quot;Dog is barking.&quot;)# 사용 예시if __name__ == &quot;__main__&quot;: animal = Animal() animal.make_sound() # &quot;Animal is making a sound.&quot; dog = Dog() dog.make_sound() # &quot;Dog is barking.&quot; 여러 형태를 가질 수 있도록 한다. 즉, 객체를 부품화할 수 있도록 한다. OOP 의 5가지 원칙* Namespace파이썬의 네임스페이스? 파이썬 내부의 모든 것은 객체로 구성되며 이들 각각은 특정 이름과 매핑 관계를 갖게 되는데, 이 매핑을 포함하고 있는 공간을 네임스페이스라고 한다. 프로그래밍 언어에서 특정한 객체(Object)를 이름(Name)에 따라 구분할 수 있는 범위를 의미 개체를 구분할 수 있는 범위 dir() : 파이썬 네임스페이스의 key 값을 list로 확인가능 _dict_ : 클래스의 네임스페이스를 확인가능 필요한 이유 프로그래밍을 할 때 모든 변수 이름과 함수 이름을 정하게 되는데, 이름들을 모두 겹치지 않게 정하기란 사실상 불가능 따라서 네임스페이스라는 개념을 도입하여, 이름 추돌을 방지 즉, 소속된 네임스페이스가 다르다면 같은 이름이 다른 객체를 가리키도록 하는 것이 가능해짐 파이썬의 네임스페이스는 크게 3가지로 분류 내장 네임스페이스(Built-in Namespace) 기본 내장 함수 및 기본 예외들의 이름들이 소속됨 123print(&quot;Hello Python!&quot;)abs(-10)len([1,2,3]) 파이썬으로 작성된 모든 코드 범위가 포함 전역 네임스페이스(Global Namespace) 각각의 모듈마다 존재하며, 모듈이 로딩될 때 생성됨 파이썬은 모듈이 로딩될 때, 해당 모듈의 소스코드를 컴파일하고, 전역 네임스페이스를 생성하여 모듈 수준의 이름들(변수, 함수, 클래스 등)을 정의 12345678910111213141516# 모듈 수준의 전역 네임스페이스# 변수의 전역 네임스페이스x = 10y = 20# 함수의 전역 네임스페이스def add(a, b): return a + b # 클래스의 전역 네임스페이스class Person: def __init__(self, name, age): self.name = name self.age = age 지역 네임스페이스(Local Namespace) 함수 및 메소드의 로컬 네임스페이스로 함수나 메소드가 호출될 때마다 생성됨 호출이 끝나면 사라지므로 함수나 메소드의 지역 변수는 호출자에게 보이지 않음 1234567891011# 함수의 지역 네임스페이스## my_function() 함수가 호출될 때마다, 함수의 지역 네임스페이스가 생성됨## a와 b 매개변수와 c 지역 변수는 모두 해당 함수의 지역 네임스페이스에 정의됨## 함수 내에서 정의된 변수들은 지역 변수로 취급되며, 함수 외부에서는 접근할 수 없음## 함수가 실행을 마치면 해당 지역 네임스페이스는 사라지게 됨def my_function(a, b): # 지역 네임스페이스 c = a + b print(c)my_function(10, 20) 1234567891011121314151617# 메소드의 지역 네임스페이스class Person: def __init__(self, name, age): self.name = name self.age = age def get_name(self): # 지역 네임스페이스 name_length = len(self.name) return name_length# 객체 생성person1 = Person(&quot;Alice&quot;, 25)# 메서드 호출name_length = person1.get_name()print(name_length) Varable Scope x 라는 이름이 여러 namespace에 존재한다면 어떠한 우선순위를 가지고 x에 접근할까? Scope! Scope란 레퍼런스하고자 하는 해당 이름이 존재하는 지를 다음과 같은 순서로 찾고 결정함 Local namespace Enclosing namespace GLobal namespace Built-in namespace 해당 부분은 아직 지식이 정리가 완전히 정리가 되지 않았지만 위의 3가지 네임 스페이스 외에 아래 두가지의 네임 스페이스가 더 있는 것으로 확인된다. 조사해본 바에 따르면 Class Namespace 및 Instance Namespace 는 위 3가지에 포함되지 않고 독립적으로 있는 것 같은데, 아무리 서치를 해보아도 python 의 5가지 namespace라는 내용은 못찾았다.개인 생각으로는 명백히 다른 개념이라 판단되어 5가지의 namespace 같아 보여 4. 5. 으로 추가해두었다.(이에 대해 지식을 알려주실 수 있다면 진심으로 감사를 드리겠습니다.) 클래스 네임스페이스(Class Namespace) 클래스의 속성(attribute)들이 저장되는 네임스페이스 12345678910111213141516class MyClass: # 클래스 네임스페이스 class_var = 10 def class_method(self): # 클래스 네임스페이스 print(MyClass.class_var)# 클래스 네임스페이스print(MyClass.class_var)# 객체 생성obj = MyClass()# 클래스 메서드 호출obj.class_method() 인스턴스 네임스페이스(Instance Namespace) 클래스의 인스턴스에서 사용되는 인스턴스 변수들이 저장되는 네임스페이스 1234567891011121314151617class MyClass: def __init__(self, inst_var): # 인스턴스 네임스페이스 self.inst_var = inst_var def inst_method(self): # 인스턴스 네임스페이스 print(self.inst_var)# 객체 생성obj = MyClass(20)# 인스턴스 변수 출력print(obj.inst_var)# 인스턴스 메서드 호출obj.inst_method() 1234567891011121314151617181920212223242526272829class Person: # 클래스 네임스페이스 count = 0 def __init__(self, name, age): # 인스턴스 네임스페이스 self.name = name self.age = age Person.count += 1 def say_hello(self): # 인스턴스 네임스페이스 print(&quot;Hello, my name is {} and I am {} years old.&quot;.format(self.name, self.age))# 클래스 네임스페이스print(Person.count)# 객체 생성person1 = Person(&quot;Alice&quot;, 25)# 인스턴스 네임스페이스print(person1.name)print(person1.age)# 클래스 네임스페이스print(Person.count)# 메서드 호출person1.say_hello() 파이썬의 네임스페이스 확인 globals(), locals(), dir() 함수를 사용하여 네임스페이스를 확인할 수 있음 globals() 전역 네임스페이스를 반환하는 함수 딕셔너리 형태로 반환되며, 키는 이름(name)을 값은 객체(object)를 나타냅니다. 1234567891011global countcount = 1name = &quot;shchoi&quot;# 전역 네임스페이스 확인print(globals())# {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;_frozen_importlib_external.SourceFileLoader object at 0x7fc79008ccd0&gt;,# '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;,# '__file__': '/Users/shchoice/TIL/Python/OOP/test.py', '__cached__': None,# 'count': 1, 'name': 'shchoi'} locals() 현재 지역 네임스페이스를 반환하는 함수 함수나 메서드 내에서 호출할 경우 해당 함수나 메서드의 지역 네임스페이스를 반환 12345# 현재 지역 네임스페이스 확인def my_func(): x = 10 print(locals()) # {'x': 10}my_func() 전역에서 호출할 경우 전역 네임스페이스를 반환. 12345678910111213global countcount = 1name = &quot;shchoi&quot;print(globals())# {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;_frozen_importlib_external.SourceFileLoader object at 0x7fc79008ccd0&gt;,# '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;,# '__file__': '/Users/shchoice/TIL/Python/OOP/test.py', '__cached__': None,# 'count': 1, 'name': 'shchoi'}print(locals())# print(globals()) 와 동일하게 출력됨 (생략) 딕셔너리 형태로 반환되며, 키는 이름(name)을 값은 객체(object)를 나타냅니다. dir() 객체의 이름(name)들을 리스트 형태로 반환하는 함수 객체의 네임스페이스(namespace)에 있는 모든 이름(name)들을 반환 전역 네임스페이스에서 호출할 경우 내장(Built-in) 네임스페이스의 이름(name)도 반환 1234567891011# 객체의 네임스페이스 확인class MyClass: def __init__(self): self.x = 10 self.y = 20obj = MyClass()print(dir(obj))# ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',# '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__',# '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__',# 'x', 'y']","link":"/Python/%E1%84%80%E1%85%A2%E1%86%A8%E1%84%8E%E1%85%A6%E1%84%8C%E1%85%B5%E1%84%92%E1%85%A3%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC/%E1%84%80%E1%85%A2%E1%86%A8%E1%84%8E%E1%85%A6%E1%84%8C%E1%85%B5%E1%84%92%E1%85%A3%E1%86%BC-%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC/"},{"title":"시퀀스","text":"시퀀스내장 시퀀스 개요타입에 따른 분류 컨테이너 시퀀스 서로 다른 자료형의 항목들을 담을 수 잇는 list, tuple, collections.deque 형 객체에 대한 참조를 담고 있음 균일 시퀀스 단 하나의 자료형만 담을 수 있는 str, bytes, bytearray, memoryview, array.array, Numpy의 ndarray 형 등 객체에 대한 참조 대신 자신의 메모리 공간에 각 항목의 값을 직접 담음 그로인해 메모리를 더 적게 사용하지만 문자, 바이트, 숫자 등 기본적인 자료형만 저장할 수 있음 가변성에 따른 분류 가변 시퀀스 list, bytearray, array.array, collections.deque, memoryview, numpy의 ndarray 형 불변 시퀀스 tuple, str, bytes 형 12345my_string = 'immutable'print(my_string) # immutablemy_string[0] = 'I'print(my_string) # TypeError: 'str' object does not support item assignmen 지능형 리스트(list comprehension)와 제너레이터(generator expression) 표현식지능형 리스트(리스트형의 경우)나 제너레이터 표현식(그 외 시퀀스의 경우)을 사용하면 시퀀스를 간단히 생성할 수 있으며, 가독성 및 실행 속도도 빠른 코드로 만들 수 있음 지능형 리스트와 가독성 123characters = 'ABCDE'codes = [ord(character) for character in characters]print(codes) # [65, 66, 67, 68, 69] 1234567characters = 'ABCDE'codes = []for character in characters: codes.append(ord(character))print(codes) # [65, 66, 67, 68, 69] 지능형 리스트는 오직 새로운 리스트를 만드는 일만 하는, 의도를 명확히 보여주는 일만 한다. 즉, 생성된 리스트를 사용하지 않을 경우에는 지능형 리스트 구문을 사용하지 말아야 함 다만 지능형 리스트 구문이 두 줄 이상 넘어가는 경우에는 코드를 분할하거나 for문을 이용하는 것이 더 낫다. 지능형 리스트는 더 이상 메모리를 누수하지 않음 즉, 지능형 리스트를 사용할 때, 반복문과 조건문을 함께 사용하여 리스트를 생성하는데, 이때 리스트의 생성 과정에서 메모리 누수를 방지하기 위해 필요한 메모리만 할당하고, 불필요한 메모리는 바로 해제하여 메모리를 효율적으로 사용한다는 것 → 프로그램의 성능을 향상시키고 메모리 사용량 줄임 지능형 리스트는 항목을 필터링 및 변환함으로써 시퀀스나 기타 반복 가능한 자료형으로부터 리스트를 만듬 지능형 리스트와 map()/filter() 비교 지능형 리스트가 속도 면에서 더 나음 지능형 리스트가 내부적으로 C 언어로 작성된 리스트 컴프리헨션과 비슷한 구조를 가지고 있는 반면에 map과 filter 함수는 함수 호출과 객체 생성 등의 오버헤드가 있기 때문 1234567characters = 'ABCDE'codes = list(filter(lambda character: character &gt; 66, map(ord, characters)))print(codes) # 67, 68, 69]characters = 'ABCDE'codes = [ord(character) for character in characters if ord(character) &gt; 66]print(codes) # 67, 68, 69]","link":"/Python/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%89%E1%85%B5%E1%84%8F%E1%85%AF%E1%86%AB%E1%84%89%E1%85%B3/"},{"title":"TDD","text":"회사에서 개발하는 솔루션이 어느새 브런치 여러개로 쪼개지게 되었다. 신규 기능이 추가되거나 유지보수를 수행할 때 하나의 부분을 고치면 3개를 수정해야하며 모두 테스트를 해야한다. 다른 회사라면 이미 여러가지 방법으로 테스트 코드로 검증을 하겠지만… 이런 저런 내부 사정으로 인해 테스트 코드가 없다. 따라서 junit/Unittest 로 코드 테스트를 하고 Jenkins로 빌드 테스트를 하고 관리도구 웹페이지는 Selenium 등으로 테스트를 수행을 하려고한다. 깊은 지식이 없기에 해당내용에 대해 공부하면서 안정적으로 솔루션을 관리하는 역량을 키워보자!","link":"/Programming/TDD/TDD/"},{"title":"1장. 잘못된 구조의 문제 깨닫기","text":"1장. 잘못된 구조의 문제 깨닫기목차 1.1 의미를 알 수 없는 이름 1.2 이해하기 어렵게 만드는 조건 분기 중첩 1.3 수많은 악마를 만들어 내는 클래스 1.4 악마 퇴치의 기본 1장 핵심 내용 이 책에서 전체적으로 말하고자하는 바는 좋은 설계, 즉 설계의 중요성에 대해 말을 하고 있음 좋은 설계를 하기 위해서는 설계를 소홀히 했을 때 어떠한 폐해가 일어나는 지를 알아야 한다고 말함 코드를 읽고 이해하는 데 시간이 오래걸림 버그가 계속해서 발생함 나쁜 구조로 인해서 더 나쁜 구조가 만들어짐 나쁜 구조와 그 폐해를 인지하는 것이 중요하다고 말함 객체지향 설계가 곧 악마를 퇴치하는 무기임을 말함 폐해는 아래로 부터 시작됨 1.1 의미를 알 수 없는 이름 기술 중싱 명명 12345class MemoryStateManager { void changeIntValue01(int changeValue) { updateState02Flag(); }} 일련번호 명명 1234class Class001 { void method001(); void method002();} 문제점 코드에서 의도를 읽어낼 수 없어 코드를 읽고 이해하는데 시간이 오래 걸림 그로인해 충분히 이해하지 못한 상태로 코드 변경 시 버그 발생 가능 로직이 필요 이상으로 복잡해질 수 있음 만약 이를 위해 클래스와 메서드의 역할과 기능을 설명하는 문서를 만들면.. 바쁜 업무로 유지보수가 이루어지지 않아 문서는 거짓말을 시작해 더 쉽게 버그가 쉽게 발생할 수 있음 대처 의도와 목적을 드러내는 이름을 사용 (10장 참고) 1.2 이해하기 어렵게 만드는 조건 분기 중첩 문제점 조건문 중첩이 많을수록 코드의 가독성이 나빠짐 어디서 어디까지가 if 조건문 처리 블록인지 확인하기 어렵기 때문 예시 1234567891011121314if （조건） { // 수십~수백 줄의 코드 if （조건） { // 수십-수백 줄의 코드 if （조건） { // 수십-수백 줄의 코드 if （조건） { // 수십-수백 줄의 코드 } } // 수십쑤백 줄의 코드 } // 수십~수백 줄의 코드} 1.3 수많은 악마를 만들어 내는 클래스 잘못된 데이터 클래스 예시 데이터 클래스 1234public class ContractAmount { public int amountlncludingTax; // 세금 포함 금액 public BigDecimal salesTaxRate; // 소비세율} 데이터 클래스에 들어갈 내용의 메서드가 다른 클래스에 구현이 됨 12345678910111213141516171819202122// 계약을 관리하는 클래스public class ContractManager { public ContractAmount contractAmount; // 세금 포함 금액 계산 public int calculateAmountIncludingTax(int amountExcludingTax, BigDecimal salesTaxRate) { BigDecimal multiplier = salesTaxRate.add(new BigDecimalC&quot;1.0&quot;))； BigDecimal amountlncludingTax = multiplier.multiply(new BigDecimal(amountExcludingTax)); return amountlncludingTax.intValue(); } // 계약 체결 public void conclude() { // 생략 int amountlncludingTax = caleutateAmountIncludingTax(amountExcludingTax, salesTaxRate); ContractAmount = new ContractAmountO; ContractAmount.amountlncludingTax = amountlncludingTax; ContractAmount.salesTaxRate = salesTaxRate; // 생략 }} 만약에 재대로 설계를 제대로 해서 작성했다면 아래와 같을 것임 123456789101112131415161718import java.math.BigDecimal;public class ContractAmount { private final int amountIncludingTax; // 세금 포함 금액 private final BigDecimal salesTaxRate; // 소비세율 public ContractAmount(int amountIncludingTax, BigDecimal salesTaxRate) { this.amountIncludingTax = amountIncludingTax; this.salesTaxRate = salesTaxRate; } public int getAmountIncludingTax() { return amountIncludingTax; } public BigDecimal getSalesTaxRate() { return salesTaxRate; } 123456789import java.math.BigDecimal;public class TaxCalculator { public static int calculateAmountIncludingTax(int amountExcludingTax, BigDecimal salesTaxRate) { BigDecimal multiplier = salesTaxRate.add(BigDecimal.ONE); BigDecimal amountIncludingTax = multiplier.multiply(new BigDecimal(amountExcludingTax)); return amountIncludingTax.intValue(); }} 1234567891011121314public class ContractManager { private ContractAmount contractAmount; public void conclude(int amountExcludingTax, BigDecimal salesTaxRate) { int amountIncludingTax = TaxCalculator.calculateAmountIncludingTax(amountExcludingTax, salesTaxRate); contractAmount = new ContractAmount(amountIncludingTax, salesTaxRate); // 여기에 추가적인 계약 체결 로직을 구현 } // 필요한 경우, ContractAmount에 접근하는 메서드를 제공 public ContractAmount getContractAmount() { return contractAmount; }} 폐해 코드 중복 관련된 코드가 서로 멀리 떨어져, 묶어서 파악하기가 힘듬 위의 코드처럼 이미 기능이 구현이 되었음에도 동료 개발자들은 아직 기능이 구현되지 않았다고 오해할 수 있어 같은 로직을 여러 곳에 구현하는 의도치 않은 코드 중복이 발생할 수 있음 수정 누락 코드 중복이 많으면, 사양이 변경될 때 중복된 코드를 모두 고쳐야 합니다. 하 지만 이 과정에서 일부 코드를 놓칠 수 있으며, 결국 버그를 낳음 가독성 저하 코드가 분산되어 있으면, 중복된 코드를 포함해서 관련된 정보를 다 찾는 것만으로도 시간이 오래 걸립 초기화되지 않은 상태(쓰레기 객체) 예시 12ContractAmount amount = new ContractAmount();System.out.println(amount.salesTaxRate.toString()); 따로 초기화하는 코드가 없기에, 초가화시 null이 되며 코드를 실행하면 NullPointerException이 발생 가능할 수 있음 잘못된 값 할당 예시 주문 건수가 음수가 나오는 경우 게임에서 Jit Point값이 최대값을 넘기는 경우 데이터 클래스는 음수를 대입해도 할당이 됨 12ContractAmount amount = new ContractAmount();amount.salesTaxRate = new BigDecimal(&quot;-0.1&quot;); 대처 데이터 클래스를 사용하는 쪽의 로직을 살짝 변경해 유효성 검사를 수행 1.4 악마 퇴치의 기본 나쁜 폐혜를 인지하는 것이 좋은 설계를 위한 첫걸음 객체 지향 설계가 곧 악마를 퇴치하는 무기","link":"/Programming/%E1%84%82%E1%85%A2%20%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B3%E1%84%80%E1%85%A1%20%E1%84%80%E1%85%B3%E1%84%85%E1%85%A5%E1%87%82%E1%84%80%E1%85%A6%20%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A1%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD/1%E1%84%8C%E1%85%A1%E1%86%BC-%E1%84%8C%E1%85%A1%E1%86%AF%E1%84%86%E1%85%A9%E1%86%BA%E1%84%83%E1%85%AC%E1%86%AB-%E1%84%80%E1%85%AE%E1%84%8C%E1%85%A9%E1%84%8B%E1%85%B4-%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A6-%E1%84%81%E1%85%A2%E1%84%83%E1%85%A1%E1%86%AE%E1%84%80%E1%85%B5/"},{"title":"2장.  설계 첫걸음","text":"목차2.1 의도를 분명히 전달할 수 있는 이름 설계하기 2.2 목적별로 변수를 따로 만들어 사용하기 2.3 단순 나열이 아니라, 의미 있는 것을 모아 메서드로 만들기 2.4 관련된 데이터와 로직을 클래스로 모으기 2장의 목적클래스를 설계 하기 전에 기본적인 설계, 즉 변수와 메서드(함수)를 설계하는 방법에 대해 알아보는 것이며 의도를 갖고 적절하게 설계하면 유지보수와 변경이 쉬워짐 2.1 의도를 분명히 전달할 수 있는 이름 설계하기 변수 이름은 의도를 쉽게 알 수 있는 쉬운 이름을 붙일 것 1234567// 잘못된 변수명int d = 0;d = p1 + p2;// 의도를 분명히 하기int damageAmount = 0;damageAmount = playerArmPower + playerWeaponPower; 2.2 목적별로 변수를 딸 만들어 사용하기 변수에 값을 다시 할당하는 것을 재할당이라고 하는데, 이는 변수의 용도가 바뀌는 문제를 일으키기 쉬움 코드를 읽는 사람을 혼란스럽게 만들고, 버그를 만들어 낼 가능성이 있기 때문이며 이를 해소한다면 전체적으로 어떤 값을 계산하는데 어떤 값을 사용하는지 관계를 파악하기 훨씬 쉬워짐 123456789// 변수의 재할당 &lt;- 사용 금지int damageAmount = 0;damageAmount = playerArmPower + playerWeaponPower; damageAmount = damageAmount - ((enemyBodyDefence + enemyArmorDefence) / 2); // 목적별로 변수를 만들어 사용int totalPlayerAttackPower = playerArmPower + playerWeaponPower;int totalEnemyDefence = enemyBodyDefence + enemyAmorDefence;int damageAmount = totalPlayerAttackPower - (totalEnemyDefence / 2); 2.3 단순 나열이 아니라, 의미 있는 것을 모아 메서드로 만들기 계산 로직들이 단순하게 나열되어 있으면, 로직이 어디에서 시작해서 어디에서 끝나는지, 무슨 일을 하는지 알기 어려움 따라서 의미 있는 로직을 모아서 메서드(함수)로 구현 2.4 관련된 데이터와 로직을 클래스로 모으기 변수 123와 변수를 조작하는 로직 이 이곳저곳에서 만들어지면 문제 발생가능 수천, 수만 줄의 소스코드로 이루어진 프로그램이라면 관련된 로직을 찾아 돌아다니는 시간만 따져도 엄청날 것 값이 잘못된 상태로 프로그램이 계속해서 동작한다면, 버그가 발생할 것 이러한 문제를 해결해주는 것이 바로 클래스 데이터를 인스턴스 변수로 갖고 인스턴스 변수를 조작하는 메서드를 함께 모아놓음 서로 밀접한 데이터와 로직을 한곳에 모아두면 이곳저곳 찾아 다니지 않아도 괜찮음 결론 클래스로 의도를 갖고 적절하게 설계하면 유지보수와 변경이 쉬워짐 설계의 기초는 클래스! 스터디 참고스레드 사용 시 고려해야 할 변수의 4가지 측면 프로그래밍에서 쓰레드를 사용하는 경우, 변수의 관리가 매우 중요. 특히 동시성과 스레드 안전성을 고려할 때 아래 4가지 주요 측면을 이해하는 것이 필수적 변수의 가변성(Mutability) 변수가 변경될 수 있는지 여부를 파악해야함 가변 변수는 동시에 여러 스레드에 의해 접근될 때 데이터 무결성 문제를 일으킬 수 있음 참조의 공유성(Reference Sharing) 변수가 프로그램의 다른 부분에서 참조되는지 여부를 파악해야함 공유되는 참조는 스레드 간 동기화 문제를 일으킬 수 있음 값 변경 가능성(Value Modification Possibility) 쓰레드 내에서 변수의 값이 변경될 가능성을 인지해야함 동시성 제어를 위한 메커니즘을 설계할 때 중요한 요소 스레드 간 참조 가능성(Cross-thread Reference Possibility) 여러 쓰레드가 동일한 변수를 참조할 수 있는지 여부를 고려해야 함 Lock 이나 기타 동시성 제어 기법을 적용할 필요가 있는지를 결정하는 데 도움이 됨","link":"/Programming/%E1%84%82%E1%85%A2%20%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B3%E1%84%80%E1%85%A1%20%E1%84%80%E1%85%B3%E1%84%85%E1%85%A5%E1%87%82%E1%84%80%E1%85%A6%20%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A1%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD/2%E1%84%8C%E1%85%A1%E1%86%BC-%E1%84%89%E1%85%A5%E1%86%AF%E1%84%80%E1%85%A8-%E1%84%8E%E1%85%A5%E1%86%BA%E1%84%80%E1%85%A5%E1%86%AF%E1%84%8B%E1%85%B3%E1%86%B7/"},{"title":"함수형 프로그래밍","text":"업무를 수행하면서 함수형 프로그래밍이 많이 대두되고 있으며, 새로 함께하는 팀장님께서는 함수형 프로그래밍으로 코드를 구현하는 것에 대해서 많이 말씀을 하고 계셨다. 학교에서 배웠던 것에 익숙한 나에게 함수형 프로그래밍의 개념은 처음에 너무 이질적이었고 배움이 더뎠었다. 하지만, 업무를 하면서 내가 커밋한 코드를 리뷰할 때 팀장님께서는 함수형으로 작성하면 코드도 간결하고 속도 및 메모리 면에서 더 이득을 볼 것이라며 예시를 들어주셨다. 이렇게 조금씩 함수형 코드를 보게 되면서 함수형 프로그래밍이라면 1줄 혹은 몇 줄 안되게 작성할 수 있는 것을 명령형 프로그래밍을 사용하면 코드가 10 여줄 가까워지며, 가독성이 좋은 코드가 아님을 여실히 느끼게 되었다. 따라서 철저히 학습 필요성을 느끼게 되어 진지하게 함수형 프로그래밍에 대해 공부한 내용을 정리해보게 되었다 프로그래밍 패러다임명령형 프로그래밍과 함수형 프로그래밍으로 나눌 수가 있으며, 이 둘을 구분하는 중요한 특징 하나는 상태에 대한 개념 명령형(imperative) 프로그래밍 순차적(procedual) 언어 객체지향(object-oriented) 언어 함수형(functional) 프로그래밍 파이썬 언어는 위 세 가지 범주에 속하는 특성을 모두 포함. 비록 파이썬은 순수한 함수형 언어는 아니지만 상당히 다양한 함수형 프로그래밍을 할 수 있음 파이썬 함수형 프로그래밍의 장점과 단점파이썬 함수형 프로그래밍의 장점 함수형 프로그래밍 언어의여러 디자인 패턴과 설계 기법을 파이썬 프로그래밍에 적용할 수 있음 간결하고 우아한 프로그래밍이 가능 파이썬의 제너레이터 식을 활용하면 메모리에 큰 데이터 구조를 만드는 일을 피할 수 있어 자원을 덜 사용하면서 더 빠르게 동작할 수 있는 프로그램을 작성 파이썬 함수형 프로그래밍의 단점 쉽고 순수한 함수형 프로그램을 작성할 수 없음 파이썬에는 순후 함수형 프로그래밍에 필요한 몇 가지 기능이 부족 재귀호출의 깊이가 한정 모든 식에 대한 지연 계산(Lazy Evaluation)을 지원하지 않음 최적화된 컴파일러가 없음 파이썬이 제공하는 함수형 프로그래밍 기능 함수가 일급 계층 객체(first class object) 함수가 함수를 인자로 받거나 결과로 반환할 수 있음 함수는 소스 코드 상의 구성 요소로만 존재하고 실행 시점의 데이터 구조로는 존재하지 않음 여러가지 고차 함수를 제공 map() filter() functools.reduce() sorted(), min(), max() 등도 있음 함수형 프로그래밍 불변적인 데이터 구조를 자주 활용 → 상태가 없는 개체를 강조하면 최적화를 유연하게 수행할 수 있음 파이썬은 tuple과 namedtuple을 불변 객체로 제공","link":"/Python/%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC/%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%92%E1%85%A7%E1%86%BC-%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC/"},{"title":"Windows CMD","text":"Intro군 복무시절 3년동안 리눅스 서버를 관리했기에 Linux 시스템과 Bash Shell에 대해서는 매우 익숙하지만, 어딘가 비슷하지만 달라 업무에 필요하거나 평소에 궁금했던 Windows CMD 명령어에 대해 정리한 내용을 기록","link":"/Ops/Window%20CMD/Windows%20CMD/"},{"title":"포트 중복 사용에 따른 포트번호 확인 및 프로세스 종료","text":"Intro개발을 수행하면서 부득이하게 프로세스를 종료시켰다고 생각했지만, 윈도우 백그라운드에서 프로그램이 동작 중일 경우가 발생한다. 그래서 프로세스를 기동하려고 하면, 포트가 중복되어 에러를 반환함을 보게 된다. 또한 IntelliJ에서 tomcat을 통해 개발 중인데 tomcat JMX port(1099) 등이 다른 프로세스가 선점해서 포트 중복으로 기동이 안되는 경우도 마주치게 된다. 이러한 경우를 위해 컴퓨터를 재기동하여 해결할 수도 있겠지만(😭) Windows CMD를 통해 프로세스를 안전하게 종료시켜보자! 핵심 키워드 netstat -nao : 현재 Windows에서 등록되어 있는 포트를 검색함 netstat -nao | findstr 포트번호 : 현재 Windows에서 등록되어 있는 포트 중에서 입력한 포트번호에 대해서만 정보를 가져옴 tasklist /svc /FI “PID eq 프로세스아이디” : 특정 PID가 어떠한 프로세스(서비스)인지 확인 taskkill /f /pid 프로세스아이디 : 특정 PID에 대해서만 프로세스 종료 Contents윈도우에서 특정 포트(port)를 사용하는 프로세스를 찾기 위해서는 netstat, findstr 명령어를 사용해야함 netstan -nao를 사용하면 현재 Windows에 등록되어 있는 포트 정보가 출력됨 netstat -nao | findstr 특정 포트번호만 확인하기 위해서는 netstat -nao | findstr 포트번호를 입력 마지막의 값이 PID 즉, 프로세스 ID이므로 해당 프로세스를 강제종료 하지만 이에 앞서 해당 PID가 어떤 서비스인지 확인을 해보자","link":"/Ops/Window%20CMD/%E1%84%91%E1%85%A9%E1%84%90%E1%85%B3%20%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%87%E1%85%A9%E1%86%A8%20%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%8B%E1%85%A6%20%E1%84%84%E1%85%A1%E1%84%85%E1%85%B3%E1%86%AB%20%E1%84%91%E1%85%A9%E1%84%90%E1%85%B3%E1%84%87%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A9%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%8B%E1%85%B5%E1%86%AB%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%89%E1%85%A6%E1%84%89%E1%85%B3%20%E1%84%8C%E1%85%A9%E1%86%BC%E1%84%85%E1%85%AD/"},{"title":"실행중인 프로세스를 확인 및 종료하기","text":"실행중인 프로세스를 확인 및 종료하기Intro어도비 프로그램을 설치하다가 OUTLOOK.EXE 파일이 실행 중이어서 설치가 불가능하다라는 메세지를 확인했다. Outlook을 이전에 기동은 했지만 종료를 한 상황! 따라서 작업관리자를 열어서 프로세스/세부 정보/서비스 탭을 확인해보았는데, Outlook이 기동중인 것 같지 않다, 설령 기동중이었다해도 프로세스가 많이 띄어져 있어 발견하기가 어려웠…🤷‍♂️) 그래서 CMD로 확인하고 종료해보았다. 핵심 키워드 tasklist /V : 자세한 작업 정보 표시 tasklist | findstr “OUTLOOK.EXE” : 실행중인 프로세스 확인(tasklist) taskkill /f /im OUTLOOK.EXE : 특정프로세스 종료 Contents윈도우에서 특정 프로세스가 기동중인지 확인하고 종료하기 위해서는 tasklist, findstr, taskkill 명령어가 필요 (필요시) 자세한 작업 정보가 표시가 된다. tasklist /V 필터 이름 유효한 연산자 유효한 값 STATUS eq, ne RUNNING IMAGENAME eq, ne 이미지/프로세스 이름 PID eq, ne, gt, lt, ge, le PID 값 SESSION# eq, ne, gt, lt, ge, le 세션 번호 SESSION NAME eq, ne 세션 이름 CPUTIME (CPU 시간) eq, ne, gt, lt, ge, le CPU 시간 MEMUSAGE eq, ne, gt, lt, ge, le 메모리 사용량 (kb) USERNAME eq, ne 모든 유효한 사용자 이름 SERVICES eq, ne 서비스 이름 WINDOWTITLE eq, ne 창 제목 MODULES eq, ne DLL 이름 특정 프로세스가 살행중인 프로세스 리스트에 있는지 확인 tasklist | findstr “OUTLOOK.EXE” 특정 프로세스를 강제 종료 taskkill /f /im OUTLOOK.EXE","link":"/Ops/Window%20CMD/%E1%84%89%E1%85%B5%E1%86%AF%E1%84%92%E1%85%A2%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%8B%E1%85%B5%E1%86%AB%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%89%E1%85%A6%E1%84%89%E1%85%B3%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%8B%E1%85%B5%E1%86%AB%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%8C%E1%85%A9%E1%86%BC%E1%84%85%E1%85%AD%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"동시성 프로그래밍","text":"동시성 프로그래밍 VS 병렬성 프로그래밍 동시성 프로그래밍 병렬성 프로그래밍 클라이언트와 서버간 통신 비디오, 오디오 또는 이미지 처리 시스템 디스크 파일 읽기/쓰기 컴퓨터 비전 데이터베이스 쿼리 작업 머신러닝 API 사용 딥러닝 코드 예제 123456789# 일반적인 프로그래밍 -&gt; 20초 def fetcher(session, url): with session.get(url) as response: return response.textdef main(): with requests.Session() as session: result = [fetcher(session, url) for url in urls] ... 123456789# 동시성 프로그래밍 -&gt; 8초 async def fetcher(session, url): async with session.get(url) as response: return await response.textasync def main(): async with requests.Session() as session: result = await asyncio.gatehr(*[fetcher(session, url) for url in urls]) ... I/O 바운드 &amp; CPU 바운드, 블로킹 CPU 바운드 프로그램이 실행될 때 실행 속도가 CPU 속도에 의해 제한됨을 의미 사례 정말 복잡한 수학 수식을 계산하는 경우에 컴퓨터의 실행 속도가 느려짐 12345678910111213def cpu_bound_func(number: int): total = 1 arrange = range(1, number + 1) for i in arrange: for j in arrange: for k in arrange: total *= i * j * k return totalif __name__ == &quot;__main__&quot;: result = cpu_bound_func(50) print(result) I/O 바운드 프로그램이 실행될 때 실행 속도가 I/O 에 의해 제한됨을 의미 사례 사용자가 키보드의 입력을 기다리는 경우 12345678def io_bound_func(): print(&quot;값을 입력해주세요.&quot;) input_value = input() return int(input_value) + 100if __name__ == &quot;__main__&quot;: result = io_bound_func() print(result) 통신을 할때의 네트워크 I/O 123456789import requestsdef io_bound_func(): result = requests.get(&quot;&lt;https://google.com&gt;&quot;) return resultif __name__ == &quot;__main__&quot;: result = io_bound_func() print(result) 블로킹 바운드에 의해 코드가 멈추게 되는 현상이 일어나는 것 동기와 비동기 코드가 동기적으로 동작한다 → 코드가 반드시 작성된 순서 그대로 실행된다. 코드가 비동기적으로 동작한다 → 코드가 반드시 작성된 순서 그대로 실행되는 것이 아니다. 파이썬 콜틴 메인 루틴 서브 루틴 코루틴 서브 루틴의 일반화된 형태 다양한 진입점과 다양한 탈출점이 있는 루틴 코루틴은 서브루틴과 달리 해당 로직들이 진행되는 중간에 멈추어서 특정 위치로 돌아 갔다가 다시 코루틴에서 진행되었던 위치로 돌아와 나머지 로직을 수행할 수 있음 파이썬 비동기 함수는 코루틴 함수(async, await)로 만들 수 있음 동시성(병행성) vs 병렬성 동시성(concurrency) 스위칭을 하면서한번에 여러 작업을 동시에 다루는 것을 의미 동시성은 논리적 개념으로 멀티 스레딩에서 사용되기도 하고 싱글 스레드에서 사용되기도 함 싱글 코어 뿐만 아니라 멀티 코어에서도 각각의 코어가 동시성을 사용할 수 있음 병렬성(Parallelism) 한번에 여러 작업을 병렬적으로 처리하는 것을 의미(at the same time) 물리적 개념으로 여러 로봇들이 여러 작업을 병렬로 수행한 것처럼, 멀티 코어에서 여러 작업을 병렬적으로 수행(여러개의 코어가 작업 및 하나의 프로세스에서도 쓰레드를 여러개 사용한다면 가능) 파이썬에서는 GIL 때문에 멀티 스레드가 병렬적인 작업을 수행할 수 없음 파이썬 멀티 스레딩과 멀티 프로세싱 멀티 스레딩의 단점 쓰레드끼리 자원을 공유, 그런데 하나의 자원을 동시에 여러 쓰레드가 가져가려는 상황이 발생할 수 있는데 이 경우 층돌이 발생, 이 경우 하나의 스레드가 다른 스레드에 의해 차단될 수 있음 파이썬 GIL은 이러한 문제점을 막음 GIL 한번에 1개의 스레드만 유지하는 락 GIL은 본질적으로 한 스레드가 다른 스레드를 차단해서 제어를 얻는 것을 막아줌 멀티 스레딩의 위험으로부터 보호를 함 이로 인해 파이썬에서는 스레드로 병렬성 연산을 수행하지 못함 파이썬 멀티 스레딩은 동시성을 사용하여 io bound 코드에서 유용하게 사용할 수 있지만 cpu bound 코드에서는 GIL에 의해 원하는 결과를 얻을 수 없음 멀티 프로세싱의 장점 멀티 프로세싱은 이러한 것을 막아줌 https://developer.mozilla.org/ko/docs/Web/HTTP https://www.redhat.com/ko/topics/api/what-are-application-programming-interfaces","link":"/Python/%E1%84%83%E1%85%A9%E1%86%BC%E1%84%89%E1%85%B5%E1%84%89%E1%85%A5%E1%86%BC%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%87%E1%85%B5%E1%84%83%E1%85%A9%E1%86%BC%E1%84%80%E1%85%B5/%E1%84%83%E1%85%A9%E1%86%BC%E1%84%89%E1%85%B5%E1%84%89%E1%85%A5%E1%86%BC-%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC/"},{"title":"CUDA 11.2 설치","text":"Cuda 11.2 설치작업 환경 OS : Ubuntu 16.04 Architecture : x86_64 GPU : NVIDIA GeForce GTX 1080 Ti Nvidia Driver : nvidia-driver-460 1. gcc 9 설치 설치 12345sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y &amp;&amp; sudo apt-get dist-upgrade -y &amp;&amp; sudo apt-get install build-essential software-properties-common -y &amp;&amp; sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y &amp;&amp; sudo apt-get update -y &amp;&amp; sudo apt-get^Cnstall gcc-9 g++-9sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 60 --slave /usr/bin/g++ g++ /usr/bin/g++-9sudo update-alternatives --config gcc 정상 설치 확인 : gcc –version 1234gcc (Ubuntu 9.4.0-1ubuntu1~16.04) 9.4.0Copyright (C) 2019 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ※ 참고사항 : ubuntu 16.04에서는 gcc 11 을 지원하지 않음, 향후 TF 버전 업으로 인해 필요 시 ubuntu 18.04, 20.04로 업데이트가 필요 2. Nvidia Graphic Driver 설치직접 설치하는 방법이며, 3번의 CUDA ToolKit 설치 시에 한번에 설치할 수도 있음(한번에 설치 시 패스) Nvidia Graphic Driver 설치 가능 버전 확인 (사전작업) sudo apt-get install (–reinstall) ubuntu-drivers-common root에서 python 버전이 꼬여있어 다음과 같은 에러 발생 (Legacy 서버로 인한 문제) 12345root@dq-mls-01:~# ubuntu-drivers devicesTraceback (most recent call last): File &quot;/usr/bin/ubuntu-drivers&quot;, line 18, in &lt;module&gt; import aptModuleNotFoundError: No module named 'apt' 해결방법 123456789101112# 1. python3.8의 python3.8의 site-packages 디렉토리를 찾음python3.8 -c &quot;import site; print(site.getsitepackages())&quot;# 2. /usr/lib/python3/dist-packages에서 apt 관련 파일을 Python 3.8의 site-packages 디렉토리로 복사sudo cp -r /usr/lib/python3/dist-packages/apt* &lt;your_python3.8_site-packages_directory&gt;(sudo cp -r /usr/lib/python3/dist-packages/apt* /usr/local/lib/python3.8/site-packages)# apt-get에도 마찬가지로 에러가 발생함cp /usr/lib/python3/dist-package/apt_pkg.cpython-35m-x86_64-linux-gnu.so apt_pkg.so# ModuleNotFoundError: No module named 'UbuntuDrivers 에러도 발생한다면cp -r /usr/lib/python3/dist-packages/UbuntuDrivers /usr/local/lib/python3.8/site-packages Nvidia Graphic Driver 설치 가능 드라이버 확인 및 최신화를 위한Repository 추가 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update 설치 가능 드라이버 확인 sudo ubuntu-drivers devices 123456789101112131415== /sys/devices/pci0000:00/0000:00:03.0/0000:03:00.0/0000:04:10.0/0000:05:00.0 ==modalias : pci:v000010DEd00001B06sv000010DEsd0000120Fbc03sc00i00vendor : NVIDIA Corporationdriver : nvidia-410 - third-party non-freedriver : nvidia-418 - third-party non-freedriver : nvidia-465 - third-party non-free recommendeddriver : nvidia-390 - third-party non-freedriver : nvidia-396 - third-party non-freedriver : nvidia-450 - third-party non-freedriver : nvidia-387 - third-party non-freedriver : nvidia-440 - third-party non-freedriver : nvidia-455 - third-party non-freedriver : nvidia-460 - third-party non-freedriver : nvidia-384 - third-party non-freedriver : xserver-xorg-video-nouveau - distro free builtin Nvidia 그래픽 드라이버 설치 재설치 시 (기존 사용 드라이버 삭제 후 재설치) 기존 Nvidia 그래픽 드라이버 삭제 12sudo systemctl stop nvidia-persistencedsudo apt-get purge nvidia* Nvidia 커널 모듈 언로드 1234sudo modprobe -r nvidia-uvm sudo modprobe -r nvidia-drm sudo modprobe -r nvidia-modeset sudo modprobe -r nvidia 재부팅 sudo reboot Nvidia Grahic Driver 460.27.04 버전 설치 1234wget https://kr.download.nvidia.com/XFree86/Linux-x86_64/460.27.04/NVIDIA-Linux-x86_64-460.27.04.run ※ 다운로드 웹 페이지 : https://www.nvidia.co.kr/content/DriverDownloads/confirmation.php?url=/XFree86/Linux-x86_64/460.27.04/NVIDIA-Linux-x86_64-460.27.04.run&amp;lang=kr&amp;type=TITANchmod +x NVIDIA-Linux-x86_64-460.27.04.run./NVIDIA-Linux-x86_64-418.165.02.run 신규 설치 시 1234wget https://kr.download.nvidia.com/XFree86/Linux-x86_64/460.27.04/NVIDIA-Linux-x86_64-460.27.04.run ※ 다운로드 웹 페이지 : https://www.nvidia.co.kr/content/DriverDownloads/confirmation.php?url=/XFree86/Linux-x86_64/460.27.04/NVIDIA-Linux-x86_64-460.27.04.run&amp;lang=kr&amp;type=TITANchmod +x NVIDIA-Linux-x86_64-460.27.04.run./NVIDIA-Linux-x86_64-418.165.02.run 3. CUDA 11.2 설치 설치 파일 다운로드 wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux.run ※ 다운로드 참고 페이지 : https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=runfilelocal chmod +x cuda_11.2.0_460.27.04_linux.run 설치 sudo sh cuda_11.2.0_460.27.04_linux.run UI를 통해 설치 작업 진행 Continue 클릭 accept 입력 옵션 선택 Nvidia Driver까지도 한번에 선택할 경우 Driver 450.27에 체크를 수행하면됨 CUDA Samples, CUDA Demo Suite CUDA Documentation 도 체크를 수행하지 않아도 됨(필요할 경우 선택 항목만 다시 설치하면 됨) 4. cuDNN 설치 설치 파일 다운로드 다운로드 파일 위치 : https://developer.nvidia.com/rdp/cudnn-download (반드시 로그인을 수행해야함) Linux x86_64 다운로드 설치 12345tar -xvf cudnn-linux-x86_64-8.8.1.3_cuda11-archive.tar.xzmv cudnn-linux-x86_64-8.8.1.3_cuda11-archive cuda-11.2sudo cp -R cuda-11.2/include/cudnn*.h /usr/local/cuda-11.2/includesudo cp -RP cuda-11.2/lib/libcudnn* /usr/local/cuda-11.2/lib64sudo chmod a+r /usr/local/cuda-11.2/include/cudnn*.h /usr/local/cuda-11.2/lib64/libcudnn* cudnn 설치 확인 cat /usr/local/cuda-11.2/include/cudnn_version.h | grep CUDNN_MAJOR -A 2 12345678cat /usr/local/cuda-11.2/include/cudnn_version.h | grep CUDNN_MAJOR -A 2# define CUDNN_MAJOR 8# define CUDNN_MINOR 8# define CUDNN_PATCHLEVEL 1--#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)/* cannot use constexpr here since this is a C-only file */ 5. 사용자 계정 환경 변수 설정 vim ~/.bashrc 12export PATH=/usr/local/cuda-11.2/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64:$LD_LIBRARY_PATH source ~/.bashrc Cuda(nvcc) 적용 확인 nvcc -V 12345nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2020 NVIDIA CorporationBuilt on Mon_Nov_30_19:08:53_PST_2020Cuda compilation tools, release 11.2, V11.2.67Build cuda_11.2.r11.2/compiler.29373293_0 6. Docker-ce 설치 Docker-ce 설치 12345678910sudo apt updatesudo apt install apt-transport-https ca-certificates curl software-properties-common$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository \\ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable&quot;sudo apt-get updatesudo apt-get install docker-ce Old Version이 기존 설치 되어 있다면 sudo apt-get remove docker docker-engine docker.io containerd runc Docker-ce 설치 확인 sudo systemctl status docker sudo docker version 7. Nvidia-Driver 설치","link":"/Ops/%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5/CUDA-11-2-%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5/"},{"title":"Ubuntu에서 python 설치하기","text":"배경DL 프로젝트를 수행하면서 mlflow를 별도로 구축해야할 필요가 생김따라서 ubuntu에 Test용 계정을 생성하여 python 환경을 구성 설치 환경 OS : Ubuntu 16.04 Python : 3.8 작업 Ubuntu 관련 계정 생성 및 권한 설정 1234567# 계정 생성 ## -m : 홈 디렉토리가 생성## -s : /bin/bash 옵션을 명시해야 쉘 환경이 설정됨)$ sudo uesradd -m -s /bin/bash 계정명# sudo 권한 부여$ sudo usermod -aG sudo 계정명 Python 버전 확인 및 업데이트 버전확인 12345678910111213# 새로 생성된 계정의 경우, python 명령어를 치면 버전이 2버전으로 symbolic link가 걸려있기에# python3 명령어를 입력하여 버전을 확인shchoice@mls-01:~$ pythonPython 2.7.12 (default, Mar 1 2021, 11:38:31) [GCC 5.4.0 20160609] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; exit()shchoice@mls-01:~$ python3Python 3.5.2 (default, Jan 26 2021, 13:30:48) [GCC 5.4.0 20160609] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; exit() 의존성 있는 라이브러리 설치 1234$ sudo apt update$ sudo apt upgrade$ sudo apt dist-upgrade$ sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev wget Python 버전 업데이트 123456789# python 3.8.13 설치 파일 다운로드(소스코드로 설치)$ wget https://www.python.org/ftp/python/3.8.13/Python-3.8.13.tgz# tgz 파일 압축 해제$ tar -xzvf Python-3.8.13.tgz$ cd Python-3.8.13# 소스코드에 대한 환경설정 작업, 작업 결과 makefile이 생성됨$ ./configure# 시스템의 python을 덮어쓰지 않도록 altinstall을 사용$ sudo make altinstall 설치 확인 1234567# python3.8 설치 제대로 되었는지 확인$ python3.8shchoice@dmls-01:~/Python-3.8.13$ python3.8Python 3.8.13 (default, Aug 10 2022, 18:22:14) [GCC 5.4.0 20160609] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; exit() 사용자 계정에 대해 python3 명령어 alias 걸기 vim ~/.bashrc 12alias python=python3.8alias pip=pip3 source ~/.bashrc 여러 python3 버전을 설치하고 원하는 버전에 맞추어 실행하기 1234567# python 3.8을 update-alternatives에 등록(우선순위는 1로 등록)$ sudo update-alternatives --install /usr/bin/python3 python3 /usr/local/bin/python3.8 1# update-alternatives: using /usr/local/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode# python 3.7을 update-alternatives에 등록(우선순위는 2로 등록)$ sudo update-alternatives --install /usr/bin/python3 python3 /usr/local/bin/python3.7 2# update-alternatives: using /usr/local/bin/python3.7 to provide /usr/bin/python3 (python3) in auto mode (과거 경험적으로) python이 아닌 python3로 하는 이유는 Ubuntu 에서 어떠한 스크립트나 파일을 돌릴 때, python2를 사용하는 경우가 있기 때문에 개인적으로는 python 에 대한 symbolic link는 건드리지 않음 다른 Python 버전으로 변경을 하고 싶을시 다음과 같이 클릭 1234567891011$ sudo update-alternatives --config python3대체 항목 python3에 대해 (/usr/bin/python3 제공) 2개 선택이 있습니다. 선택 경로 우선순ꡬ태------------------------------------------------------------ 0 /usr/local/bin/python3.7 2 자동 모드 1 /usr/local/bin/python3.7 2 수동 모드* 2 /usr/local/bin/python3.8 1 수동 모드 Press &lt;enter&gt; to keep the current choice[*], or type selection number: 2 현재 python 3.7을 사용하는 0번이 선택되어져 있음 python 3.8 사용 희망으로 2 를 누르고 느낀점이용하는 Ubuntu 서버에서 이유는 모르겠지만 apt 패키지 매니저로 python package를 설치하면 설치가 불가능하다. 1234567search2@dq-mls-01:~$ sudo apt install python3.6패키지 목록을 읽는 중입니다... 완료의존성 트리를 만드는 중입니다 상태 정보를 읽는 중입니다... 완료E: python3.6 패키지를 찾을 수 없습니다E: Couldn't find any package by glob 'python3.6'E: 'python3.6' 정규식에 해당하는 패키지가 없습니다 sudo apt update &amp;&amp; upgrade를 수행하면 다음과 같은 에러로 수행이 불가능하기 때문일 것이라 판단한다. 12345678910111213141516171819오류:21 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease 다음 서명들은 공개키가 없기 때문에 인증할 수 없습니다: NO_PUBKEY A4B469963BF863CC무시:27 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 InRelease 기존:28 http://kr.archive.ubuntu.com/ubuntu xenial-updates InRelease 기존:29 https://apt.kubernetes.io kubernetes-xenial InRelease 기존:30 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 Release 기존:31 http://kr.archive.ubuntu.com/ubuntu xenial-backports InRelease 기존:33 http://mariadb.mirror.liquidtelecom.com/repo/10.4/ubuntu xenial InRelease 기존:34 http://ppa.launchpad.net/linuxuprising/java/ubuntu xenial InRelease 기존:35 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease 기존:36 http://ppa.launchpad.net/webupd8team/y-ppa-manager/ubuntu xenial InRelease 패키지 목록을 읽는 중입니다... 완료 E: 설치 방법 드라이버 /usr/lib/apt/methods/&lt;https을(를) 찾을 수 없습니다.E: 설치 방법 드라이버 /usr/lib/apt/methods/&lt;https을(를) 찾을 수 없습니다.E: 설치 방법 드라이버 /usr/lib/apt/methods/oops을(를) 찾을 수 없습니다.W: GPG 오류: http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease: 다음 서명들은 공개키가 없기 때문에 인증할 수 없습니다: NO_PUBKEY A4B469963BF863CCE: The repository 'http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease' is not signed.N: Updating from such a repository can't be done securely, and is therefore disabled by default.N: See apt-secure(8) manpage for repository creation and user configuration details. 여기 부분을 수정하면 apt- install 이 가능하다 판단을 하는데, 추후 해당 오류를 수정하는 방법을 찾아보아야겠다.다른 업무가 많으니.. 우선 소스코드로 설치해 업무 처리를 빠르게 하자!","link":"/Ops/%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5/Ubuntu%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5%20python%20%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8E%E1%85%B5%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"리액티브 프로그래밍","text":"리액티브 프로그래밍1. 리액티브 프로그래밍이란 리액티브 프로그래밍은 데이터 또는 이벤트의 변경이 발생하면 이에 반응해 처리하는 프로그래밍 기법을 말함 비동기 프로그래밍을 처리하는 새로운 접근 방식 데이터의 통지, 완료, 에러에 대한 처리를 옵저버 패턴에 영감을 받아 설계되었고 데이터의 손쉬운 비동기 처리를 위해 함수형 언어의 접근 방식을 사용 2010년 에릭 마이어에 의해 마이크로소프트 .NET 에코 시스템으로 정의됨 리액티브 프로그래밍 이전의 비동기 프로그래밍 리액티브 프로그래밍이 나오기 전 비동기 프로그래밍은 대부분 콜백기반의 비동기 처리 방식을 사용 간단한 콜백은 이해하기 쉬울 수 있지만 콜백이 많아져서 발생하는 콜백 헬로 인해 코드의 복잡도가 늘어남 12345678etch(&quot;/api/users/me&quot;) { user-&gt; fetch(&quot;/api/users/${user.id}/followers&quot;) { followers -&gt; fetch(&quot;/api/users/${user.id}/likes&quot;) { likes -&gt; fetch(&quot;/api/users/${user.id}/contacts&quot;) { contacts -&gt; } } }} 리액티브 프로그래밍을 적용한 사례 리액티브 프로그래밍을 사용하면 콜백 헬 문제를 함수형 프로그래밍 관점으로 해결할 수 있음 콜백 헬 없이 비동기 코드를 쉽게 작성할 수 있기 때문에 서버나 UI 애플리케이션 개발 시 리액티브 프로그래밍이 유용하게 사용됨 1234567fetchReactive(&quot;/api/users/me&quot;) .zip { user -&gt; fetchReactive(&quot;/api/users/${user.id}/followers&quot;) } .zip { user -&gt; fetchReactive(&quot;/api/users/${user.id}/likes&quot;) } .zip { user -&gt; fetchReactive(&quot;/api/users/${user.id}/contacts&quot;) } .flatMap { followers, likes, contacts -&gt; // Fh직구현 } 리액티브 스트림 리액티브 스트림은 리액티브 프로그램의 표준 API 사양을 말함 비동기 데이터 스트림과 Non-blocking Back-Pressure에 대한 사양을 제공 리액티브 스트림 이전의 비동기식 애플리케이션에서는 CPU의 멀티 코어를 제대로 활용하기 위해 복잡한 병렬 처리 코드가 필요했음 기존의 방식은 처리할 데이터가 무한정 많아져서 시프템의 한계를 넘어서는 경우 애플리케이션은 병목현상이 발생하거나 심각한 경우 애플리케이션이 정지되는 경우도 있음 Netflix, Vmware, Redhat, Twitter, Lightbend 등과 같은 유명 회사들이 표준화에 참여 중 리액티브 스트림의 다양한 구현체들 리액티브 스트림은TCK(Technology Compatibility Kit) 를 지원하기 때문에 라이브러리가 정해진 사양에 맞게 구현되었는지 보장 TCK는 라이브러리가 정해진 사양에 맞게 구현되었는지 보장하기 위해 만들어진 테스트 도구자바 진영에선 Java SE 표준을 따른 JDK인지 검증하기 위해 TCK를 사용 리액티브 스트림은 TCK만 통과한다면 각 구현체들은 표준 사양에 포함되지 않은 라이브러리만의 추가 기능도 자유롭게 지원할 수 있게 함 리액티브 스트림을 표준 사양을 채택한 대표적인 구현체들 Project Reactor RxJava JDK9 Flow Akka Streams Vert.x 리액티브 스트림 사양 리액티브 스트림 사양(specification)은 핵심 인터페이스와 프로토콜로 구성됨","link":"/Kotlin/WebFlux/%E1%84%85%E1%85%B5%E1%84%8B%E1%85%A2%E1%86%A8%E1%84%90%E1%85%B5%E1%84%87%E1%85%B3-%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC/"},{"title":"코틀린의 현재와 미래","text":"코틀린의 현재와 미래 코틀린을 배워야하는 이유 코틀린은 IntelliJ로 유명한 젯브레인사에서 만든 언어이기 때문에 IntelliJ에서 자동완성, 자바-코틀린 변환, 코루틴 등 코틀린 관련 편의 기능을 완벽하게 지원 자바는 발표된지 20년이 넘었지만 코틀린, C#, 스위프트와 같은 현대적 언어에 비해 기능 이 부족함 간결한 문법: 최근의 언어들은 간결하고 표현력이 높은 문법인 경우가 많습니다. 이에 반해 자바는 상대적으로 길고 복잡한 문법을 가지고 있습니다. 예를 들어, 코틀린과 스위프트는 람다식, 타입 추론 등의 기능을 제공하여 코드를 더 간결하게 작성할 수 있습니다. 확장성 및 유연성: 코틀린, C#, 스위프트 등 현대적 언어는 확장성과 유연성 측면에서 더 많은 기능을 제공합니다. 예를 들어, C#은 확장 메서드를 통해 기존 클래스에 새로운 메서드를 추가할 수 있으며, 스위프트는 프로토콜 지향 프로그래밍을 통해 유연한 설계를 가능하게 합니다. 표현력: 최근의 언어들은 고차함수, 패턴 매칭, 디스트럭처링 등의 고급 기능을 제공하여 코드의 가독성과 표현력을 높이고 있습니다. 이에 비해 자바는 이러한 기능들이 상대적으로 부족합니다. 언어 기능 업데이트 속도: 자바는 오랜 역사와 거대한 사용자 층을 감안할 때, 기존 코드와의 호환성을 유지하면서 새로운 기능을 추가하기가 어렵습니다. 그래서 언어 기능의 업데이트 속도가 상대적으로 느리며, 현대적 언어들이 제공하는 다양한 기능을 적용하는데 한계가 있습니다. 자바에서 Best-Practice로 불리는 기법들을 언어적 차원에서 기본 제공 이펙티브 자바, 디자인 패턴 등 자바에 비해 문법이 간결하기 때문에 가독성과 생산성이 높고 오류 가능성이 적어진다 자바와 코틀린의 차이점 자바에는 있지만 코틀린에는 없는 기능 Checked Exception Error : 시스템에 비정상적인 상황이 발생, 예측이 어렵고 기본적으로 복구가 불가능함 OutOfMemoryError, StackOverflowError, Etc Throwable : 예외 계층의 최상위 클래스 Exception: 시스템에서 포착 가능하여(try-catch) 복구 가능, 예외 처리 강제 IOException, FileNotFoundException, etc @Transactional 에서 해당 예외가 발생하면 기본적으론 롤백이 동작하지 않음 rollbackFor를 사용해야함 RuntimeException : 런타임 시에 발생하는 예외, 예외처리를 강제하지 않음 e.g. NullPointerException, ArrayIndexOutOfBoundsException, etc 코틀린에서는 자바의 Exception 계층을 코틀린 패키지로 래핑 코틀린에는 있지만자바에는 없는 기능","link":"/Kotlin/%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%8F%E1%85%A9%E1%84%90%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%86%AB%E1%84%8B%E1%85%B4-%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8C%E1%85%A2%E1%84%8B%E1%85%AA-%E1%84%86%E1%85%B5%E1%84%85%E1%85%A2/"},{"title":"파이썬과 자바의 참조 차이 비교","text":"Python과 Java의 차이점 Python 모든 것이 객체로 취급됨. 파이썬에서는 기본 데이터 타입도 객체로 취급되며, 변수는 항상 객체에 대한 참조를 저장 가변 객체(mutable objects)는 함수 내에서 변경할 수 있으며, 이러한 변경은 함수 외부에도 영향을 미침 하지만 불변 객체는 함수 내에서의 변경이 외부에 영향을 미치지 않음 Java 기본 데이터 타입과 객체 타입 사이에 구분이 있음 기본 데이터 타입: int, float, double, boolean 등과 같은 기본 데이터 타입은 값(value)으로 전달됨. 이들은 스택 메모리에 저장되며, 함수에 전달될 때 값의 복사본이 생성됨 객체 타입: String, List, 사용자 정의 클래스 객체 등은 힙 메모리에 저장되며, 변수는 이 객체의 참조를 저장. 함수에 객체를 전달할 때 참조의 복사본이 전달되므로, 함수 내에서 객체의 상태를 변경하면 원본 객체에도 영향을 미침 객체 타입은 참조로 전달되며, 함수 내에서 객체의 상태를 변경할 수 있음. 이 변경은 함수 외부에도 영향을 미침. 하지만 기본 데이터 타입은 값으로 전달되어, 함수 내에서의 변경이 외부에 영향을 미치지 않음 Java의 객체 참조 방식 2가지 Call-by-Value Java에서 모든 기본 데이터 타입(primitives)은 Call-by-Value 방식으로 전달됩니다. 이는 메서드에 인자로 전달될 때, 그 값의 복사본이 생성되어 전달된다는 의미입니다. 예를 들어, int, float, double, boolean 등과 같은 기본 타입은 이 방식을 따릅니다. 메서드 내에서 이러한 인자의 값을 변경해도 원래 변수의 값에는 영향을 미치지 않습니다. 객체 참조의 전달 (Passing Object References by Value) Java에서 객체는 실제로 참조(reference)에 의해 전달됩니다. 하지만 이것이 “Call-by-Reference”는 아닙니다. 객체를 메서드에 인자로 전달할 때, 실제로 전달되는 것은 객체의 참조의 복사본입니다. 이것이 “Call-by-Value”입니다. 이 방식으로 전달된 객체의 참조를 통해 객체의 상태를 변경하면 원본 객체에 영향을 미칩니다. 그러나 참조 자체(즉, 메모리 주소)를 변경하면 원본 참조에는 영향을 주지 않습니다. 따라서 Java에서는 기본적으로 “Call-by-Value”가 적용되지만, 객체에 대해서는 그 참조의 복사본이 전달되기 때문에 실제 객체의 상태 변경이 가능합니다. 이것은 때때로 혼동을 일으키지만, Java의 핵심적인 메모리 관리 방식 중 하나 파이썬에서의 객체 참조: Reference-by-assignment 파이썬에서는 모든 변수는 객체 파이썬에서 변수는 객체에 대한 참조로 작동한다. 즉, 변수에 객체를 할당할 때, 저장되는 것은 객체의 메모리 주소이다. 이러한 reference-by-assignment라는 개념은 파이썬의 객체 지향 프로그래밍에서 핵심적인 역할을 함 Java에서 “Call-by-Value”가 핵심적인 메모리 관리 방식을 차용하는 이유 일관된 메모리 관리: Java는 모든 함수 호출에서 ‘값에 의한 호출’(Call-by-Value)을 사용합니다. 이는 원시 타입이든 객체 참조든 간에, 함수에 전달되는 것은 항상 값의 복사본입니다. 원시 타입의 경우 실제 값이 복사되고, 객체의 경우 참조(메모리 주소)의 복사본이 전달됩니다. 이 일관된 접근 방식은 메모리 관리를 단순화하고 예측 가능하게 만듭니다. 객체 상태의 변경 가능성: 객체를 메서드에 전달할 때, 전달되는 것은 객체 참조의 복사본입니다. 이 참조를 통해 객체의 상태를 변경하면, 원본 객체에 영향을 미치게 됩니다. 이는 객체의 참조가 가리키는 실제 객체가 동일하기 때문입니다. 즉, 메서드 내부에서 객체의 필드를 변경하면 원본 객체도 그 변경을 반영합니다. 메모리 안정성: Java에서는 객체의 실제 메모리 주소를 직접 다루는 것이 허용되지 않습니다. 이는 메모리를 직접 조작하는 데서 발생할 수 있는 오류와 보안 문제를 방지합니다. 참조의 복사본을 전달하는 방식은 이러한 메모리 안정성을 유지하는 데 도움이 됩니다. 가비지 컬렉션과의 호환성: Java의 가비지 컬렉터는 메모리를 효율적으로 관리하기 위해 참조를 추적합니다. 객체에 대한 모든 참조가 사라지면, 해당 객체는 가비지 컬렉터에 의해 수집될 수 있습니다. 참조의 복사본을 전달하는 방식은 가비지 컬렉션과 잘 호환되어 메모리 누수를 방지하는 데 도움이 됩니다.","link":"/Python/%E1%84%86%E1%85%AE%E1%86%AB%E1%84%87%E1%85%A5%E1%86%B8/%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8A%E1%85%A5%E1%86%AB%E1%84%80%E1%85%AA-%E1%84%8C%E1%85%A1%E1%84%87%E1%85%A1%E1%84%8B%E1%85%B4-%E1%84%8E%E1%85%A1%E1%86%B7%E1%84%8C%E1%85%A9-%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%B5-%E1%84%87%E1%85%B5%E1%84%80%E1%85%AD/"},{"title":"파이썬에서는 왜 오버로딩이 없을까요","text":"파이썬에서는 왜 오버로딩이 없을까요파이썬에서 메서드 오버로딩이 전통적인 의미에서 지원되지 않는 주된 이유는 파이썬의 핵심 철학(간결함과 가독성)과 동적 타이핑 시스템 때문 동적 타이핑 파이썬은 동적으로 타입이 결정되는 언어로 함수나 메서드에 전달되는 인자의 타입을 미리 지정할 필요 없이, 실행 시간에 타입이 결정됨 이로 인해 동일한 함수가 다양한 타입의 인자를 받을 수 있음 기본값과 키워드 파라미터 파이썬은 함수 파라미터에 기본값을 설정할 수 있으며, 키워드 파라미터를 사용하여 함수 호출 시 파라미터의 이름을 명시적으로 지정할 수 있음 이를 통해 하나의 함수가 다양한 형태의 파라미터를 유연하게 처리 가능 1234567891011def sum_numbers(number1, number2=10): return number1 + number2# 기본값을 사용하는 경우print(sum_numbers(5)) # number2의 기본값 10이 사용되어 결과는 15# 모든 인자를 명시적으로 제공하는 경우print(sum_numbers(5, 5)) # number1=5, number2=5, 결과는 10# 키워드 인자를 사용하는 경우print(sum_numbers(number1=5, number2=20)) # number1=5, number2=20, 결과는 25 가변 인자 파이썬 함수는 가변 인자(*args, **kwargs)를 사용하여 임의 개수의 인자를 받을 수 있음 이를 통해 하나의 함수로 다양한 조합의 인자를 처리할 수 있으며, 일종의 오버로딩과 유사항 기능을 제공 1234567891011121314def print_arguments(*args, **kwargs): &quot;&quot;&quot; 주어진 모든 위치 인자와 키워드 인자를 출력합니다. &quot;&quot;&quot; for arg in args: print(arg, end=', ') for key, value in kwargs.items(): print(f&quot;{key} = {value}&quot;, end=', ')# 가변 인자 사용 예시print_arguments(1, 2, 3, name=&quot;Alice&quot;, age=30) # 1, 2, 3, name = Alice, age = 30,# print_arguments(1, 2, 3, name=&quot;Alice&quot;, 4, age=30) # 에러 발생 SyntaxError: positional argument follows keyword argument# print_arguments(name=&quot;Alice&quot;, age=30, 1, 2, 3) # 에러 발생 SyntaxError: positional argument follows keyword argument 덕 타이핑(Duck Typing) 파이썬은 객체의 타입보다는 객체가 가진 메서드와 속성에 더 집중 “오리처럼 걷고, 오리처럼 소리를 낸다면 그것은 오리다”라는 원칙에 따라, 객체의 실제 타입보다는 해당 객체가 필요한 메서드나 속성을 가지고 있는지에 초점을 맞츰 이러한 특성들 때문에 파이썬에서는 전통적인 오버로딩이 필요하지 않게 되며, 파이썬의 방식으로 함수의 유연성과 다양성을 제공","link":"/Python/%E1%84%86%E1%85%AE%E1%86%AB%E1%84%87%E1%85%A5%E1%86%B8/%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8A%E1%85%A5%E1%86%AB%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB-%E1%84%8B%E1%85%AB-%E1%84%8B%E1%85%A9%E1%84%87%E1%85%A5%E1%84%85%E1%85%A9%E1%84%83%E1%85%B5%E1%86%BC%E1%84%8B%E1%85%B5-%E1%84%8B%E1%85%A5%E1%86%B9%E1%84%8B%E1%85%B3%E1%86%AF%E1%84%81%E1%85%A1%E1%84%8B%E1%85%AD/"},{"title":"파이썬에서의 가변성과 불변성: 객체 참조의 이해","text":"파이썬에서의 가변성과 불변성: 객체 참조의 이해배경최근 아래의 smart_tokenizer_and_embedding_resize 함수를 분석하는 과정에서 궁금증이 생겼다. 함수 내에서 return 문이 명시적으로 없음에도 불구하고, tokenizer와 model 객체의 상태에 변화가 발생하는 것을 확인했다. 이러한 상태 변화의 원리를 이해하고자 조사를 해보았고, 이 현상이 파이썬의 객체 참조 메커니즘과 객체의 가변성(mutable) 및 불변성(immutable)의 특성과 밀접하게 관련되어 있음을 발견 123456789101112131415161718192021222324252627282930def smart_tokenizer_and_embedding_resize( special_tokens_dict: Dict, tokenizer: transformers.PreTrainedTokenizer, model: transformers.PreTrainedModel,): &quot;&quot;&quot;Resize tokenizer and embedding. Note: This is the unoptimized version that may make your embedding size not be divisible by 64. &quot;&quot;&quot; num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict) model.resize_token_embeddings(len(tokenizer)) if num_new_tokens &gt; 0: input_embeddings = model.get_input_embeddings().weight.data output_embeddings = model.get_output_embeddings().weight.data input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True) output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True) input_embeddings[-num_new_tokens:] = input_embeddings_avg output_embeddings[-num_new_tokens:] = output_embeddings_avgdef train()...(생략)...if tokenizer.pad_token is None: smart_tokenizer_and_embedding_resize( special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN), tokenizer=tokenizer, model=model, ) 파이썬에서의 객체 참조: Reference-by-assignment 이해하기 파이썬에서 변수는 객체에 대한 참조로 작동한다. 즉, 변수에 객체를 할당할 때, 저장되는 것은 객체의 메모리 주소이다. 이러한 reference-by-assignment라는 개념은 파이썬의 객체 지향 프로그래밍에서 핵심적인 역할을 함 가변 객체와 불변 객체: 차이점파이썬에서의 객체는 크게 두 가지 유형으로 나뉩니다: 가변 객체와 불변 객체 불변 객체 (Immutable Objects) 불변 객체는 한 번 생성되면 그 상태를 변경할 수 없는 객체 이러한 객체의 내용을 변경하려고 하면, 사실은 새로운 객체가 생성되고 기존 객체는 변경되지 않음 불변 객체의 예 숫자형 (Numbers): 정수, 부동 소수점 숫자 등 문자열 (String): 문자열은 한 번 생성되면 변경할 수 없습니다. 튜플 (Tuple): 요소의 순서와 내용이 고정되어 있습니다. 불변 객체의 특징 메모리 효율성: 동일한 값을 가진 불변 객체는 메모리에서 한 번만 생성되고, 다른 변수들이 같은 객체를 공유할 수 있음 12345678910111213# 메모리 효율성 확인을 위한 코드# 불변 객체a = 3b = 3# a와 b가 같은 객체(메모리 주소)를 참조하는지 확인print(id(a) == id(b)) # 출력: Truestr1 = &quot;Hello&quot;str2 = &quot;Hello&quot;# str1과 str2가 같은 객체(메모리 주소)를 참조하는지 확인print(id(str1) == id(str2)) # 출력: True 1234567891011def test(a): a_old_id = id(a) a = a + 1 a_new_id = id(a) print(id(a_old_id) == id(a_new_id)) # Falsea= 5test(a)print(a) # 5 123456# 가변 객체list1 = [1, 2, 3]list2 = [1, 2, 3]# list1과 list2가 같은 객체(메모리 주소)를 참조하는지 확인print(id(list1) == id(list2)) # 출력: False Thrread Safe: 여러 스레드에서 동시에 접근해도 데이터가 변경되지 않으므로 안전 Hash 가능: Dictonary의 key나 set의 element로 사용될 수 있음 가변 객체 (Mutable Objects) 가변 객체는 생성된 후에도 상태(값)를 변경할 수 있는 객체입니다. 이러한 객체의 내용을 변경하면, 원본 객체 자체가 변경됩니다. 가변 객체의 예: 리스트 (List): 요소를 추가, 삭제, 수정할 수 있음 딕셔너리 (Dictionary): 키-값 쌍을 추가, 삭제, 수정할 수 있음 집합 (Set): 요소를 추가하거나 제거할 수 있음 가변 객체의 특징: 유연성: 객체의 크기나 내용을 동적으로 변경할 수 있음 주의 필요: 가변 객체를 함수의 인자로 전달할 때는 객체 자체가 변경될 수 있으므로 주의해야 함, 특히 Thread.. Hash불가능: Dictionary의 key나 set의 element로 사용될 수 없음 상태 변화의 실제 사례 분석123456789101112131415161718def test(a, b): a = a + 1 b = b + 2 # 여기서 a와 b는 새로운 정수 객체를 참조 # 하지만 이 변경은 함수 외부로 전파되지 않음a, b = 5, 5test(a, b)print(a, b) # 5, 5def test(nums): nums[0] += 1 nums[1] += 2 numbers = [5, 5]test(numbers)print(numbers) # 출력: [6, 7] 결론파이썬에서 객체를 다룰 때 상태 변화와 관련된 현상을 명확하게 이해하는 데 도움이 됨. 특히 객체 지향 프로그래밍에서 이러한 가변성과 불변성의 개념은 코드의 동작 방식을 예측하고 효율적으로 관리하는 데 중요한 역할을 한다.","link":"/Python/%E1%84%86%E1%85%AE%E1%86%AB%E1%84%87%E1%85%A5%E1%86%B8/%E1%84%91%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8A%E1%85%A5%E1%86%AB%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5%E1%84%8B%E1%85%B4-%E1%84%80%E1%85%A1%E1%84%87%E1%85%A7%E1%86%AB%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA-%E1%84%87%E1%85%AE%E1%86%AF%E1%84%87%E1%85%A7%E1%86%AB%E1%84%89%E1%85%A5%E1%86%BC-%E1%84%80%E1%85%A2%E1%86%A8%E1%84%8E%E1%85%A6-%E1%84%8E%E1%85%A1%E1%86%B7%E1%84%8C%E1%85%A9%E1%84%8B%E1%85%B4-%E1%84%8B%E1%85%B5%E1%84%92%E1%85%A2/"},{"title":"Java Spring","text":"스프링 IoC와 빈IoC(Inversion of Control, 제어권의 역전)란 무엇인가 의존 관계 주입(Dependency injection)이라고도 불림 객체의 생성, 생명주기의 관리까지 모든 객체에 대한 제어권을 스프링 프레임워크에 맡김 객체의 생성을 책임지고 의존성을 관리 POJO 의 생성, 초기화, 서비스 , 소멸에 대한 권한을 갖음 개발자들이 직접 POJO를 생성할 수 있지만 컨테이너에 맡김 해당 내용의 의미를 코드로 살펴보자 일반적인 Java 프로그래밍 코드 프로그래머가 직접 인스턴스의 생성을 작성 123class PaymentController { private PaymentRepository paymentRepository = new PaymentRepository();} Spring Framework에서의 DI를 바탕으로 한 코드 인스턴스 및 의존성에 대한 관리를 Spring Framework가 직접해주기에 new를 직접적으로 사용하지 않아도 됨 내가 사용할 의존성의 타입(또는 인터페이스)만 맞으면 어떤거든 상관없다 그래야 내 코드 테스트 하기도 편함 PaymentController 123456789101112@Controllerpublic class PaymentController { PaymentRepository paymentRepository; public PaymentController(PaymentRepository paymentRepository) { // 해당 객체에 맞는 bean을 가져와 주입을 함(스프링이 빈들의 의존성을 관리) this.paymentRepository = paymentRepository; } public void cardPayment() { this.paymentRepository.save(); // 스프링이 아니었다면 new를 하지 않았기 때문에 NullPointException 이어야함 }} PaymentRepository 12345@Controllerpublic class PaymentRepository { public void save() { }} 하지만 Test 코드에서는 다음과 같이 직접 객체의 생성 및 의존성을 주입해야 한다. 1234567891011@RunWith(SpringRunner.class)@SpringBootTestclass PaymentControllerTest { @Test public void testCardPayment() { PaymentRepository paymentRepository = new PaymentRepository(); PaymentController paymentController = new PaymentController(paymentRepository); paymentController.cardPayment(); }} Spring IoC 컨테이너 BeanFactory 애플리케이션 컴퍼넌트의 중앙 저장소 빈 설정 소스로부터 빈 정의를 읽어 드리고 , 빈을 구성하고 제공 BEAN 스프링이 관리하는 객체 스프링 IoC 컨테이너가 관리하는 객체 최상위 인터페이스는 BeanFactory라는 인터페이스 applcation context 가 관리하는 객체에서 가져와야 함(일반객체와 차이) → 그래야 의존성 주입이 됨 초기에는 XML 방식을 사용했지만, 시간이 지나면서 annotation 방식으로 사용 그렇다면 Bean은 무엇일까? 아래의 코드들를 보자 Book Class 12345678910111213141516public class Book { private Date created; private BookStatus bookStatus; public void setCreated(Date date) { this.created = created; } public BookStatus getBookStatus() { return bookStatus; } public void setBookStatus(BookStatus bookStatus) { this.bookStatus = bookStatus; }} Spring IoC 컨테이너가 관리하는 Bean이 맞을까? Spring IoC 컨테이너가 관리하는 Bean이 아니다. (그냥 Java Bean 이라고는 할 수 있다. getter, setter 를 준수 하기때문) BookRepository 1234567@Repositorypublic class BookRepository { public Book save(Book book) { return null; }} Repository annotation을 사용했기 때문에 Spring IoC 컨테이너가 관리하는 Bean이 맞음 BookService 1234567891011121314@Servicepublic class BookService { private BookRepository bookRepository; public BookService(BookRepository bookRepository) { this.bookRepository = bookRepository; } public Book save(Book book) { book.setCreated(new Date()); book.setBookStatus(BookStatus.DRAFT); return bookRepository.save(book); }} Service annotation을 사용했기 때문에 Spring IoC 컨테이너가 관리하는 Bean이 맞음 BookStatus 123public enum BookStatus { DRAFT, PUBLISHED;} Spring IoC 컨테이너가 관리하는 Bean이 아님, 그렇다면 왜 @Service, @Repository 등은 왜 bean으로 들어가있을까/Spring IoC 컨테이너가 관리하게 했을까? 즉 Bean의 장점은 무엇일까? 의존성 관리를 받고 싶기 때문, 의존성 주입을 하려면 반드시 Bean으로 되어있어야 함 Bean의 Scope 때문 - 오직 하나만 사용하기에 적합하기 때문 - 싱글톤으로 사용하고 싶을 때 BookService나 BookRepository는 하나만 사용하면 되기 때문 Spring IoC에서 관리되는 Bean들은 모두 싱글톤 Scope으로 관리됨 싱글톤 : 객체는 하나, 즉 모두 같은 객체로 받기 때문에 효율적, 런타임 시 성능 최적화에도 유리 , Reopository 등은 사이즈가 클텐데 한번만 사용하면 이득 프로토타입 : 매번 다른 객체를 사용 라이프 사이클 인터페이스 빈이 만들어졌을 때 추가적인 작업을 하고 싶을경우 @PostConstruct ApplicationContext스프링 IoC 컨테이너의 역할 빈 인스턴스 생성 의존 관계 설정 빈 제공 ApplicationContext classPathXmlApplicationContext (XML) AnnotationConfigApplicationContext (Java) 빈 설정 빈 명세서 빈에 대한 정의를 담고 있음 이름 클래스 스코프 생성자 아규먼트(constructor) 프로퍼티(setter) … @Autowire필요한 의존 객체의 “타입”에 해당하는 빈을 찾아서 주입 @Autowired required: 기본값은 true(따라서 못찾으면 애플리케이션 구동 실패) 사용할 수 있는 위치 생성자 ( 스프링 4.3 부터는 생략 가능) Setter Feild 경우의 수 해당 타입의 빈이 없는 경우 해당 타입의 빈이 한 개인 경우 해당 타입의 빈이 여러 개인 경우 빈 이름으로 시도 같은 이름의 빈 찾으면 해당 빈 사용 같은 이름 못 찾으면 실패 같은 타입의 빈이 여러개 일 때 @Primary 해당 타입의 빈 모두 주입 받기 @Qualifier(빈 이름으로 주입) primary가 더 type safe 하기에 primary를 사용하는 것이 더 나음 동작원리 첫 시간에 잠깐 언급했던 빈 라이프 사이클을 기억하세요? BeanPostProcessor 새로 만든 빈 인스턴스를 수정할 수 있는 라이프 사이클 인터페이스 AutowiredAnnotationBeanPostProcessor extends BeanPostProcessor 스프링이 제공하는 @Autowired와 @Value 어노테이션 그리고 JSR-330의 @Inject 애노테이션을 지원하는 애노테이션 처리기 어떻게 Sptring Container 안에 빈을 생성할 수 있을까? Component Scan @Component @Repository @Service @Controller @Configuration @SpringBootApplication 을 들어가서 보면 @ComponentScan 을 확인할 수 있음, 그래서 특별히 작성 안해도 다 빈을 찾고 등록함. 만약 다른 패키지에서 의존성을 주입하는 코드가 있다면 스캔이 되지 않음, 스캔의 범위가 있다. 만약에 코드 작성중 의존성 주입이 잘 되지 않는다면 ComponentScan 범위를 확인해보자 주요기능 스캔 위치 설정 필터 : 어떤 어노테이션을 스캔할지 또는 하지 않을지 굳이 단점이라면 싱글톤 타입일 경우 초기 구동 시간이 느려질 수 있다. 만약에 이를 대처하려면 펑션을 사용한 빈 등록 방법이 있다.(리플렉션과 프록시를 사용하지 않기 때문) Component Scan @Component @Repository @Service @Controller @Configuration @SpringBootApplication 을 들어가서 보면 @ComponentScan 을 확인할 수 있음, 그래서 특별히 작성 안해도 다 빈을 찾고 등록함. 만약 다른 패키지에서 의존성을 주입하는 코드가 있다면 스캔이 되지 않음, 스캔의 범위가 있다. 만약에 코드 작성중 의존성 주입이 잘 되지 않는다면 ComponentScan 범위를 확인해보자 주요기능 스캔 위치 설정 필터 : 어떤 어노테이션을 스캔할지 또는 하지 않을지 굳이 단점이라면 싱글톤 타입일 경우 초기 구동 시간이 느려질 수 있다. 만약에 이를 대처하려면 펑션을 사용한 빈 등록 방법이 있다.(리플렉션과 프록시를 사용하지 않기 때문) 직접 등록 XML 이나 자바 설정 파일에 등록 근래에는 Java 설정 파일에 등록을 하여 더 사용 Configuration 예제 PaymentConfig 1234567@Configurationpublic class PaymentConfig { @Bean public PaymentController paymentController() { return new PaymentController(); }} PaymentController - @Controller는 이제 빼주어야함 → @Configuartion 으로 관리 12345678910111213public class PaymentController { @Autowired private PaymentRepository paymentRepository; public PaymentController() {} public PaymentController(PaymentRepository paymentRepository) { //Spring IoC 컨테이너 즉, ApplicationContext가 bean을 찾아서 넣어줌) 의존성 주입은 bean끼리만 가능함, 즉 Spring IoC 컨테이너 안에 들어있는 것 끼리만 의존성 주입이 가능함 this.paymentRepository = paymentRepository; } public void cardPayment() { this.paymentRepository.save(); // 스프링이 아니었다면 new를 하지 않았기 때문에 NullPointException 이어야함 }} 어떻게 꺼내 쓸까? @Autowired 또는 @Inject Autowired 스프링 4.3 부터는 생성자가 1개이며 생성자로 주입받는 레퍼런스 변수들이 bean으로 등록되어져 있을 경우 자동으로 주입하여 사용 가능하여 @Autowired 어노테이션 안적어도됨 생성자, 필드, Setter 등에서 사용이 가능하지만 생성자에서 Autowired 하는 것을 권장함 왜냐하면 필수적으로 사용하는 reference 없이는 instance를 만들지 못하도록 강제할 수 있기 때문이다. 순환참조가 일어나는 경우는 field, setter injection을 사용하여 해결, 생성자 injection 으로는 에러가 발생 생성자에 주입 123456789101112131415161718192021222324252627@Controllerclass OwnerController { private static final String VIEWS_OWNER_CREATE_OR_UPDATE_FORM = &quot;owners/createOrUpdateOwnerForm&quot;; @Autowired private final OwnerRepository owners; private final VetRepository vetRepository; private final ApplicationContext applicationContext; @Autowired public OwnerController(OwnerRepository clinicService, VetRepository vetService, ApplicationContext applicationContext) { this.owners = clinicService; this.vetRepository = vetService; this.applicationContext = applicationContext; } @GetMapping(&quot;/bean&quot;) @ResponseBody public String bean() { return &quot;bean: &quot; + applicationContext.getBean(OwnerController.class) + &quot;\\\\n&quot; // 싱글톤 스코프 객체 : 객체 하나를 어플리케이션 전반에서 계속해서 재사용해서 사용(아래 2개는 주소값이 같음) + applicationContext.getBean(OwnerRepository.class) + &quot;\\\\n&quot; + &quot;owners: &quot; + this.owners; // 싱글톤 스코프 객체 : 객체 하나를 어플리케이션 전반에서 계속해서 재사용해서 사용 }} 필드에 주입 12@Autowiredprivate OwnerRepository owners; setter에 주입 1234@Autowiredpublic void getOwners(OwnerRepository owners) { this.owners = owners;} 12345@Controllerpublic class PaymentRepository { public void save() {}} bean으로 등록하지 않고 주입을 하게 되면 에러가 발생함 의존성 주입이 되지 않았기 때문에 에러가 발생 예제 PaymentRepository : 일부러 @Controller 명령어를 빼버림 1234public class PaymentRepository { public void save() {}} PaymentController 123456@Controllerpublic class PaymentController { @Autowired private PaymentRepository paymentRepository;} 에러명 12345678910111213141516***************************APPLICATION FAILED TO START***************************Description:Field paymentRepository in org.springframework.samples.petclinic.payment.PaymentController required a bean of type 'org.springframework.samples.petclinic.payment.PaymentRepository' that could not be found.The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true)Action:Consider defining a bean of type 'org.springframework.samples.petclinic.payment.PaymentRepository' in your configuration.Process finished with exit code 0 @Autowired 또는 @Inject Autowired 스프링 4.3 부터는 생성자가 1개이며 생성자로 주입받는 레퍼런스 변수들이 bean으로 등록되어져 있을 경우 자동으로 주입하여 사용 가능하여 @Autowired 어노테이션 안적어도됨 생성자, 필드, Setter 등에서 사용이 가능하지만 생성자에서 Autowired 하는 것을 권장함 왜냐하면 필수적으로 사용하는 reference 없이는 instance를 만들지 못하도록 강제할 수 있기 때문이다. 순환참조가 일어나는 경우는 field, setter injection을 사용하여 해결, 생성자 injection 으로는 에러가 발생 생성자에 주입 123456789101112131415161718192021222324252627@Controllerclass OwnerController { private static final String VIEWS_OWNER_CREATE_OR_UPDATE_FORM = &quot;owners/createOrUpdateOwnerForm&quot;; @Autowired private final OwnerRepository owners; private final VetRepository vetRepository; private final ApplicationContext applicationContext; @Autowired public OwnerController(OwnerRepository clinicService, VetRepository vetService, ApplicationContext applicationContext) { this.owners = clinicService; this.vetRepository = vetService; this.applicationContext = applicationContext; } @GetMapping(&quot;/bean&quot;) @ResponseBody public String bean() { return &quot;bean: &quot; + applicationContext.getBean(OwnerController.class) + &quot;\\\\n&quot; // 싱글톤 스코프 객체 : 객체 하나를 어플리케이션 전반에서 계속해서 재사용해서 사용(아래 2개는 주소값이 같음) + applicationContext.getBean(OwnerRepository.class) + &quot;\\\\n&quot; + &quot;owners: &quot; + this.owners; // 싱글톤 스코프 객체 : 객체 하나를 어플리케이션 전반에서 계속해서 재사용해서 사용 }} 필드에 주입 12@Autowiredprivate OwnerRepository owners; setter에 주입 1234@Autowiredpublic void getOwners(OwnerRepository owners) { this.owners = owners;} 12345@Controllerpublic class PaymentRepository { public void save() {}} bean으로 등록하지 않고 주입을 하게 되면 에러가 발생함 의존성 주입이 되지 않았기 때문에 에러가 발생 예제 PaymentRepository : 일부러 @Controller 명령어를 빼버림 1234public class PaymentRepository { public void save() {}} PaymentController 123456@Controllerpublic class PaymentController { @Autowired private PaymentRepository paymentRepository;} 에러명 12345678910111213141516***************************APPLICATION FAILED TO START***************************Description:Field paymentRepository in org.springframework.samples.petclinic.payment.PaymentController required a bean of type 'org.springframework.samples.petclinic.payment.PaymentRepository' that could not be found.The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true)Action:Consider defining a bean of type 'org.springframework.samples.petclinic.payment.PaymentRepository' in your configuration.Process finished with exit code 0 또는 ApplicationContext에서 getBean()으로 직접 꺼냄 예제 PaymentController 123456789101112131415161718192021222324252627@Controllerclass PaymentController { @Autowired private PaymentRepository paymentRepository; @Autowired public void getOwners(PaymentRepository paymentRepository) { this.paymentRepository= paymentRepository; } private final ApplicationContext applicationContext; public PaymentController(PaymentRepository paymentService, ApplicationContext applicationContext) { this.owners = paymentService; this.applicationContext = applicationContext; } @GetMapping(&quot;/bean&quot;) @ResponseBody public String bean() { return &quot;bean: &quot; + applicationContext.getBean(OwnerController.class) + &quot;\\\\n&quot; // 싱글톤 스코프 객체 : 객체 하나를 어플리케이션 전반에서 계속해서 재사용해서 사용(아래 2개는 주소값이 같음) + applicationContext.getBean(OwnerRepository.class) + &quot;\\\\n&quot; + &quot;owners: &quot; + this.owners; }} 결과 123bean: org.springframework.samples.petclinic.owner.OwnerController@2aef3021 org.springframework.data.jpa.repository.support.SimpleJpaRepository@49bf2291 owners: org.springframework.data.jpa.repository.support.SimpleJpaRepository@49bf2291 오로지 빈들만 의존성 주입을 해야한다. IoC(Inversion of Control) 컨테이너 ApplicationContext(BeanFactory) bean을 만들고 bean들 사이의 의존성을 엮어주며 만들어진 bean들을 제공해준다 빈 설정 이름 또는 ID 타입 스코프 컨테이너를 직접 쓸 일은 많지 않음 빈의 스코프싱글톤 아무런 설정을 하지 않으면 기본 설정은 싱글톤 스코프의 빈이다. 프로토차입 Request Session WebSocket … 프로토타입 빈이 싱글톤 빈을 참조하면? → 아무 문제가 없음 싱글톤 빈이 프로토타입 빈을 참조하면? → 프로토타입 빈이 업데이트가 안됨 업데이트 하려면 scoped-proxy 빈을 선언하는 부분에서만 사용하는 것이기에 가장 나은 판단일듯 Object-Provider Provider (표준) Environment - 프로파일프로파일과 프로퍼티를 다루는 인터패이스 ApplicationContext extends EnvironmentCapable getEnvironment() 프로파일 빈들의 그룹 Environment의 역할은 활성화할 프로파일 확인 및 설정 프로파일 유즈케이스 테스트 환경에서는 A라는 빈을 사용하고, 배포 환경에서는 B라는 빈을 쓰고 싶음 이 빈은 모니터링 용도니까 테스트 때는 필요가없고 배포할 때만 등록이 되면 좋겠다 프로파일 정의하기 클래스에 정의 @Configuration @Profile(”test”P 메소드에 정의 @Bean @Profile(”test”) 프로파일 설정하기 Dspring.profiles.active=”test,A,B,…” @ActiveProfiles(테스트용) 프로파일 표현식 ! (not) &amp; (and) | (or) 프록시 싱글톤 객체 사용시 주의할점 프로퍼티가 공유 ApplicationContext 초기 구동 시 인스턴스 생성 Environment - 프로퍼티프로퍼티 다양한 방법으로 정의할 ㅅ ㅜ있는 설정값 Environment의 역할은 프로퍼티 소스 설정 및 프로퍼티 값 가져오기 프로퍼티에는 우선 순위가 있다. StandardServiceEnvironment의 우선순위 ServletConfig 매개변수 ServletContext 매개변수 JNDI (java:comp/env/) JVM 시스템 프로퍼티(-DKey=”value”) JVM 시스템 환경 변수(운영 체제 환경 변수) @PropertySource Environment를 통해 프로퍼티 추가하는 방법 스프링부트의 외부 설정 참고 기본 프로퍼티 소스 지원(application.properties) 프로파일까지 고려한 계층형 프로퍼티 우선 순위 제공 MessageSource국제화(i18n) 기능을 제공하는 인터페이스 ApplicationContext extends MessageSource getMessage(String code, Object[] args, String default, Locale, loc) 스프링 부트를 사용한다면 별다른 설정 필요없이 messages.properties 사용할 수 있음 messages.properties messages_ko_KR.properteis … 예제 messages.properties 1**greeting=Hello!, {0}** messages_ko_KR.properties 1greeting=안녕하세요, {0} messages_en_US.properties 1greeting=Hello!, {0} SpringMessageSourceApplication 123456@SpringBootApplicationpublic class SpringMessageSourceApplication { public static void main(String[] args) { SpringApplication.run(SpringMessageSourceApplication.class, args); }} AppRunner.java 123456789101112131415@Componentpublic class AppRunner implements ApplicationRunner { @Autowired MessageSource messageSource; @Override public void run(ApplicationArguments args) throws Exception { System.out.println(messageSource.getMessage(&quot;greeting&quot;, new String[]{&quot;스프링&quot;}, Locale.getDefault())); System.out.println(messageSource.getMessage(&quot;greeting&quot;, new String[]{&quot;스프링&quot;}, Locale.KOREA)); System.out.println(messageSource.getMessage(&quot;greeting&quot;, new String[]{&quot;Spring&quot;}, Locale.US)); // 안녕하세요, 스프링 // 안녕하세요, 스프링 // Hello!, Spring }} 출력: 각 언어에 맞는 프로퍼티 대로 결과값이 출력이 됨을 확인할 수 있다. 다만, getDefault()를 사용 시에 원하는 처음에는 기대값인 영어가 나올 것이라 생각했었다. 그 이유는 messages.properties 기본 파일이 영어이기 때문이었다. 하지만 예상과 달리 한글이 나오는 것을 확인할 수 있었음 그 이유는 Locale.getDefault()는 운영체제의 기본 언어값을 읽어오기 때문이었음, 따라서 Locale.getDefault()는 개발 시에 해당 서버의 OS의 기본 언어값이 무엇인지를 알기 위한 것으로만 사용하도록 하자! 영어 버전이 필요하다면, messages_en_US.properties 를 통해 처리하도록 하자 릴로딩 기능이 있는 메시지 소스 사용하기 ApplicationEventPublisher이벤트 프로그래밍에 필요한 인터페이스 제공, 옵저버 패턴 구현체 이벤트는 빈이 아니다. 이벤트핸들러는 다만 빈으로 등록해서 Spring에 알려야한다. ApplicationContext extends ApplicationEventPublisher publushEvent(ApplicationEvent event) 이벤트 만들기 ApplicationEvent 스프링 4.2 부터는 이 클래스를 상속받지 않아도 이벤트로 사용할 수 있음 이벤트 발생 시키는 방법 ApplicationEventPublisher.publish Event() 이벤트를 처리하는 방법 ApplicationListener&lt;이벤트&gt; 구현한 클래스 만들어서 빈으로 등록하기 스프링 4.2부터는 @EventListener를 사용해서 빈의 메소드에 사용할 수 있음 기본적으로는 synchronzied 순서를 정하고 싶다면 @Order와 함께 사용 비동기적으로 실행하고 싶다면 @Async와 함께 사용 스프링이 제공하는 기본 이벤트 ContextRefreshedEvent: ApplicationContext를 초기화 했더나 리프래시 했을 때 발생. ContextStartedEvent: ApplicationContext를 start()하여 라이프사이클 빈들이 시작 신호를 받은 시점에 발생 ContextStoppedEvent: ApplicationContext를 stop()하여 라이프사이클 빈들이 정지 신호를 받은 시점에 발생. ContextClosedEvent: ApplicationContext를 close()하여 싱글톤 빈 소멸되는 시점에 발생 RequestHandledEvent: HTTP 요청을 처리했을 때 발생. ResourceLoader리소스를 읽어오는 기능을 제공하는 인터페이스 AplicationContext extends ResourceLoader 리소스 읽어오기 파일 시스템에서 읽어오기 클래스패스에서 읽어오기 URL로 읽어오기 상대/절대 경로로 읽어오기 Resource getResource(java.lang.String location) Resource 추상화org.springframework.core.io.Resource 특징 java.net.URL을 추상화한 것 스프링 내부에서 많이 사용하는 인터페이스 추상화한 이유 클래스패스 기준으로 리소스를 읽어오는 기능 부재 ServletContext를 기준으로 상대 경로를 읽어오는 기능 부재 새로운 핸들러를 등록하여 특별한 URL 접미사를 만들어 사용할 수는 있지만 구현이 복잡하고 편의성 메소드가 부족 인터페이스 둘러보기 상속 받은 인터페이스 주요 메소드 getInputStream() exist() isOpen() getDescription(): 전체 경로 포함한 파일 이름 또는 실제 URL 구현체 UrlResource: java.net.URL 참고, 기본으로 지원하는 프로토콜 http, https, ftp, file, jar. ClassPathResource: 지원하는 접두어 classpath: FileSystemResource ServletContextResource: 웹 애플리케이션 루트에서 상대 경로로 리소스 찾는다. … 리소스 읽어오기 Resource의 타입은 locaion 문자열과 ApplicationContext의 타입에 따라 결정 된다. ClassPathXmlApplicationContext -&gt; ClassPathResource FileSystemXmlApplicationContext -&gt; FileSystemResource WebApplicationContext -&gt; ServletContextResource ApplicationContext의 타입에 상관없이 리소스 타입을 강제하려면 java.net.URL 접두어(+ classpath:)중 하나를 사용할 수 있다. classpath:me/whiteship/config.xml -&gt; ClassPathResource file:///some/resource/path/config.xml -&gt; FileSystemResource Validation 추상화org.springframework.validation.Validator 애플리케이션에서 사용하는 객체 검증용 인터페이스. 특징 어떤한 계층과도 관계가 없다. =&gt; 모든 계층(웹, 서비스, 데이터)에서 사용해도 좋다. 구현체 중 하나로, JSR-303(Bean Validation 1.0)과 JSR-349(Bean Validation 1.1)을 지원한다. (LocalValidatorFactoryBean) DataBinder에 들어가 바인딩 할 때 같이 사용되기도 한다. 인터페이스 boolean supports(Class clazz): 어떤 타입의 객체를 검증할 때 사용할 것인지 결정함 void validate(Object obj, Errors e): 실제 검증 로직을 이 안에서 구현 구현할 때 ValidationUtils 사용하며 편리 함. 스프링 부트 2.0.5 이상 버전을 사용할 때 LocalValidatorFactoryBean 빈으로 자동 등록 JSR-380(Bean Validation 2.0.1) 구현체로 hibernate-validator 사용. https://beanvalidation.org/ PSA(Portable Service Abstraction) 추상화 계층인 이유 편의성 제공 Portable 다른 것으로 바꿀 수 있음 spring5 webflux @Transactional : all or nothing","link":"/Spring/Spring%20Framework/Spring-Framework/"},{"title":"Spring MVC","text":"어떻게 GetMapping으로 요청을 처리하고 Model model이 받는 파라미터는 어떤것들이 있고 어떻게 확장할 수있는지 return은 어떻게하는지 등을 파악 Spring MVC 원리, 설정, 사용법을 배우게 됨 서블릿 소개1. Servlet 자바 엔터프라이즈 에디션은 웹 애플리케이션 개발용 스펙과 API 제공 요청 당 한 프로세스 내에서 자원을 공유하는 쓰레드를 만들거나 풀에서 가져다가 사용 그 중에 가장 중요한 클래스 중 하나가 HttpServlet 2. 서블릿 등장 이전에 사용하던 기술인 CGI(Common Gateway Interface)를 요청당 프로세스를 만들어 사용3.서블릿의 장점(CGI에 비해) 빠르다 플랫폼 독립적 보안 이식성 서블릿 생명 주기 서블릿 컨테이너가 서블릿 인스턴스의 init() 메소드를 호출하여 초기화 최초 요청을 받았을 때 한번 초기화 하고 나면 그 다음 요청부터는 이 과정을 생략 실제 필드에서는 초기에 모든 URL에 요청을 보내 모든 서블릿이 init()을 하게 하여 속도를 빠르게 처리할 수 있도록 함 서블릿이 초기화된 다음부터 클라이언트의 요청을 처리할 수 있음. 각 요청은 별도의 쓰레드로 처리하고 이때 서블릿 인스턴스의 service() 메소드를 호출 이 안에서 HTTP 요청을 받고 클라이언트로 보낼 HTTP 응답을 만든다. service()는 보통 HTTP Method에 따라 doGet(), doPost() 등으로 처리를 위임 따라서 보통 doGet() 또는 doPost()를 구현 서블릿 컨테이너 판단에 따라 해당 서블릿을 메모리에서 내려야할 시점에 destory()를 호출 SpringMVC는 HTTPServlet 기반으로 만들어짐 annotation으로 controller, requestMapping을 하는데 어떻게 HTTPServletRequest, HttpServletResponse를 사용하지 않고 사용했을까? 뷰를 어떻게 타임리프를 사용할 수 있었고 web.xml을 만들지 않고 웹 어플리케이션을 띄울 수 있었을까 서블릿 리스너와 필터서블릿 리스너 웹 어플리케이션에서 발생하는 주요 이벤트를 감지하고 각 이벤트에 특별한 작업이 필요한 경우에 사용할 수 있음 서블릿 컨텍스트 수준의 이벤트 컨텍스트 라이프사이클 이벤트 컨텍스트 에트리뷰트 변경 이벤트 세션 수준의 이벤트 세션 라이프사이클 이벤트 세션 애트리뷰트 변경 이벤트 서블릿 필터 들어온 요청을 서블릿으로 보내고, 또 서블릿이 작성한 응답을 클라이언트로 보내기 전에 특별한 처리가 필요한 경우 사용할 수 있음 체인 형태의 구조 DispatcherServlet Root WebApplicationConetext 에서는 다른 Dispathcer Servlet들도 공용으로 사용할 수 있는 자원 웹과 관련된 빈들은 Servelt WebApplicationContext는 해당 Dispatcher Servlet에 한정됨.. 서블릿 애플리케이션에 스프링 연동하기 서블릿에서 스프링이 제공하는 IoC컨테이너 활용하는 방법 ContextLoaderListener : Spring Ioc Container 즉, ApplicationContext를 서블릿 애플리케이션의 생명 주기에 맞추어 바인딩 해줌, 서블릿이 종료될때는 ApplicationContext를 종료시켜줌 스프링이 제공하는 서블릿 구현체 DispatrcherServlet 사용하기 DispatcherServlet 스프링 MVC의 핵심 Front Controller 역할을 함 DispatcherServlet 동작원리DispatcherServlet 초기화 다음의 특별한 타입의 빈들을 찾거나 기본 전략에 해당하는 빈들을 등록 HandlerMapping : 핸들러를 찾아주는 인터페이스 HandlerAdapter : 핸들러를 실행하는 인터페이스 HandlerExceptionResolver ViewResolver … DispatcherServlet 동작 순서 요청을 분석(로케일, 테마, 멀티파트 등) (핸들러 맵핑에게 위임하여) 요청을 처리할 핸들러를 찾는다 (등록되어 있는 핸들러 어탭터 중에) 해당 핸들러를 실행할 수 있는 “핸들러 어댑터”를 찾음 찾아낸 핸들러 어댑터를 사용해서 핸들러의 응답을 처리 핸들러의 리턴값을 보고 어떻게 처리할지 판단 뷰 이름에 해당하는 뷰를 찾아서 모델 데이터를 랜더링함 @ResponseBody가 있다면, Converter를 사용해서 응답 본문을 만들고 (부가적으로) 예외가 발생했다면, 예외 처리 핸들러에 요청 처리를 위임 최종적으로 응답을 보냄 HandlerMapping RequestMappingHanelerMapping : Controller에서의 GetMapping, PostMapping 등의 Handler를 찾아주는 역할을 함 HandlerAdapter RequestMappingHandlerAdapter ViewResolverViewResolver InternalResourceViewResolver Prefix Suffix 1234567891011@Configuration @ComponentScan public class WebConfig { @Bean public InternalResourceViewResolver viewResolver() { InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setPrefix(&quot;/WEB-INF/&quot;); viewResolver.setSuffix(&quot;.jsp&quot;); return viewResolver; } } 1234567@org.springframework.stereotype.Controller(&quot;/simple&quot;) public class SimpleController implements Controller { @Override public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception { return new ModelAndView(&quot;simple&quot;); } } 스프링 MVC 구성 요소https://lh4.googleusercontent.com/HBnBgU7p_hQVPspumg9wLYpOktj-mUjxt_uuvLsYlVPn7YsupUNHy1AtI_LBrLLvY3Cjl6Hy5pf6Fs5VcNjqaHkybCTnk9t3D9SsZx3piOpCIFW0McfR_50vA-InPIyfPD0RjetqLQj4yiN0r8FZcLlJuV3SiOii2xTcWuyNHhjKek5ZuKMP0ZKlDwJr DispatcherSerlvet의 기본 전략 DispatcherServlet.properties MultipartResolver 파일 업로드 요청 처리에 필요한 인터페이스 HttpServletRequest를 multipartHttpServletRequest로 변환해주어 요청이 담고 있는 File을 꺼낼 수 있는 API 제공 LocaleResolver 클라이언트의 위치(Locale) 정보를 파악하는 인터페이스 기본 전략은 요청의 accept-language를 보고 판단 ThemeResolver 애플리케이션에 설정된 테마를 파악하고 변경할 수 있는 인터페이스 참고: https://memorynotfound.com/spring-mvc-theme-switcher-example/ HandlerMapping 요청을 처리할 핸들러를 찾는 인터페이스 HandlerAdapter HandlerMapping이 찾아낸 핸들러를 처리하는 인터페이스 스프링 MVC 확장력으 ㅣ핵심 HandlerExceptionResolver 요청 처리 중에 발생한 에러 처리하는 인터페이스 RequestToViewNameTraslator 핸들러에서 뷰 이름을 명시적으로 리턴하지 않은 경우, 요청을 기반으로 뷰 이름을 판단하는 인터페이스 ViewResolver 뷰 이름(string)에 해당하는 뷰를 찾아내는 인터페이스 FlashMapManager FlashMap 인터페이스를 가져오고 저장하는 인터페이스 FlashMap은 주로 리다이렉션을 사용할 때 요청 매개변수를 사용하지 않고 데이터를 전달하고 정리할 때 사용 redirect:/events Spring MVC 동작원리 정리결국엔 굉장히 복잡한 서블릿 = DispatcherServelt DispatcherServlet 초기화 특정 타입에 해당하는 빈을 찾음 없으면 기본 전략을 사용(DispatcherServlet.properties) 스프링부트 사용하지 않는 스프링 MVC 서블릿 컨테이너(ex. 톰캣)에 등록한 웹 어플리케이션(WAR)에 DispatcherServlet을 등록 web.xml에 서블릿 등록 또는 WebApplicationInitializer 에 자바 코드를 서블릿 등록 (Spring 3.1+, 서블릿 3.0+) 세부 구성 요소는 빈 설정하기 나름 스프링부트를 사용하는 스프링 MVC 자바 애플리케이션에 내장 톰캣을 만들고 그 안에 DispatcherServlet을 등록 스프링 부트 자동 설정이 자동으로 해줌 스프링 부트의 주관에 따라 여러 인터페이스 구현체를 빈으로 등록 스프링 MVC 설정 스프링 MVC 구성요소 직접 빈으로 등록하기 @EnableWebMVC 애노테이션 기반 스프링 MVC를 사용할 때 편리한 웹 MVC 기본 설정 1234@Configuration @EnableWebMvc public class WebConfig { } WebMvcConfigurer 인터페이스 @EnableWebMvc 가 제공하는 빈을 커스터마이징할 수 있는 기능을 제공하는 인터페이스 12345678@Configuration @EnableWebMvc public class WebConfig implements WebMvcConfigurer { @Override public void configureViewResolvers(ViewResolverRegistry registry) { registry.jsp(&quot;/WEB-INF/&quot;, &quot;.jsp&quot;); } } 스프링 부트의 스프링 MVC 설정https://lh6.googleusercontent.com/D3i-tOMRbgwI-D_F3lw8R6drbp3nxCstmFddjAEQsRq5sIuf9twcYERm3N5df1k0J_1VieOzf0ffkonI-Bs00cIuCviTgD9HMnGejgW8c_ubm4aPP07gmcIFZLnapJDodR4wOQthhk163j9dBwIGAUhvhFW6QycGrSXHkp3xgQlZocK7rjYRS5uCcG_R 스프링 부트의 “주관”이 적용된 자동 설정이 동작한다. JSP 보다 Thymeleaf 선호 JSON 지원 정적 리소스 지원 (+ 웰컴 페이지, 파비콘 등 지원) 스프링 MVC 커스터마이징 application.properties @Configuration + Implements WebMvcConfigurer: 스프링 부트의 스프링 MVC 자동설정 + 추가 설정 @Configuration + @EnableWebMvc + Imlements WebMvcConfigurer : 스프링 부트의 스프링 MVC 자동설정 사용하지 않음 스프링 부트에서 JSP 사용하기“If possible, JSPs should be avoided. There are several known limitations when using them with embedded servlet containers.” https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-feature s-spring-mvc-template-engines 제약 사항 JAR 프로젝트로 만들 수 없음, WAR 프로젝트로 만들어야 함 Java -JAR로 실행할 수는 있지만 “실행가능한 JAR 파일”은 지원하지 않음 언더토우(JBoss에서 만든 서블릿 컨테이너)는 JSP를 지원하지 않음 Whitelabel 에러 페이지를 error.jsp로 오버라이딩 할 수 없음. 참고 https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-jsp-li mitations https://github.com/spring-projects/spring-boot/tree/v2.1.1.RELEASE/spring-boot-samples /spring-boot-sample-web-jsp (샘플 프로젝트) 의존성 추가 123456789&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 태그 선언 1&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;&lt;http://java.sun.com/jsp/jstl/core&gt;&quot;%&gt; application.properties 12spring.mvc.view.prefix=/WEB-INF/jsp/spring.mvc.view.suffix=.jsp WAR 파일 배포하기java -jar를 사용해서 실행하기 https://lh3.googleusercontent.com/CcYem9DNu8vSWWn7Rd9AXY2DeFLkci44FewsmGuNmlyrZqR7kdHc4znCeRHcFu2wCECGfl8ElRk3qDUHMJjPE6aJts-Q0FSk_uykjoUnjIvVMbNc5v8PrDi4X_KhtgMMCwEb5vMKtBMzAJ03bOhGRbkmxgXhNviQ5PO4eKbgcoPIrnZuAa4D-gQSbwmF SpringApplication.run 사용하기 서블릿 컨테이너에 배포하기 https://lh3.googleusercontent.com/QRYt58b7-JGHWUv2-LwTTdxN7jGr2q3qxqMHcdjsoJqxSs6C_KcnBGGtwGNe0mNIev5hpF2VVbLTbWo2mCvVFWAlB1tRJ1SSR4tCqRNOQ1R4uF6FDj0QVvta5u5sAO8f7ADX45RZEnsBQnTah6FDOiS-5X_mGaXhBHLyxYJJZT1bofGsW6XH93gDANPS SpringBootServletInitializer (WebApplicationInitializer) 사용하기 포매터 추가하기 https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/servlet/config/annotation/WebMvcConfigurer.html#addFormatters-org.springframework.format.FormatterRegistry Formatter Printer : 해당 객체를(Locale 정보를 참고하여) 문자열로 어떻게 출력할 것인가 Parser : 어떤 문자열을(Locale 정보를 참고하여) 객체로 어떻게 변환할 것인가 https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/format/Fo rmatter.html 포매터 추가하는 방법 1 WebMvcConfigurer의 addFormatters(FormatterRegistry) 메소드 정의 포매터 추가하는 방법 2 (스프링 부트 사용시에만 가능 함) 해당 포매터를 빈으로 등록 도메인 클래스 컨버터 자동 등록스프링 데이터 JPA는 스프링 MVC용 도메인 클래스 컨버터를 제공 도메인 클래스 컨버터 스프링 데이터 JPA가 제공하는 Repository를 사용해서 ID에 해당하는 엔티티를 읽어옴 의존성 설정 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;/dependency&gt; 엔티티 맵핑 123456@Entity public class Person { @Id @GeneratedValue private Integer id; ... Repository 추가 1public interface PersonRepository extends JpaRepository&lt;Person, Integer&gt; { } 테스트코드 수정 테스트용 이벤트 객체 생성 이벤트 Repository에 저장 저장한 이벤트의 ID로 조회 시도 핸들러 인터셉터 https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/servl et/config/annotation/WebMvcConfigurer.html#addInterceptors-org.springframework.web.servlet. config.annotation.InterceptorRegistry HandlerInterceptor 핸들러 맵핑에 설정할 수 있는 인터셉터 핸들러를 실행하기 전,후(아직 렌더링 전) 그리고 완료(렌더링까지 끝난 이후) 시점에 부가 작업을 하고싶은 경우에 사용할 수 있음 여러 핸들러에서 반복적으로 사용하는 코드를 줄이고 싶을 때 사용할 수 있음 로깅, 인증 체크, Locale 변경 등 boolean preHandle(request, response, handler) 핸들러 실행하기 전에 호출됨 핸들러에 대한 정보를 사용할 수 있기 때문에 서블릿 필터에 비해 보다 세밀한 로직을 구현할 수 있음 리턴갑으로 계속 다음 인터셉터 또는 핸들러로 요청, 응답을 전달할지(true) 응답 처리가 이곳에서 끝났는지(false) 알린다. void postHandle(request, response, modelAndView) 핸들러 실행이 끝나고 아직 뷰를 렌더링 하기 이전에 호출됨 뷰에 전달할 추가적이거나 여러 핸들러에 공통적인 모델 정보를 담는데 사용할 수 있음 이 메소드는 인터셉터 역순으로 호출됨 비동기적인 요청 처리 시에는 호출되지 않음 void afterCompletion(request, response, handle, ex) 요청 처리가 완전히 끝난 뒤(뷰 렌더링 끝난 뒤)에 호출됨 preHandler에서 true가 리턴한 경우에만 호출 됨 이 메소드는 인터셉터 역순으로 호출됨 비동기적인 요청 처리 시에는 호출되지 않음 vs 서블릿 필터 서블릿 보다 구체적인 처리가 가능 서블릿은 보다 일반적인 용도의 기능을 구현하는데 사용하는게 좋다. 참고 https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/w eb/servlet/HandlerInterceptor.html https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/w eb/servlet/AsyncHandlerInterceptor.html http://forum.spring.io/forum/spring-projects/web/20146-what-is-the-difference-between-u sing-a-filter-and-interceptor (스프링 개발자 Mark Fisher의 서블릿 필터와의 차이점에 대한 답변 참고) 핸들러 인터셉터 구현 123456789101112131415public class GreetingInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(&quot;preHandle 1&quot;); return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(&quot;postHandle 1&quot;); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(&quot;afterCompletion 1&quot;); } } 핸들러 인터셉터 등록 12345678910@Configuration public class WebConfig implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new GreetingInterceptor()).order(0); registry.addInterceptor(new AnotherInterceptor()) .addPathPatterns(&quot;/keeun&quot;) .order(-1); } } 특정 패턴에 해당하는 요청에만 적용할 수도 있음 순서를 지정할 수 있음 리소스 핸들러 https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/web/servl et/config/annotation/WebMvcConfigurer.html#addResourceHandlers-org.springframework.web. servlet.config.annotation.ResourceHandlerRegistry 이미지, 자바스크립트, CSS 및 HTML 파일과 같은 정적인 리소스를 처리하는 핸들러를 등록하는 방법 디폴트(default) 서블릿 서블릿 컨테이너가 기본으로 제공하는 서블릿으로 정적인 리소스를 처리할때 사용 https://tomcat.apache.org/tomcat-9.0-doc/default-servlet.html 스프링 MVC 리소스 핸들러 맵핑 등록 가장 낮은 우선 순위로 등록 다른 핸들러 맵핑이 “/” 이하 요청을 처리하도록 허용하고 최종적으로 리소스 핸들러가 처리하도록 DefaultServletHandlerConfigurer 리소스 핸들러 설정 어떤 요청 패턴을 지원할 것인가 어디서 리소스를 찾을 것인가 캐싱 ResourceResolver: 요청에 해당하는 리소스를 찾는 전략 캐싱, 인코딩(gzip, brotli), WebJar, … ResourceTransformer: 응답으로 보낼 리소스를 수정하는 전략 캐싱, CSS 링크 ,HTML5 AppCache, … 스프링 부트 기본 정적 리소스 핸들러와 캐싱 제공 참고 https://www.slideshare.net/rstoya05/resource-handling-spring-framework-41","link":"/Spring/Spring%20MVC/Spring-MVC/"},{"title":"(1) 통계학 기본 - 통계학이란","text":"Intro전공으로 통계를 공부하지 않았기 때문에 머신러닝에 대한 대학 원서나 확률과 통계에 대한 개념을 접할 때 간혹 전문용어 등이 나타날 때 갸우뚱하는 경우가 있었다. 그래서 용어에 대한 개념체계를 명백히 하면서 자격증이 남을 수 있는 ADsP 를 준비하게 되었다.따라서 전반적으로 엔지니어로써의 관점보다는, 원론적인 내용에 대해 알아두면 언젠가는 도움이 되지 않을까라는 지적 호기심의 관점으로 작성하였다.(대부분의 내용은 이기적 ADsP 책을 기본으로 하였습니다.) 통계학(Statistics)이란 관심 대상인 모집단의 특성을 파악하기 위해 문제 : 모집단을 조사하기란 매우 어려움, 현실세계에서 99%이상은 표본추출로 실행할 것임 모집단으로부터 관련된 일부 자료인 표본을 수집하고 표본추출(sampling) 수집된 표본의 자료를 요약하여 표본의 특성을 파악하고 기술통계학(Descriptive Statistics) 표본의 자료를 이용하여 모집단의 특성에 대해 확률을 이용해 추론(95% 신뢰도로, σ=1.96)하는 학문 추론통계학(Inferential Statistics) ) 기술통계학(Descriptive Statistics) 조사 및 측정된 자료를 통해 그 자료가 가지고 있는 특징을 수치, 표, 그래프로 정리하는 과정 수치 중심위치 평균(산술평균, 기하평균, 조화평균, 가중평균) 중앙값 최빈값 산포경향 범위 편차 표준편차 변동계수(Coefficient of Variation) 표 그래프 추론통계학(Inferential Statistics) 모집단으로부터 플을 추출 및 분석하여 그 결과로부터 전체 모집단에 대한 특성을 예측하는 과정 추리통계학 작업 가설형태 통계적 결정오류 및 통계적 유의도 가설검증 추리통계학 검증방법 모수에 의한 검증 T-test ANOVA Z 검증 방법 비모수 통계 검증 카이제곱 검증 Mann-Whitney U 검증 Kruskal-Wallis 검증 기본 용어 정리 모집단과 표본 모집단(Population) 관심 있는 연구대상 전체의 집합 무한모집단: 모집단의 크기가 무한한 경우(전 세계인구, 축구를 좋아하는 사람들의 수) 유한모집단: 모집단의 크기가 유한한 경우(A 대학교 재학생의 수) 전수조사(Census) 관심있는 모집단 전체를 조사하는 경우로서 주로 모집단의 규모가 작을 경우에 실시 전수조사의 어려움 조사 불가능 (모집단 전체를 대상으로 조사자체가 불가능) 엄청난 시간과 비용 소모 해결책 전수조사 -&gt; 표본조사 표본(Sample) 실제로 조사 및 측정되는 모집단의 일부 조사하기 위해 추출한 모집단의 일부 원소들 표본조사 모집단에서 추출된 일부분인 표본을 가지고하는 조사 수집방법 실험(Eperiment) 조사(Survey) 출판 자료(Published data) 모수(Parameter) 와 통계량(Statistic) 모수(Parameter) 통계적 관점에서의 의미 모집단에 대한 수치 특성값 모집단의 특성을 나타내는 양적인 측도로서 주어진 모집단을 따르는 고유의 상수(상수=모집단은 진실된 하나의 값임) 모집단의 특성(모평균(𝜇), 모분산(𝜎) 등)을 나타내는 값으로 모집단을 전수조사해야만 알 수 있는 값 실제로는 모집단의 크기와 범위가 너무 방대하기에 전수조사를 실시하지 않고 표본 조사를 하는데, 표본평균, 표본분산 등으로 모평균, 모분산등을 추정할 수 있음 표본 관측으로 구하고자 하는 모집단에 대한 정보 예시 서울시 송파구 소재의 고등학교 사교육비 평균 확률적 관점에서의 의미 정규 분포(Normal Distribution)의 경우 모수: 평균 (μ, Mean), 분산(σ², Variance) 수학적 관점에서의 의미$$f(x) = ax^2+bx+c$$ 모수는 a,b,c를 뜻함, x는 함수의 인수(the function’s argument) 특정 시스템을 정의하거나 분류하는데 도움이 되는 특성 함수의 특정한 성질을 나타내는 변수를 의미 일반적으로 θ로 표현 통계량(Statistic) 표본에서 얻은 수치 특성값 표본의 특성을 나타내는 양적인 측도로서 모집단의 분포를 따르는 확률변수 확률변수 = 표본에 따라 값이 변함을 의미 표본평균(x̅), 표본표준편차(𝑠) 예시 우리나라 고등학교 1학년 중에서 1000명만 뽑아 조사하여 얻은 평균 사 교육비 표본오차와 통계적 추론 표본오차(Sampling Error) 모집단에서 표본을 추출해서 조사하기 때문에, 모수와 표본 통계량 사이에 생기는 오차 표본의 크기를 크게함으로써 표본 오차를 감소 -&gt; 통계학에서 표본의 크기를 크게 하라는 이유 표본오차는 아무리 표본을 크게 해도 전수조사를 하지 않는 이상 존재 표본오차의 허용범위를 확률로 구하는 것이 통계의 목적 통계적추론 우리가 실제로 알고 싶은 것은 표본의 값(통계량: statistic)이 아니고 모집단의 값(모수:parameter) 통계학의 목적 : 추론(Inference) -&gt; 표본에서 구한 값을 이용해 우리가 구하고자하는 모집단의 값도 어떠할 것이다 추론","link":"/Deep%20Learning/%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB/01.%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%8B%E1%85%B5%E1%84%85%E1%85%A1%E1%86%AB/"},{"title":"3장. 딥러닝 Overview - Working Process","text":"우리의 목표!주어진 데이터에 대해서 결과를 내는 가상의 함수를 모사하는 함수를 만드는 것 예시 주어진 숫자 그림을 보고 맞추기! https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d Working Process https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3 문제 정의 가장 중요한 부분 풀고자 하는 문제를 단계뼐로 나누고 simplify 하여야 한다. 신경망이라는 함수에 넣기 위한 x와 결과값 y가 정의되어야 한다. 𝑦=𝑓(𝑥) : 라면 끓는 이미지를 넣으면 물의 온도가 나온다 등 데이터 수집 문제 정의에 따라 정해진 𝑥와 𝑦 풀고자 하는 문제의 영역에 따라 수집 방법이 다름 NLP, CV : crawling 데이터분석 : 실제 수집한 데이터 필요에 따라 레이블링(labeling) 작업을 수행 자동적으로 label이 y로 주어질 수도 있지만, 대부분 레이블이 따로 필요함, 비지도학습 기대하지 말자 데이터 전처리 및 분석 수집된 데이터를 신경망에 넣어주기 위한 형태로 가공하는 과정 입출력 값을 정제(Cleansing &amp; normalization) 이 과정에서 탐험적 분석(EDA)이 필요 데이터 알맞은 알고리즘 찾기 위함(NLP, CV 생략되기도) CV의 경우 데이터 증강(augmentation)이 수행됨 rotation, flipping, shifting 등의 연산 알고리즘 적용 데이터에 대해 가설을 세우고, 해당 가설을 위한 알고리즘(모델)을 적용 평가 문제 정의에 따른 공정하고 올바른 평가 방법 필요 (가설을 검증하기 위한 실험 설계) 테스트 셋 구성 너무 쉽거나 어렵다면 판별력이 떨어짐, 실제 데이터와 가장 가깝게 구성되야함 정성적 평가(extrinsic evaluation)와 정성적 평가(intrinsic evaluation)로 나뉨 배포 학습과 평가가 완료된 모델 weights 파일을 배포함 RESTful API 등을 통해 wrapping 후 배포 데이터 분포의 변화에 따른 모델 업데이트 및 유지보수가 필요할 수 있음","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-Working-Process/"},{"title":"(2) 통계학 기본 - 데이터 구성","text":"통계분석 - 자료분석데이터 구성 데이터는 기본적으로 **관측치(행)**와 **변수(변수)**들로 구성 관측치(Observations) 모집단으로부터 추출된 표본의 수 변수(Variable) 속성 혹은 특성이라고 함 독립변수와 종속변수 변수들 간의 상호 관련성, 즉 인과관계가 있는지에 따라 구분 $$y=f(x)$$ x를 독립변수, y를 종속변수라 한다. 온도에 따른 아이스크림의 판매량 상관관계를 예시로 들었을 때, 날씨가 영하일 경우 아이스크림의 판매량이 떨어지고, 날씨가 더울 경우 아이스크림 판매량이 늘어나는 것처럼 원인이 되는 변수를 독립변수 원인의 결과인 아이스크림 판매량은 독립변수라 한다. 구분 내용 독립 변수(Qualitative variable) 설명변수로 원인이 되는 변수 종속 변수(Quantitative variable) 결과변수로 독립변수에 영향을 받아서 결과가 되는 변수 질적/양적 변수 속성을 수량화 할 수 있는가에 따라 구분 구분 내용 질적 변수(Qualitative variable) 수치로 나타낼 수 없는 변수 (ex. 회사명, 직종, 혈액형 등) 양적 변수 (Quantitative variable) 수치로 나타낼 수 있는 변수 (ex. 체중, 온도, 나이, 키 등) 이산변수 및 종속변수 변수가 어떠한 값이라도 가질 수 있는지 아니면 특정 수치만 가질 수 있는지에 따라 구분 구분 내용 이산 변수(Discrete variable) 셀 수 있는 정수 값을 갖는변수(ex. 학생수, 직원수 등) 연속 변수(Continuous variable) 연속적인 모든 실수 값을 갖는 변수 (ex. 길이, 무게, 온도 등) 명목변수와 서열변수 구분한 범주기 서열이 존재하는지에 따라 구분 구분 내용 명목 변수(Discrete variable) 자료를 서로 다른 범주로 구분, 각 범주에 수치를 부여(ex. 남자-1 여자-2 등) 서열 변수(Continuous variable) 자료에 서열을 부여하기 위해서 수치를 사용(ex. 5는 만족 3은 보통 1은 불만족으로 구분) 등간변수와 비율변수 구분 내용 등간변수 (Interval variable) 자료를 서열뿐 아니라 상대척 차이까지 제시(ex. 온도 20도와 30도의 차이는 10도 이다. 비율 변수 (Ratio variable) 자료를 분류, 서열, 차이와 함께 절대영점까지 표현 (ex. 키 180cm = 90cm * 2) 잔차(Residual) 관측값과 계산 값의 차이를 의미하며, 잔차를 바탕으로 각종 오차를 계산 통계분석 - 표본조사(Sample) 표본조사란 모집단의 특성을 나타내는 일부 표본을 추출하기 위해서 자료를 수집하는 행위 모집단은 대상이 너무에 현실적으로 모집단을 전수조사하는 것은 불가능 모집단을 전수조사하는 것보다 표본 조사가 오히려 오차가 적을 수 있음 표본방법 확률표집 정의 수학적인 지침에 의해 선정되는 표본추출법 표본의 오차를 계산해야 함 종류 단순 무작위 표집(Simple random sampling) 랜덤하게 묘수에서 표본을 반복적으로 추출하는 것으로 난수표 및 체계적 표집법을 사용 층화표집법(Stratified sampling) 모집단을 기준에 따라서 소집단으로 분류하고, 각 소집단으로부터 무작위로 표본을 추출 모집단에 대한 특성을 이해해야 하기에, 소집단 구분에 많은 비용과 노력이 발생 발생비율이 낮은 소집단은 해당 표본을 찾기 어려운 문제가 있음 군집표집법(Clustser sampling) 모집단을 특정 군집으로 분류하고, 군집 중 일부를 선택해서 군집의 모든 구성원을 전수조사 하는 방법 비확률표집 정의 수학적으로 계산할 수 없는 경우에 사용되는 표본추출법 표본의 오차를 계산할 수 없음 방법 할당표본추출법(Quota sampling) 가장 널리 사용되는 방법으로 모집단의 특성이 잘 반영되도록 특성별로 비례해서 표본을 추출 사전에 정해놓은 분류기준에 의해서 집단을 소집단으로 분류하고 집단별 대상을 선정 마케팅 조사, 연령별, 성별 설문조사 등에 사용됨 편의표본추출법(Convenience sampling) 가장 간단한 형태로 임의의 선정지역, 조사시간 등을 정의해 표본을 선택하는 방법 표본추출 비용이 거의 발생하지 않고 조사가 아주 간단 추출된 표본이 모집단을 대표하지 못함 응답거부자의 특성이 반영되지 않음 판단표본추출법(Purposive sampling) 모집단의 의견을 반영할 수 있을 것이라고 판단될 때 사용하는 방법 조사문제에 대해서 잘 알고있을 때 사용 적은 비용으로 의미 있는 자료를 수집할 수 있는 장점 모집단의 성격을 대표하지 못할 수 있음 통계분석 - 기술통계 분석 통계에는 기술통계와 추리통계 2가지가 있다. 기술통계 : 표 및 그래프, 객관적인 수치를 사용해 데이터를 요약 추리통계 : 확률을 사용해서 모수와 표본 간의 오차 범위를 예측 기술통계 분석 구분 요약방법 자료 정리 그래프 질적자료 도표그래프 도수분포표분할표 막대그래프(Bar Chart, 빈도)막대그래프(Bar Chart, 퍼센트)원 그래프(Pie Chart) 연속자료 수치그래프 산술평균중앙값조화평균 히스토그램(Histogram)상자그래프(Box Plot)시계열(Time Series)산점도(Scatter Plot) 질적자료 기술통계 도수분포표(Frequency Table) 수집된 자료애 대해서 적절한 등급으로 분류해서 정리한 표 각 자료에 대해서 출현 빈도를 계산하고 여러 구간으로 분료한 표 관측값을 여러 개의 그룹으로 나누고 관측값의 수를 요약 정리한 표 이러한 표를 바탕으로 그래프(Bar/Pie Graph)를 그린다 분할표(Contingency Table) 질적변수가 2개일 때 관측치를 몇 개의 그룹으로 분할해서 빈도수를 정리한 것 예시 - 2개의 질적 범주(성별, 플레이 유무) 연속자료 기술통계 자료의 분포 특성을 파악하기 위해서 숫자로 표현 중심위치와 산포경향을 파악하여 요약 중심위치 산포경향 관측 잘가 어디에 집중되어 있는지를 분석 자료가 중심위치를 기준으로 어느 정도 흩어져 있는지를 분석 산술평균, 중앙값, 최빈값, 기하평균, 조화평균, 가중편균 범위, 편차, 표준편차 중심위치 평균은 자료의 분포상 무게중심을 하지만, 평균은 데이터의 분포에 따라 역할을 제대로 하지 못할 수 있다. 아래의 예는 평균은 178.46이지만 정작 170-180의 사람은 가장 적은 것을 볼 수있음 따라서 표존편차를 함께 사용한다. 표준편차는 평ㅇ균을 기준으로 데이터가 떨어져 있는 거리 값으로 표준편차가 크게 나타난다면 평균을 기준으로 데이터가 멀리 흩어져 있다는 것을 알 수 있다. 평균, 중앙값, 최빈값 구분 평균(Mean) 중앙값(Median) 최빈값(Mode) 의미 자료의 합을 개수로 나눈 값 작은값부터 큰 값을 나열할 때 중앙에 있는 값 자료 중 빈도가 가장 많이 나타나는 값 장점 자료의 값을 모두 사용 극단적인 값이 있을 때 자료의 특성을 잘 반영 쉽게 계산이 가능 단점 극단적인 값이 있으면 자료의 특성을 잘 반영하지 못함 모든 자료를 사용하지 않음 자료의 수가 적으면 중심 경향을 잘 바영하지 못함 가중평균(Weighted mean) 자료의 중요도 및 영향 등에 따라서 가중치를 반영한 평균값 기하평균(Geometric mean) 변화량을 계산할 때 많이 사용 연간 경제성장률, 물가 인상률, 연간 이자율 등과 같은 곳에 사용 2010년부터 2022년까지 경제 성장률에 대한 평균, 즉 2010년의 기준에서 2022년의 경쟁률로 발전하려면 매년 몇 퍼센트가 증가해야 하는가를 의미 조화평균(Harmonic mean) 여러 단위가 결합될 때 평균적인 변화를 계산 상대적인 비를 계산 예시 경기도에서 서울까지 자동차로 다녀왔는데 갈 때에는 고속도로를 이요하여 100km/h로 갔고, 올때는 국도를 이용해서 70km.h로 돌아왔다면 왕복 평균 속도는 얼마인가? 평균이라면 85이지만 조화평균의 경우 82.3km/h이다 $$H= \\frac{n}{\\frac{1}{a_1}+…+\\frac{1}{a_n}} = \\frac{2}{\\frac{1}{100}+\\frac{1}{70}}=2/(0.0143+0.01)=82.3$$ 절단평균(Trimmed Mean) 가장 큰 값과 작은 값을 잘라내고 산술평균을 구한 것 산포경향 범위 분산 자료가 평균을 중심으로 얼마나 븐포하고 있는가를 수치로 나타냄 확률변수가 기대값으로부터 얼마나 떨어진 곳에 분포하는지를 나타내는 숫자값 표준편차 자료의 분포와 변동에 대한 정보를 제공 자료에 있는 이상치를 점검 가설검정을 함 체비쇼프의 정리(Chebyshev’s Theorem) 어떤 k에 대해서 적어도 자료의 (1-(1/k*2)) 만큼의 비율이 k표준편차 내에 있음을 의미 k=2 이면 3/4인 75%의 자료가 2 표준편차 내에 있다는 것을 의미 k= 3이라면 88.9%의 자료가 3 표준편차 내에 있다는 것을 의미 변동계수(Coefficient of Variation) 측정 단위가 다른 자료나 자료 값의 차이가 너무 큰 경우 사용 상대 표준편차라고도 한다, 즉 상대적인 산포를 계산한다 변동계수는 표준편차를 산술평균으로 나눈 값이다.$$CV=\\frac{σ}{\\overset{-}{x}}$$ 그래프 히스토그램(Histogram) 각 구간별 현황 및 대칭 여부를 확인하기 위해서 히스토그램을 사용할 수 있음 데이터의 이상값(outler) 유무를 확인할 수 있음 상자그림(Box Plot) 대칭여부, 이상값, 자료의 분포, 최대값, 최소값, 중위값 등을 확인할 수 있음 사분위수 사분위수 설명 제1사분위수(Q1) 데이터의 25%가 이 값보다 작거나 같음 제2사분위수(Q2) 중위수 데이터의 50%가 이 값보다 작거나 같다 제3사분위수(Q3) 데이터의 75%가 이 값보다 작거나 같다 사분위간범위(InterQuantile Range) 재1사분위수(Q1)과 제3사분위수(Q3) 간의 거리(Q3-Q1)이고, 데이터의 50%의 범위 Boxplot의 의미 박스의 길이가 길면 자료가 넓게 펴져있는 것 박스의 길이가 짧으면 자료가 평균을 중심으로 모여있는 것 시계열(TimeSeries) 분석 시계열 분석이란 시계열 데이터는 관측치가 시간적 순서를 가지고 있음 미래를 예측하는 것을 목적으로 함 시계열 데이터를 사용해서 추세(Trend)분석, 원인 예측, 전망 등을 분석 주가 환율, 거래량 변동, 기온, 습도 등의 데이터이다. 시계열 데이터 구성요소 구성요소 설명 추세(Trend) 기술혁신, 인구증가, 문화의 변화 등과 같이 장기간에 걸쳐 일정한 방향으로 지속적으로 사읏ㅇ하거나 하강하는 경향이다. 계절적 변동(Seasonal variation) 봄, 여름, 가을, 겨울에 따라서 특정 소비가 증가하거나, 감소하는 형태로 나타남 주기적 변동(Cyclical variation) 경기동향, 실업률, 이자율과 같이 일정한 주기를 가지고 장기간에 걸쳐 변동됨 임의 변동(Random variation) 불규칙 변동이라고 하며, 우연한 요인에 의해 발생되기 때문에 패턴을 가지고 있지 않음 추세(Trend)분해 방법 방법 내용 Lowess.Loess 회귀 특정 범위에 다항 회귀선을 구하여 병합하는 방법 이동평균(Moving Average) 특정 기간 동안의 값의 평균변화를 분석 통계분석 - 추리통계1. 확률 확률은 어떤 사건이 발생할 가능성으로 0과 1사이의 숫자로 표현 표본 자료를 사용해서 구한 통계량과 모집단의 모수를 추론 2. 확률계산 표본공간(Sample space) S로 표시하며 통계적 시험에서 발생할 수 있는 경우의 수 사건(Event) 특정 결과가 발생하는 모임으로 A,B,C 등으로 표현 P(A) : 사건 A가 일어날 확률 n(A) : 사건 A가 일어날 수 있는 경우의 수 n(S) : 전체 경우의 수 $$확률 = \\frac{사건이 발생할 경우의 수}{전체경우의수}$$ $$P(A) = \\frac{n(A)}{n(S)}$$ 3. 추리통계(Inferential Statistics)란기술통계에서 자료의 특성이 분석되면 표본을 사용하여 모집단의 특성을 추정하는 분석 기술통계와 추리 통계의 차이점 기술통계 추리통계 수집한 데이터의 특성을 파악하기 위해서 요약정리하는 통계방법 수집한 데이터에서 표본(sample)을 추출하여 모집단의 특성을 추정 평균, 중위값, 최빈수, 범위, 분산, 표준편차와 같은 분석으로 데이터의 특성을 파악 표본을 사용해서 미래를 예측하는 것차이검정 및 관계검정 등 추리통계학 작업 가설형태 통계적 결정오류 및 통계적 유의도 가설검증 추리통계학 검증방법 모수에 의한 검증 T-test ANOVA Z 검증 방법 비모수 통계 검증 카이제곱 검증 Mann-Whitney U 검증 Kruskal-Wallis 검증 통계분석 - 통계적 추론1. 확률분포 확률변수란 확률변수가 특정 값을 가질 확률을 나타내는 함수 통계량을 분석하여 통계적 의사결정을 내릴 수 있는 기준을 제시 이산확률분포와 연속활률분포로 분류(어떤 종류의 값을 가지고 있는가에 따라 구분) 확률분포의 종류 이산확률분포 : 일양균등분포, 이항분포, 포아송분포, 초기하분포, 기하분포 연속확률분포 평균분포 : 정규분포, t-분포 분산분포 : 카이제곱분포, f-분포 확률변수와 확률분포의 관계 확률변수는 모든 원소를 실수로 대응하는 함수이고 확률분포는 확률변수로 얻어진 실수를 확률 값으로 반환하는 함수 2. 이산확률분포 이산확률분포란 확률변수가 0,1,2,와 같이 이산적인 형태를 이루는 분포 로또 1등으로 당첨될 확률, 1남 3녀가 될 확률 등을 계산하는 형태 이산확률변수(Discreate Random Variable) 특정 수치만을 가지고 확률변수로 정수로 표현된다 P(X) = 180 이항분포(Binomial Distribution) = 베르누이분포 베르누이 과정의 시행을 반복 베르누이 시행은 두 가지 결과 중 하나만 나타나게 시행하는 것으로 보통 “성공”, “실패”로 표현 이전의 실행 결과에 독립적이므로 영향을 주지 않는다. 각 시행의 성공 혹은 실패의 확률은 처음부터 끝까지 변하지 않음 베르누이분포(Bernoulli ditribution) 베르누이분포는 확률변수가 0과 1 두 가지 결과 값만을 가지고 서로 독립적으로 시행됨 모든 실험결과에서 결과 확률은 항상 동일 포아송분포(poisson distribution) 데이터 분석자가 설정한 시간에서 사건이 발생하는 건수 주어진 시간, 거리, 공간 범위에서 발생 할 확률이 아주 낮은 사건들의 발생에 관한 이산확률 분포 시간 단위당 도착에 대한 모델에 많이 사용됨 예시 주어지 기간 동안 살인 사건의 수 생산 공장에서 작업 중에 재해가 발생하여 사망할 건수 일정한 거리의 전선에서 결점 수 지수분포(Exponential Distribution) 포아송분포의 반대로 도착시간에 따른 시간을 측정할 때 사용하는 연속확률분포 두 사건 사이에서 시간에 대한 확률을 의미 초기하분포(Hypergeometric distribution) 주어진 횟수만큼 반복되는 경우 성공할 횟수를 예측 과거의 결과는 현재, 미래의 결과에 영향을 미치는 것으로 분석(이항확률분포는 연속되는 시행이 독립적) 시행마다 발생할 결과가 이항분포처럼 두 가지만 있지만 유한 모집단에서 비복원 추출되기 때문에 베르누이 시행조건에 만족되지 안흔ㄴ 경우 사용되는 확률분포 베르누이 과정을 따르지 않는다. 3. 연속확률분포 연속확률분포란 연속 확률변수의 값에 대응하는 확률을 표시 확률밀도함수를 사용해서 분포를 표현할 수 있음 관측값이 연속적인 값을 가지고 있는 확률변수 연속확률변수(Continuous Random Variable) 어떤 범위에서 연속적인 값을 가질 수 있는 실수 연속확률변수의 자료는 각각 고유의 값을 가지고 있음 몸무게, 체온, 수명 등의 변수가 있음 P(175.0 &lt;= X &lt;= 185.0) 정규분포(Normal Distribution) 통계이론에서 중요한 확률분포로 샘플을 추출해서 모집단의 모수를 예측할 때 사용 모집단의 분포를 정규분포로 가정하고 통계분석을 수행 정규분포는 평균을 중심으로 좌우대칭 구조를 가지고 잇는 확률분포 표준정규분포(Stadard Normal Distribution) 정규분포에서 확률변수를 측정단위와 관계없이 자료를 표준화시켜 측정한 확률분포 표준 확률변수(Standardized Random Variable) 표준 확률변수는 측정단위와 관계없이 자료를 표준화시킴 평균으로부터 떨어진 거리를 계산할 수 있음 경험적 법칙(Empirical Rule) k=1, 68.26% 이상의 데이터가 μ ± 1 시그마에 있다 k=2, 95.44% 이상의 데이터가 μ ± 2 시그마에 있다 k=3, 99.73% 이상의 데이터가 μ ± 3 시그마에 있다 z 함수를 이요해서 평균과 표준편차로 면적을 계산할 수 있음(적분 사용 안해도됨) 4. 통계적 추론(Statistical Inference) 통계적 추론이란 우리가 알지 못하는 대상에 대해서 통계적으로 접근하여 알아가는 과정 예시 스마트폰을 가장 많이 사용하는 시간? 스마트폰을 남자와 여자 중에 누가 더 많이 사용할까? 급여수준과 사용하는 스마트폰의 종류는 관계가 있을까? 모수적 추론(Parametric inference) 어떤 대상인 모집단의 분포가 어떤 분포일 것이라고 가정하고 모수에 대해서 추론 모집단이 정규분포를 따른다면 분포의 모수는 평균과 분산일 것 따라서 모수적 추론의 가정은 최종 결론에 아주 큰 영향을 줌 정규분포, 이항분포, 포아송분포 등을 가정 비모수적 추론(Non-parametric inference) 모집단에 대해서 어떠한 가정도 하지 않고 추론 모집단을 몇개의 모수로 결정하기 어려워서 많은 모수를 사용해야 할 때 비모수적 추론을 함 비모수적 추론을 사용하는 경우 정규분포를 따르지 않는 것이 증명 표본의 수가 적어서 정규분포를 가정할 수가 없음 모집단에 대한 아무런 정보가 없음 정규분포를 가정하지 않기 때문에 평균과 분산이 없고 평균값의 차이, 신뢰구간을 구할 수가 없음 따라서 비모수적 추론은 해석이 복잡해지고 실제 값을 사용하기 보다 부호나 순위 등의 형태를 사용하는 경우가 많음 베이지안 추론(Bayesian Inference) 베이지안 확률을 사용해서 추론하는 방법으로 모수적 추론에서 가정한 분포의 모수로 추론 실험을 통해서 정보를 획득하고 베이즈 정리를 사용하여 가설 확률을 수정하는 통계적 추정방법 인공지능에서 사전 데이터로부터 학습된 지식을 추가 데이터로 업데이트할 때 사용됨$$P(H|E) = \\frac{P(E|H)P(H)}{P(E)}$$ 베이즈 정리(Bayes’ theorm) A와 B는 모두 독립사건이고, A에 대해서 B의 조건부 확률 P(A|B)와 B에 대한 A의 조건부확률(B|A)는 일반적으로 같지 않음 A와 B의 사전확률(Prior probability)인 P(A)와 P(B)가 같지 않기 때문 결론적으로 베이즈 정리는 P(A|B), P(B|A) 사이의 연관규칙이 존재하며 해당 규칙의 관계를 설명 $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$ A와 B는 모두 독립사건이고 B가 발생활 확률 P(B)가 0 이 아님 사건 A가 일어날 확률 P(A)와 사건 B가 일어날 확률 P(B)에서 B에 대한 사건 A의 조건부확률(P(B|A))를 알고 있으면 A에 대한 B가 일어날 확률을 알지 못해도 추정이 가능하다는 것 베이즈 정리를 사용한 베이지안 추론 사전 경험과 현재 데이터를 사용해서 어떤 사건의 확률을 베이즈 정리를 사용해서 추론한 것 P(A)는 사전확률이고 “사건A가 발생한다”라는 명제에 대한 확률값으로 정의됨 P(B)는 증거로 측정을 통해서 얻어진 B가 발생할 확률 P(B|A)는 가능도(Likelihood)로 “사건 A가 발생할 때 명제 B가 발생할 조건부확률” P(B|A)와 P(A), P(B)를 통해서 P(A|B)를 얻을 수 있으며, P(A|B)는 사후확률로 B라는 증거가 관찰된 후의 명제에 대한 확률 베이지안 추론$$P(H|E) = \\frac{P(E|H)P(H)}{P(E)}$$ 사건 A가 발생한다라는 명제를 H로 정의하고 믿음의 정도는 P(H)로 나타냄 증거B를 E로 나타냄 통계적 추론 방법에 따른 분류 통계적 추론 추정(Estimation) 점추정(Point Estimation) 미지의 모수에 대해 표본의 통계량을 사용해서 어떤 값으로 추정하는 과정 모집단의 특성을 단일값으로 추정하는 방법 모집단의 평균이 표본평균과 일치하는 세타를 찾는 방법을 적률방법(Moment Method)라고 함 예 표본평균 표본분산 구간추정(Interval Estimation) 모수의 값이 포함될 것이라 생각되는 범위를 통해서 모수를 추정 모수의 구간 값을 계산해서 모수가 특정 구간에 포함될 것을 확률로 분석 신뢰수준으로 85%, 97% 등으로 확률로 나타냄 신뢰구간(Confidnece interval) P(L ≤ θ ≤ U) - 1 - α 가설검정(Testing hypothesis) 모수에 대한 가설을 세우고 해당 가설의 옳고 그름을 판단 가설에 대한 검정을 통해서 기각할 것인지 채택할 것인지 결정 검정 통계량(Test statsitic)은 귀무가설을 기각하고 대립가설을 채택할지 아니면 귀무가설을 채택하고 대립가설을 기각할 것인지에 대한 통계량 통계분석 - 5.가설가설검증 가설검증(hypothesis test)이란 표본 데이터를 기반으로 모집단에 대한 새로운 주장의 옳고 그름을 추론하는 과정 가설의 진설여부를 증명하는 것 통계적 유의성을 검정하는 것으로, 유의적 검정(Significance Test)라고도 함 모수에서 표본을 사용하여 진실여부를 True, False로 판단 귀무가설(H0)이 사실이라고 가정하고 검증 통계적 검정(statstical test) 라고도 함 귀무가설(Null, Hypothesis ,H0) 기존의 주장 또는 기존에 알려진 사실(일반적으로 진실이라고 믿고 있는 것) 통계적 검정 대상이 됨 제네시스는 연비가 10km 이다 대립가설(Alternative Hypothesis, H1) 모집단에 대한 새로운 주장 (모집단과 표본의 평균은 다르다.) 귀무가설과 대립하는 가설로 새로운 사실을 입증 연구가설이라고도 함 모수의 표본을 사용해서 검증 제네시스는 연비가 10km가 아니다. 가설검정(귀무가설을 채택할 것인지 기각할 것인지 검증)의 종류 우측 검정(Right-sided Test) 오른쪽 5% 내에 있는지를 확인 95% 구간을 벗어나면 귀무가설을 기각됨 좌측 검정(Left-sided Test) 좌측 5%로 검정하여 귀무가설을 채택할지 기각할지 결정 양측 검정(Two-sided Test) 우측과 좌측 2.5% 구간을 기준으로 귀무가설을 채택할지 기각할지 결정 통계분석 - 7. 통계분석 기법 통계분석이란 특정집단을 대상으로 자료를 수집하여 대상집단의 정보를 구체적 통계분석 기법으로 통계적 추론을 하는 일련의 과정을 의미 평균차이검정 평균차이검정 평균검정(T-test) 분석해야하는 집단의 수가 2개 미만일 때 사용하는 방법 집단 간에 평균 값을 비교하는 분석기법 3개의 집단에 대해서 평균분석을 하면 1종 오류가 발생할 확률이 높아짐 평균검정은 집단이 1,2,3 이면 1:2, 2:3, 1:3 처럼 3번 평균을 비교하지만 분산분석(ANOVA)은 한번에 평균을 비교 종류 종류 내용 One Sample T-test 하나의 집단에 평균이 얼마인지를 검사하는 방법 Independent Samples T-test 독립된 두 집단 간에 평균의 차이를 검사하는 방법 Paired Samples T-test 하나의 집단을 처리 전과 처리 후로 나누어 분석하는 방법 분산분석(ANOVA, Analysis of Variance) ANOVA는 전체분산을 여러 개로 분할하여 분석하는 것으로 어떤 요인(factor)의 영향이 유의한지를 검정 집단이 3개 이상인 경우 사용 두개 이상의 집단을 비교할 때 사용하며 각 집단의 평균 차이에 의해서 발생되는 집단 간의 분산을 비교 F-분포(F-distribution, 연속확률분포)를 사용해서 가설을 검증하는 방법 종류 기법 내용 One way ANOVA 하나의 집단구분 변수를 사용 Two way ANOVA 동시에 두 집단의 집단구분 변수를 사용평균 반응 프로파일(두 변수 간의 상호작용을 확인하여 변화량을 확인)을 사용하여 두 개의 벼누 간에 상호작용(변화)를 확인 Repeated Measured ANOVA 집단이 3개이고 반복적으로 측정반복해서 측정하여 변화된 것을 비교하는 방법 Two way Repeated Measured ANOVA 시점 데이터와 집단 데이터를 사용해서 분석하는 형태 F값의 의미 F값 = 집단 간의 변량 / 집단 내의 변량 F값이 클수록 집단 간 변량이 집단 내 변량보다 커진다는 것을 의미 독립변수의 설명력이 크다는 것은 집단 간의 변량이 크다는 것을 의미하기에 독립변수의 설명력이 커질수록 F값이 커진다 변량은 변수나 분산의 뜻으로 사용됨 관계검정 차이검정은 그룹 간의 차이를 분석(평균, 분산)하는 것이고 관계검정은 벼누솨 변수의 관계(연관성)울 검정 종류 상관분석(Correlation) 연속변수와 연속변수를 분석 두 개의 변수 간에 관계를 통계적 기법으로 분석 인과관계가 명확하지 않을 때 분석 선형관계를 전제 두 개의 변수는 균등한 관계 ex. 키와 몸무게 -&gt; 어떤 변수가 원인이고 어떤 변수가 결과인지 알 수 없을 때 변수는 균등관계 종류 공분산(Covariance) 상관계수(Correlation Coefficient) Pearson 상관계수 Spearman 상관계수 회귀분석(Regression) 연속변수와 연속변수를 분석 변수 간의 인과관계를 분석 상관관계에서는 두가지 변수는 변수간에 원인과 결과가 없는 균등한 변수이고 회귀분석은 변수 간에 원인과 결과가 있는 변수 상관분석은 1:1의 관계이지만, 회귀분석은 1:N의 관계에서 데이터를 분석 인과 분석(원인 결과 분석)으로 독립변수의 변화가 종속변수의 변화를 어떻게 유발하는지 분석 독립변수 : 독립변수의 변화가 종속변수에 여향을 주는 변수 x1, x2, … 종속변수 : 독깁변수에 영향을 받는 변수 목적 예측 원인변수에 영향을 받는 기울기(회귀계수)를 찾아 y를 예측 변수를 비표준화하여 사용 종류 Multiple Linear Regression Logistic Regression 교차분석(Cross-tabulation) 질적변수와 질적변수를 분석","link":"/Deep%20Learning/%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB/%E1%84%80%E1%85%B5%E1%84%89%E1%85%AE%E1%86%AF%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8/"},{"title":"3장. 딥러닝 Overview - 딥러닝이란","text":"딥러닝 개요딥러닝이란? Deep Neural Network(D NN)을 학습시켜 문제를 해결 인경신경망(Artificial Neural Networks)의 적통을 이어받음 neuron 들로 구성된 신경망을 학습하여 문제를 해겨하도록 동작하는 함수 딥러닝은 인공신경망의 부분집합 차이점이라면 ANN은 얇게, DNN은 깊게 쌓음 기존 신경망에 비하여 더 깊은 구조를 갖는 것이 특징 이유 1. GPU를 활용한 병렬 연산 방법이 대중화되며, 신경망의 학습/추론 속도의 비약적 증가 이유 2. 인터넷의 발달로 빅데이터가 널리 활용되고 이를 통해 깊은 신경망 학습시킬 수 있게 됨 왜 딥러닝인가? 비선형 함수로 기존 머신러닝에 비해 패턴 인식 능력이 월등함 ※ 이 세상 어떠한 유형의 함수든 모사할 수 있는 능력이 있다는 것이 수학적으로 증명됨, UAT(Universal Approach Theorem) 이미지나 텍스트, 음성과 같은 분야들에서 비약적인 성능 개선을 만듬 기존 머신러닝과 달리 hand-crafted feature가 필요없음 단순히 raw값을 넣는 것으로, 자동으로 특징(feature)을 학습함 딥러닝의 주요역사 1980년대 역전파(Back-propagation) 알고리즘의 개발로 인한 중흥기 하지만 모델을 깊세 쌓지 못함으로써 절망감 2010년 초 ImageNet 우승과 음성 인식(Speech Recognition)의 상용화 2015년 기계번역(Machine Translation)의 상용화 자연어 처리(SequenceToSequence, seq2seq)의 시작 2017년 알파고의 승리 2018년 GAN을 통한 이미지 합성의 발전 딥러닝 패러다임의 변화 기존 패러다임 Hand-Crafted Feature를 추출하여 머신러닝 모델에 넣고 학습 ※ Hand-Crafted Feature : 얼굴은 둥그렇게 되어져 있으며, 눈 코 입의 위치가 있다라는 가정들을 넣는 것 즉, 여러 sub-모듈 여러 단계의 sub-module로 이루어져 있었음 여러 sub-module로 구성되어져 있어서 시스템이 무거웠으며, 여러 사람이 작업을 해야했음 가정이 들어감, 하지만 사람의 가정이 틀릴 수도 있는 문제점이 있음 새로운 패러다임 Raw 값을 신경망에 넣으면 자동으로 특징(Feature)을 학습 하지만, 사람이 해석하기가 어려움, 얼굴 인식이 안되면 왜 안되는지 알기 어려움(블랙박스) 하나의 task에 대해서, 하나의 신경망 모델이 존재하는 end-to-end 방식 훨씬 가볍고 혼자서도 오픈소스로 충분히 작업이 가능함","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%B4%EB%9E%80/"},{"title":"4장. PyTorch Tutorials - Tensor","text":"TensorTensor란 무엇인가 Scalar : 점 Vector : 1차원 배열 Matrix : 2차원 배열 Tensor : 3차원 이상의 배열 Tensor Shape ​ $x \\in \\mathbb{R}^{k \\times n \\times m} \\rightarrow |x|=(k,n,m)$ Matrix Shape $x \\in \\mathbb{R}^{k \\times n} \\rightarrow |x|=(k,n)$ Typical Tensor Shape : Tabular Dataset $|x|=(n,) \\Rightarrow x \\in \\mathbb{R}^n \\text{ (vector)}$ Mini batch: Consider Parallel Operations $|x|=(k,n) \\Rightarrow x \\in \\mathbb{R}^{k \\times n}$ Typical Tensor Shape : NLP $x \\in \\mathbb{R}^{k \\times n \\times m} \\rightarrow |x|=(k,n,m)$ |𝑥|=(#𝒔,#𝐰,#𝒇) $|x_{(i,j)}|$=(#𝑓, ) $|x_i|$=(#𝑤, #𝑓, ) Typical Tensor Shape : Computer Vision(GrayScale) |𝑥|=(#𝐢𝐦𝐠, 𝐡, 𝒘) Typical Tensor Shape : Computer Vision(Color) |𝑥|=(#𝐢𝐦𝐠, #𝐜𝐡𝐚𝐧𝐧𝐞𝐥, 𝐡, 𝒘) (4차원이 됨) $|x_i|$=(#𝑐ℎ, ℎ, 𝑤)","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/4%EC%9E%A5-PyTorch-Tutorials-Tensor/"},{"title":"3장. 딥러닝 Overview - 좋은 인공지능이란","text":"좋은 인공지능이란인공지능 모델이란? 𝑥 가 주어졌을 때, 𝑦 를 반환하는 함수 𝑦=𝑓(𝑥) 파라미터(weight parameter, 𝜃)란 𝑓가 동작하는 방식(𝑥 가 들어왔을 때, 어떤 𝑦 를 뱉어낼 것인가?)를 결정 학습이란 𝑥와 𝑦의 쌍으로 이루어진 데이터가 주어졌을 때, 𝑥로부터 𝑦로 가는 관계를 배우는 것 𝑥와 𝑦를 통해 적절한 파라미터(𝜃)를 찾아내는 것 모델이란 상황에 따라 알고리즘 자체를 이르거나 파라미터를 이름 좋은 인공지능 모델이란? 일반화(Generalization)를 잘 하는 모델 보지 못한(unseen) 데이터에 대해서 좋은 예측(prediction)을 하는 모델 우리는 모든 경우의 수에 대해서 데이터를 모을 수 없기 때문 보지 못한 경우에 대해서, 모델은 좋은 판단을 내릴 수 있어야 함 기존 머신러닝의 한계 기존 머신러닝은 주로 선형 또는 낮은 차원의 데이터를 다루기 위해 설계되었음 Kernel 등을 사용하여 비선형 데이터를 다룰 수 있지만 한계가 명확함 이미지, 텍스트, 음성과 같은 훨씬 더 높은 차원의 데이터들에 대해 낮은 성능을 보임","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-%EC%A2%8B%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%B4%EB%9E%80/"},{"title":"5장. 신경망의 기본 구성요소 살펴보기 - Linear Layer","text":"목표우리는 다음의 이미지를 통해 3이라고 머리가 인식하지만, 컴퓨터가 어떻게 이 이미지를 3으로 근사하도록 함수를 만들어야한다 우리는 $f^*$(f optimal)을 모사하는 최적의 $\\hat{f}$ (f hat)을 찾아야한다 https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d Linear Layer 란 신경망의 가장 기본 구성 요소, 딥러닝을 통해 모사하는 함수를 만들때 가장 기본이 되는 것이 Linear Layer Fully-connected(FC) Layer 라고 불리기도 함 입력의 모든 노드는 출력의 모든 노드와 컨넥션이 있음 Dense Layer 라고도 불리기도 함 내부 파라미터에 따른 선형 변환을 수행하는 함수 내부 파라미터를 잘 찾아내면 우리가 원하는 출력을 얻을 수 있음 https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942 Linear Layer 동작방식 각 입력 노드들에 weight(가중치)를 곱하고 모두 합친 뒤, bias(편향)을 더함 https://github.com/shchoice/shchoice.github.io/assets/100276387/ed702032-9ad9-491c-b1cd-481a7a412907 |𝜃|=(18,) , // 18개의 파라미터가 있음! 𝑊 = 5x3 =15, 𝑏 = 3 Linear Layer Equations 행렬 곱으로 구현 가능 n차원에서 m차원으로의 1선형 변환 함수 $x \\in R^{k \\times n}, w \\in R^{n \\times m} \\rightarrow y \\in R^{k \\times m}$ $y = f(k) = x \\cdot w + b$ 같은 표현 𝑥를 미니배치에 관계없이 단순히 벡터로 볼 경우 : (m,n) x (n,1) = (m,1) $y = f(k) = W^T \\cdot x + b$ $\\text{ where } x \\in \\mathbb{R}^n, W^T \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m \\text{ and } y \\in \\mathbb{R}^m$ 𝑥를 미니배치(N개) 텐서로 표현할 경우 : (N,n) x (n,m) = (N,m) $y = f(k) = x \\cdot W + b$ $\\text{ where } x \\in \\mathbb{R}^{k \\times n}, W \\in \\mathbb{R}^{n \\times m}, b \\in \\mathbb{R}^{n \\times m} \\text{ and } y \\in \\mathbb{R}^{n \\times m}$ https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942 코드로 구현해보기 parameter 정보 확인 예제 gradient에 관해서는 다음 gradient descent 파트에서 다룸 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import torchimport torch.nn as nn# 간단한 신경망 구조를 정의class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) # 10개의 입력을 받아 5개의 출력을 내는 선형 계층 self.fc2 = nn.Linear(5, 1) # 5개의 입력을 받아 1개의 출력을 내는 선형 계층 def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x# 신경망 객체를 생성model = SimpleNet()# 입력 데이터input_data = torch.FloatTensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]).unsqueeze(0)print(f&quot;input_data : {input_data}&quot;)# input_data : tensor([[ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]])# 신경망을 통해 입력 데이터를 전달output_data = model(input_data)print(f&quot;output: {output_data}&quot;)# output: tensor([[2.3264]], grad_fn=&lt;AddmmBackward&gt;)# 파라미터들에 대한 정보를 출력for name, param in model.named_parameters(): print(f&quot;name: {name}, param.data: {param.data}&quot;) ''' name: fc1.weight, param.data: tensor([[-0.2895, -0.1230, 0.1624, 0.0381, 0.2252, 0.2265, -0.1498, 0.0806, -0.1704, 0.2421], [-0.1162, 0.0786, -0.1140, 0.0178, 0.0470, 0.2920, 0.2933, 0.2919, 0.0493, -0.0025], [ 0.0196, 0.0492, -0.2049, 0.1628, -0.1038, 0.1221, 0.0516, -0.1309, -0.2128, -0.3086], [ 0.0129, 0.1872, -0.1641, 0.0406, 0.1779, 0.1346, -0.1623, 0.1618, 0.0410, -0.1538], [ 0.1166, -0.0591, 0.0349, -0.0866, 0.2066, -0.0777, 0.3119, -0.1021, -0.2297, 0.2657]]) name: fc1.bias, param.data: tensor([ 0.0787, -0.0037, -0.2033, 0.0398, -0.1233]) name: fc2.weight, param.data: tensor([[0.0168, 0.2259, 0.2410, 0.0145, 0.2553]]) name: fc2.bias, param.data: tensor([0.2295]) ''' # Gradient 계산을 위한 랜덤 타깃 값 생성target = torch.FloatTensor([0.5]).unsqueeze(0)print(f&quot;target {target}&quot;) # target tensor([[0.5000]])# 손실 함수로 평균 제곱 오차를 사용loss_fn = nn.MSELoss()# 손실 계산loss = loss_fn(output_data, target)# 역전파를 수행하여 그라디언트를 계산loss.backward()# 파라미터들의 그라디언트 정보를 출력for name, param in model.named_parameters(): print(f&quot;name: {name}, param.grad: {param.grad}&quot;) ''' name: fc1.weight, param.grad: tensor([[0.0614, 0.1229, 0.1843, 0.2458, 0.3072, 0.3687, 0.4301, 0.4915, 0.5530, 0.6144], [0.8253, 1.6507, 2.4760, 3.3013, 4.1267, 4.9520, 5.7774, 6.6027, 7.4280, 8.2534], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0529, 0.1057, 0.1586, 0.2114, 0.2643, 0.3171, 0.3700, 0.4228, 0.4757, 0.5285], [0.9324, 1.8648, 2.7972, 3.7296, 4.6621, 5.5945, 6.5269, 7.4593, 8.3917, 9.3241]]) name: fc1.bias, param.grad: tensor([0.0614, 0.8253, 0.0000, 0.0529, 0.9324]) name: fc2.weight, param.grad: tensor([[11.5118, 23.9645, 0.0000, 2.8603, 7.8742]]) name: fc2.bias, param.grad: tensor([3.6528]) ''' Raw Linear Layer 예제 (1) – nn.Module 추상 클래스를 활용 $y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 12345678910111213141516171819202122import torchW = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])b = torch.FloatTensor([2, 2])print(W.size()) # torch.Size([3, 2])print(b.size()) # torch.Size([2])def linear(x, W, b): y = torch.matmul(x, W) + b return yx = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])y = linear(x, W, b)print(y.size()) # torch.Size([4, 2]) Raw Linear Layer 예제 (2) – nn.Module 추상 클래스를 활용 $f(x) = y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 12345678910111213141516171819202122232425262728293031323334353637import torch import torch.nn as nnclass MyLinear(nn.Module): def __init__(self, input_dim=3, output_dim=2): self.input_dim = input_dim self.output_dim = output_dim super().__init__() self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim)) self.b = nn.Parameter(torch.FloatTensor(output_dim)) def forward(self, x): # |x| = (batch_size, input_dim) y = torch.matmul(x, self.W) + self.b # |y| = (batch_size, input_dim) * (input_dim, output_dim) # = (batch_size, output_dim) return yx = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])linear = MyLinear(3, 2)y = linear(x)print(y.size()) # torch.Size([4, 2])for p in linear.parameters(): print(p)'''Parameter containing:tensor([[-3.7895e+32, 7.2868e-43], [ 2.8026e-45, 0.0000e+00], [-3.7896e+32, 7.2868e-43]], requires_grad=True)''' Raw Linear Layer 예제 (3) – nn.Linear 이용 $f(x) = y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 1234567891011121314151617181920212223import torch import torch.nn as nnlinear = nn.Linear(3, 2)x = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])y = linear(x)print(y.size()) # torch.Size([4, 2])for p in linear.parameters(): print(p)'''Parameter containing:tensor([[-0.4061, 0.0483, 0.0804], [ 0.0581, 0.0730, 0.4323]], requires_grad=True)Parameter containing:tensor([0.4551, 0.4209], requires_grad=True)''' Raw Linear Layer 예제 (4) – nn.Linear 이용 $f(x) = y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 123456789101112131415161718192021222324252627282930313233343536373839import torch import torch.nn as nnclass MyLinear(nn.Module): def __init__(self, input_dim=3, output_dim=2): self.input_dim = input_dim self.output_dim = output_dim super().__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): # |x| = (batch_size, input_dim) y = self.linear(x) # |y| = (batch_size, output_dim) return yx = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])linear = MyLinear(3, 2)y = linear(x)print(y.size()) # torch.Size([4, 2])for p in linear.parameters(): print(p)'''Parameter containing:tensor([[-0.1267, 0.0563, 0.3951], [ 0.2291, 0.3214, 0.2595]], requires_grad=True)Parameter containing:tensor([0.3659, 0.4013], requires_grad=True)''' Summary Linear Layer 는 선형 함수 내부 가중치 파라미터(weight parameter) 𝑊와 𝑏에 의해 정의됨 우린 이 함수의 파라미터를 잘 조절하면, 주어진 입력에 대해 원하는 출력을 만들 수 있음","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-Linear-Layer-1/"},{"title":"5장. 신경망의 기본 구성요소 살펴보기 - 행렬의 곱셈과 벡터의 곱셈","text":"행렬의 곱셈(Matrix Multiplication) 행렬의 곱셈 2개의 행렬을 입력으로 받아 새로운 행렬을 생성 첫 번째 행렬의 각 행과 두 번째 행렬의 각 열 사이의 내적을 요소로 가지는 새로운 행렬을 만듬 행렬 곱셈은 내적의 총합을 사용하지만 자체적으로는 다른 연산 12345678# 행렬 곱셈M = torch.tensor([[1, 2, 3], [4, 5, 6]])N = torch.tensor([[7, 8], [9, 10], [11, 12]])matrix_product = torch.mm(M, N) # 두 텐서가 모두 2차원 이상인 경우, '@'는 행렬곱(matrix multiplication)을 계산print(f&quot;Matrix Product:\\\\n {matrix_product}&quot;)# Matrix Product:# tensor([[ 58, 64],# [139, 154]]) 1. Matrix Multiplication(행렬 곱) 2. Vector Matrix Multiplication (벡터와 행렬의 곱) 3. Batch Matrix Multiplication 같은 갯수의 행렬 쌍들에 대해서 병렬로 행렬 곱 실행 만약 4차원 텐서라면 (N1, N2, n, h) X (N1, N2, h, m) 이 됨 벡터의 곱셈(Vector Multiplication)벡터의 곱셈에는 주로 2가지의 형태로 있음 내적 (Dot Product, Inner Product, 점곱) 두 개의 벡터를 입력으로 받아 스칼라(단일 수치) 값을 출력 벡터의 내적은 같은 위치에 있는 요소들끼리 곱한 후, 그 결과를 모두 더해서 하나의 숫자를 얻음 내적은 벡터들 사이의 유사성을 측정하는 데 사용 123456# 벡터 내적A = torch.tensor([1, 2, 3])B = torch.tensor([4, 5, 6])dot_product = torch.mm(A, B) # 두 텐서가 모두 1차원인 경우, '@'는 벡터 내적(dot product)을 계산print(f&quot;Dot Product: {dot_product}&quot;)# Dot Product: 32 외적 (Cross Product) 3차원 벡터에 한정하며, 두 벡터의 외적은 새로운 벡터를 생성 새로운 벡터는 두 입력 벡터에 수직인 방향을 가지며, 그 크기는 두 입력 벡터 사이의 각도에 따라 달라짐 물리학에서 힘의 방향 계산 등에 사용 벡터의 내적 vs 코사인 유사도 Dot Product $a \\cdot b = |a| |b| \\cos \\theta$ 얼마나 같은 방향을 가지고 있는지 정보를 담으며, 벡터의 크기에도 영향을 받음 Cosine Similarity $\\text{cosine-similarity}(a, b) = \\frac{a \\cdot b}{|a| |b|}$ 방향성만 고려함 벡터의 크기 고려x","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88%EA%B3%BC-%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"},{"title":"6장. 신경망이 잘 학습되는지 판단하기 - Loss Function","text":"Again, Our object is 데이터를 넣었을 때 출력을 반환하는 가상의 함수를 모사하는 것 Linear Layer 함수를 통해 원하는 함수를 모사해보자 Linear Layer 함수가 얼마나 원하는 만큼 동작하는지 측정해 보자 얼마나 잘 동작하는지, 점수로 나타내보자 Loss Loss(손실값): 원하는 출력값(target,𝑦)가 실제 출력값(output, $\\hat{y}$)의 차이의 합 $\\text{Loss} = \\sum_{i=1}^{N} | y_i - \\hat{y}i | = \\sum{i=1}^{N} | y_i - f(x_i) |$ 그러므로 우리는 Loss가 작을수록 가상의 함수를 잘 모사하고 있다고 할 수 있음 Loss가 작은 Linear Layer를 선택하면 됨 Loss Function Linear Layer의 파라미터를 바꿀 때마다 Loss를 계산 Loss Function 입력 : Linear Layer의 파라미터(𝜃, 즉, 𝑊,𝑏가 파라미터) 출력 : Looss 𝐿(𝜃)=$\\sum_{i=1}^{n} | y_i - f_{\\theta}(x_j) |, \\text{ where } \\theta = {W, b}$ 종류 Euclidean Distance $| y - \\hat{y} |_2 (L2) = \\sqrt{(y_1 - \\hat{y}_1)^2 + \\ldots + (y_n - \\hat{y} n)^2} = \\sqrt{\\sum {i=1}^{n} (y_i - \\hat{y}_i)^2}, \\text{ where } y \\in \\mathbb{R}^n \\text{ and } \\hat{y} \\in \\mathbb{R}^n$ 딥러닝은 차원제약이 없어서 고차원으로 가면 차이가 굉장히 커질 수 있기 때문에 RMSE 가 등장 cf) $| y - \\hat{y} | : L1$, 절대값 RMSE(Root Mean Square Error) Euclidean Distance와 비슷한 개념 $\\text{RMSE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$ ```MSE 1234567891011121314151617181920212223242526272829 (Mean Square Error) - $\\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n}(\\| y - \\hat{y} \\|_2)^2 = \\frac{1}{n}\\| y - \\hat{y} \\|_2^2 ∝ \\| y - \\hat{y} \\|_2^2$ - Root와 상수를 뺏지만 크기 차이로 인한 순서 결과는 바뀌지 않음 - **손실함수로 가장 많이 사용**## 코드 구현하기- Loss Function 예제 (1) – 직접 구현하기 ```python import torch import torch.nn as nn def mse(x_hat, x): # |x_hat| = (batch_size, dim) # |x| = (batch_size, dim) y = ((x - x_hat)**2).mean() return y x = torch.FloatTensor([[1, 1], [2, 2]]) x_hat = torch.FloatTensor([[0, 0], [0, 0]]) print(x.size(), x_hat.size()) # torch.Size([2, 2]) torch.Size([2, 2]) mse(x_hat, x) # tensor(2.5000) https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83 Loss Function 예제 (2) – 라이브러리 활용 12345678910111213import torchimport torch.nn.functional as Fx = torch.FloatTensor([[1, 1], [2, 2]])x_hat = torch.FloatTensor([[0, 0], [0, 0]])print(x.size(), x_hat.size()) # torch.Size([2, 2]) torch.Size([2, 2])print(F.mse_loss(x_hat, x)) # tensor(2.5000)print(F.mse_loss(x_hat, x, reduction='sum')) # tensor(10.)print(F.mse_loss(x_hat, x, reduction='none')) # tensor([[1., 1.], [4., 4.]]) Loss Function 예제 (3) – 라이브러리 활용 1234import torch.nn as nnmse_loss = nn.MSELoss()print(mse_loss(x_hat, x)) # tensor(2.5000) 요약 우리는 목표로 하는 함수를 모사하기 위해 학습용 입력 데이터들을 Linear Layer에 넣어 출력 값들을 구하고 출력값($\\hat{y}$)들과 목표값(${y}$)들의 차이의 합(Loss)를 최소화 해야함 결국, Linear Layer 파라미터(𝜃)를 바꾸면서 loss를 최소화 해야함","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/6%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%B4-%EC%9E%98-%ED%95%99%EC%8A%B5%EB%90%98%EB%8A%94%EC%A7%80-%ED%8C%90%EB%8B%A8%ED%95%98%EA%B8%B0-Loss-Function/"},{"title":"5장. 신경망의 기본 구성요소 살펴보기 - Linear Layer","text":"목표우리는 다음의 이미지를 통해 3이라고 머리가 인식하지만, 컴퓨터가 어떻게 이 이미지를 3으로 근사하도록 함수를 만들어야한다 우리는 $f^*$(f optimal)을 모사하는 최적의 $\\hat{f}$ (f hat)을 찾아야한다 Linear Layer 란 신경망의 가장 기본 구성 요소, 딥러닝을 통해 모사하는 함수를 만들때 가장 기본이 되는 것이 Linear Layer Fully-connected(FC) Layer 라고 불리기도 함 입력의 모든 노드는 출력의 모든 노드와 컨넥션이 있음 Dense Layer 라고도 불리기도 함 내부 파라미터에 따른 선형 변환을 수행하는 함수 내부 파라미터를 잘 찾아내면 우리가 원하는 출력을 얻을 수 있음 Linear Layer 동작방식 각 입력 노드들에 weight(가중치)를 곱하고 모두 합친 뒤, bias(편향)을 더함 |𝜃|=(18,) , // 18개의 파라미터가 있음! 𝑊 = 5x3 =15, 𝑏 = 3 Linear Layer Equations 행렬 곱으로 구현 가능 n차원에서 m차원으로의 선형 변환 함수 $x \\in R^{k \\times n}, w \\in R^{n \\times m} \\rightarrow y \\in R^{k \\times m}$ $y = f(k) = x \\cdot w + b$ 같은 표현 𝑥를 미니배치에 관계없이 단순히 벡터로 볼 경우 : (m,n) x (n,1) = (m,1) $y = f(k) = W^T \\cdot x + b$ $\\text{ where } x \\in \\mathbb{R}^n, W^T \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m \\text{ and } y \\in \\mathbb{R}^m$ 𝑥를 미니배치(N개) 텐서로 표현할 경우 : (N,n) x (n,m) = (N,m) $y = f(k) = x \\cdot W + b$ $\\text{ where } x \\in \\mathbb{R}^{k \\times n}, W \\in \\mathbb{R}^{n \\times m}, b \\in \\mathbb{R}^{n \\times m} \\text{ and } y \\in \\mathbb{R}^{n \\times m}$ 코드로 구현해보기 parameter 정보 확인 예제 gradient에 관해서는 다음 gradient descent 파트에서 다룸 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import torchimport torch.nn as nn# 간단한 신경망 구조를 정의class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) # 10개의 입력을 받아 5개의 출력을 내는 선형 계층 self.fc2 = nn.Linear(5, 1) # 5개의 입력을 받아 1개의 출력을 내는 선형 계층 def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x# 신경망 객체를 생성model = SimpleNet()# 입력 데이터input_data = torch.FloatTensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]).unsqueeze(0)print(f&quot;input_data : {input_data}&quot;)# input_data : tensor([[ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]])# 신경망을 통해 입력 데이터를 전달output_data = model(input_data)print(f&quot;output: {output_data}&quot;)# output: tensor([[2.3264]], grad_fn=&lt;AddmmBackward&gt;)# 파라미터들에 대한 정보를 출력for name, param in model.named_parameters(): print(f&quot;name: {name}, param.data: {param.data}&quot;) ''' name: fc1.weight, param.data: tensor([[-0.2895, -0.1230, 0.1624, 0.0381, 0.2252, 0.2265, -0.1498, 0.0806, -0.1704, 0.2421], [-0.1162, 0.0786, -0.1140, 0.0178, 0.0470, 0.2920, 0.2933, 0.2919, 0.0493, -0.0025], [ 0.0196, 0.0492, -0.2049, 0.1628, -0.1038, 0.1221, 0.0516, -0.1309, -0.2128, -0.3086], [ 0.0129, 0.1872, -0.1641, 0.0406, 0.1779, 0.1346, -0.1623, 0.1618, 0.0410, -0.1538], [ 0.1166, -0.0591, 0.0349, -0.0866, 0.2066, -0.0777, 0.3119, -0.1021, -0.2297, 0.2657]]) name: fc1.bias, param.data: tensor([ 0.0787, -0.0037, -0.2033, 0.0398, -0.1233]) name: fc2.weight, param.data: tensor([[0.0168, 0.2259, 0.2410, 0.0145, 0.2553]]) name: fc2.bias, param.data: tensor([0.2295]) ''' # Gradient 계산을 위한 랜덤 타깃 값 생성target = torch.FloatTensor([0.5]).unsqueeze(0)print(f&quot;target {target}&quot;) # target tensor([[0.5000]])# 손실 함수로 평균 제곱 오차를 사용loss_fn = nn.MSELoss()# 손실 계산loss = loss_fn(output_data, target)# 역전파를 수행하여 그라디언트를 계산loss.backward()# 파라미터들의 그라디언트 정보를 출력for name, param in model.named_parameters(): print(f&quot;name: {name}, param.grad: {param.grad}&quot;) ''' name: fc1.weight, param.grad: tensor([[0.0614, 0.1229, 0.1843, 0.2458, 0.3072, 0.3687, 0.4301, 0.4915, 0.5530, 0.6144], [0.8253, 1.6507, 2.4760, 3.3013, 4.1267, 4.9520, 5.7774, 6.6027, 7.4280, 8.2534], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0529, 0.1057, 0.1586, 0.2114, 0.2643, 0.3171, 0.3700, 0.4228, 0.4757, 0.5285], [0.9324, 1.8648, 2.7972, 3.7296, 4.6621, 5.5945, 6.5269, 7.4593, 8.3917, 9.3241]]) name: fc1.bias, param.grad: tensor([0.0614, 0.8253, 0.0000, 0.0529, 0.9324]) name: fc2.weight, param.grad: tensor([[11.5118, 23.9645, 0.0000, 2.8603, 7.8742]]) name: fc2.bias, param.grad: tensor([3.6528]) ''' Raw Linear Layer 예제 (1) – nn.Module 추상 클래스를 활용 $y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 12345678910111213141516171819202122import torchW = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])b = torch.FloatTensor([2, 2])print(W.size()) # torch.Size([3, 2])print(b.size()) # torch.Size([2])def linear(x, W, b): y = torch.matmul(x, W) + b return yx = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])y = linear(x, W, b)print(y.size()) # torch.Size([4, 2]) Raw Linear Layer 예제 (2) – nn.Module 추상 클래스를 활용 $f(x) = y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 12345678910111213141516171819202122232425262728293031323334353637import torch import torch.nn as nnclass MyLinear(nn.Module): def __init__(self, input_dim=3, output_dim=2): self.input_dim = input_dim self.output_dim = output_dim super().__init__() self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim)) self.b = nn.Parameter(torch.FloatTensor(output_dim)) def forward(self, x): # |x| = (batch_size, input_dim) y = torch.matmul(x, self.W) + self.b # |y| = (batch_size, input_dim) * (input_dim, output_dim) # = (batch_size, output_dim) return yx = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])linear = MyLinear(3, 2)y = linear(x)print(y.size()) # torch.Size([4, 2])for p in linear.parameters(): print(p)'''Parameter containing:tensor([[-3.7895e+32, 7.2868e-43], [ 2.8026e-45, 0.0000e+00], [-3.7896e+32, 7.2868e-43]], requires_grad=True)''' Raw Linear Layer 예제 (3) – nn.Linear 이용 $f(x) = y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 1234567891011121314151617181920212223import torch import torch.nn as nnlinear = nn.Linear(3, 2)x = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])y = linear(x)print(y.size()) # torch.Size([4, 2])for p in linear.parameters(): print(p)'''Parameter containing:tensor([[-0.4061, 0.0483, 0.0804], [ 0.0581, 0.0730, 0.4323]], requires_grad=True)Parameter containing:tensor([0.4551, 0.4209], requires_grad=True)''' Raw Linear Layer 예제 (4) – nn.Linear 이용 $f(x) = y = x \\cdot W + b, \\text{ where } x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, \\text{ Thus, } W \\in \\mathbb{R}^{n \\times m} \\text{ and } b \\in \\mathbb{R}^{m}$ 123456789101112131415161718192021222324252627282930313233343536373839import torch import torch.nn as nnclass MyLinear(nn.Module): def __init__(self, input_dim=3, output_dim=2): self.input_dim = input_dim self.output_dim = output_dim super().__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): # |x| = (batch_size, input_dim) y = self.linear(x) # |y| = (batch_size, output_dim) return yx = torch.FloatTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])print(x.size()) # torch.Size([4, 3])linear = MyLinear(3, 2)y = linear(x)print(y.size()) # torch.Size([4, 2])for p in linear.parameters(): print(p)'''Parameter containing:tensor([[-0.1267, 0.0563, 0.3951], [ 0.2291, 0.3214, 0.2595]], requires_grad=True)Parameter containing:tensor([0.3659, 0.4013], requires_grad=True)''' Summary Linear Layer 는 선형 함수 내부 가중치 파라미터(weight parameter) 𝑊와 𝑏에 의해 정의됨 우린 이 함수의 파라미터를 잘 조절하면, 주어진 입력에 대해 원하는 출력을 만들 수 있음","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-Linear-Layer/"},{"title":"7장. 기초 최적화 방법 Gradient Descent - Gradient Descent","text":"Gradient DescentAgain, Our Objective is 주어진 데이터에 대해서 출력 값을 똑같이 모사하는 함수를 찾고 싶다. Loss 값을 최소로 하는 Loss Function의 입력 값(𝜃)를 찾자. How? 𝜃 값을 하나하나 다 랜덤하게 넣어볼 수가 없다!! 따라서 Loss Function을 최소화하는 𝜃를 얻는 방법이 바로 Gradient Descent! $D = {(x_i, y_i)}_{i=1}^N$ // (데이터 셋들이 모여져 있을때) $L(\\theta) = \\sum_{i=1}^{N} |y_i - \\hat{y_i}|^2_2 = \\sum_{i=1}^{N} |y_i - f_\\theta(x_i)|^2_2, \\text { where } \\theta={W,b}, \\text{ }f(x)=x \\cdot W+b$ $\\hat{\\theta} = \\operatorname{argmin}_{\\theta} L(\\theta)$ Loss 함수의 출력 결과가 최소가 되고 싶어하는 입력값을 점진적으로 찾고 싶겠다. 잘된다면 목적함수를 근사할 수 있음($f^* \\approx f_{\\hat{\\theta}}$) Gradient Descent 1D Case 𝑥로 미분하여 기울기를 활용하여 좀 더 낮은 곳으로 점차 나아가자 https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2 $x \\gets x - \\eta \\frac{dy}{dx}, \\text{ where } y = f(x)$ 1⬇️ $\\theta \\gets \\theta - \\eta \\frac{\\partial L(\\theta)}{\\partial \\theta} = \\theta - \\eta \\nabla_\\theta L(\\theta)$ ※ 𝜂 : Learning rate(0~1, hyper parameter), ⅆ𝑦/ⅆ𝑥 : 기울기 ※ 𝐿(𝜃) : 손실 함수(loss function)로써 스칼라 값을 갖음 , 𝜃: 파라미터 또는 가중치 벡터로 vector값을 갖음(고차원의 신경망에서는 𝜃가 벡터 뿐만 아니라 행렬, 또는 고차원 텐서의 형태를 가질 수도 있음, 여기서는 1D로 가정하기에 Vector) 따라서 $∇_θL(θ)$는 벡터 θ에 대한 L(θ)의 그래디언트를 나타내며, 이는 θ의 각 요소에 대해 L(θ)를 편미분한 결과를 벡터 형태로 표현한 것 가장 loss가 낮은 곳이 아닌 골짜기에 빠질 가능성이 있음 https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e 그림은 2차원 이지만, 사실 파라미터의 개수만큼의 차원으로 이루어져 있음 Convex 한 2차 함수가 아닌 이상, Global Minima를 알 수가 없다. Loss Minimization using Gradient Descenthttps://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444 https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444 1D 케이스를 높은 차원의 파라미터(*θ)*로 확장하자 $\\hat{\\theta} = \\operatorname{argmin}_{\\theta} L(\\theta)$ $W \\gets W - \\eta \\frac{\\partial L(\\theta)}{\\partial W} \\text{, }$ $b \\gets b - \\eta \\frac{\\partial L(\\theta)}{\\partial b},$ $\\text{where } \\theta = {W,b}$ Number of Parameters in Linear Layer 파라미터 수 : n x m + m = (n + 1) x m https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942 |𝜃|=(18,) , // 18개의 파라미터가 있음! 𝑊 = 5x3 =15, 𝑏 = 3 $y = f(k) = x \\cdot W + b$ $\\text{ where } x \\in \\mathbb{R}^{k \\times n}, W \\in \\mathbb{R}^{n \\times m}, b \\in \\mathbb{R}^{n \\times m} \\text{ and } y \\in \\mathbb{R}^{n \\times m}$ Local Minima in Practice 실제 딥러닝의 경우에 파라미터의 크기가 수백만 단위 수백만 차원의 loss 함수 surface 에서 global minma를 찾는 문제 수 많은 차원에서 동시에 local minima를 위한 조건이 만족되기는 어려움 따라서 local mima에 대한 걱정을 크게 할 필요 없음 코드로 구현하기 Gradient Descent 실습 - backward() &amp; requires_grad_() $x = \\begin{bmatrix} x_{(1,1)} &amp; x_{(1,2)} \\ x_{(2,1)} &amp; x_{(2,2)} \\end{bmatrix}$ $x_1=x+2$ $x_2=x-2$ $x_3=x^2-4$ $y=sum(x_3)=x_{3(1,1)} + x_{3(1,2)} + x_{3(2,1)} + x_{3(2,2)}$ $x.grad=\\begin{bmatrix} \\frac{\\partial y}{\\partial x_{(1,1)}} &amp; \\frac{\\partial y}{\\partial x_{(1,2)}} \\ \\frac{\\partial y}{\\partial x_{(2,1)}} &amp; \\frac{\\partial y}{\\partial x_{(2,2)}} \\end{bmatrix}$ $\\frac{dy}{dx}=2x$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import torchx = torch.FloatTensor([[1, 2], [3, 4]]).requires_grad_(True)x1 = x + 2x2 = x - 2x3 = x1 * x2y = x3.sum() # 스칼라 값이어야 미분 가능하기 때문에 .sum()이 붙음 없으면 RuntimeError: grad can be implicitly created only for scalar outputs 에러 발생print(x1)#tensor([[3., 4.],# [5., 6.]], grad_fn=&lt;AddBackward0&gt;)print(x2)# tensor([[-1., 0.],# [ 1., 2.]], grad_fn=&lt;SubBackward0&gt;)print(x3)# tensor([[-3., 0.],# [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)print(y)# tensor(14., grad_fn=&lt;SumBackward0&gt;)y.backward() # 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분'''.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? 이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # 가중치 벡터, y.backward(v)이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, 이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행'''print(x.grad)# tensor([[2., 4.],# [6., 8.]])'''print(x3.numpy())# RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.print(x3.detach_().numpy())# array([[-3., 0.],# [ 5., 12.]], dtype=float32)PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, 이 속성이 True로 설정되면, 해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. 이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. 따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. 이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.''' 요약 정리 DNN을 통해 문제 해결을 위한 함수를 모사하고 싶을 경우, 우리가 만든 함수가 내뱉는 출력과 실제 정답의 차이는 loss이다. loss를 loss function으로 만들고 loss function은 파라미터를 입력으로 받음 따라서 loss function의 출력을 최소로 하는 파라미터를 찾는 것이 목적이 됨 loss를 최소화하는 입력 파라미터를 Gradiend Descent로 찾을 수 있음","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Gradient-Descent/"},{"title":"7장. 기초 최적화 방법 Gradient Descent - Learning Rate","text":"Learning RateLearning Rate in Gradient Descent 파라미터가 업데이트 될 때, gradient의 크기에 영향을 받게 됨 이 때, learning rate가 step-size를 정해주게 됨 Equation $\\theta \\gets \\theta - \\eta \\frac{\\partial L(\\theta)}{\\partial \\theta} = \\theta - \\eta \\nabla_\\theta L(\\theta)$ Learning Rate 에 따른 최적화 (데이터나 모델 아키텍처에 따라 lr은 변함) Large LR 너무 큰 Loss가 발산할 수 있음 Small LR 너무 작은 LR은 수렴이 늦음 자칫 local minima에 빠질 수 있음 https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589 Learning Rate 는 중요한 하이퍼파라미터 실험을 통해 최적화하는 것이 필요 초보자들은 처음에 어떤 값을 정해야 할지 난감 고민할 바에 그냥 아주 작은 값(eg. 1e-4)으로 오래 돌려도 괜찮음 나중에 Adam Optimizer를 통해 Learning Rate에 대한 고민을 없앨 수 있음 ※ 하이퍼파라미터 : 모델 성능에 영향을 끼치지만, 데이터를 통해 학습할 수 없는 파라미터 (우리가 직접 테스트하고 튜닝을 해야함) 코드로 구현하기 Gradient Descent + Learning Rate 실습 $L(x)=| targert - x |_2^2$ $x \\gets x - \\eta \\nabla_x L(x)$ cf) $\\nabla_x L(x) = x.grad$ $\\hat{x} = \\text{argmin}L(x)$, $x \\in \\mathbb{R}^{3,3}$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import torchimport torch.nn.functional as Ftarget = torch.FloatTensor([[.1, .2, .3], [.4, .5, .6], [.7, .8, .9]])x = torch.rand_like(target)# This means the final scalar will be differentiate by x.x.requires_grad = True# You can get gradient of x, after differentiation.print(x)# tensor([[0.4176, 0.6465, 0.3522],# [0.9164, 0.7576, 0.0892],# [0.4854, 0.0136, 0.9467]], requires_grad=True)loss = F.mse_loss(x, target)print(loss) # tensor(0.1737, grad_fn=&lt;MseLossBackward&gt;)threshold = 1e-5learning_rate = 1.iter_cnt = 0while loss &gt; threshold: iter_cnt += 1 loss.backward() # Calculate gradients. x = x - learning_rate * x.grad x.detach_() x.requires_grad_(True) loss = F.mse_loss(x, target) print('%d-th Loss: %.4e' % (iter_cnt, loss)) print(x) ''' 1 - th Loss: 1.0510e-01 tensor([[0.3470, 0.5473, 0.3406], [0.8016, 0.7003, 0.2027], [0.5331, 0.1883, 0.9363]], requires_grad = True) 2 - th Loss: 6.3576e-02 tensor([[0.2921, 0.4701, 0.3316], [0.7124, 0.6558, 0.2910], [0.5702, 0.3242, 0.9282]], requires_grad = True) 3 - th Loss: 3.8460e-02 tensor([[0.2494, 0.4101, 0.3246], [0.6430, 0.6212, 0.3597], [0.5990, 0.4300, 0.9220]], requires_grad = True) 19 - th Loss: 1.2370e-05 tensor([[0.1027, 0.2038, 0.3004], [0.4044, 0.5022, 0.5957], [0.6982, 0.7934, 0.9004]], requires_grad = True) 20 - th Loss: 7.4833e-06 tensor([[0.1021, 0.2029, 0.3003], [0.4034, 0.5017, 0.5966], [0.6986, 0.7948, 0.9003]], requires_grad = True) ''' 7장 Wrap upWhy we do gradient descent? 실재하지만 알 수 없는 함수 $f^*$를 근사하고 싶음 나의 모델(함수)의 $f_\\theta$ 파라미터 𝜽를 조절 손실 함수(Loss Function)를 최소화 하도록 파라미터 𝜽를 조절 미분을 통해 gradient($\\frac{\\partial Loss}{\\partial \\theta}$)를 얻고, loss를 낮추는 방향으로 파라미터를 업데이트","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Learning-Rate/"},{"title":"7장. 기초 최적화 방법 Gradient Descent - 편미분","text":"편미분다변수 함수(Multivariation Function) 여러 개의 변수(multivariate)를 입력으로 받는 함수 $z = f(x,y)$ $y = f(x_1, x_2)$ $x = \\begin{bmatrix} x_1 \\ x_2 \\end{bmatrix}$ https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2 편미분 다변수 x와 y를 입력으로 받는 함수 f를 x로 미분할 경우 하나의 변수만 남겨 놓고 나머지를 상수 취급하는 미분 방법 함수 f를 x변수(x축)으로 미분 편미분 기호 𝜕 (round 혹은 partial 라고 부름) $\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h,y) - f(x,y)}{(x+h) - x}$ Y값에 대해 뚝 잘랐을 때 x축에 대한 기울기 https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61 함수의 입출력 형태 함수의 입력이 벡터인 경우 $y=f(\\begin{bmatrix} x_1 \\⦙\\x_n \\end{bmatrix})=f(x), \\text{ where } x \\in \\mathbb{R}^n$ 함수의 출력이 벡터인 경우 $y=f(\\begin{bmatrix} y_1 \\⦙\\y_n \\end{bmatrix})=f(x)=\\begin{bmatrix} f_1(x) \\⦙\\f_n(x) \\end{bmatrix}, \\text{ where } y \\in \\mathbb{R}^n$ 함수의 입력이 행렬인 경우 $y=f(\\begin{bmatrix} x_{1,1} ⋯ x_{1,m} \\⦙ \\text{ }\\text{ } ⋱ \\text{ }\\text{ } ⦙ \\ x_{n,1} ⋯ x_{n,m} \\end{bmatrix})=f(X), \\text{ where } X \\in \\mathbb{R}^{n \\times m}$ 함수의 출력이 행렬인 경우 $Y=f(\\begin{bmatrix} y_{1,1} ⋯ y_{1,m} \\⦙ \\text{ }\\text{ } ⋱ \\text{ }\\text{ } ⦙ \\ y_{n,1} ⋯ y_{n,m} \\end{bmatrix})=f(x), \\text{ where } Y \\in \\mathbb{R}^{n \\times m}$ 입력과 출력이 벡터인 함수 $y=f(\\begin{bmatrix} y_1 \\⦙\\y_n \\end{bmatrix})=f(x)= f(\\begin{bmatrix} x_1 \\⦙\\x_n \\end{bmatrix}), \\text{ where } f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ 스칼라를 벡터로, 스칼라를 행렬로 미분 미분 결과는 gradient 벡터가 되어 방향과 크기를 모두 나타냄 스칼라를 벡터로 미분 스칼라 함수를 벡터로 미분한다는 것을 의미 결과는 벡터가 됨 각 벡터의 요소는 스칼라 함수를 해당 방향을 미분한 결과를 나타냄, 이것을 gradient라고 부르며, 함수의 기울기를 나타나는데 사용 $\\frac{\\partial f}{\\partial x} = \\nabla_x f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\ ⦙\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}, \\text {where }x \\in \\mathbb{R}^n$ 예제 $f(x,y)=3x2+2xy+y2$ 라는 스칼라 함수와 벡터 변수 $x$ 와 $y$ 가 있음 이 함수를 각 변수에 대해 편미분 하면 다음과 같음 $\\frac{\\partial f}{\\partial x} = 6x + 2y$ $\\frac{\\partial f}{\\partial y} = 2x + 2y$ 따라서 함수 $f(x,y)$ 에 대한 그래디언트는 다음과 같음 $\\nabla f = \\left[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right] = [6x + 2y, 2x + 2y]$ 이 그래디언트 벡터는 각 점$(x,y)$에서 함수의 값이 가장 크게 증가하는 방향을 가리킴, 따라서 그래디언트는 최적화 문제에서 가장 중요한 역할을 함 (최적화 알고리즘은 일반적으로 그래디언트의 반대 방향으로 이동하여 함수의 최소값을 찾습니다. 이것이 그래디언트 디센트 방법의 기본 개념) 코드로 확인해보기 123456789101112131415161718192021222324252627from sympy import symbols, diffx, y = symbols('x y')f = 3*x**2 + 2*x*y + y**2df_dx = diff(f, x) # x에 대한 편미분df_dy = diff(f, y) # y에 대한 편미분print(f'df/dx: {df_dx}') # df/dx: 6*x + 2*yprint(f'df/dy: {df_dy}') # df/dy: 2*x + 2*y# 임의의 시작점x_value = 1.0y_value = 1.0# 학습률eta = 0.01for _ in range(100): # 100번 반복 gradient_x = df_dx.evalf(subs={x: x_value, y: y_value}) # x 위치에서의 그래디언트 계산 gradient_y = df_dy.evalf(subs={x: x_value, y: y_value}) # y 위치에서의 그래디언트 계산 x_value -= eta * gradient_x # 그래디언트의 반대 방향으로 이동 y_value -= eta * gradient_y # 그래디언트의 반대 방향으로 이동print(f'Optimized x: {x_value}') # Optimized x: -0.0627121858146143print(f'Optimized y: {y_value}') # Optimized y: 0.154295508359253 스칼라를 행렬로 미분 스칼라 함수를 행렬 변수에 대해 미분하는 것을 나타냄 결과는 행렬이 됨 이 행렬의 각 요소는 해당 스칼라 함수를 행렬의 해당 요소로 편미분한 값, 스칼라 함수를 행렬로 미분한 결과는 자코비안 행렬이라고 부르며, 이는 함수의 지역적 변화율을 나타냄 $\\frac{\\partial f}{\\partial x} = \\nabla_x f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{1,1} } ⋯ \\frac{\\partial f}{\\partial x_{1,m}} \\ ⦙ \\text{ }\\text{ } ⋱ \\text{ }\\text{ } ⦙ \\ \\frac{\\partial f}{\\partial x_{n,1} } ⋯ \\frac{\\partial f}{\\partial x_{n,m}} \\end{bmatrix}, \\text {where }x \\in \\mathbb{R}^{n \\times m}$ 스칼라 함수 f가 행렬 X에 의존한다고 하면, 이 함수를 행렬 X에 대해 미분한 결과는 행렬이 됨 이 행렬의 (i, j)번째 요소는 f를 X의 (i, j)번째 요소에 대해 편미분한 값 스칼라 함수 f를 행렬 $X = \\begin{bmatrix} x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22} \\end{bmatrix}$에 대해 미분하면 다음과 같은 형태의 행렬이 나옴 $\\nabla_X f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{11}} &amp; \\frac{\\partial f}{\\partial x_{12}} \\ \\frac{\\partial f}{\\partial x_{21}} &amp; \\frac{\\partial f}{\\partial x_{22}} \\end{bmatrix}$ 이때 $\\frac{\\partial f}{\\partial x_{ij}}$는 함수 f를 행렬 X의 (i,j)번째 요소에 대해 편미분한 것으로, 이렇게 구해진 행렬을 자코비안 행렬(Jacobian matrix) 이라고 함 자코비안 행렬의 각 요소는 스칼라 함수가 각 변수를 조금씩 변화시킬 때, 함수의 출력이 얼마나 변화하는지를 나타냄, 따라서 자코비안 행렬은 함수의 지역적인 변화율을 설명하는 데 사용될 수 있음 스칼라-행렬 미분은 딥러닝에서 매우 중요한 개념 신경망의 가중치는 일반적으로 행렬 형태를 가지며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 함. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 오차 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산 스칼라, 벡터, 행렬 스칼라는 단일한 수치 값입니다. 예를 들어, 10이나 2.5와 같은 단일 숫자를 스칼라라고 합니다. 벡터는 숫자들의 배열입니다. 예를 들어, [1, 2]와 같은 1차원 배열이 벡터입니다. 행렬은 숫자들의 2차원 배열입니다. 예를 들어, [[1, 2], [3, 4]]와 같은 2차원 배열이 행렬입니다. 자코비안 행렬, 헤시안 행렬 자코비안 행렬(Jacobian matrix) 자코비안 행렬은 벡터 값을 가진 함수를 벡터 변수에 대해 미분할 때 사용 함수의 입력과 출력이 모두 벡터일 때, 각 입력 변수에 대한 각 출력 변수의 편미분을 행렬 형태로 나타낸 것이 자코비안 행렬 즉, 다변수 벡터 함수의 첫 번째 도함수를 나타냅니다. 헤시안 행렬(Hessian matrix) 헤시안 행렬은 스칼라 값을 가진 함수를 행렬 변수에 대해 두 번 미분할 때 사용 즉, 함수의 두 번째 도함수(2차 미분)를 나타냄 헤시안 행렬의 각 성분은 원래 함수의 두 변수에 대한 두 번째 편미분 헤시안 행렬은 주로 함수의 곡률, 즉 최솟값, 최댓값, 또는 안장점(saddle point) 등을 판별하는 데 사용됨 따라서, 자코비안은 함수의 기울기(1차 도함수)를, 헤시안은 곡률(2차 도함수)을 나타내며, 두 행렬 모두 함수의 지역적인 동작을 이해하는 데 중요한 역할을 함 Gradient 상미분과 달리 미분 결과가 벡터 https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61 $\\nabla f(x,y) = \\begin{bmatrix} 2x+y \\ x+3y^2 +10y\\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$ 벡터를 스칼라로, 벡터를 벡터로 미분 벡터를 스칼라로 미분 $\\frac{\\partial f}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x}, … \\text { }, \\frac{\\partial f_n}{\\partial x} \\end{bmatrix}, \\text {where }x \\in \\mathbb{R}^{n}$ 벡터를 벡터로 미분 $\\frac{\\partial f}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\ ⦙ \\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x}, … \\text { }, \\frac{\\partial f_m}{\\partial x} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_{1} } ⋯ \\frac{\\partial f_m}{\\partial x_{1}} \\ ⦙ \\text{ }\\text{ } ⋱ \\text{ }\\text{ } ⦙ \\ \\frac{\\partial f_1}{\\partial x_{n} } ⋯ \\frac{\\partial f_m}{\\partial x_{n}} \\end{bmatrix}, \\text {where }x \\in \\mathbb{R}^{n}\\text{ } and\\text{ } f(x) \\in \\mathbb{R}^m$ 코드로 구현하기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 import torchx = torch.FloatTensor([[1, 2], [3, 4]]).requires_grad_(True)x1 = x + 2x2 = x - 2x3 = x1 * x2y = x3.sum()print(x1)#tensor([[3., 4.],# [5., 6.]], grad_fn=&lt;AddBackward0&gt;)print(x2)# tensor([[-1., 0.],# [ 1., 2.]], grad_fn=&lt;SubBackward0&gt;)print(x3)# tensor([[-3., 0.],# [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)print(y)# tensor(14., grad_fn=&lt;SumBackward0&gt;)y.backward() # 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분'''.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? 이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # 가중치 벡터, y.backward(v)이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, 이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행'''print(x.grad)# tensor([[2., 4.],# [6., 8.]])'''print(x3.numpy())# RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.print(x3.detach_().numpy())# array([[-3., 0.],# [ 5., 12.]], dtype=float32)PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, 이 속성이 True로 설정되면, 해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. 이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. 따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. 이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.''' Why we laern this? Loss 함수 결과값인 스칼라 힘수를 파라미터 행렬(𝜃)로 미분해야 한다면? DNN의 중간 결과물 벡터(ℎ)를 파라미터 행렬(𝜃)로 미분해야 한다면? 딥러닝에서는 일반적으로 가중치(파라미터)가 행렬 형태를 가지고 있으며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 합니다. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 손실 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 또한, DNN에서는 각 레이어의 입력(중간 결과 값)을 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 따라서, 이러한 미분 개념을 이해하는 것은 딥러닝 모델을 이해하고 최적화하는 데 매우 중요합니다. 따라서, 이러한 이유들로 인해 파라미터 행렬(𝜃)에 대한 스칼라 함수, 즉 손실 함수의 결과 값을 미분하거나, 파라미터 행렬(𝜃)에 대한 중간 결과 값 벡터 (ℎ)를 미분하는 개념을 배우는 것이 딥러닝에서 매우 중요합니다","link":"/Deep%20Learning/%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%80%E1%85%B5%E1%86%B7%E1%84%80%E1%85%B5%E1%84%92%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%B4%20%E1%84%8E%E1%85%A5%E1%84%8B%E1%85%B3%E1%86%B7%E1%84%87%E1%85%AE%E1%84%90%E1%85%A5%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-%ED%8E%B8%EB%AF%B8%EB%B6%84/"}],"tags":[{"name":"Spring이란","slug":"Spring이란","link":"/tags/Spring%EC%9D%B4%EB%9E%80/"},{"name":"객체지향","slug":"객체지향","link":"/tags/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5/"},{"name":"SOLID","slug":"SOLID","link":"/tags/SOLID/"},{"name":"다형성","slug":"다형성","link":"/tags/%EB%8B%A4%ED%98%95%EC%84%B1/"},{"name":"함수형 프로그래밍","slug":"함수형-프로그래밍","link":"/tags/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"},{"name":"1급 시민","slug":"1급-시민","link":"/tags/1%EA%B8%89-%EC%8B%9C%EB%AF%BC/"},{"name":"What to do","slug":"What-to-do","link":"/tags/What-to-do/"},{"name":"Function Interface","slug":"Function-Interface","link":"/tags/Function-Interface/"},{"name":"Lambda Expression","slug":"Lambda-Expression","link":"/tags/Lambda-Expression/"},{"name":"Anonymous Class","slug":"Anonymous-Class","link":"/tags/Anonymous-Class/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"BERT","slug":"BERT","link":"/tags/BERT/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"FLAN","slug":"FLAN","link":"/tags/FLAN/"},{"name":"Instruction Finetuning","slug":"Instruction-Finetuning","link":"/tags/Instruction-Finetuning/"},{"name":"Tramsformers","slug":"Tramsformers","link":"/tags/Tramsformers/"},{"name":"padding","slug":"padding","link":"/tags/padding/"},{"name":"max_length","slug":"max-length","link":"/tags/max-length/"},{"name":"GPT","slug":"GPT","link":"/tags/GPT/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"KoNLPy","slug":"KoNLPy","link":"/tags/KoNLPy/"},{"name":"Mecab","slug":"Mecab","link":"/tags/Mecab/"},{"name":"AutoEncoder","slug":"AutoEncoder","link":"/tags/AutoEncoder/"},{"name":"Hidden Representation","slug":"Hidden-Representation","link":"/tags/Hidden-Representation/"},{"name":"Latent Space","slug":"Latent-Space","link":"/tags/Latent-Space/"},{"name":"Embedding vector","slug":"Embedding-vector","link":"/tags/Embedding-vector/"},{"name":"Representation Learning","slug":"Representation-Learning","link":"/tags/Representation-Learning/"},{"name":"Feature Vector","slug":"Feature-Vector","link":"/tags/Feature-Vector/"},{"name":"one-hot Encoding","slug":"one-hot-Encoding","link":"/tags/one-hot-Encoding/"},{"name":"Embedding Vectors","slug":"Embedding-Vectors","link":"/tags/Embedding-Vectors/"},{"name":"Subword Embedding","slug":"Subword-Embedding","link":"/tags/Subword-Embedding/"},{"name":"Document Embedding","slug":"Document-Embedding","link":"/tags/Document-Embedding/"},{"name":"Word Embedding","slug":"Word-Embedding","link":"/tags/Word-Embedding/"},{"name":"정보이론","slug":"정보이론","link":"/tags/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/"},{"name":"Entropy","slug":"Entropy","link":"/tags/Entropy/"},{"name":"정보량","slug":"정보량","link":"/tags/%EC%A0%95%EB%B3%B4%EB%9F%89/"},{"name":"엔트로피","slug":"엔트로피","link":"/tags/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC/"},{"name":"cross entropy","slug":"cross-entropy","link":"/tags/cross-entropy/"},{"name":"KL Divergence","slug":"KL-Divergence","link":"/tags/KL-Divergence/"},{"name":"Probabilistic Perspective","slug":"Probabilistic-Perspective","link":"/tags/Probabilistic-Perspective/"},{"name":"MAP","slug":"MAP","link":"/tags/MAP/"},{"name":"MLE","slug":"MLE","link":"/tags/MLE/"},{"name":"Log-Likelihood","slug":"Log-Likelihood","link":"/tags/Log-Likelihood/"},{"name":"Cross Entropy Loss","slug":"Cross-Entropy-Loss","link":"/tags/Cross-Entropy-Loss/"},{"name":"NLL","slug":"NLL","link":"/tags/NLL/"},{"name":"Manifold hypothesis","slug":"Manifold-hypothesis","link":"/tags/Manifold-hypothesis/"},{"name":"Curse of Dimensionality","slug":"Curse-of-Dimensionality","link":"/tags/Curse-of-Dimensionality/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"Multi Stage Build&quot;","slug":"Multi-Stage-Build","link":"/tags/Multi-Stage-Build/"},{"name":"Docker Orchestration Tools","slug":"Docker-Orchestration-Tools","link":"/tags/Docker-Orchestration-Tools/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Persistence Data","slug":"Persistence-Data","link":"/tags/Persistence-Data/"},{"name":"Data Volume","slug":"Data-Volume","link":"/tags/Data-Volume/"},{"name":"Bind Mounts","slug":"Bind-Mounts","link":"/tags/Bind-Mounts/"},{"name":"mlflow","slug":"mlflow","link":"/tags/mlflow/"},{"name":"default method","slug":"default-method","link":"/tags/default-method/"},{"name":"static method","slug":"static-method","link":"/tags/static-method/"},{"name":"함수형 인터페이스","slug":"함수형-인터페이스","link":"/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"},{"name":"람다식","slug":"람다식","link":"/tags/%EB%9E%8C%EB%8B%A4%EC%8B%9D/"},{"name":"packing","slug":"packing","link":"/tags/packing/"},{"name":"unpacking","slug":"unpacking","link":"/tags/unpacking/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"python 설치","slug":"python-설치","link":"/tags/python-%EC%84%A4%EC%B9%98/"},{"name":"Tensor","slug":"Tensor","link":"/tags/Tensor/"},{"name":"Scalar","slug":"Scalar","link":"/tags/Scalar/"},{"name":"Vector","slug":"Vector","link":"/tags/Vector/"},{"name":"Matrix","slug":"Matrix","link":"/tags/Matrix/"},{"name":"Linear Layer","slug":"Linear-Layer","link":"/tags/Linear-Layer/"},{"name":"파라미터","slug":"파라미터","link":"/tags/%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/"},{"name":"행렬의 곱셈","slug":"행렬의-곱셈","link":"/tags/%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88/"},{"name":"벡터의 곱셈","slug":"벡터의-곱셈","link":"/tags/%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"},{"name":"Learning Rate","slug":"Learning-Rate","link":"/tags/Learning-Rate/"},{"name":"Gradient Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"}],"categories":[{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"핵심 원리 - 기본편","slug":"Spring/핵심-원리-기본편","link":"/categories/Spring/%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8/"},{"name":"함수형 프로그래밍","slug":"Java/함수형-프로그래밍","link":"/categories/Java/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"},{"name":"인공지능","slug":"인공지능","link":"/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/categories/ElasticSearch/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"OpenSearch","slug":"OpenSearch","link":"/categories/OpenSearch/"},{"name":"딥러닝","slug":"딥러닝","link":"/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"데이터 마이닝","slug":"데이터-마이닝","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A7%88%EC%9D%B4%EB%8B%9D/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"개념","slug":"인공지능/개념","link":"/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EA%B0%9C%EB%85%90/"},{"name":"FastAPI","slug":"FastAPI","link":"/categories/FastAPI/"},{"name":"JPA","slug":"JPA","link":"/categories/JPA/"},{"name":"Github Pages","slug":"Github-Pages","link":"/categories/Github-Pages/"},{"name":"Paper","slug":"Deep-Learning/Paper","link":"/categories/Deep-Learning/Paper/"},{"name":"REST","slug":"REST","link":"/categories/REST/"},{"name":"자연어처리","slug":"인공지능/자연어처리","link":"/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"자연어생성 개념","slug":"Deep-Learning/자연어생성-개념","link":"/categories/Deep-Learning/%EC%9E%90%EC%97%B0%EC%96%B4%EC%83%9D%EC%84%B1-%EA%B0%9C%EB%85%90/"},{"name":"Transformers","slug":"Deep-Learning/Transformers","link":"/categories/Deep-Learning/Transformers/"},{"name":"개념 정리","slug":"인공지능/개념-정리","link":"/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EA%B0%9C%EB%85%90-%EC%A0%95%EB%A6%AC/"},{"name":"자연어 처리","slug":"인공지능/자연어-처리","link":"/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/"},{"name":"자연어처리","slug":"딥러닝/자연어처리","link":"/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"기본","slug":"데이터-마이닝/기본","link":"/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B8%B0%EB%B3%B8/"},{"name":"중급 개념","slug":"Deep-Learning/중급-개념","link":"/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/"},{"name":"컨테이너 오케스트레이션","slug":"컨테이너-오케스트레이션","link":"/categories/%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%98%A4%EC%BC%80%EC%8A%A4%ED%8A%B8%EB%A0%88%EC%9D%B4%EC%85%98/"},{"name":"Basic","slug":"JPA/Basic","link":"/categories/JPA/Basic/"},{"name":"Text Summarization","slug":"Deep-Learning/Text-Summarization","link":"/categories/Deep-Learning/Text-Summarization/"},{"name":"MLOps","slug":"MLOps","link":"/categories/MLOps/"},{"name":"Java8","slug":"Java/Java8","link":"/categories/Java/Java8/"},{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"클린코드","slug":"Java/클린코드","link":"/categories/Java/%ED%81%B4%EB%A6%B0%EC%BD%94%EB%93%9C/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"파이썬","slug":"파이썬","link":"/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/"},{"name":"TIL","slug":"TIL","link":"/categories/TIL/"},{"name":"Ops","slug":"Ops","link":"/categories/Ops/"},{"name":"TainingArugments","slug":"Deep-Learning/Transformers/TainingArugments","link":"/categories/Deep-Learning/Transformers/TainingArugments/"},{"name":"OPS","slug":"OPS","link":"/categories/OPS/"},{"name":"Kotlin","slug":"Kotlin","link":"/categories/Kotlin/"},{"name":"Spring Framework","slug":"Spring/Spring-Framework","link":"/categories/Spring/Spring-Framework/"},{"name":"Spring MVC","slug":"Spring/Spring-MVC","link":"/categories/Spring/Spring-MVC/"},{"name":"통계학","slug":"통계학","link":"/categories/%ED%86%B5%EA%B3%84%ED%95%99/"},{"name":"기본 개념","slug":"Deep-Learning/기본-개념","link":"/categories/Deep-Learning/%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/"},{"name":"Docker","slug":"컨테이너-오케스트레이션/Docker","link":"/categories/%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%98%A4%EC%BC%80%EC%8A%A4%ED%8A%B8%EB%A0%88%EC%9D%B4%EC%85%98/Docker/"},{"name":"MLflow","slug":"MLOps/MLflow","link":"/categories/MLOps/MLflow/"},{"name":"Agile","slug":"Programming/Agile","link":"/categories/Programming/Agile/"},{"name":"Clean Code","slug":"Programming/Clean-Code","link":"/categories/Programming/Clean-Code/"},{"name":"Advanced Concept","slug":"Python/Advanced-Concept","link":"/categories/Python/Advanced-Concept/"},{"name":"개념","slug":"Python/개념","link":"/categories/Python/%EA%B0%9C%EB%85%90/"},{"name":"개념","slug":"파이썬/개념","link":"/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/%EA%B0%9C%EB%85%90/"},{"name":"UML","slug":"Programming/UML","link":"/categories/Programming/UML/"},{"name":"1만 시간의 법칙","slug":"TIL/1만-시간의-법칙","link":"/categories/TIL/1%EB%A7%8C-%EC%8B%9C%EA%B0%84%EC%9D%98-%EB%B2%95%EC%B9%99/"},{"name":"객체지향 프로그래밍","slug":"Python/객체지향-프로그래밍","link":"/categories/Python/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"},{"name":"TDD","slug":"Programming/TDD","link":"/categories/Programming/TDD/"},{"name":"내 코드가 그렇게 이상한가요","slug":"Programming/내-코드가-그렇게-이상한가요","link":"/categories/Programming/%EB%82%B4-%EC%BD%94%EB%93%9C%EA%B0%80-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9D%B4%EC%83%81%ED%95%9C%EA%B0%80%EC%9A%94/"},{"name":"함수형 프로그래밍","slug":"Python/함수형-프로그래밍","link":"/categories/Python/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"},{"name":"Windows CMD","slug":"Ops/Windows-CMD","link":"/categories/Ops/Windows-CMD/"},{"name":"동시성 및 비동기","slug":"Python/동시성-및-비동기","link":"/categories/Python/%EB%8F%99%EC%8B%9C%EC%84%B1-%EB%B0%8F-%EB%B9%84%EB%8F%99%EA%B8%B0/"},{"name":"설치","slug":"Ops/설치","link":"/categories/Ops/%EC%84%A4%EC%B9%98/"},{"name":"WebFlux","slug":"Kotlin/WebFlux","link":"/categories/Kotlin/WebFlux/"},{"name":"개념","slug":"Kotlin/개념","link":"/categories/Kotlin/%EA%B0%9C%EB%85%90/"},{"name":"문법","slug":"Python/문법","link":"/categories/Python/%EB%AC%B8%EB%B2%95/"},{"name":"기본","slug":"통계학/기본","link":"/categories/%ED%86%B5%EA%B3%84%ED%95%99/%EA%B8%B0%EB%B3%B8/"},{"name":"함께자리기","slug":"Programming/Agile/함께자리기","link":"/categories/Programming/Agile/%ED%95%A8%EA%BB%98%EC%9E%90%EB%A6%AC%EA%B8%B0/"}],"pages":[]}