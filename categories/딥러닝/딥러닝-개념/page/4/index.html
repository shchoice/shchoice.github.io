<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: 딥러닝 개념 - Shawn&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Shawn&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon_sh.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Shawn&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:type" content="blog"><meta property="og:title" content="Shawn&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Shawn&#039;s Blog"><meta property="og:description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Seohwan Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Shawn's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Seohwan Choi"},"publisher":{"@type":"Organization","name":"Shawn's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"차분하고 겸손하지만 확실하게!!"}</script><link rel="icon" href="/img/favicon_sh.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-D7QRVGYDET" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-D7QRVGYDET');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></li><li class="is-active"><a href="#" aria-current="page">딥러닝 개념</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-16T15:05:57.000Z" title="7/17/2023, 12:05:57 AM">2023-07-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:30.000Z" title="9/6/2024, 12:02:30 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">10 minutes read (About 1436 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-Linear-Layer/">5장. 신경망의 기본 구성요소 살펴보기 - Linear Layer</a></h1><div class="content"><h2 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h2><p>우리는 다음의 이미지를 통해 3이라고 머리가 인식하지만, 컴퓨터가 어떻게 이 이미지를 3으로 근사하도록 함수를 만들어야한다</p>
<p>우리는 $f^*$(f optimal)을 모사하는 최적의 $\hat{f}$ (f hat)을 찾아야한다</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d" alt="DigitMnist"></p>
<h3 id="Linear-Layer-란"><a href="#Linear-Layer-란" class="headerlink" title="Linear Layer 란"></a>Linear Layer 란</h3><ul>
<li><code>신경망의 가장 기본 구성 요소</code>, 딥러닝을 통해 모사하는 함수를 만들때 가장 기본이 되는 것이 Linear Layer</li>
<li>Fully-connected(FC) Layer 라고 불리기도 함<ul>
<li>입력의 모든 노드는 출력의 모든 노드와 컨넥션이 있음</li>
<li>Dense Layer 라고도 불리기도 함</li>
</ul>
</li>
<li>내부 파라미터에 따른 선형 변환을 수행하는 함수<ul>
<li>내부 파라미터를 잘 찾아내면 우리가 원하는 출력을 얻을 수 있음</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<h3 id="Linear-Layer-동작방식"><a href="#Linear-Layer-동작방식" class="headerlink" title="Linear Layer 동작방식"></a>Linear Layer 동작방식</h3><ul>
<li>각 입력 노드들에 weight(가중치)를 곱하고 모두 합친 뒤, bias(편향)을 더함</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/ed702032-9ad9-491c-b1cd-481a7a412907" alt="FCLayer02"></p>
<ul>
<li>|𝜃|&#x3D;(18,) , &#x2F;&#x2F; 18개의 파라미터가 있음! 𝑊 &#x3D; 5x3 &#x3D;15, 𝑏 &#x3D; 3</li>
</ul>
<h3 id="Linear-Layer-Equations"><a href="#Linear-Layer-Equations" class="headerlink" title="Linear Layer Equations"></a>Linear Layer Equations</h3><ul>
<li><p><strong>행렬 곱으로 구현 가능</strong></p>
</li>
<li><p>n차원에서 m차원으로의 <code>선형 변환 함수</code></p>
<ul>
<li>$x \in R^{k \times n}, w \in R^{n \times m} \rightarrow y \in R^{k \times m}$</li>
<li>$y &#x3D; f(k) &#x3D; x \cdot w + b$</li>
</ul>
</li>
<li><p>같은 표현</p>
<ul>
<li><p>𝑥를 미니배치에 관계없이 단순히 벡터로 볼 경우 : (m,n) x (n,1) &#x3D; (m,1)</p>
<ul>
<li><p>$y &#x3D; f(k) &#x3D; W^T \cdot x + b$</p>
<p>$\text{ where } x \in \mathbb{R}^n, W^T \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m \text{ and } y \in \mathbb{R}^m$</p>
</li>
</ul>
</li>
<li><p>𝑥를 미니배치(N개) 텐서로 표현할 경우 : (N,n) x (n,m) &#x3D; (N,m)</p>
<ul>
<li><p>$y &#x3D; f(k) &#x3D; x \cdot W + b$</p>
<p>$\text{ where } x \in \mathbb{R}^{k \times n}, W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^{n \times m} \text{ and } y \in \mathbb{R}^{n \times m}$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<h3 id="코드로-구현해보기"><a href="#코드로-구현해보기" class="headerlink" title="코드로 구현해보기"></a>코드로 구현해보기</h3><ul>
<li><p>parameter 정보 확인 예제</p>
<ul>
<li>gradient에 관해서는 다음 gradient descent 파트에서 다룸</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 간단한 신경망 구조를 정의</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)  <span class="comment"># 10개의 입력을 받아 5개의 출력을 내는 선형 계층</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)  <span class="comment"># 5개의 입력을 받아 1개의 출력을 내는 선형 계층</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 신경망 객체를 생성</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 데이터</span></span><br><span class="line">input_data = torch.FloatTensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>, <span class="number">10.0</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;input_data : <span class="subst">&#123;input_data&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># input_data : tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 신경망을 통해 입력 데이터를 전달</span></span><br><span class="line">output_data = model(input_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;output: <span class="subst">&#123;output_data&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># output: tensor([[2.3264]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 파라미터들에 대한 정보를 출력</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;name: <span class="subst">&#123;name&#125;</span>, param.data: <span class="subst">&#123;param.data&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    name: fc1.weight, param.data: tensor([[-0.2895, -0.1230,  0.1624,  0.0381,  0.2252,  0.2265, -0.1498,  0.0806, -0.1704,  0.2421],</span></span><br><span class="line"><span class="string">        [-0.1162,  0.0786, -0.1140,  0.0178,  0.0470,  0.2920,  0.2933,  0.2919, 0.0493, -0.0025],</span></span><br><span class="line"><span class="string">        [ 0.0196,  0.0492, -0.2049,  0.1628, -0.1038,  0.1221,  0.0516, -0.1309, -0.2128, -0.3086],</span></span><br><span class="line"><span class="string">        [ 0.0129,  0.1872, -0.1641,  0.0406,  0.1779,  0.1346, -0.1623,  0.1618, 0.0410, -0.1538],</span></span><br><span class="line"><span class="string">        [ 0.1166, -0.0591,  0.0349, -0.0866,  0.2066, -0.0777,  0.3119, -0.1021, -0.2297,  0.2657]])</span></span><br><span class="line"><span class="string">    name: fc1.bias, param.data: tensor([ 0.0787, -0.0037, -0.2033,  0.0398, -0.1233])</span></span><br><span class="line"><span class="string">    name: fc2.weight, param.data: tensor([[0.0168, 0.2259, 0.2410, 0.0145, 0.2553]])</span></span><br><span class="line"><span class="string">    name: fc2.bias, param.data: tensor([0.2295])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Gradient 계산을 위한 랜덤 타깃 값 생성</span></span><br><span class="line">target = torch.FloatTensor([<span class="number">0.5</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;target <span class="subst">&#123;target&#125;</span>&quot;</span>) <span class="comment"># target tensor([[0.5000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실 함수로 평균 제곱 오차를 사용</span></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실 계산</span></span><br><span class="line">loss = loss_fn(output_data, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 역전파를 수행하여 그라디언트를 계산</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파라미터들의 그라디언트 정보를 출력</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;name: <span class="subst">&#123;name&#125;</span>, param.grad: <span class="subst">&#123;param.grad&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    name: fc1.weight, param.grad: tensor([[0.0614, 0.1229, 0.1843, 0.2458, 0.3072, 0.3687, 0.4301, 0.4915, 0.5530, 0.6144],</span></span><br><span class="line"><span class="string">        [0.8253, 1.6507, 2.4760, 3.3013, 4.1267, 4.9520, 5.7774, 6.6027, 7.4280, 8.2534],</span></span><br><span class="line"><span class="string">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0529, 0.1057, 0.1586, 0.2114, 0.2643, 0.3171, 0.3700, 0.4228, 0.4757, 0.5285],</span></span><br><span class="line"><span class="string">        [0.9324, 1.8648, 2.7972, 3.7296, 4.6621, 5.5945, 6.5269, 7.4593, 8.3917, 9.3241]])</span></span><br><span class="line"><span class="string">    name: fc1.bias, param.grad: tensor([0.0614, 0.8253, 0.0000, 0.0529, 0.9324])</span></span><br><span class="line"><span class="string">    name: fc2.weight, param.grad: tensor([[11.5118, 23.9645,  0.0000,  2.8603,  7.8742]])</span></span><br><span class="line"><span class="string">    name: fc2.bias, param.grad: tensor([3.6528])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Raw Linear Layer 예제 (1) – nn.Module 추상 클래스를 활용</p>
<ul>
<li><p>$y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">W = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">                       [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b = torch.FloatTensor([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(W.size()) <span class="comment"># torch.Size([3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(b.size()) <span class="comment"># torch.Size([2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, W, b</span>):</span><br><span class="line">    y = torch.matmul(x, W) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">y = linear(x, W, b)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (2) – nn.Module 추상 클래스를 활용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, output_dim=<span class="number">2</span></span>):</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))</span><br><span class="line">        self.b = nn.Parameter(torch.FloatTensor(output_dim))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># |x| = (batch_size, input_dim)</span></span><br><span class="line">        y = torch.matmul(x, self.W) + self.b</span><br><span class="line">        <span class="comment"># |y| = (batch_size, input_dim) * (input_dim, output_dim)</span></span><br><span class="line">        <span class="comment">#     = (batch_size, output_dim)        </span></span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-3.7895e+32,  7.2868e-43],</span></span><br><span class="line"><span class="string">        [ 2.8026e-45,  0.0000e+00],</span></span><br><span class="line"><span class="string">        [-3.7896e+32,  7.2868e-43]], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (3) – nn.Linear 이용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-0.4061,  0.0483,  0.0804],</span></span><br><span class="line"><span class="string">        [ 0.0581,  0.0730,  0.4323]], requires_grad=True)</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0.4551, 0.4209], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (4) – nn.Linear 이용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, output_dim=<span class="number">2</span></span>):</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.linear = nn.Linear(input_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># |x| = (batch_size, input_dim)</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="comment"># |y| = (batch_size, output_dim)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-0.1267,  0.0563,  0.3951],</span></span><br><span class="line"><span class="string">        [ 0.2291,  0.3214,  0.2595]], requires_grad=True)</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0.3659, 0.4013], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Linear Layer 는 선형 함수</li>
<li>내부 가중치 파라미터(weight parameter) 𝑊와 𝑏에 의해 정의됨</li>
<li>우린 이 함수의 파라미터를 잘 조절하면, 주어진 입력에 대해 원하는 출력을 만들 수 있음</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-16T14:58:32.000Z" title="7/16/2023, 11:58:32 PM">2023-07-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:45.000Z" title="9/6/2024, 12:02:45 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 591 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88%EA%B3%BC-%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/">5장. 신경망의 기본 구성요소 살펴보기 - 행렬의 곱셈과 벡터의 곱셈</a></h1><div class="content"><h2 id="행렬의-곱셈-Matrix-Multiplication"><a href="#행렬의-곱셈-Matrix-Multiplication" class="headerlink" title="행렬의 곱셈(Matrix Multiplication)"></a>행렬의 곱셈(Matrix Multiplication)</h2><ul>
<li><p>행렬의 곱셈</p>
<ul>
<li>2개의 행렬을 입력으로 받아 <code>새로운 행렬</code>을 생성</li>
<li>첫 번째 행렬의 각 행과 두 번째 행렬의 각 열 사이의 내적을 요소로 가지는 새로운 행렬을 만듬</li>
<li>행렬 곱셈은 내적의 총합을 사용하지만 자체적으로는 다른 연산</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행렬 곱셈</span></span><br><span class="line">M = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">N = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">matrix_product = torch.mm(M, N) <span class="comment"># 두 텐서가 모두 2차원 이상인 경우, &#x27;@&#x27;는 행렬곱(matrix multiplication)을 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Matrix Product:\\n <span class="subst">&#123;matrix_product&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Matrix Product:</span></span><br><span class="line"><span class="comment">#  tensor([[ 58,  64],</span></span><br><span class="line"><span class="comment">#         [139, 154]])</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-Matrix-Multiplication-행렬-곱"><a href="#1-Matrix-Multiplication-행렬-곱" class="headerlink" title="1. Matrix Multiplication(행렬 곱)"></a>1. Matrix Multiplication(행렬 곱)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/29973390-4e07-4bf5-a650-be12868b240b" alt="MatrixMultiplication"></p>
<h3 id="2-Vector-Matrix-Multiplication-벡터와-행렬의-곱"><a href="#2-Vector-Matrix-Multiplication-벡터와-행렬의-곱" class="headerlink" title="2. Vector Matrix Multiplication (벡터와 행렬의 곱)"></a>2. Vector Matrix Multiplication (벡터와 행렬의 곱)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/321590d0-9da0-44bf-945f-90d828b4f084" alt="VectorMatrixMultiplication01"></p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4e1cd1aa-2d96-4e75-bb3e-6db31d46b176" alt="VectorMatrixMultiplication02"></p>
<h3 id="3-Batch-Matrix-Multiplication"><a href="#3-Batch-Matrix-Multiplication" class="headerlink" title="3. Batch Matrix Multiplication"></a>3. Batch Matrix Multiplication</h3><ul>
<li>같은 갯수의 행렬 쌍들에 대해서 병렬로 행렬 곱 실행</li>
<li>만약 4차원 텐서라면 (N1, N2, n, h) X (N1, N2, h, m) 이 됨</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4baa6300-8ffc-4ee4-bee2-81c14936e11c" alt="BatchMatrixMultiplication"></p>
<h2 id="벡터의-곱셈-Vector-Multiplication"><a href="#벡터의-곱셈-Vector-Multiplication" class="headerlink" title="벡터의 곱셈(Vector Multiplication)"></a>벡터의 곱셈(Vector Multiplication)</h2><p>벡터의 곱셈에는 주로 2가지의 형태로 있음</p>
<ul>
<li><p>내적 (Dot Product, Inner Product, 점곱)</p>
<ul>
<li>두 개의 벡터를 입력으로 받아 <code>스칼라</code>(단일 수치) 값을 출력</li>
<li>벡터의 내적은 같은 위치에 있는 요소들끼리 곱한 후, 그 결과를 모두 더해서 하나의 숫자를 얻음</li>
<li>내적은 벡터들 사이의 <code>유사성</code>을 측정하는 데 사용</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 벡터 내적</span></span><br><span class="line">A = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">B = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">dot_product = torch.mm(A, B) <span class="comment"># 두 텐서가 모두 1차원인 경우, &#x27;@&#x27;는 벡터 내적(dot product)을 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dot Product: <span class="subst">&#123;dot_product&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Dot Product: 32</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>외적 (Cross Product)</p>
<ul>
<li>3차원 벡터에 한정하며, 두 벡터의 외적은 새로운 벡터를 생성</li>
<li>새로운 벡터는 두 입력 벡터에 수직인 방향을 가지며, 그 크기는 두 입력 벡터 사이의 각도에 따라 달라짐</li>
<li>물리학에서 힘의 방향 계산 등에 사용</li>
</ul>
</li>
</ul>
<h2 id="벡터의-내적-vs-코사인-유사도"><a href="#벡터의-내적-vs-코사인-유사도" class="headerlink" title="벡터의 내적 vs 코사인 유사도"></a>벡터의 내적 vs 코사인 유사도</h2><ul>
<li>Dot Product<ul>
<li>$a \cdot b &#x3D; |a| |b| \cos \theta$</li>
<li>얼마나 같은 방향을 가지고 있는지 정보를 담으며,</li>
<li>벡터의 크기에도 영향을 받음</li>
</ul>
</li>
<li>Cosine Similarity<ul>
<li>$\text{cosine-similarity}(a, b) &#x3D; \frac{a \cdot b}{|a| |b|}$</li>
<li>방향성만 고려함</li>
<li>벡터의 크기 고려x</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-15T14:58:24.000Z" title="7/15/2023, 11:58:24 PM">2023-07-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:24.000Z" title="9/6/2024, 12:02:24 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">2 minutes read (About 333 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/4%EC%9E%A5-PyTorch-Tutorials-Tensor/">4장. PyTorch Tutorials - Tensor</a></h1><div class="content"><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><h3 id="Tensor란-무엇인가"><a href="#Tensor란-무엇인가" class="headerlink" title="Tensor란 무엇인가"></a>Tensor란 무엇인가</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5b8d117f-0487-4131-b90b-261a8d54914d" alt="Tensor"></p>
<ul>
<li>Scalar : 점</li>
<li>Vector : 1차원 배열</li>
<li>Matrix : 2차원 배열</li>
<li>Tensor : 3차원 이상의 배열</li>
</ul>
<h3 id="Tensor-Shape"><a href="#Tensor-Shape" class="headerlink" title="Tensor Shape"></a>Tensor Shape</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/1873595c-eae5-4056-b07f-ed0a4158ea28" alt="Tensor Shape"></p>
<p>​	$x \in \mathbb{R}^{k \times n \times m} \rightarrow |x|&#x3D;(k,n,m)$</p>
<h3 id="Matrix-Shape"><a href="#Matrix-Shape" class="headerlink" title="Matrix Shape"></a>Matrix Shape</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/fe5c94b3-57fe-4bd4-832b-cce05c96b097" alt="Matrix Shape"></p>
<ul>
<li>$x \in \mathbb{R}^{k \times n} \rightarrow |x|&#x3D;(k,n)$</li>
</ul>
<h3 id="Typical-Tensor-Shape-Tabular-Dataset"><a href="#Typical-Tensor-Shape-Tabular-Dataset" class="headerlink" title="Typical Tensor Shape : Tabular Dataset"></a>Typical Tensor Shape : Tabular Dataset</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/088e479a-0a60-4b2e-bde5-95873c2bcc55" alt="Tabular Shape"></p>
<ul>
<li>$|x|&#x3D;(n,) \Rightarrow x \in \mathbb{R}^n \text{ (vector)}$</li>
</ul>
<h3 id="Mini-batch-Consider-Parallel-Operations"><a href="#Mini-batch-Consider-Parallel-Operations" class="headerlink" title="Mini batch: Consider Parallel Operations"></a>Mini batch: Consider Parallel Operations</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/3f3b8ada-ec71-404a-9357-782900187dfa" alt="MiniBatch"></p>
<ul>
<li>$|x|&#x3D;(k,n) \Rightarrow x \in \mathbb{R}^{k \times n}$</li>
</ul>
<h3 id="Typical-Tensor-Shape-NLP"><a href="#Typical-Tensor-Shape-NLP" class="headerlink" title="Typical Tensor Shape : NLP"></a>Typical Tensor Shape : NLP</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b90beaa4-dfd0-464d-a463-abb9e9f18353" alt="NLP_Tensor_Shape"></p>
<ul>
<li>$x \in \mathbb{R}^{k \times n \times m} \rightarrow |x|&#x3D;(k,n,m)$</li>
<li>|𝑥|&#x3D;(#𝒔,#𝐰,#𝒇)</li>
<li>$|x_{(i,j)}|$&#x3D;(#𝑓, )</li>
<li>$|x_i|$&#x3D;(#𝑤, #𝑓, )</li>
</ul>
<h3 id="Typical-Tensor-Shape-Computer-Vision-GrayScale"><a href="#Typical-Tensor-Shape-Computer-Vision-GrayScale" class="headerlink" title="Typical Tensor Shape : Computer Vision(GrayScale)"></a>Typical Tensor Shape : Computer Vision(GrayScale)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/802d848b-a788-4bbb-8753-210de6260e4c" alt="CV_GrayScale_Tensor_Shape"></p>
<ul>
<li>|𝑥|&#x3D;(#𝐢𝐦𝐠, 𝐡, 𝒘)</li>
</ul>
<h3 id="Typical-Tensor-Shape-Computer-Vision-Color"><a href="#Typical-Tensor-Shape-Computer-Vision-Color" class="headerlink" title="Typical Tensor Shape : Computer Vision(Color)"></a>Typical Tensor Shape : Computer Vision(Color)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b287b7a4-88de-4dc2-83cb-a58d5da8e751" alt="CV_Color_Tensor_Shape"></p>
<ul>
<li>|𝑥|&#x3D;(#𝐢𝐦𝐠, #𝐜𝐡𝐚𝐧𝐧𝐞𝐥, 𝐡, 𝒘) (4차원이 됨)</li>
<li>$|x_i|$&#x3D;(#𝑐ℎ, ℎ, 𝑤)</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-13T14:51:23.000Z" title="7/13/2023, 11:51:23 PM">2023-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:06.000Z" title="9/6/2024, 12:02:06 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 598 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-Working-Process/">3장. 딥러닝 Overview - Working Process</a></h1><div class="content"><h3 id="우리의-목표"><a href="#우리의-목표" class="headerlink" title="우리의 목표!"></a>우리의 목표!</h3><p>주어진 데이터에 대해서 결과를 내는 가상의 함수를 모사하는 함수를 만드는 것</p>
<ul>
<li><p>예시</p>
<ul>
<li><p>주어진 숫자 그림을 보고 맞추기!</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d" alt="DigitMnist"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d">https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d</a></p>
</li>
</ul>
</li>
</ul>
<h3 id="Working-Process"><a href="#Working-Process" class="headerlink" title="Working Process"></a>Working Process</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3" alt="WorkingProcess"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3">https://github.com/shchoice/shchoice.github.io/assets/100276387/bf6feb46-8811-4897-86a1-9097e688f0f3</a></p>
<ul>
<li>문제 정의<ul>
<li>가장 중요한 부분</li>
<li>풀고자 하는 문제를 단계뼐로 나누고 simplify 하여야 한다.</li>
<li>신경망이라는 함수에 넣기 위한 x와 결과값 y가 정의되어야 한다.<ul>
<li>𝑦&#x3D;𝑓(𝑥) : 라면 끓는 이미지를 넣으면 물의 온도가 나온다 등</li>
</ul>
</li>
</ul>
</li>
<li>데이터 수집<ul>
<li>문제 정의에 따라 정해진 𝑥와 𝑦</li>
<li>풀고자 하는 문제의 영역에 따라 수집 방법이 다름<ul>
<li>NLP, CV : crawling</li>
<li>데이터분석 : 실제 수집한 데이터</li>
</ul>
</li>
<li>필요에 따라 레이블링(labeling) 작업을 수행<ul>
<li>자동적으로 label이 y로 주어질 수도 있지만, 대부분 레이블이 따로 필요함, 비지도학습 기대하지 말자</li>
</ul>
</li>
</ul>
</li>
<li>데이터 전처리 및 분석<ul>
<li>수집된 데이터를 신경망에 넣어주기 위한 형태로 가공하는 과정<ul>
<li>입출력 값을 정제(Cleansing &amp; normalization)</li>
</ul>
</li>
<li>이 과정에서 탐험적 분석(EDA)이 필요<ul>
<li>데이터 알맞은 알고리즘 찾기 위함(NLP, CV 생략되기도)</li>
</ul>
</li>
<li>CV의 경우 데이터 증강(augmentation)이 수행됨<ul>
<li>rotation, flipping, shifting 등의 연산</li>
</ul>
</li>
</ul>
</li>
<li>알고리즘 적용<ul>
<li>데이터에 대해 가설을 세우고, 해당 가설을 위한 알고리즘(모델)을 적용</li>
</ul>
</li>
<li>평가<ul>
<li>문제 정의에 따른 공정하고 올바른 평가 방법 필요 (가설을 검증하기 위한 실험 설계)</li>
<li>테스트 셋 구성</li>
<li>너무 쉽거나 어렵다면 판별력이 떨어짐, 실제 데이터와 가장 가깝게 구성되야함</li>
<li>정성적 평가(extrinsic evaluation)와 정성적 평가(intrinsic evaluation)로 나뉨</li>
</ul>
</li>
<li>배포<ul>
<li>학습과 평가가 완료된 모델 weights 파일을 배포함</li>
<li>RESTful API 등을 통해 wrapping 후 배포</li>
<li>데이터 분포의 변화에 따른 모델 업데이트 및 유지보수가 필요할 수 있음</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-12T14:53:08.000Z" title="7/12/2023, 11:53:08 PM">2023-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:19.000Z" title="9/6/2024, 12:02:19 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">2 minutes read (About 363 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-%EC%A2%8B%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%B4%EB%9E%80/">3장. 딥러닝 Overview - 좋은 인공지능이란</a></h1><div class="content"><h2 id="좋은-인공지능이란"><a href="#좋은-인공지능이란" class="headerlink" title="좋은 인공지능이란"></a>좋은 인공지능이란</h2><h3 id="인공지능-모델이란"><a href="#인공지능-모델이란" class="headerlink" title="인공지능 모델이란?"></a>인공지능 모델이란?</h3><ul>
<li>𝑥 가 주어졌을 때, 𝑦 를 반환하는 함수<ul>
<li>𝑦&#x3D;𝑓(𝑥)</li>
</ul>
</li>
<li>파라미터(weight parameter, 𝜃)란<ul>
<li>𝑓가 동작하는 방식(𝑥 가 들어왔을 때, 어떤 𝑦 를 뱉어낼 것인가?)를 결정</li>
</ul>
</li>
<li>학습이란<ul>
<li>𝑥와 𝑦의 쌍으로 이루어진 데이터가 주어졌을 때, 𝑥로부터 𝑦로 가는 관계를 배우는 것</li>
<li><code>𝑥와 𝑦를 통해 적절한 파라미터(𝜃)를 찾아내는 것</code></li>
</ul>
</li>
<li>모델이란<ul>
<li>상황에 따라 <code>알고리즘 자체</code>를 이르거나 <code>파라미터</code>를 이름</li>
</ul>
</li>
</ul>
<h3 id="좋은-인공지능-모델이란"><a href="#좋은-인공지능-모델이란" class="headerlink" title="좋은 인공지능 모델이란?"></a>좋은 인공지능 모델이란?</h3><ul>
<li><code>일반화(Generalization)</code>를 잘 하는 모델</li>
<li>보지 못한(unseen) 데이터에 대해서 좋은 예측(prediction)을 하는 모델<ul>
<li>우리는 모든 경우의 수에 대해서 데이터를 모을 수 없기 때문</li>
<li>보지 못한 경우에 대해서, 모델은 좋은 판단을 내릴 수 있어야 함</li>
</ul>
</li>
</ul>
<h3 id="기존-머신러닝의-한계"><a href="#기존-머신러닝의-한계" class="headerlink" title="기존 머신러닝의 한계"></a>기존 머신러닝의 한계</h3><ul>
<li>기존 머신러닝은 주로 선형 또는 낮은 차원의 데이터를 다루기 위해 설계되었음</li>
<li>Kernel 등을 사용하여 비선형 데이터를 다룰 수 있지만 한계가 명확함<ul>
<li>이미지, 텍스트, 음성과 같은 훨씬 더 높은 차원의 데이터들에 대해 낮은 성능을 보임</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-11T14:44:26.000Z" title="7/11/2023, 11:44:26 PM">2023-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:11.000Z" title="9/6/2024, 12:02:11 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">5 minutes read (About 750 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/3%EC%9E%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-Overview-%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%B4%EB%9E%80/">3장. 딥러닝 Overview - 딥러닝이란</a></h1><div class="content"><h2 id="딥러닝-개요"><a href="#딥러닝-개요" class="headerlink" title="딥러닝 개요"></a>딥러닝 개요</h2><h3 id="딥러닝이란"><a href="#딥러닝이란" class="headerlink" title="딥러닝이란?"></a>딥러닝이란?</h3><ul>
<li>Deep Neural Network(D NN)을 학습시켜 문제를 해결</li>
<li>인경신경망(Artificial Neural Networks)의 적통을 이어받음<ul>
<li>neuron 들로 구성된 신경망을 학습하여 문제를 해겨하도록 동작하는 함수<ul>
<li>딥러닝은 인공신경망의 부분집합</li>
<li>차이점이라면 ANN은 얇게, DNN은 깊게 쌓음</li>
</ul>
</li>
</ul>
</li>
<li>기존 신경망에 비하여 더 깊은 구조를 갖는 것이 특징<ul>
<li>이유 1. GPU를 활용한 병렬 연산 방법이 대중화되며, 신경망의 학습&#x2F;추론 속도의 비약적 증가</li>
<li>이유 2. 인터넷의 발달로 빅데이터가 널리 활용되고 이를 통해 깊은 신경망 학습시킬 수 있게 됨</li>
</ul>
</li>
</ul>
<h3 id="왜-딥러닝인가"><a href="#왜-딥러닝인가" class="headerlink" title="왜 딥러닝인가?"></a>왜 딥러닝인가?</h3><ul>
<li>비선형 함수로 기존 머신러닝에 비해 패턴 인식 능력이 월등함 ※ 이 세상 어떠한 유형의 함수든 모사할 수 있는 능력이 있다는 것이 수학적으로 증명됨, UAT(Universal Approach Theorem)</li>
<li>이미지나 텍스트, 음성과 같은 분야들에서 비약적인 성능 개선을 만듬<ul>
<li>기존 머신러닝과 달리 hand-crafted feature가 필요없음</li>
<li>단순히 raw값을 넣는 것으로, 자동으로 특징(feature)을 학습함</li>
</ul>
</li>
</ul>
<h3 id="딥러닝의-주요역사"><a href="#딥러닝의-주요역사" class="headerlink" title="딥러닝의 주요역사"></a>딥러닝의 주요역사</h3><ul>
<li><p>1980년대 역전파(Back-propagation) 알고리즘의 개발로 인한 중흥기</p>
</li>
<li><p>하지만 모델을 깊세 쌓지 못함으로써 절망감</p>
</li>
<li><p>2010년 초 ImageNet 우승과 음성 인식(Speech Recognition)의 상용화</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/80f13ae3-fc0f-408b-a6ff-37deb09e19ac" alt="ImageNet"></p>
</li>
<li><p>2015년 기계번역(Machine Translation)의 상용화 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f3fc9ae0-7502-4424-9c1a-3b107a67599c" alt="Machine Translation"></p>
<ul>
<li>자연어 처리(SequenceToSequence, seq2seq)의 시작</li>
</ul>
</li>
<li><p>2017년 알파고의 승리</p>
</li>
<li><p>2018년 GAN을 통한 이미지 합성의 발전<br> <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/80646f7d-686d-4132-afa1-295c966f776c" alt="GAN"></p>
</li>
</ul>
<h3 id="딥러닝-패러다임의-변화"><a href="#딥러닝-패러다임의-변화" class="headerlink" title="딥러닝 패러다임의 변화"></a>딥러닝 패러다임의 변화</h3><ul>
<li><p>기존 패러다임</p>
<ul>
<li><p>Hand-Crafted Feature를 추출하여 머신러닝 모델에 넣고 학습</p>
<p>※ Hand-Crafted Feature : 얼굴은 둥그렇게 되어져 있으며, 눈 코 입의 위치가 있다라는 가정들을 넣는 것 즉, 여러 sub-모듈</p>
</li>
<li><p>여러 단계의 sub-module로 이루어져 있었음</p>
<ul>
<li>여러 sub-module로 구성되어져 있어서 시스템이 무거웠으며, 여러 사람이 작업을 해야했음</li>
</ul>
</li>
<li><p>가정이 들어감, 하지만 사람의 가정이 틀릴 수도 있는 문제점이 있음</p>
</li>
</ul>
</li>
<li><p>새로운 패러다임</p>
<ul>
<li><p>Raw 값을 신경망에 넣으면 자동으로 특징(Feature)을 학습</p>
<ul>
<li>하지만, 사람이 해석하기가 어려움, 얼굴 인식이 안되면 왜 안되는지 알기 어려움(블랙박스)</li>
</ul>
</li>
<li><p>하나의 task에 대해서, </p>
<ul>
<li>하나의 신경망 모델이 존재하는 end-to-end 방식</li>
</ul>
<ul>
<li>훨씬 가볍고 혼자서도 오픈소스로 충분히 작업이 가능함</li>
</ul>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-11-16T14:46:11.000Z" title="11/16/2022, 11:46:11 PM">2022-11-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-11T13:25:10.978Z" title="9/11/2024, 10:25:10 PM">2024-09-11</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a></span><span class="level-item">22 minutes read (About 3295 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%8F%E1%85%A6%E1%84%85%E1%85%A1%E1%84%89%E1%85%B3%20%E1%84%8E%E1%85%A1%E1%86%BC%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A6%E1%84%80%E1%85%A6%20%E1%84%87%E1%85%A2%E1%84%8B%E1%85%AE%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/%EB%AA%A8%EB%8D%B8-%EC%B5%9C%EC%A0%81%ED%99%94/">모델 최적화</a></h1><div class="content"><h1 id="모델-최적화-훈련-성능과-일반화-성능의-균형"><a href="#모델-최적화-훈련-성능과-일반화-성능의-균형" class="headerlink" title="모델 최적화: 훈련 성능과 일반화 성능의 균형"></a>모델 최적화: 훈련 성능과 일반화 성능의 균형</h1><h2 id="개요"><a href="#개요" class="headerlink" title="개요"></a>개요</h2><ul>
<li>모델 최적화는 <strong>훈련 성능을 극대화</strong>하고, <strong>과대적합</strong>을 방지하여 <strong>일반화 성능</strong>을 유지하는 두 가지 방향에서 이루어져야함</li>
<li><strong>과대적합</strong>은 훈련 데이터에 지나치게 맞추는 현상이며, 반대로 <strong>일반화</strong>는 새로운 데이터에서도 좋은 성능을 유지하는 것을 의미합</li>
<li>이 글에서는 <strong>훈련 성능을 향상</strong>시키면서도 <strong>과대적합을 방지</strong>하는 방법들을 설명</li>
</ul>
<h2 id="1-과대적합-Overfitting-과-일반화-Generalization"><a href="#1-과대적합-Overfitting-과-일반화-Generalization" class="headerlink" title="1. 과대적합(Overfitting)과 일반화(Generalization)"></a><strong>1. 과대적합(Overfitting)과 일반화(Generalization)</strong></h2><h3 id="과대적합-Overfitting"><a href="#과대적합-Overfitting" class="headerlink" title="과대적합(Overfitting)"></a><strong>과대적합(Overfitting)</strong></h3><ul>
<li><strong>과대적합</strong>은 모델이 훈련 데이터의 <strong>세부적인 패턴</strong>에 지나치게 맞춰지면서, 훈련 데이터에서는 매우 좋은 성능을 내지만, <strong>새로운 데이터</strong>에서는 성능이 저하되는 현상을 의미</li>
<li>과대적합된 모델은 학습한 데이터에만 적합하고, 현실 세계의 데이터에 대한 <strong>예측 능력이 떨어짐</strong></li>
</ul>
<h3 id="일반화-Generalization"><a href="#일반화-Generalization" class="headerlink" title="일반화(Generalization)"></a><strong>일반화(Generalization)</strong></h3><ul>
<li><strong>일반화</strong>는 모델이 <strong>훈련 데이터 이외의 데이터</strong>에서 <strong>잘 작동하는 능력</strong>을 의미</li>
<li>딥러닝 모델의 성능을 제대로 평가하려면 <strong>훈련 데이터와는 다른 검증 데이터</strong>나 테스트 데이터에서 모델이 얼마나 잘 작동하는지 살펴보는 것이 중요</li>
<li>일반화를 달성하기 위해서는 훈련 중 <strong>과대적합을 방지</strong>하는 다양한 기법이 필요</li>
</ul>
<h2 id="2-훈련-성능-향상-과대적합-가능성-증가"><a href="#2-훈련-성능-향상-과대적합-가능성-증가" class="headerlink" title="2. 훈련 성능 향상 (과대적합 가능성 증가)"></a><strong>2. 훈련 성능 향상 (과대적합 가능성 증가)</strong></h2><ul>
<li>훈련 성능을 향상시키는 것은 모델이 훈련 데이터에서 좋은 성능을 내도록 <strong>최적화</strong>하는 과정</li>
<li>하지만 이 과정에서 <strong>과대적합의 위험</strong>이 증가할 수 있으므로 주의가 필요</li>
</ul>
<h3 id="1-경사하강법의-핵심-파라미터-튜닝"><a href="#1-경사하강법의-핵심-파라미터-튜닝" class="headerlink" title="1) 경사하강법의 핵심 파라미터 튜닝"></a><strong>1) 경사하강법의 핵심 파라미터 튜닝</strong></h3><ul>
<li><p><strong>옵티마이저 선택</strong> </p>
<ul>
<li>딥러닝 모델을 훈련할 때 <strong>Adam</strong>이나 <strong>SGD(Stochastic Gradient Descent)</strong> 같은 옵티마이저를 선택할 수 있음(Adam은 <strong>적응형 학습률</strong>을 사용하여 빠르고 안정적인 학습을 제공)</li>
</ul>
</li>
<li><p><strong>학습률(Learning rate)</strong></p>
<ul>
<li>학습률이 너무 높으면 훈련이 불안정해지고, 너무 낮으면 학습 속도가 느려짐</li>
<li><strong>적절한 학습률</strong>을 선택하는 것이 훈련 성능을 향상시키는 중요한 요소</li>
<li><strong>학습률 스케줄링</strong>(Learning rate scheduling)을 적용하면, 학습 도중에 학습률을 점진적으로 줄여 과대적합을 방지할 수 있음</li>
</ul>
</li>
<li><p><strong>배치 크기(Batch size)</strong></p>
<ul>
<li>배치 크기가 클수록 <strong>훈련의 안정성</strong>이 증가하지만, <strong>작은 배치 크기</strong>는 <strong>더 빠른 수렴</strong>과 <strong>일반화 성능 향상</strong>에 도움이 될 수 있음</li>
<li>프랑소와 숄레는 작은 배치 크기를 사용할 경우, <strong>노이즈</strong>를 통해 모델이 <strong>더 나은 일반화 성능</strong>을 가질 수 있다고 언급</li>
</ul>
</li>
</ul>
<h3 id="2-모델-구조-조정"><a href="#2-모델-구조-조정" class="headerlink" title="2) 모델 구조 조정"></a><strong>2) 모델 구조 조정</strong></h3><ul>
<li><p><strong>적합한 모델 선택</strong></p>
<ul>
<li>모델 구조는 해결하려는 문제에 맞게 설계되어야 함</li>
<li>간단한 문제를 해결할 때는 <strong>작은 모델</strong>로도 충분하지만, 복잡한 문제에서는 <strong>더 큰 네트워크</strong>가 필요</li>
<li>프랑소와 숄레는 모델 선택 시 <strong>베이스라인 모델</strong>을 먼저 구현하고, 이를 바탕으로 더 복잡한 모델을 설계하는 방법을 권장</li>
</ul>
</li>
<li><p><strong>층을 늘리기</strong></p>
<ul>
<li>딥러닝 모델의 깊이를 늘리는 것은 <strong>복잡한 패턴</strong>을 학습할 수 있도록 도와줌</li>
<li>하지만, 너무 많은 층을 사용하면 과대적합의 위험이 커질 수 있으므로 주의해야 함</li>
</ul>
</li>
</ul>
<h3 id="3-모델-용량-증가"><a href="#3-모델-용량-증가" class="headerlink" title="3) 모델 용량 증가"></a><strong>3) 모델 용량 증가</strong></h3><ul>
<li><strong>노드 수 증가</strong><ul>
<li>각 층의 **뉴런 수(노드 수)**를 늘리면 모델의 용량이 커져 <strong>더 복잡한 패턴</strong>을 학습할 수 있음</li>
<li>하지만, <strong>너무 큰 모델</strong>은 훈련 데이터에 과도하게 맞춰지면서 일반화 성능이 저하될 수 있음</li>
</ul>
</li>
</ul>
<h2 id="3-일반화-성능-향상-과대적합-방지"><a href="#3-일반화-성능-향상-과대적합-방지" class="headerlink" title="3. 일반화 성능 향상 (과대적합 방지)"></a><strong>3. 일반화 성능 향상 (과대적합 방지)</strong></h2><ul>
<li>과대적합을 방지하고 <strong>모델의 일반화 성능을 높이기 위해</strong>서는 다양한 <strong>정규화 기법</strong>과 <strong>모델 조정 방법</strong>을 적용할 수 있음</li>
<li>프랑소와 숄레는 딥러닝 모델이 데이터에 너무 맞춰지지 않도록 하기 위해 아래와 같은 기법들을 권장</li>
</ul>
<h3 id="1-데이터셋-큐레이션"><a href="#1-데이터셋-큐레이션" class="headerlink" title="1) 데이터셋 큐레이션"></a><strong>1) 데이터셋 큐레이션</strong></h3><ul>
<li><p><strong>더 많은 데이터 확보</strong></p>
<ul>
<li>더 많은 훈련 데이터를 확보하면 <strong>모델이 더 다양한 패턴을 학습</strong>할 수 있어 일반화 성능이 향상됨</li>
<li>데이터를 <strong>증가시키는 데이터 증강(data augmentation)</strong> 기법도 매우 유용</li>
</ul>
</li>
<li><p><strong>정확한 레이블 할당</strong></p>
<ul>
<li>잘못된 레이블을 포함한 데이터는 모델의 성능을 저하시키고 과대적합을 유발할 수 있음</li>
<li><strong>레이블 오류를 최소화</strong>하는 것이 중요</li>
</ul>
</li>
</ul>
<h3 id="2-더-나은-특성-feature-개발"><a href="#2-더-나은-특성-feature-개발" class="headerlink" title="2) 더 나은 특성(feature) 개발"></a><strong>2) 더 나은 특성(feature) 개발</strong></h3><ul>
<li>특성 공학은 모델이 학습할 수 있는 <strong>의미 있는 특성</strong>을 만들어내는 과정</li>
<li>프랑소와 숄레는 딥러닝에서 자동으로 <strong>특성을 추출</strong>할 수 있는 장점이 있지만, 여전히 <strong>좋은 특성 설계</strong>가 모델 성능에 중요하다고 강조</li>
</ul>
<h3 id="3-네트워크-용량-줄이기"><a href="#3-네트워크-용량-줄이기" class="headerlink" title="3) 네트워크 용량 줄이기"></a><strong>3) 네트워크 용량 줄이기</strong></h3><ul>
<li><strong>간소한 모델 설계</strong><ul>
<li>모델의 복잡성을 줄여 <strong>과대적합을 방지</strong></li>
<li>층의 수나 노드 수가 너무 많으면 훈련 데이터에 지나치게 맞춰지기 때문에, 문제에 적합한 <strong>적당한 크기의 모델</strong>을 사용하는 것이 중요</li>
</ul>
</li>
</ul>
<h3 id="4-가중치-규제-Regularization"><a href="#4-가중치-규제-Regularization" class="headerlink" title="4) 가중치 규제(Regularization)"></a><strong>4) 가중치 규제(Regularization)</strong></h3><ul>
<li><strong>L1, L2 규제</strong>: 모델의 가중치 크기를 제한하여 <strong>너무 큰 가중치 값</strong>을 방지</li>
<li>이를 통해 모델이 훈련 데이터에 너무 맞춰지지 않도록 하고, 더 <strong>일반화된 모델</strong>을 만들 수 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L2 규제(가중치 감쇠)를 사용하는 Dense 층 예시</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line">model.add(Dense(<span class="number">64</span>, kernel_regularizer=regularizers.l2(<span class="number">0.01</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h3 id="5-드롭아웃-Dropout"><a href="#5-드롭아웃-Dropout" class="headerlink" title="5) 드롭아웃(Dropout)"></a><strong>5) 드롭아웃(Dropout)</strong></h3><ul>
<li><strong>드롭아웃</strong>은 훈련 중에 <strong>무작위로 일부 뉴런을 비활성화</strong>하여 모델이 특정 뉴런에 <strong>의존하지 않도록</strong> 하는 정규화 기법입니다.</li>
<li>이를 통해 과대적합을 방지하고 <strong>더 나은 일반화 성능</strong>을 얻을 수 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout</span><br><span class="line"></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>

<h3 id="6-교차-검증-Cross-validation"><a href="#6-교차-검증-Cross-validation" class="headerlink" title="6) 교차 검증(Cross-validation)"></a><strong>6) 교차 검증(Cross-validation)</strong></h3><ul>
<li><strong>교차 검증</strong>은 데이터를 여러 번 나누어 학습하고 검증하여 모델이 훈련 데이터에 과적합되지 않도록 방지하는 방법입니다. 특히 <strong>소규모 데이터셋</strong>에서 유용하며, 데이터 분할에 따른 성능 차이를 줄일 수 있습니다.</li>
</ul>
<h3 id="7-조기-종료-Early-Stopping"><a href="#7-조기-종료-Early-Stopping" class="headerlink" title="7) 조기 종료(Early Stopping)"></a><strong>7) 조기 종료(Early Stopping)</strong></h3><ul>
<li><strong>조기 종료</strong>는 검증 데이터에서 성능이 더 이상 개선되지 않을 때 훈련을 멈추는 기법</li>
<li><strong>과대적합</strong>이 일어나는 것을 방지하기 위해 **검증 손실(validation loss)**이 일정 기간 동안 향상되지 않으면 훈련을 중단할 수 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">&#x27;val_loss&#x27;</span>, patience=<span class="number">5</span>)</span><br><span class="line">model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=<span class="number">100</span>, callbacks=[early_stopping])</span><br></pre></td></tr></table></figure>


<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a><strong>결론</strong></h2><h2 id="결론-1"><a href="#결론-1" class="headerlink" title="결론"></a>결론</h2><ul>
<li>딥러닝 모델의 최적화는 <strong>훈련 성능을 극대화</strong>하는 것과 동시에 <strong>과대적합을 방지</strong>하여 <strong>일반화 성능을 높이는 것</strong>이 핵심</li>
<li><strong>프랑소와 숄레</strong>는 이 두 가지 목표를 균형 있게 달성하는 것이 중요하다고 강조하며, 모델이 <strong>현실 세계에서도 높은 성능</strong>을 유지하도록 다양한 <strong>정규화 기법</strong>과 <strong>훈련 기법</strong>을 적용해야 한다고 설명</li>
<li>훈련 성능을 향상시키면서도 <strong>과대적합을 방지</strong>하는 전략을 적절히 사용하면, 모델은 더 나은 <strong>일반화 성능</strong>을 유지하며, 새로운 데이터에 대해 높은 예측력을 가질 수 있음</li>
</ul>
<h2 id="부록-1-일반화-성능을-최적화하는-전략-학습-그래프-기반-접근"><a href="#부록-1-일반화-성능을-최적화하는-전략-학습-그래프-기반-접근" class="headerlink" title="부록 1. 일반화 성능을 최적화하는 전략: 학습 그래프 기반 접근"></a>부록 1. <strong>일반화 성능을 최적화하는 전략: 학습 그래프 기반 접근</strong></h2><ul>
<li>모델 학습 과정에서 **훈련 곡선(Training curve)**과 **검증 곡선(Validation curve)**을 주의 깊게 관찰하는 것은 <strong>과대적합</strong>과 <strong>과소적합</strong>을 방지하고, 모델이 <strong>일반화 성능</strong>을 갖출 수 있도록 돕는 강력한 방법</li>
<li>이를 기반으로 모델의 <strong>네트워크 크기</strong>나 <strong>하이퍼파라미터</strong>를 점진적으로 조정하여 최적의 성능을 찾는 것이 핵심</li>
</ul>
<h3 id="1-먼저-모델을-과대적합시키기"><a href="#1-먼저-모델을-과대적합시키기" class="headerlink" title="1. 먼저 모델을 과대적합시키기"></a>1. <strong>먼저 모델을 과대적합시키기</strong></h3><ul>
<li><strong>초기에는 충분한 복잡성을 가진 네트워크</strong>로 시작하여 모델이 훈련 데이터에 잘 맞도록 해야함</li>
<li>이때 <strong>훈련 데이터에서 성능이 높아지고</strong>, <strong>검증 데이터에서 성능이 떨어지는</strong> <strong>과대적합</strong>이 발생하는 시점을 확인.</li>
<li><strong>과대적합의 징후</strong>는 훈련 세트에서는 손실이 계속해서 낮아지는데, 검증 세트에서는 손실이 낮아지다가 다시 증가하는 패턴을 보임</li>
</ul>
<h3 id="2-과대적합-발생-시-네트워크-크기-줄이기"><a href="#2-과대적합-발생-시-네트워크-크기-줄이기" class="headerlink" title="2. 과대적합 발생 시 네트워크 크기 줄이기"></a>2. <strong>과대적합 발생 시 네트워크 크기 줄이기</strong></h3><ul>
<li>모델이 <strong>과대적합</strong>되기 시작하면, 네트워크의 <strong>복잡도를 줄이기</strong> 위해 <strong>층의 수</strong>나 <strong>노드 수</strong>를 줄임</li>
<li>이를 통해 모델이 더 단순해지고, 과대적합을 방지할 수 있음<ul>
<li><strong>층 수</strong> 또는 <strong>뉴런 수</strong>를 줄여서 모델이 과도하게 복잡하지 않도록 하되, <strong>훈련 데이터에 대한 성능이 여전히 유지</strong>되는지 확인</li>
</ul>
</li>
</ul>
<h3 id="3-과소적합-발견-시-네트워크-크기-증가"><a href="#3-과소적합-발견-시-네트워크-크기-증가" class="headerlink" title="3. 과소적합 발견 시 네트워크 크기 증가"></a>3. <strong>과소적합 발견 시 네트워크 크기 증가</strong></h3><ul>
<li>네트워크를 단순화한 후 <strong>과소적합</strong>이 발생할 수 있음 (과소적합은 <strong>훈련 데이터에서의 성능</strong>이 충분히 올라가지 않는 경우)<ul>
<li><strong>훈련 곡선과 검증 곡선 모두가</strong> 손실이 계속 높은 상태에 머무르는 경우 과소적합을 의미합</li>
<li>이때는 <strong>네트워크 크기를 다시 키워야</strong> 함</li>
</ul>
</li>
<li><strong>층의 수</strong>나 <strong>노드 수</strong>를 다시 늘려 모델이 충분한 표현력을 가질 수 있도록 조정</li>
</ul>
<h3 id="4-최적의-일반화-성능을-찾기"><a href="#4-최적의-일반화-성능을-찾기" class="headerlink" title="4. 최적의 일반화 성능을 찾기"></a>4. <strong>최적의 일반화 성능을 찾기</strong></h3><ul>
<li><strong>과대적합</strong>과 <strong>과소적합</strong> 사이의 <strong>균형점</strong>을 찾기 위해, 네트워크의 복잡성을 조정하면서 <strong>훈련 데이터</strong>와 <strong>검증 데이터</strong>에서 성능이 모두 향상되는 지점을 찾아야 함</li>
<li><strong>훈련 손실</strong>은 꾸준히 낮아지고, <strong>검증 손실</strong>은 더 이상 증가하지 않는 상태가 <strong>최적의 일반화 성능</strong>을 의미</li>
<li>이를 확인하기 위해서는 <strong>훈련과 검증 곡선이 비슷하게 수렴</strong>하는 지점을 찾는 것이 중요</li>
</ul>
<h2 id="학습-곡선을-통해-모델-성능-조정하기"><a href="#학습-곡선을-통해-모델-성능-조정하기" class="headerlink" title="학습 곡선을 통해 모델 성능 조정하기"></a><strong>학습 곡선을 통해 모델 성능 조정하기</strong></h2><h3 id="훈련-및-검증-손실-그래프"><a href="#훈련-및-검증-손실-그래프" class="headerlink" title="훈련 및 검증 손실 그래프"></a><strong>훈련 및 검증 손실 그래프</strong></h3><ul>
<li><p>훈련 중 **훈련 손실(Training Loss)**과 **검증 손실(Validation Loss)**의 그래프를 주의 깊게 살펴보는 것이 중요</p>
</li>
<li><p>각 손실 값의 변화 추이를 통해 과대적합 또는 과소적합 여부를 판별할 수 있음</p>
<ul>
<li><p><strong>과대적합 발생 시</strong>: 훈련 손실은 계속해서 감소하지만, 검증 손실은 증가하기 시작</p>
</li>
<li><p><strong>과소적합 발생 시</strong>: 훈련 손실과 검증 손실 모두 높은 상태로 남아있음</p>
</li>
<li><p><strong>일반화가 잘 이루어질 때</strong>: 훈련 손실과 검증 손실이 함께 낮아지며, 검증 손실이 더 이상 증가하지 않을 때가 최적의 상태</p>
</li>
</ul>
</li>
</ul>
<h3 id="훈련-및-검증-정확도-그래프"><a href="#훈련-및-검증-정확도-그래프" class="headerlink" title="훈련 및 검증 정확도 그래프"></a><strong>훈련 및 검증 정확도 그래프</strong></h3><ul>
<li><p><strong>정확도 그래프(Accuracy Curve)</strong> 또한 학습 곡선에서 중요한 지표</p>
</li>
<li><p>훈련 정확도와 검증 정확도 사이에 큰 차이가 생기면 <strong>과대적합</strong>의 가능성이 있음</p>
<ul>
<li><p><strong>과대적합</strong>: 훈련 정확도는 계속해서 높아지지만, 검증 정확도는 일정 수준에서 멈추거나 떨어짐</p>
</li>
<li><p><strong>과소적합</strong>: 훈련 정확도 자체가 낮고, 검증 정확도도 낮은 상태가 지속</p>
</li>
<li><p><strong>일반화</strong>: 훈련과 검증 정확도가 비슷한 수준에서 안정되면 모델이 <strong>일반화</strong>가 잘 되고 있다는 신호</p>
</li>
</ul>
</li>
</ul>
<h2 id="모델-학습-시의-실용적인-예시"><a href="#모델-학습-시의-실용적인-예시" class="headerlink" title="모델 학습 시의 실용적인 예시"></a><strong>모델 학습 시의 실용적인 예시</strong></h2><p>아래는 케라스를 사용해 학습 중 <strong>훈련 및 검증 손실&#x2F;정확도</strong>를 시각화하여 네트워크 크기를 조정하고 일반화 성능을 찾는 방법</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 정의</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(input_dim,)))</span><br><span class="line">model.add(Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(output_dim, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 컴파일</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습</span></span><br><span class="line">history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=<span class="number">50</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 곡선 시각화</span></span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>], label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>], label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;accuracy&#x27;</span>], label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_accuracy&#x27;</span>], label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-11-16T14:45:29.000Z" title="11/16/2022, 11:45:29 PM">2022-11-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-09T13:05:15.098Z" title="9/9/2024, 10:05:15 PM">2024-09-09</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a></span><span class="level-item">17 minutes read (About 2514 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%E1%84%8F%E1%85%A6%E1%84%85%E1%85%A1%E1%84%89%E1%85%B3%20%E1%84%8E%E1%85%A1%E1%86%BC%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A6%E1%84%80%E1%85%A6%20%E1%84%87%E1%85%A2%E1%84%8B%E1%85%AE%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC/%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%B0%8F%20%EB%AA%A8%EB%8D%B8%20%EC%B5%9C%EC%A0%81%ED%99%94(%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%88%84%EC%88%98,%20%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9%EA%B3%BC%20%EC%9D%BC%EB%B0%98%ED%99%94)/">데이터 최적화(데이터 누수, 데이터 전처리, 데이터 분할 전략)</a></h1><div class="content"><h2 id="개요"><a href="#개요" class="headerlink" title="개요"></a>개요</h2><ul>
<li><strong>데이터 최적화</strong>는 <strong>머신러닝 모델 성능</strong>을 극대화하기 위한 필수 단계</li>
<li>모델을 학습시키기 위해서는 <strong>적절한 데이터 처리</strong>와 <strong>전처리 과정</strong>이 필요하며, 이를 통해 모델이 <strong>과대적합을 방지</strong>하고 <strong>일반화 성능</strong>을 높일 수 있음</li>
<li><strong>데이터 누수 방지</strong>와 함께 <strong>데이터 전처리</strong> 및 <strong>데이터 분할 전략</strong>을 중심으로 효율적인 데이터 준비 방법에 대해 알아봄</li>
</ul>
<h2 id="1-데이터-누수-Data-Leakage-방지"><a href="#1-데이터-누수-Data-Leakage-방지" class="headerlink" title="1. 데이터 누수(Data Leakage) 방지"></a>1. <strong>데이터 누수(Data Leakage) 방지</strong></h2><ul>
<li><strong>데이터 누수</strong>는 모델이 학습하는 동안 <strong>미래 정보를 포함하거나</strong> 평가 과정에서 <strong>테스트 데이터의 일부를 사용하는</strong> 경우 발생</li>
<li>이는 모델 성능이 부풀려질 수 있지만, <strong>실제 환경</strong>에서의 성능은 크게 떨어지게 됨</li>
<li>데이터 누수는 훈련 데이터에 <strong>부정확한 정보를 제공</strong>하여 모델이 잘못 학습하게 만드는 중요한 문제</li>
</ul>
<h3 id="데이터-누수-방지-방법"><a href="#데이터-누수-방지-방법" class="headerlink" title="데이터 누수 방지 방법"></a><strong>데이터 누수 방지 방법</strong></h3><h4 id="1-미래-정보-포함-여부-확인"><a href="#1-미래-정보-포함-여부-확인" class="headerlink" title="1) 미래 정보 포함 여부 확인"></a>1) <strong>미래 정보 포함 여부 확인</strong></h4><ul>
<li><strong>미래의 데이터를 학습에 포함</strong>시키는 것은 누수의 가장 흔한 사례</li>
<li><strong>시계열 데이터</strong>나 <strong>시간 의존적인 데이터</strong>에서는 과거 데이터만 사용해야 하며, <strong>미래 데이터를 절대 포함하지 않도록</strong> 주의해야 함</li>
</ul>
<ul>
<li>금융 시장에서 주식 가격 예측 모델을 만들 때, <strong>미래의 주가 데이터</strong>가 학습에 포함되지 않도록 해야함 → 이를 방지하기 위해 <strong>시간순으로 데이터를 분할</strong>하여 과거 데이터를 학습에 사용하고, <strong>미래 데이터를 검증용</strong>으로 사용함</li>
</ul>
<h4 id="2-중복-샘플-방지"><a href="#2-중복-샘플-방지" class="headerlink" title="2) 중복 샘플 방지"></a>2) <strong>중복 샘플 방지</strong></h4><ul>
<li>같은 샘플이 훈련과 테스트 세트에 <strong>중복</strong>되어 포함되면, 모델은 이미 학습한 데이터를 기반으로 평가되어 <strong>성능이 과대평가</strong>될 수 있음</li>
<li>데이터 분할 시에는 <strong>샘플 중복을 방지</strong>하여 데이터 누수가 발생하지 않도록 해야함</li>
</ul>
<ul>
<li>예시: 고객 데이터를 기반으로 머신러닝 모델을 학습할 때, 같은 고객이 훈련과 테스트 세트에 <strong>중복</strong>으로 포함되지 않도록 주의해야 함</li>
</ul>
<h4 id="3-전처리-단계에서의-주의"><a href="#3-전처리-단계에서의-주의" class="headerlink" title="3) 전처리 단계에서의 주의"></a>3) <strong>전처리 단계에서의 주의</strong></h4><ul>
<li>전처리 과정에서 데이터를 잘못 처리하면 <strong>데이터 누수</strong>가 발생할 수 있음</li>
<li>특히 <strong>훈련 세트와 테스트 세트</strong>를 구분하지 않고 전체 데이터에 대한 <strong>통계값</strong>을 사용하여 전처리를 하면 <strong>테스트 데이터의 정보가 학습 과정에 누출</strong>될 수 있음</li>
</ul>
<ul>
<li>예시: 결측값 처리 시 전체 데이터의 <strong>평균값</strong>이나 <strong>중앙값</strong>을 사용하는 대신, <strong>훈련 데이터에서만</strong> 통계치를 계산하고 테스트 데이터에는 그 값을 적용해야 함</li>
</ul>
<h4 id="4-특징-공학-Feature-Engineering-에서의-주의"><a href="#4-특징-공학-Feature-Engineering-에서의-주의" class="headerlink" title="4) 특징 공학(Feature Engineering)에서의 주의"></a>4) <strong>특징 공학(Feature Engineering)에서의 주의</strong></h4><ul>
<li>예측할 타깃 변수와 강하게 <strong>연관된 변수</strong>를 모델 학습에 포함하면 <strong>미래 정보가 누출</strong>되어 모델이 과대평가될 수 있음</li>
</ul>
<ul>
<li>예시: 의료 데이터를 활용한 예측 모델에서, 환자의 <strong>퇴원 여부</strong>를 예측하는 경우, 이미 퇴원 여부가 포함된 변수를 학습에 포함시키면 안됨</li>
</ul>
<h4 id="5-레이블-누수-확인"><a href="#5-레이블-누수-확인" class="headerlink" title="5) 레이블 누수 확인"></a>5) <strong>레이블 누수 확인</strong></h4><ul>
<li>모델이 학습 과정에서 <strong>타깃 변수</strong>와 관련된 정보를 <strong>직접적으로 사용하는 것</strong>은 데이터 누수의 전형적인 사례입</li>
<li>타깃 변수와 <strong>직접적인 상관관계</strong>가 있는 변수를 학습에 포함시키지 않도록 해야 함</li>
</ul>
<ul>
<li>예시: 대출 승인 여부를 예측할 때, <strong>이미 승인 여부가 기록된 변수</strong>를 모델이 학습하도록 하면 안됨</li>
</ul>
<h2 id="2-데이터-분할-전략"><a href="#2-데이터-분할-전략" class="headerlink" title="2. 데이터 분할 전략"></a>2. <strong>데이터 분할 전략</strong></h2><ul>
<li><strong>데이터 분할</strong>은 모델의 성능을 제대로 평가하기 위해 필수적인 과정</li>
<li>데이터를 적절히 분할함으로써 모델의 <strong>일반화 성능</strong>을 평가하고, <strong>과대적합</strong>을 방지할 수 있음</li>
<li>프랑소와 숄레는 데이터 분할의 중요성을 강조하며, 올바른 분할이 <strong>성능 평가</strong>에 핵심적인 역할을 한다고 언급</li>
</ul>
<h3 id="데이터-분할-방법"><a href="#데이터-분할-방법" class="headerlink" title="데이터 분할 방법"></a><strong>데이터 분할 방법</strong></h3><h4 id="1-훈련-x2F-검증-x2F-테스트-데이터-분할"><a href="#1-훈련-x2F-검증-x2F-테스트-데이터-분할" class="headerlink" title="1) 훈련&#x2F;검증&#x2F;테스트 데이터 분할"></a>1) <strong>훈련&#x2F;검증&#x2F;테스트 데이터 분할</strong></h4><ul>
<li><p>일반적으로 데이터는 세 개의 세트로 나누어 사용</p>
<ul>
<li><p><strong>훈련 세트(Train)</strong>: 모델을 학습시키는 데 사용됨</p>
</li>
<li><p><strong>검증 세트(Validation)</strong>: 하이퍼파라미터 튜닝 및 모델 선택에 사용됨</p>
</li>
<li><p><strong>테스트 세트(Test)</strong>: 최종적으로 모델 성능을 평가하는 데 사용됨 테스트 세트는 <strong>한 번만 사용</strong>하고, 이후에는 모델 학습에 포함되지 않아야함</p>
</li>
</ul>
</li>
</ul>
<h4 id="2-데이터-섞기-Shuffle"><a href="#2-데이터-섞기-Shuffle" class="headerlink" title="2) 데이터 섞기(Shuffle)"></a>2) <strong>데이터 섞기(Shuffle)</strong></h4><ul>
<li>데이터는 무작위로 섞는 것이 중요, 그렇지 않으면 <strong>데이터 순서</strong>에 따라 모델이 <strong>편향된 학습</strong>을 할 수 있음</li>
<li>특히 <strong>지역별</strong>이나 <strong>시간순</strong>으로 정렬된 데이터는 <strong>무작위로 섞어야</strong> 모델이 다양한 패턴을 학습할 수 있음</li>
</ul>
<ul>
<li>단, <strong>시계열 데이터</strong>의 경우에는 데이터를 섞으면 안 되고, <strong>시간순</strong>으로 유지해야 함, 그렇지 않으면 <strong>미래 데이터를 학습</strong>에 사용하게 되어 <strong>데이터 누수</strong>가 발생할 수 있음</li>
</ul>
<h4 id="3-층화-샘플링-Stratified-Sampling"><a href="#3-층화-샘플링-Stratified-Sampling" class="headerlink" title="3) 층화 샘플링(Stratified Sampling)"></a>3) <strong>층화 샘플링(Stratified Sampling)</strong></h4><ul>
<li>데이터셋이 <strong>불균형</strong>한 경우, 데이터 분할 시 <strong>층화 샘플링</strong>을 사용하여 <strong>클래스 비율을 유지</strong>해야함</li>
<li>이를 통해 <strong>소수 클래스</strong>가 훈련 및 검증 세트에서 <strong>충분히 포함</strong>될 수 있도록 보장</li>
</ul>
<ul>
<li>예시: <code>train_test_split</code> 함수에서 <code>stratify</code> 옵션을 사용하여 <strong>불균형 데이터</strong>의 클래스 비율을 유지</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, stratify=y, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>


<h2 id="3-데이터-전처리-Data-Preprocessing"><a href="#3-데이터-전처리-Data-Preprocessing" class="headerlink" title="3. 데이터 전처리(Data Preprocessing)"></a>3. <strong>데이터 전처리(Data Preprocessing)</strong></h2><ul>
<li><strong>데이터 전처리</strong>는 모델 학습에 있어서 중요한 단계</li>
<li>프랑소와 숄레는 데이터를 적절히 전처리해야만 모델이 <strong>일반화 성능</strong>을 높일 수 있다고 설명</li>
<li>이 과정에서 잘못된 전처리는 모델 성능을 저하시킬 수 있으며, 데이터 누수의 원인이 될 수도 있음</li>
</ul>
<h3 id="전처리-방법"><a href="#전처리-방법" class="headerlink" title="전처리 방법"></a><strong>전처리 방법</strong></h3><h4 id="1-결측값-처리-Missing-Data-Handling"><a href="#1-결측값-처리-Missing-Data-Handling" class="headerlink" title="1) 결측값 처리(Missing Data Handling)"></a>1) <strong>결측값 처리(Missing Data Handling)</strong></h4><ul>
<li>결측값을 처리하지 않고 모델에 포함하면 <strong>성능 저하</strong>가 발생할 수 있음</li>
<li>결측값은 <strong>평균값</strong>, <strong>중앙값</strong> 등으로 대체할 수 있으며, 때로는 특정 모델을 사용하여 결측값을 <strong>예측</strong>할 수 있음</li>
</ul>
<ul>
<li>중요한 점은, <strong>훈련 세트의 통계값만</strong> 사용하여 결측값을 처리해야 한다는 것, 테스트 데이터에는 훈련 세트에서 계산된 통계값을 사용해야 데이터 누수를 방지할 수 있음</li>
</ul>
<h4 id="2-범주형-변수-인코딩-Categorical-Encoding"><a href="#2-범주형-변수-인코딩-Categorical-Encoding" class="headerlink" title="2) 범주형 변수 인코딩(Categorical Encoding)"></a>2) <strong>범주형 변수 인코딩(Categorical Encoding)</strong></h4><ul>
<li><p>머신러닝 모델은 <strong>숫자형 데이터</strong>만 처리할 수 있기 때문에 <strong>범주형 변수</strong>는 숫자로 변환해야함</p>
<ul>
<li><p><strong>One-Hot Encoding</strong>: 범주형 변수를 이진 변수로 변환하여 처리하는 방법</p>
</li>
<li><p><strong>Label Encoding</strong>: 범주를 숫자로 변환하는 방식</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-스케일링-Scaling"><a href="#3-스케일링-Scaling" class="headerlink" title="3) 스케일링(Scaling)"></a>3) <strong>스케일링(Scaling)</strong></h4><ul>
<li>변수 간 스케일이 크게 다른 경우, <strong>정규화</strong>나 <strong>표준화</strong>를 통해 스케일을 맞춰줘야함</li>
<li>이는 특히 <strong>거리 기반 알고리즘</strong>에서 매우 중요함</li>
<li>이때도 스케일링은 <strong>훈련 세트</strong>로부터 계산된 값을 사용하여 <strong>테스트 세트</strong>에 적용해야함</li>
</ul>
<h2 id="4-데이터-증강-Data-Augmentation"><a href="#4-데이터-증강-Data-Augmentation" class="headerlink" title="4. 데이터 증강(Data Augmentation)"></a>4. <strong>데이터 증강(Data Augmentation)</strong></h2><ul>
<li>데이터가 충분하지 않거나 특정 클래스의 데이터가 부족한 경우, <strong>데이터 증강</strong>을 통해 데이터를 <strong>인위적으로 늘릴 수 있음</strong></li>
<li>특히 <strong>딥러닝 모델</strong>에서는 데이터 증강이 중요한 역할을 함</li>
</ul>
<h3 id="데이터-증강-방법"><a href="#데이터-증강-방법" class="headerlink" title="데이터 증강 방법"></a><strong>데이터 증강 방법</strong></h3><h4 id="1-이미지-데이터-증강"><a href="#1-이미지-데이터-증강" class="headerlink" title="1) 이미지 데이터 증강"></a>1) <strong>이미지 데이터 증강</strong></h4><ul>
<li><strong>이미지 데이터</strong>의 경우 <strong>회전</strong>, <strong>확대</strong>, <strong>축소</strong>, <strong>뒤집기</strong> 등을 통해 <strong>다양한 훈련 데이터</strong>를 생성할 수 있음</li>
<li>이는 <strong>과대적합</strong>을 방지하고, 모델이 다양한 패턴을 학습할 수 있도록 도와줌</li>
</ul>
<h4 id="2-텍스트-데이터-증강"><a href="#2-텍스트-데이터-증강" class="headerlink" title="2) 텍스트 데이터 증강"></a>2) <strong>텍스트 데이터 증강</strong></h4><ul>
<li>텍스트 데이터에서는 <strong>문장의 순서를 변경</strong>하거나 <strong>동의어로 대체</strong>하는 방식으로 데이터를 증강할 수 있음</li>
<li>이를 통해 텍스트 데이터를 더욱 다양하게 만들 수 있음</li>
</ul>
<h4 id="3-합성-데이터-생성"><a href="#3-합성-데이터-생성" class="headerlink" title="3) 합성 데이터 생성"></a>3) <strong>합성 데이터 생성</strong></h4><ul>
<li>**GAN(생성적 적대 신경망)**이나 <strong>SMOTE</strong> 기법을 통해 <strong>소수 클래스 데이터를 생성</strong>할 수 있음</li>
<li>이 방법은 데이터가 부족하거나 <strong>불균형 데이터셋</strong>에서 사용될 수 있음</li>
</ul>
<h2 id="5-데이터-품질-유지-및-관리"><a href="#5-데이터-품질-유지-및-관리" class="headerlink" title="5. 데이터 품질 유지 및 관리"></a>5. <strong>데이터 품질 유지 및 관리</strong></h2><ul>
<li>데이터 최적화의 마지막 단계는 <strong>데이터 품질 관리</strong></li>
<li>프랑소와 숄레는 <strong>데이터 일관성</strong>과 <strong>버전 관리</strong>가 중요하다고 강조하며, 데이터가 변경될 경우 <strong>모델 성능</strong>에 미치는 영향을 추적하는 것이 필수적이라고 설명</li>
</ul>
<h3 id="데이터-품질-관리-방법"><a href="#데이터-품질-관리-방법" class="headerlink" title="데이터 품질 관리 방법"></a><strong>데이터 품질 관리 방법</strong></h3><ul>
<li><strong>데이터 버전 관리</strong>: 데이터 수집 및 처리 과정에서 <strong>버전 관리</strong>를 명시하여 추적합니다. 이를 통해 데이터 변경 시 <strong>모델에 미치는 영향</strong>을 추적할 수 있음</li>
<li><strong>데이터 소스의 일관성 유지</strong>: 여러 소스에서 데이터를 수집할 때, <strong>스키마</strong>와 <strong>포맷</strong>을 일관되게 유지해야함</li>
<li><strong>데이터 품질 모니터링</strong>: 정기적으로 <strong>데이터 품질</strong>을 모니터링하여 <strong>이상값</strong>이나 <strong>패턴</strong>을 확인하고, 잘못된 데이터가 모델에 학습되지 않도록 방지해야함</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/page/3/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/page/5/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">1</a></li><li><a class="pagination-link" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/page/2/">2</a></li><li><a class="pagination-link" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/page/3/">3</a></li><li><a class="pagination-link is-current" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/matterhorn.jpg" alt="Shawn Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shawn Choi</p><p class="is-size-6 is-block">노력 백줌 열정 천줌의 소프트웨어 개발자</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, Republic of Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">138</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">64</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">110</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/shchoice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/shchoice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DevOps/"><span class="level-start"><span class="level-item">DevOps</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/"><span class="level-start"><span class="level-item">CI/CD 파이프라인</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/"><span class="level-start"><span class="level-item">Jenkins</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/"><span class="level-start"><span class="level-item">버전 관리</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5/"><span class="level-start"><span class="level-item">버전 관리 및 배포 전략</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/MLOps/"><span class="level-start"><span class="level-item">MLOps</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/MLOps/Cuda/"><span class="level-start"><span class="level-item">Cuda</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/MLOps/MLflow/"><span class="level-start"><span class="level-item">MLflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Ops/Windows-CMD/"><span class="level-start"><span class="level-item">Windows CMD</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/%EB%82%B4-%EC%BD%94%EB%93%9C%EA%B0%80-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9D%B4%EC%83%81%ED%95%9C%EA%B0%80%EC%9A%94/"><span class="level-start"><span class="level-item">내 코드가 그렇게 이상한가요</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Spring/%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8/"><span class="level-start"><span class="level-item">핵심 원리 - 기본편</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/"><span class="level-start"><span class="level-item">기타</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/Github-Pages/"><span class="level-start"><span class="level-item">Github Pages</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/"><span class="level-start"><span class="level-item">데이터베이스 &amp; 검색엔진</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/OpenSearch/"><span class="level-start"><span class="level-item">OpenSearch</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="level-start"><span class="level-item">딥러닝</span></span><span class="level-end"><span class="level-item tag">47</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/Text-Summarization/"><span class="level-start"><span class="level-item">Text Summarization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/"><span class="level-start"><span class="level-item">Transformers</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/TainingArugments/"><span class="level-start"><span class="level-item">TainingArugments</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/"><span class="level-start"><span class="level-item">논문 리뷰</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 개념</span></span><span class="level-end"><span class="level-item tag">38</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 기본 개념</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-NLP-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝을 활용한 자연어 처리(NLP) 개념</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%86%B5%EA%B3%84%ED%95%99-%EB%B0%8F-%EC%88%98%ED%95%99/"><span class="level-start"><span class="level-item">딥러닝을 위한 통계학 및 수학</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/"><span class="level-start"><span class="level-item">성능과 튜닝</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%B0%8F-%EB%B2%A4%EC%B9%98%EB%A7%88%ED%82%B9/"><span class="level-start"><span class="level-item">테스트 및 벤치마킹</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/UML/"><span class="level-start"><span class="level-item">UML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">소프트웨어 아키텍처</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/API-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">API 설계 및 아키텍처</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">마이크로서비스 아키텍처</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">웹 프로그래밍</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/FastAPI/"><span class="level-start"><span class="level-item">FastAPI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/HTTP-%EB%B0%8F-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/"><span class="level-start"><span class="level-item">HTTP 및 네트워크</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Core/"><span class="level-start"><span class="level-item">Spring Core</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Data-JPA/"><span class="level-start"><span class="level-item">Spring Data JPA</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-MVC/"><span class="level-start"><span class="level-item">Spring MVC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/"><span class="level-start"><span class="level-item">개발 환경 설정</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/"><span class="level-start"><span class="level-item">보안</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%95%94%ED%98%B8%ED%99%94/"><span class="level-start"><span class="level-item">데이터 암호화</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%84%9C%EB%B2%84-%EB%B0%8F-%EC%9D%B8%ED%94%84%EB%9D%BC/"><span class="level-start"><span class="level-item">서버 및 인프라</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/"><span class="level-start"><span class="level-item">클라우드 컴퓨팅</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EB%8F%84%EC%BB%A4-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4/"><span class="level-start"><span class="level-item">도커 &amp; 쿠버네티스</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EC%84%9C%EB%B2%84%EB%A6%AC%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">서버리스 아키텍처</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">프로그래밍</span></span><span class="level-end"><span class="level-item tag">31</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Effective-Java/"><span class="level-start"><span class="level-item">Effective Java</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">함수형 프로그래밍</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java&quot;</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Java8/"><span class="level-start"><span class="level-item">Java8</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%8F%99%EC%8B%9C%EC%84%B1-%EB%B3%91%EB%A0%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">동시성 &amp; 병렬 프로그래밍</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/Agile/"><span class="level-start"><span class="level-item">Agile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%ED%81%B4%EB%A6%B0-%EC%BD%94%EB%93%9C/"><span class="level-start"><span class="level-item">클린 코드</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-27T14:37:53.000Z">2024-09-27</time></p><p class="title"><a href="/Jenkins-Notification-Teams-Email/">Jenkins Notification(Teams, Email)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-26T14:14:55.000Z">2024-09-26</time></p><p class="title"><a href="/Jenkins-SVM-%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%83%E1%85%A9%E1%86%BC-Multibranch-Pipeline-%E1%84%89%E1%85%A5%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8/">Jenkins - SVM 연동 &gt; Multibranch Pipeline 설정 방법</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-23T14:58:06.000Z">2024-09-23</time></p><p class="title"><a href="/Jenkins-SVM-%EC%97%B0%EB%8F%99-Freestyle-Project-%EC%84%A4%EC%A0%95-%EB%B0%A9%EB%B2%95/">Jenkins - SVM 연동 &gt; Freestyle Project 설정 방법</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-22T14:51:41.000Z">2024-09-22</time></p><p class="title"><a href="/Jenkins%EC%97%90%EC%84%9C-%EB%8B%A4%EC%96%91%ED%95%9C-%EB%B9%8C%EB%93%9C-%EC%98%B5%EC%85%98-%EC%84%A0%ED%83%9D%ED%95%98%EA%B8%B0-Multi-branch-Pipeline-vs-Freestyle-Project/">Jenkins에서 다양한 빌드 옵션 선택하기 &gt; Multi-branch Pipeline vs Freestyle Project</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/">CI/CD 파이프라인</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/">Jenkins</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-20T17:29:08.000Z">2024-09-21</time></p><p class="title"><a href="/DevOps/CICD%20%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/Docker-Installation-CentOS-7/">Docker Installation - CentOS 7</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/">CI/CD 파이프라인</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/">Docker</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/1%EA%B8%89-%EC%8B%9C%EB%AF%BC/"><span class="tag">1급 시민</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AES/"><span class="tag">AES</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASGI/"><span class="tag">ASGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Anonymous-Class/"><span class="tag">Anonymous Class</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoEncoder/"><span class="tag">AutoEncoder</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bind-Mounts/"><span class="tag">Bind Mounts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CGI/"><span class="tag">CGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CORS/"><span class="tag">CORS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Entropy-Loss/"><span class="tag">Cross Entropy Loss</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Curse-of-Dimensionality/"><span class="tag">Curse of Dimensionality</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Volume/"><span class="tag">Data Volume</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker-Orchestration-Tools/"><span class="tag">Docker Orchestration Tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Document-Embedding/"><span class="tag">Document Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-Vectors/"><span class="tag">Embedding Vectors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-vector/"><span class="tag">Embedding vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Entropy/"><span class="tag">Entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FLAN/"><span class="tag">FLAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FastAPI/"><span class="tag">FastAPI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Feature-Vector/"><span class="tag">Feature Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Forward-Proxy/"><span class="tag">Forward Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function-Interface/"><span class="tag">Function Interface</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradient-Descent/"><span class="tag">Gradient Descent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gunicorn/"><span class="tag">Gunicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Representation/"><span class="tag">Hidden Representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instruction-Finetuning/"><span class="tag">Instruction Finetuning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Jenkins/"><span class="tag">Jenkins</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL-Divergence/"><span class="tag">KL Divergence</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KoNLPy/"><span class="tag">KoNLPy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/L4-%EC%8A%A4%EC%9C%84%EC%B9%98/"><span class="tag">L4 스위치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lambda-Expression/"><span class="tag">Lambda Expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latent-Space/"><span class="tag">Latent Space</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learning-Rate/"><span class="tag">Learning Rate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Layer/"><span class="tag">Linear Layer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Load-Testing/"><span class="tag">Load Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Likelihood/"><span class="tag">Log-Likelihood</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MAP/"><span class="tag">MAP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLE/"><span class="tag">MLE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Manifold-hypothesis/"><span class="tag">Manifold hypothesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix/"><span class="tag">Matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mecab/"><span class="tag">Mecab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-Stage-Build/"><span class="tag">Multi Stage Build&quot;</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLL/"><span class="tag">NLL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OSI-7%EA%B3%84%EC%B8%B5/"><span class="tag">OSI 7계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Persistence-Data/"><span class="tag">Persistence Data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probabilistic-Perspective/"><span class="tag">Probabilistic Perspective</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RPS/"><span class="tag">RPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSA/"><span class="tag">RSA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Response-Time/"><span class="tag">Response Time</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reverse-Proxy/"><span class="tag">Reverse Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SOLID/"><span class="tag">SOLID</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scalar/"><span class="tag">Scalar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spring%EC%9D%B4%EB%9E%80/"><span class="tag">Spring이란</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stress-Testing/"><span class="tag">Stress Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword-Embedding/"><span class="tag">Subword Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TCP-IP-4%EA%B3%84%EC%B8%B5/"><span class="tag">TCP/IP 4계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TPS/"><span class="tag">TPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensor/"><span class="tag">Tensor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Testing-Types/"><span class="tag">Testing Types</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Throughput/"><span class="tag">Throughput</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tramsformers/"><span class="tag">Tramsformers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UML/"><span class="tag">UML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Uvicorn/"><span class="tag">Uvicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vector/"><span class="tag">Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WAS/"><span class="tag">WAS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WSGI/"><span class="tag">WSGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/What-to-do/"><span class="tag">What to do</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cross-entropy/"><span class="tag">cross entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/default-method/"><span class="tag">default method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-commit-rule/"><span class="tag">git commit rule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-flow/"><span class="tag">git flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-merge/"><span class="tag">git merge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-rebase/"><span class="tag">git rebase</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github-flow/"><span class="tag">github flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gitlab-flow/"><span class="tag">gitlab flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/max-length/"><span class="tag">max_length</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mlflow/"><span class="tag">mlflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/one-hot-Encoding/"><span class="tag">one-hot Encoding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/packing/"><span class="tag">packing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/padding/"><span class="tag">padding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-%EC%84%A4%EC%B9%98/"><span class="tag">python 설치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/static-method/"><span class="tag">static method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unpacking/"><span class="tag">unpacking</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5/"><span class="tag">객체지향</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8B%A4%ED%98%95%EC%84%B1/"><span class="tag">다형성</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%88%84%EC%88%98/"><span class="tag">데이터 누수</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%9E%8C%EB%8B%A4%EC%8B%9D/"><span class="tag">람다식</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1/"><span class="tag">로드 밸런싱</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A6%AC%EB%B2%84%EC%8A%A4-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">리버스 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">벡터의 곱셈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%EC%A7%80%ED%91%9C/"><span class="tag">성능 지표</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%ED%85%8C%EC%8A%A4%ED%8A%B8/"><span class="tag">성능 테스트</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC/"><span class="tag">엔트로피</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9B%B9-%EC%84%9C%EB%B2%84/"><span class="tag">웹 서버</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EB%9F%89/"><span class="tag">정보량</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/"><span class="tag">정보이론</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%8B%A4%EC%9D%B4%EC%96%B4%EA%B7%B8%EB%9E%A8/"><span class="tag">클래스 다이어그램</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%8F%AC%EC%9B%8C%EB%93%9C-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">포워드 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"><span class="tag">함수형 인터페이스</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="tag">함수형 프로그래밍</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">행렬의 곱셈</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Seohwan Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>