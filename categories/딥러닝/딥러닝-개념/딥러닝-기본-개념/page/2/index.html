<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: 딥러닝 기본 개념 - Shawn&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Shawn&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon_sh.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Shawn&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:type" content="blog"><meta property="og:title" content="Shawn&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Shawn&#039;s Blog"><meta property="og:description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Seohwan Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Shawn's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Seohwan Choi"},"publisher":{"@type":"Organization","name":"Shawn's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"차분하고 겸손하지만 확실하게!!"}</script><link rel="icon" href="/img/favicon_sh.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-D7QRVGYDET" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-D7QRVGYDET');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></li><li><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a></li><li class="is-active"><a href="#" aria-current="page">딥러닝 기본 개념</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-27T14:59:02.000Z" title="7/27/2023, 11:59:02 PM">2023-07-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:04:14.000Z" title="9/6/2024, 12:04:14 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">2 minutes read (About 356 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/2%EC%9E%A5-Probabilistic-Perspective-Introduction/">2장. Probabilistic Perspective - Introduction</a></h1><div class="content"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Again-our-objective-is"><a href="#Again-our-objective-is" class="headerlink" title="Again, our objective is"></a>Again, our objective is</h3><ul>
<li>가상의 함수를 모사하여, 원하는 출력값을 반환하는 신경망의 파라미터를 찾자.</li>
<li>그래서 우리는 Deep Neural Networks를 이야기할 때,<ul>
<li>Gradient Descent</li>
<li>Back-Propagation</li>
<li>Feature Vector</li>
<li>and blah blah..</li>
</ul>
</li>
<li>이제는 우리의 생각을 확장시켜야 할 때! → Probabilistic Perspective!</li>
</ul>
<h3 id="Probabilistic-Perspective"><a href="#Probabilistic-Perspective" class="headerlink" title="Probabilistic Perspective"></a>Probabilistic Perspective</h3><ul>
<li><p>이 세상은 확률에 기반</p>
<ul>
<li><p>아래의 그림에 대해서 모두가 같은 대답을 하지는 않을 것 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702" alt="duckrabbit"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702">https://github.com/shchoice/shchoice.github.io/assets/100276387/e711d4ed-678f-4058-ae5b-fc6bff6fc702</a></p>
</li>
<li><p>우리의 새로운 목표: <code>확률 분포</code>를 학습하는 것</p>
</li>
</ul>
</li>
<li><p>Before vs After</p>
<ul>
<li>Before<ul>
<li>함수를 배우자(모사하자)</li>
</ul>
</li>
<li>After<ul>
<li>확률 분포 함수를 배우자<ul>
<li>수학적으로 더 설명이 가능해짐</li>
<li>불확실성까지 학습</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a" alt="BeforeAfter"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a">https://github.com/shchoice/shchoice.github.io/assets/100276387/172bd68c-53e9-4b00-8506-929d68b9ca1a</a></p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>Neural Networks는 확률 분포 함수를 모델링할 수 있음</li>
<li>이를 통해 가상의 확률 분포 함수 𝑃(𝑦 | 𝑥)를 근사(approximation)할 것</li>
<li>대부분의 최신 기술들은 이 관점에 기반을 두고 만들어짐</li>
<li>DNN을 확률 분포로 보았을 때, 가능한 이론들에 대해서 앞으로 이야기 할 것<ul>
<li>Likelihood</li>
<li>Maximum Likelihood Estimation(MLE)</li>
<li>Maximum A Posterior(MAP) Estimation</li>
<li>Cross Entropy &amp; KL-Divergence</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-26T14:21:28.000Z" title="7/26/2023, 11:21:28 PM">2023-07-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:40.000Z" title="9/6/2024, 12:03:40 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/1%EC%9E%A5-Representation-Learning-AutoEncoders/">1장. Representation Learning - AutoEncoders</a></h1><div class="content"><h2 id="AutoEncoders"><a href="#AutoEncoders" class="headerlink" title="AutoEncoders"></a>AutoEncoders</h2><ul>
<li><p>Overview</p>
<ul>
<li>인코더(encoder)와 디코더(decoder)를 통해 압축과 해제를 실행<ul>
<li>인코더는 입력(𝑥)의 정보를 최대한 보존하도록 손실 압축을 수행</li>
<li>디코더는 중간 결과물(𝑧)의 정보를 입력(𝑥)과 같아지도록 압축 해제(복원)를 수행</li>
</ul>
</li>
<li>복원을 성공적으로 하기 위해, autoencoder는 특<strong>징(feature)을 추출하는 방법을 자동으로 학습</strong></li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f9947a9e-f1f4-4e4c-b8ab-bba2427ffa45" alt="Encoder-Decoder"></p>
</li>
<li><p>Encoder</p>
<ul>
<li><p>복원에 필요한 정보를 중심으로 손실 압축을 수행</p>
</li>
<li><p>필요없는 정보(뻔한 특징)는 버릴 수도 있음</p>
<ul>
<li><p>예시1) 일반적인 사람의 얼굴을 학습할 때: 사람의 얼굴에서 눈은 2개이다 등</p>
</li>
<li><p>예시2)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2e64e11e-6879-4644-bb6c-d416e8cb5ebc" alt="NoMeaningMNIST"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Bottleneck(𝒛)</p>
<ul>
<li>입력(𝒙)에 비해 작은 차원으로 구성</li>
<li>따라서 정보의 선택과 압축이 발생, 차원에 따라 압축의 정도를 결정함<ul>
<li>집에 불이 나서 탈출할 때, 무엇을 들고 나갈 것인가?</li>
</ul>
</li>
<li>그러므로 𝒛 는 입력(𝒙)에 대한 feature vector라고 할 수 있다.</li>
<li>압축의 효율이 높아야 하므로, 입력에 비해 dense vector일 것</li>
</ul>
</li>
<li><p>Decoder</p>
<ul>
<li>압축된 중간 결과물(𝒙)을 바탕으로 최대한 입력(𝒛)과 비슷하게 복원 : $\hat{x}$</li>
<li>보통 MSELoss 를 통해 최적화 수행 ($MSE&#x3D;|\hat{x}-x|^2_2$)</li>
<li>뻔한 정보는 주어지지 않더라도 어차피 알 수 있기에 복원 가능</li>
</ul>
</li>
</ul>
<h2 id="Hidden-Representation"><a href="#Hidden-Representation" class="headerlink" title="Hidden Representation"></a>Hidden Representation</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul>
<li><p>인코더로부터 나온 중간 결과물(𝒛)은 입력(𝒙)에 대한 feature vector이다.</p>
</li>
<li><p>feature vector의 각 차원은 어떤 의미를 내포하고 있을까?</p>
<ul>
<li><p>인코더의 결과물 𝒛를 plot 하였을 때, 비슷한 샘플들은 비슷한 곳에 위치함을 확인</p>
</li>
<li><p>이 plot이 뿌려진 공간을 hidden(latent) space라고 부름(잠재공간, feature vector가 위치하는 곳)</p>
<ul>
<li>Input space의 MNIST 샘플이 latent space에 embedding 된 것</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c02a4df6-74c9-49f3-b269-87abd0764629" alt="LatentSpace"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Mapping-to-Hidden-Latent-Space"><a href="#Mapping-to-Hidden-Latent-Space" class="headerlink" title="Mapping to Hidden(Latent) Space"></a>Mapping to Hidden(Latent) Space</h3><ul>
<li>각 <strong>레이어의 결과물</strong>을 <code>hidden vector</code> 라고 부름</li>
<li>모두 feature vector라고 볼 수 있음</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/118c8c4d-dd4d-41cf-bc22-e524f3c77867" alt="hiddenSpace"></p>
<h3 id="Hidden-Latent-Representation"><a href="#Hidden-Latent-Representation" class="headerlink" title="Hidden(Latent) Representation"></a>Hidden(Latent) Representation</h3><ul>
<li><p>Tabular data의 feature vector와 달리, hidden vector는 해석이 어려움</p>
<ul>
<li>해석하고자 하는 연구(XAI. Explainable AI)들이 이어지고 있으나, 아직 갈 길이 멀다.</li>
</ul>
</li>
<li><p>하지만, 비슷한 특징을 가진 샘플은 비슷한 hidden vector를 가질 것</p>
</li>
<li><p>만약 각 차원이 명확하게 하나의 의미를 지닌다면 각 차원의 숫자를 조절하여 원하는 이미지를 합성해 낼 수 있을 것</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4acbea3d-5fa3-4503-8cd8-4662a03b798f" alt="hiddenSpace02"></p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>오토인코더(AE)는 압축과 해제를 반복하며 특징 추출을 자동으로 학습<ul>
<li>필요한 정보와 필요없는 정보를 구분할 수 있게되는 것</li>
</ul>
</li>
<li>인코더로부터 나온 중간 결과물(𝒛)은 입력(𝑥)에 대한 feature vector이다.<ul>
<li>a.k.a Embedding vector</li>
<li>인코더에 통과시키는 것은 feature vector에 대한 embedding 과정이라고 볼 수 있음</li>
</ul>
</li>
<li>Hidden layer의 결과값들을 hidden vectors라 부르며, 이들은 샘플의 feature를 담고 있음<ul>
<li>여러개의 hidden vector들을 feature vector라 부를 수 있음</li>
<li>딥러닝에서는 label을 classify하기 위한 feature 를 자동으로 추출하여 학습(전통적인 머신러닝과의 차이점)</li>
</ul>
</li>
<li>신경망(또는 레이어)을 통과시키는 것은 입력 공간(input space)에서 잠재 공간(latent space)로의 맵핑 과정<ul>
<li>고차원 공간(high-dimensional space) → 저차원 공간(lower-dimensional space)</li>
</ul>
</li>
<li>Hidden representaion을 해석하는 것은 매우 어려움<ul>
<li>하지만 비슷한 샘플은 비슷한 hidden representation을 지닐 것!</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-25T14:56:47.000Z" title="7/25/2023, 11:56:47 PM">2023-07-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:50.000Z" title="9/6/2024, 12:03:50 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">8 minutes read (About 1171 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/1%EC%9E%A5-Representation-Learning-One-hot-Encoding-%EB%B0%8F-Embedding-Vector/">1장. Representation Learning - One-hot Encoding 및 Embedding Vector</a></h1><div class="content"><h2 id="One-hot-Encoding"><a href="#One-hot-Encoding" class="headerlink" title="One-hot Encoding"></a>One-hot Encoding</h2><h3 id="Categorical-Value-vs-Continuous-Value"><a href="#Categorical-Value-vs-Continuous-Value" class="headerlink" title="Categorical Value vs Continuous Value"></a>Categorical Value vs Continuous Value</h3><ul>
<li>Categorical Value<ul>
<li>보통은 discrete value</li>
<li>단어, 클래스</li>
</ul>
</li>
<li>Continuous Value<ul>
<li>키, 몸무게</li>
</ul>
</li>
<li>Categorical Value와 Continuous Value의 가장 결정적인 차이점<ul>
<li>Continous value는 비슷한 값은 비슷한 의미를 지니지만</li>
<li>Categorical value는 비슷한 값일지라도 상관없는 의미를 지닌다.</li>
</ul>
</li>
</ul>
<h3 id="One-hot-Encoding의-필요성"><a href="#One-hot-Encoding의-필요성" class="headerlink" title="One-hot Encoding의 필요성"></a>One-hot Encoding의 필요성</h3><ul>
<li><p>One-hot Encoding의 필요성을 느끼기 위해 아래의 단어를 사전 순으로 index에 mapping 해보자</p>
<table>
<thead>
<tr>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody><tr>
<td>공책</td>
<td>딱풀</td>
<td>볼펜</td>
<td>샤프</td>
<td>연필</td>
<td>자</td>
<td>필기장</td>
</tr>
</tbody></table>
<ul>
<li>상식적으로는<ul>
<li>distance(연필, 볼펜) &lt; distance(연필, 자)</li>
<li>distance(공책, 필기장) &lt; distance(공책, 딱풀)</li>
</ul>
</li>
<li>하지만 이 테이블에서는 아래와 같은 결과가 나온다.<ul>
<li>|연필 - 볼펜| &#x3D; 2	&gt;	1 &#x3D; |연필 - 자|</li>
<li>|공책 - 필기장| &#x3D; 6	&gt;	1 &#x3D; |공책 - 딱풀|</li>
</ul>
</li>
<li>즉, 범주형 데이터를 임의의 숫자로 표현하게 되면, 텍스트 간에 원래 존재하지 않았던 ‘크기’나 ‘순서’의 개념이 부여됨. 이로 인해 범주형 데이터 간에 실제로는 없는 거리 혹은 차이가 존재하는 것처럼 해석되어, 불필요하거나 잘못된 정보를 학습하는 결과를 초래할 수 있음</li>
</ul>
</li>
</ul>
<h3 id="One-hot-Encoding-이란"><a href="#One-hot-Encoding-이란" class="headerlink" title="One-hot Encoding 이란"></a>One-hot Encoding 이란</h3><ul>
<li><p>One-hot 인코딩은 범주형 데이터를 컴퓨터가 이해할 수 있는 형태로 변환하는 방법</p>
</li>
<li><p>각각의 범주를 벡터의 형태로 표현하며, 각 범주의 위치에 해당하는 인덱스만 1로 표시하고 나머지는 0으로 표시</p>
</li>
<li><p>크기가 의미를 갖는 integer 값 대신, 1개의 1과 n-1개의 0으로 이루어진 n차원의 벡터</p>
<table>
<thead>
<tr>
<th></th>
<th>index</th>
<th>공책</th>
<th>딱풀</th>
<th>볼펜</th>
<th>샤프</th>
<th>연필</th>
<th>자</th>
</tr>
</thead>
<tbody><tr>
<td>딱풀</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>연필</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>자</td>
<td>5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<ul>
<li>n개의 항목 → n차원</li>
<li>즉, 6개의 항목 → 6차원</li>
</ul>
</li>
<li><p>Vector의 대부분의 element가 0인 경우 Sparse Vector라고 부름</p>
<ul>
<li>반대 개념 : Dense Vector ↔ Sparse Vector</li>
</ul>
</li>
<li><p>One-hot Encoding의 장점</p>
<ul>
<li>범주형 데이터를 숫자로 변환</li>
<li>범주 간의 순서 없음</li>
<li>특성 간의 독립성 보장</li>
</ul>
</li>
<li><p>One-hot Encoding의 단점</p>
<ul>
<li>서로 다른 두 벡터는 항상 직교(orthogonal)한다. (element-wise 곱 &#x3D; 0)<ul>
<li>Cosine Similarity가 0, 따라서 두 샘플 사이의 유사도(거리)를 구할 수 없음</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Embedding-Vectors"><a href="#Embedding-Vectors" class="headerlink" title="Embedding Vectors"></a>Embedding Vectors</h2><h3 id="Motivation-of-Embedding-Vectors"><a href="#Motivation-of-Embedding-Vectors" class="headerlink" title="Motivation of Embedding Vectors"></a>Motivation of Embedding Vectors</h3><ul>
<li><p>NLP에서 단어는 categorical value &amp; discrete value의 속성을 갖음</p>
<ul>
<li>따라서 one-hot representation으로 표현</li>
<li>하지만 이는 실제 존재하는 단어 사이의 유사도를 표현할 수 없음</li>
<li>따라서 실제적으로는 좀더 고급화된 Embedding 기법을 사용</li>
</ul>
</li>
<li><p>다른 Embedding Vectors 표현 기법</p>
<ul>
<li><p>Contextual Word Embedding (문맥적 단어 임베딩)</p>
<ul>
<li><p>기존의 단어 임베딩 방법은 단어의 의미가 문맥에 따라 달라질 수 있다는 점을 고려하지 못했는데 이를 해결하기 위해 등장</p>
</li>
<li><p>단어를 임베딩할 때 주변 문맥을 고려해 동일한 단어라도 다른 문맥에서는 다른 임베딩 벡터를 갖게함</p>
</li>
<li><p>BERT, ELMO 등이 해당됨</p>
</li>
<li><p>코드로 구현하기</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># BERT 모델과 토크나이저 초기화</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 문장</span></span><br><span class="line">sentences = [<span class="string">&quot;I went to the store&quot;</span>, <span class="string">&quot;I went to the school&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 토큰화 및 임베딩</span></span><br><span class="line">inputs = tokenizer(sentences, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 각 문장의 [CLS] 토큰에 대한 임베딩 가져오기</span></span><br><span class="line">embeddings = outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line"><span class="built_in">print</span>(embeddings)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Subword Embedding (서브워드 임베딩)</p>
<ul>
<li>특정 언어들은 단어를 더 작은 의미 단위로 분리할 수 있음</li>
<li>이런 경우, 서브워드 임베딩을 사용하여 작은 단위들을 학습할 수 있음</li>
<li>BPE(Byte Pair Encoding), SentencePiece 등이 해당됨</li>
</ul>
</li>
<li><p>Document Embedding (문서 임베딩)</p>
<ul>
<li>문서 전체를 하나의 벡터로 표현하는 방법</li>
<li>문서 임베딩은 문서의 전체적인 의미를 이해하는데 도움이 됨</li>
<li>Doc2Vec, FastText 등이 해당</li>
</ul>
</li>
<li><p>Word Embedding (단어 임베딩)</p>
<ul>
<li>고차원의 One-hot 벡터를 저차원의 실수 벡터로 변환하는 기법</li>
<li>비슷한 의미를 가진 단어들이 벡터 공간에서 가까이 위치하도록 학습</li>
<li>Word2Vec, GloVe 등이 해당됨</li>
</ul>
</li>
</ul>
</li>
<li><p>개인적인 경험으로는 Contextual Word Embedding + Subword Embedding을 결합해서 가장 많이 사용하는 것 같으며, Word Embedding은 김기현 님의 말씀에 의하면 Embedding에 적합하지 않다고 말씀해주신 것으로 알고 있다.</p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>Categorical Value는 One-hot Encoding을 통해 벡터로 표현됨</li>
<li>Sparse Vector는 벡터 간 유사도 계산이 어려움 → One-hot의 단점</li>
<li>따라서 Dense Vector로 표현할 필요가 있음 → Contextual Embedding 등을 사용!</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-24T14:40:25.000Z" title="7/24/2023, 11:40:25 PM">2023-07-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:45.000Z" title="9/6/2024, 12:03:45 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">6 minutes read (About 889 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EC%A4%91%EA%B8%89%20%EA%B0%9C%EB%85%90/1%EC%9E%A5-Representation-Learning-Feature-Vector/">1장. Representation Learning - Feature Vector</a></h1><div class="content"><h2 id="Representation-Learning-표현-학습"><a href="#Representation-Learning-표현-학습" class="headerlink" title="Representation Learning (표현 학습)"></a>Representation Learning (표현 학습)</h2><ul>
<li>머신러닝의 하위 분야로서, 데이터의 숨겨진 구조를 학습하여 복잡한 데이터를 효율적이고 쉽게 이해할 수 있는 표현 형태로 변환하는 방법을 연구하는 분야</li>
<li>이 변환된 형태를 통해 원본 데이터의 중요한 특성이나 패턴을 찾아내고, 이를 사용해 효과적으로 학습 및 예측을 수행</li>
<li>원본 데이터의 복잡성을 줄이고, 노이즈를 제거하며, 데이터의 중요한 특성을 보다 명확하게 강조하는 역할, 이를 통해 머신러닝 모델의 성능을 향상시키고, 학습 과정을 단순화</li>
<li>오토인코더, 딥 비지도 학습, 임베딩 학습 등은 표현 학습의 대표적인 예시<ul>
<li>원본 데이터에서 중요한 특성을 추출하고</li>
<li>차원 공간에서 표현하는 방법을 학습하며</li>
<li>결과적으로 데이터의 가장 핵심적인 특성만을 잘 보존하는 표현을 찾아냄</li>
</ul>
</li>
</ul>
<h2 id="Feature-특징"><a href="#Feature-특징" class="headerlink" title="Feature(특징)"></a>Feature(특징)</h2><h3 id="Feature란"><a href="#Feature란" class="headerlink" title="Feature란?"></a>Feature란?</h3><ul>
<li>샘플을 잘 설명하는 특징</li>
<li>사람을 설명할 때 좋은 특징<ul>
<li>Continuous: 나이, 키, 몸무게, 소득</li>
<li>Categorical: 성별, 직업, 거주지, 출신 학교&#x2F;학과</li>
</ul>
</li>
<li>나쁜 특징<ul>
<li>(생물 분류학적) 종<ul>
<li>모두가 호모 사피엔스이므로 구분이 불가능</li>
</ul>
</li>
<li>주민등록번호, 이름<ul>
<li>전 국민의 수 만큼 momory가 필요할 것</li>
<li>주민등록번호만으로는 (비록 일부 특징이 유사하여도) 두 사람의 유사도를 알 수 없음</li>
<li>Categorical value로 볼 수 있음</li>
</ul>
</li>
</ul>
</li>
<li><strong>특징을 통해 우리는 특정 샘플을 수치화</strong> 할 수 있음</li>
<li>현실에서의 대표적인 Feature의 예 - 몽타주(Montage)<ul>
<li>범인의 얼굴을 특정하기 위해서, 목격자들에게 물어서 나온 특징들을 합쳐 만든 것<ul>
<li>좋은 단서<ul>
<li>뺨에 큰 붉은 반점</li>
<li>쳐진 눈</li>
<li>긴 생머리</li>
</ul>
</li>
<li>나쁜 단서<ul>
<li>눈이 2개</li>
<li>귀가 2개</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Feature-in-Machine-Learning"><a href="#Feature-in-Machine-Learning" class="headerlink" title="Feature in Machine Learning"></a>Feature in Machine Learning</h3><ul>
<li>MNIST Classifcation<ul>
<li>특정 위치에 곧은(휘어진) 선이 얼마나 있는가?</li>
<li>특정 위치에 선이 얼마나 굵은가?</li>
<li>특정 위치에 선이 얼마나 기울어져 있는가?</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de">https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de</a></p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/7b2f4861-9861-41c1-b00b-a9f519e4c5de" alt="MNIST"></p>
<h3 id="No-Need-of-Hand-crafted-Feature-in-Deep-Learning"><a href="#No-Need-of-Hand-crafted-Feature-in-Deep-Learning" class="headerlink" title="No Need of Hand-crafted Feature in Deep Learning"></a>No Need of Hand-crafted Feature in Deep Learning</h3><ul>
<li>Traditional Machine Learning<ul>
<li>사람이 데이터를 면밀히 분석 후, 가정을 세움</li>
<li>가정에 따라 전처리를 하여 feature를 추출</li>
<li>추출된 feature를 model에 넣어 학습</li>
<li>장점 : 사람이 해석하기 쉬움</li>
<li>단점 : 사람이 미처 생각하지 못한 특징의 존재 가능성</li>
</ul>
</li>
<li>Current Deep Learning<ul>
<li>Raw 데이터에 <strong>최소한의 전처리</strong>(e.g. scale)를 수행</li>
<li>데이터를 model에 넣어 학습</li>
<li>장점 : 구현이 용이함, 미처 발견하지 못한 특징도 활용</li>
<li>단점 : 사람이 해석하기 어려움</li>
</ul>
</li>
</ul>
<h3 id="Feature-Vector"><a href="#Feature-Vector" class="headerlink" title="Feature Vector"></a>Feature Vector</h3><ul>
<li>각 <code>특징들을 모아서 하나의 vector</code>로 만든 것<ul>
<li>Tabular Dataset의 각 row도 이에 해당</li>
</ul>
</li>
<li>각 차원(dimension)은 어떤 속성에 대한 level을 나타냄<ul>
<li>각 속성에 대한 level이 비슷할수록 비슷한 샘플이라고 볼 수 있음</li>
</ul>
</li>
<li>우리는 feature vector를 통해 샘플 사이의 거리(유사도)를 계산할 수 있음</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>키</th>
<th>몸무게</th>
<th>나이</th>
<th>월 소득</th>
<th>출신</th>
</tr>
</thead>
<tbody><tr>
<td>로버트</td>
<td>174cm</td>
<td>78kg</td>
<td>42</td>
<td>100</td>
<td>미국 매사추세츠</td>
</tr>
<tr>
<td>캡틴아메리카</td>
<td>183cm</td>
<td>88kg</td>
<td>58</td>
<td>1000</td>
<td>미국 뉴욕</td>
</tr>
</tbody></table>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-23T14:44:53.000Z" title="7/23/2023, 11:44:53 PM">2023-07-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:25.000Z" title="9/6/2024, 12:03:25 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 565 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Learning-Rate/">7장. 기초 최적화 방법 Gradient Descent - Learning Rate</a></h1><div class="content"><h2 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h2><h3 id="Learning-Rate-in-Gradient-Descent"><a href="#Learning-Rate-in-Gradient-Descent" class="headerlink" title="Learning Rate in Gradient Descent"></a>Learning Rate in Gradient Descent</h3><ul>
<li>파라미터가 업데이트 될 때, gradient의 크기에 영향을 받게 됨<ul>
<li>이 때, learning rate가 step-size를 정해주게 됨</li>
</ul>
</li>
<li>Equation<ul>
<li>$\theta \gets \theta - \eta \frac{\partial L(\theta)}{\partial \theta} &#x3D; \theta - \eta \nabla_\theta L(\theta)$</li>
</ul>
</li>
</ul>
<h3 id="Learning-Rate-에-따른-최적화-데이터나-모델-아키텍처에-따라-lr은-변함"><a href="#Learning-Rate-에-따른-최적화-데이터나-모델-아키텍처에-따라-lr은-변함" class="headerlink" title="Learning Rate 에 따른 최적화 (데이터나 모델 아키텍처에 따라 lr은 변함)"></a>Learning Rate 에 따른 최적화 (데이터나 모델 아키텍처에 따라 lr은 변함)</h3><ul>
<li>Large LR<ul>
<li>너무 큰 Loss가 발산할 수 있음</li>
</ul>
</li>
<li>Small LR<ul>
<li>너무 작은 LR은 수렴이 늦음</li>
<li>자칫 local minima에 빠질 수 있음</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589" alt="LearningRate"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589">https://github.com/shchoice/shchoice.github.io/assets/100276387/045601bd-7157-43fd-9d93-d9eaacbc7589</a></p>
<h3 id="Learning-Rate-는-중요한-하이퍼파라미터"><a href="#Learning-Rate-는-중요한-하이퍼파라미터" class="headerlink" title="Learning Rate 는 중요한 하이퍼파라미터"></a>Learning Rate 는 중요한 하이퍼파라미터</h3><ul>
<li><strong>실험을 통해 최적화</strong>하는 것이 필요</li>
<li>초보자들은 처음에 어떤 값을 정해야 할지 난감<ul>
<li>고민할 바에 그냥 아주 작은 값(eg. 1e-4)으로 오래 돌려도 괜찮음</li>
</ul>
</li>
<li>나중에 Adam Optimizer를 통해 Learning Rate에 대한 고민을 없앨 수 있음</li>
</ul>
<p>※ 하이퍼파라미터 : 모델 성능에 영향을 끼치지만, 데이터를 통해 학습할 수 없는 파라미터 (우리가 직접 테스트하고 튜닝을 해야함)</p>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><ul>
<li><p>Gradient Descent + Learning Rate 실습</p>
<ul>
<li>$L(x)&#x3D;| targert - x |_2^2$</li>
<li>$x \gets x - \eta \nabla_x L(x)$<ul>
<li>cf) $\nabla_x L(x) &#x3D; x.grad$</li>
</ul>
</li>
<li>$\hat{x} &#x3D; \text{argmin}L(x)$, $x \in \mathbb{R}^{3,3}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">target = torch.FloatTensor([[<span class="number">.1</span>, <span class="number">.2</span>, <span class="number">.3</span>],</span><br><span class="line">                            [<span class="number">.4</span>, <span class="number">.5</span>, <span class="number">.6</span>],</span><br><span class="line">                            [<span class="number">.7</span>, <span class="number">.8</span>, <span class="number">.9</span>]])</span><br><span class="line"></span><br><span class="line">x = torch.rand_like(target)</span><br><span class="line"><span class="comment"># This means the final scalar will be differentiate by x.</span></span><br><span class="line">x.requires_grad = <span class="literal">True</span></span><br><span class="line"><span class="comment"># You can get gradient of x, after differentiation.</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor([[0.4176, 0.6465, 0.3522],</span></span><br><span class="line"><span class="comment">#         [0.9164, 0.7576, 0.0892],</span></span><br><span class="line"><span class="comment">#         [0.4854, 0.0136, 0.9467]], requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">loss = F.mse_loss(x, target)</span><br><span class="line"><span class="built_in">print</span>(loss) <span class="comment"># tensor(0.1737, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"></span><br><span class="line">threshold = <span class="number">1e-5</span></span><br><span class="line">learning_rate = <span class="number">1.</span></span><br><span class="line">iter_cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> loss &gt; threshold:</span><br><span class="line">    iter_cnt += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment"># Calculate gradients.</span></span><br><span class="line"></span><br><span class="line">    x = x - learning_rate * x.grad</span><br><span class="line">    </span><br><span class="line">    x.detach_()</span><br><span class="line">    x.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    loss = F.mse_loss(x, target)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%d-th Loss: %.4e&#x27;</span> % (iter_cnt, loss))</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    1 - th Loss: 1.0510e-01</span></span><br><span class="line"><span class="string">    tensor([[0.3470, 0.5473, 0.3406],</span></span><br><span class="line"><span class="string">            [0.8016, 0.7003, 0.2027],</span></span><br><span class="line"><span class="string">            [0.5331, 0.1883, 0.9363]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    2 - th Loss: 6.3576e-02</span></span><br><span class="line"><span class="string">    tensor([[0.2921, 0.4701, 0.3316],</span></span><br><span class="line"><span class="string">            [0.7124, 0.6558, 0.2910],</span></span><br><span class="line"><span class="string">            [0.5702, 0.3242, 0.9282]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    3 - th Loss: 3.8460e-02</span></span><br><span class="line"><span class="string">    tensor([[0.2494, 0.4101, 0.3246],</span></span><br><span class="line"><span class="string">            [0.6430, 0.6212, 0.3597],</span></span><br><span class="line"><span class="string">            [0.5990, 0.4300, 0.9220]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    19 - th Loss: 1.2370e-05</span></span><br><span class="line"><span class="string">    tensor([[0.1027, 0.2038, 0.3004],</span></span><br><span class="line"><span class="string">            [0.4044, 0.5022, 0.5957],</span></span><br><span class="line"><span class="string">            [0.6982, 0.7934, 0.9004]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    20 - th Loss: 7.4833e-06</span></span><br><span class="line"><span class="string">    tensor([[0.1021, 0.2029, 0.3003],</span></span><br><span class="line"><span class="string">            [0.4034, 0.5017, 0.5966],</span></span><br><span class="line"><span class="string">            [0.6986, 0.7948, 0.9003]],  requires_grad = True)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="7장-Wrap-up"><a href="#7장-Wrap-up" class="headerlink" title="7장 Wrap up"></a>7장 Wrap up</h2><h3 id="Why-we-do-gradient-descent"><a href="#Why-we-do-gradient-descent" class="headerlink" title="Why we do gradient descent?"></a>Why we do gradient descent?</h3><ul>
<li>실재하지만 알 수 없는 함수 $f^*$를 근사하고 싶음</li>
<li>나의 모델(함수)의 $f_\theta$ 파라미터 𝜽를 조절</li>
<li><strong>손실 함수(Loss Function)를 최소화 하도록 파라미터 𝜽를 조절</strong></li>
<li>미분을 통해 gradient($\frac{\partial Loss}{\partial \theta}$)를 얻고, loss를 낮추는 방향으로 파라미터를 업데이트</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-21T14:58:47.000Z" title="7/21/2023, 11:58:47 PM">2023-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:19.000Z" title="9/6/2024, 12:03:19 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">12 minutes read (About 1774 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-Gradient-Descent/">7장. 기초 최적화 방법 Gradient Descent - Gradient Descent</a></h1><div class="content"><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><h3 id="Again-Our-Objective-is"><a href="#Again-Our-Objective-is" class="headerlink" title="Again, Our Objective is"></a>Again, Our Objective is</h3><ul>
<li><p>주어진 데이터에 대해서 출력 값을 똑같이 모사하는 함수를 찾고 싶다.</p>
</li>
<li><p>Loss 값을 최소로 하는 Loss Function의 입력 값(𝜃)를 찾자. How?</p>
<ul>
<li><p>𝜃 값을 하나하나 다 랜덤하게 넣어볼 수가 없다!! 따라서 Loss Function을 최소화하는 𝜃를 얻는 방법이 바로 Gradient Descent!</p>
<p>$D &#x3D; {(x_i, y_i)}_{i&#x3D;1}^N$    &#x2F;&#x2F; (데이터 셋들이 모여져 있을때)</p>
<p>$L(\theta) &#x3D; \sum_{i&#x3D;1}^{N} |y_i - \hat{y_i}|^2_2 &#x3D; \sum_{i&#x3D;1}^{N} |y_i - f_\theta(x_i)|^2_2, \text { where } \theta&#x3D;{W,b}, \text{ }f(x)&#x3D;x \cdot W+b$</p>
<p>$\hat{\theta} &#x3D; \operatorname{argmin}_{\theta} L(\theta)$</p>
<p>Loss 함수의 출력 결과가 최소가 되고 싶어하는 입력값을 점진적으로 찾고 싶겠다. 잘된다면 목적함수를 근사할 수 있음($f^* \approx f_{\hat{\theta}}$)</p>
</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Descent-1D-Case"><a href="#Gradient-Descent-1D-Case" class="headerlink" title="Gradient Descent 1D Case"></a>Gradient Descent 1D Case</h3><ul>
<li><p>𝑥로 미분하여 기울기를 활용하여 좀 더 낮은 곳으로 점차 나아가자</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2" alt="GradientDescent05"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2">https://github.com/shchoice/shchoice.github.io/assets/100276387/fb0d94f4-622c-4a79-af25-3526e39efec2</a></p>
<p>$x \gets x - \eta \frac{dy}{dx}, \text{ where } y &#x3D; f(x)$</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">⬇️</span><br></pre></td></tr></table></figure>

<p>$\theta \gets \theta - \eta \frac{\partial L(\theta)}{\partial \theta} &#x3D; \theta - \eta \nabla_\theta L(\theta)$</p>
<p>※ 𝜂 : Learning rate(0~1, hyper parameter), ⅆ𝑦&#x2F;ⅆ𝑥 : 기울기</p>
<p>※ 𝐿(𝜃) : 손실 함수(loss function)로써 스칼라 값을 갖음 , 𝜃: 파라미터 또는 가중치 벡터로 vector값을 갖음(고차원의 신경망에서는 𝜃가 벡터 뿐만 아니라 행렬, 또는 고차원 텐서의 형태를 가질 수도 있음, 여기서는 1D로 가정하기에 Vector) 따라서 $∇_θL(θ)$는 벡터 <em>θ에 대한 L</em>(<em>θ</em>)의 그래디언트를 나타내며, 이는 <em>θ의 각 요소에 대해 L</em>(<em>θ</em>)를 편미분한 결과를 벡터 형태로 표현한 것</p>
</li>
<li><p>가장 loss가 낮은 곳이 아닌 골짜기에 빠질 가능성이 있음 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e" alt="GradientDescent06"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e">https://github.com/shchoice/shchoice.github.io/assets/100276387/2c81e7ab-68e0-4fc9-8306-112d39cec17e</a></p>
<ul>
<li>그림은 2차원 이지만, 사실 파라미터의 개수만큼의 차원으로 이루어져 있음 Convex 한 2차 함수가 아닌 이상, Global Minima를 알 수가 없다.</li>
</ul>
</li>
</ul>
<h3 id="Loss-Minimization-using-Gradient-Descent"><a href="#Loss-Minimization-using-Gradient-Descent" class="headerlink" title="Loss Minimization using Gradient Descent"></a>Loss Minimization using Gradient Descent</h3><p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444">https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444">https://github.com/shchoice/shchoice.github.io/assets/100276387/e26fa081-eeb0-441e-8ea0-90d3aa418444</a></p>
<ul>
<li><p>1D 케이스를 높은 차원의 파라미터(*θ)*로 확장하자</p>
<ul>
<li><p>$\hat{\theta} &#x3D; \operatorname{argmin}_{\theta} L(\theta)$</p>
<p>$W \gets W - \eta \frac{\partial L(\theta)}{\partial W} \text{, }$</p>
<p>$b \gets b - \eta \frac{\partial L(\theta)}{\partial b},$</p>
<p>$\text{where } \theta &#x3D; {W,b}$</p>
</li>
</ul>
</li>
<li><p>Number of Parameters in Linear Layer</p>
<ul>
<li><p>파라미터 수 : n x m + m &#x3D; (n + 1) x m</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942">https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942</a></p>
<ul>
<li><p>|𝜃|&#x3D;(18,) , &#x2F;&#x2F; 18개의 파라미터가 있음! 𝑊 &#x3D; 5x3 &#x3D;15, 𝑏 &#x3D; 3</p>
</li>
<li><p>$y &#x3D; f(k) &#x3D; x \cdot W + b$</p>
<p>$\text{ where } x \in \mathbb{R}^{k \times n}, W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^{n \times m} \text{ and } y \in \mathbb{R}^{n \times m}$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Local-Minima-in-Practice"><a href="#Local-Minima-in-Practice" class="headerlink" title="Local Minima in Practice"></a>Local Minima in Practice</h3><ul>
<li>실제 딥러닝의 경우에 파라미터의 크기가 수백만 단위</li>
<li>수백만 차원의 loss 함수 surface 에서 global minma를 찾는 문제</li>
<li>수 많은 차원에서 동시에 local minima를 위한 조건이 만족되기는 어려움</li>
<li><strong>따라서 local mima에 대한 걱정을 크게 할 필요 없음</strong></li>
</ul>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><ul>
<li><p>Gradient Descent 실습 -  backward() &amp; requires_grad_()</p>
<ul>
<li>$x &#x3D; \begin{bmatrix} x_{(1,1)} &amp; x_{(1,2)} \ x_{(2,1)} &amp; x_{(2,2)} \end{bmatrix}$</li>
<li>$x_1&#x3D;x+2$</li>
<li>$x_2&#x3D;x-2$</li>
<li>$x_3&#x3D;x^2-4$</li>
<li>$y&#x3D;sum(x_3)&#x3D;x_{3(1,1)} + x_{3(1,2)} + x_{3(2,1)} + x_{3(2,2)}$</li>
<li>$x.grad&#x3D;\begin{bmatrix} \frac{\partial y}{\partial x_{(1,1)}} &amp; \frac{\partial y}{\partial x_{(1,2)}} \ \frac{\partial y}{\partial x_{(2,1)}} &amp; \frac{\partial y}{\partial x_{(2,2)}} \end{bmatrix}$</li>
<li>$\frac{dy}{dx}&#x3D;2x$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>]]).requires_grad_(<span class="literal">True</span>)</span><br><span class="line">x1 = x + <span class="number">2</span></span><br><span class="line">x2 = x - <span class="number">2</span></span><br><span class="line">x3 = x1 * x2</span><br><span class="line">y = x3.<span class="built_in">sum</span>() </span><br><span class="line"><span class="comment"># 스칼라 값이어야 미분 가능하기 때문에 .sum()이 붙음 없으면 RuntimeError: grad can be implicitly created only for scalar outputs 에러 발생</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="comment">#tensor([[3., 4.],</span></span><br><span class="line"><span class="comment">#       [5., 6.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="comment"># tensor([[-1.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.]], grad_fn=&lt;SubBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"><span class="comment"># tensor([[-3.,  0.],</span></span><br><span class="line"><span class="comment">#        [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(14., grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며</span></span><br><span class="line"><span class="string">파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨</span></span><br><span class="line"><span class="string">그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? </span></span><br><span class="line"><span class="string">이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 </span></span><br><span class="line"><span class="string">이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음</span></span><br><span class="line"><span class="string">예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)  # 가중치 벡터, y.backward(v)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, </span></span><br><span class="line"><span class="string">이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 </span></span><br><span class="line"><span class="string">이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[2., 4.],</span></span><br><span class="line"><span class="comment">#        [6., 8.]])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">print(x3.numpy())</span></span><br><span class="line"><span class="string"># RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</span></span><br><span class="line"><span class="string">print(x3.detach_().numpy())</span></span><br><span class="line"><span class="string"># array([[-3.,  0.],</span></span><br><span class="line"><span class="string">#       [ 5., 12.]], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, </span></span><br><span class="line"><span class="string">이 속성이 True로 설정되면, </span></span><br><span class="line"><span class="string">해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨</span></span><br><span class="line"><span class="string">따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. </span></span><br><span class="line"><span class="string">이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. </span></span><br><span class="line"><span class="string">따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여</span></span><br><span class="line"><span class="string"> 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. </span></span><br><span class="line"><span class="string">이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="요약-정리"><a href="#요약-정리" class="headerlink" title="요약 정리"></a>요약 정리</h3><ul>
<li>DNN을 통해 문제 해결을 위한 함수를 모사하고 싶을 경우, 우리가 만든 함수가 내뱉는 출력과 실제 정답의 차이는 loss이다.</li>
<li>loss를 loss function으로 만들고 loss function은 파라미터를 입력으로 받음</li>
<li>따라서 loss function의 출력을 최소로 하는 파라미터를 찾는 것이 목적이 됨</li>
<li>loss를 최소화하는 입력 파라미터를 Gradiend Descent로 찾을 수 있음</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-20T14:21:57.000Z" title="7/20/2023, 11:21:57 PM">2023-07-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:32.000Z" title="9/6/2024, 12:03:32 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">21 minutes read (About 3218 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/7%EC%9E%A5-%EA%B8%B0%EC%B4%88-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95-Gradient-Descent-%ED%8E%B8%EB%AF%B8%EB%B6%84/">7장. 기초 최적화 방법 Gradient Descent - 편미분</a></h1><div class="content"><h2 id="편미분"><a href="#편미분" class="headerlink" title="편미분"></a>편미분</h2><h3 id="다변수-함수-Multivariation-Function"><a href="#다변수-함수-Multivariation-Function" class="headerlink" title="다변수 함수(Multivariation Function)"></a>다변수 함수(Multivariation Function)</h3><ul>
<li>여러 개의 변수(multivariate)를 입력으로 받는 함수<ul>
<li>$z &#x3D; f(x,y)$</li>
<li>$y &#x3D; f(x_1, x_2)$</li>
<li>$x &#x3D; \begin{bmatrix} x_1 \ x_2 \end{bmatrix}$</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2" alt="MultivariateFunction"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2">https://github.com/shchoice/shchoice.github.io/assets/100276387/b827a50c-a2cb-452a-852b-28242a4154f2</a></p>
<h3 id="편미분-1"><a href="#편미분-1" class="headerlink" title="편미분"></a>편미분</h3><ul>
<li><p>다변수 x와 y를 입력으로 받는 함수 f를 x로 미분할 경우</p>
<ul>
<li>하나의 변수만 남겨 놓고 나머지를 상수 취급하는 미분 방법</li>
</ul>
</li>
<li><p>함수 f를 x변수(x축)으로 미분</p>
<ul>
<li>편미분 기호 𝜕 (round 혹은 partial 라고 부름)</li>
</ul>
<p>$\frac{\partial f}{\partial x} &#x3D; \lim_{h \to 0} \frac{f(x+h,y) - f(x,y)}{(x+h) - x}$</p>
</li>
<li><p>Y값에 대해 뚝 잘랐을 때 x축에 대한 기울기</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61" alt="GradientDescent01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61">https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61</a></p>
</li>
</ul>
<h3 id="함수의-입출력-형태"><a href="#함수의-입출력-형태" class="headerlink" title="함수의 입출력 형태"></a>함수의 입출력 형태</h3><ul>
<li><p>함수의 입력이 벡터인 경우</p>
<p>$y&#x3D;f(\begin{bmatrix} x_1 \⦙\x_n \end{bmatrix})&#x3D;f(x), \text{ where } x \in \mathbb{R}^n$</p>
</li>
<li><p>함수의 출력이 벡터인 경우 $y&#x3D;f(\begin{bmatrix} y_1 \⦙\y_n \end{bmatrix})&#x3D;f(x)&#x3D;\begin{bmatrix} f_1(x) \⦙\f_n(x) \end{bmatrix}, \text{ where } y \in \mathbb{R}^n$</p>
</li>
<li><p>함수의 입력이 행렬인 경우</p>
<p>$y&#x3D;f(\begin{bmatrix} x_{1,1} ⋯ x_{1,m} \⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ x_{n,1} ⋯ x_{n,m} \end{bmatrix})&#x3D;f(X), \text{ where } X \in \mathbb{R}^{n \times m}$</p>
</li>
<li><p>함수의 출력이 행렬인 경우 $Y&#x3D;f(\begin{bmatrix} y_{1,1} ⋯ y_{1,m} \⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ y_{n,1} ⋯ y_{n,m} \end{bmatrix})&#x3D;f(x), \text{ where } Y \in \mathbb{R}^{n \times m}$</p>
</li>
<li><p>입력과 출력이 벡터인 함수</p>
<p>$y&#x3D;f(\begin{bmatrix} y_1 \⦙\y_n \end{bmatrix})&#x3D;f(x)&#x3D; f(\begin{bmatrix} x_1 \⦙\x_n \end{bmatrix}), \text{ where } f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</p>
</li>
</ul>
<h3 id="스칼라를-벡터로-스칼라를-행렬로-미분"><a href="#스칼라를-벡터로-스칼라를-행렬로-미분" class="headerlink" title="스칼라를 벡터로, 스칼라를 행렬로 미분"></a>스칼라를 벡터로, 스칼라를 행렬로 미분</h3><ul>
<li>미분 결과는 gradient 벡터가 되어 방향과 크기를 모두 나타냄</li>
</ul>
<ol>
<li><p>스칼라를 벡터로 미분</p>
<ul>
<li><p>스칼라 함수를 벡터로 미분한다는 것을 의미</p>
</li>
<li><p>결과는 벡터가 됨</p>
</li>
<li><p>각 벡터의 요소는 스칼라 함수를 해당 방향을 미분한 결과를 나타냄, 이것을 gradient라고 부르며, 함수의 기울기를 나타나는데 사용</p>
</li>
<li><p>$\frac{\partial f}{\partial x} &#x3D; \nabla_x f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_1} \ ⦙\ \frac{\partial f}{\partial x_n} \end{bmatrix}, \text {where }x \in \mathbb{R}^n$</p>
</li>
<li><p>예제</p>
<ul>
<li><p>$f(x,y)&#x3D;3x2+2xy+y2$ 라는 스칼라 함수와 벡터 변수 $x$ 와 $y$ 가 있음</p>
<ul>
<li><p>이 함수를 각 변수에 대해 편미분 하면 다음과 같음</p>
<p>$\frac{\partial f}{\partial x} &#x3D; 6x + 2y$</p>
<p>$\frac{\partial f}{\partial y} &#x3D; 2x + 2y$</p>
</li>
<li><p>따라서 함수 $f(x,y)$ 에 대한 그래디언트는 다음과 같음</p>
<p>$\nabla f &#x3D; \left[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right] &#x3D; [6x + 2y, 2x + 2y]$</p>
</li>
<li><p>이 그래디언트 벡터는 각 점$(x,y)$에서 함수의 값이 가장 크게 증가하는 방향을 가리킴, 따라서 그래디언트는 최적화 문제에서 가장 중요한 역할을 함 (최적화 알고리즘은 일반적으로 그래디언트의 반대 방향으로 이동하여 함수의 최소값을 찾습니다. 이것이 그래디언트 디센트 방법의 기본 개념)</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>코드로 확인해보기</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sympy <span class="keyword">import</span> symbols, diff</span><br><span class="line"></span><br><span class="line">x, y = symbols(<span class="string">&#x27;x y&#x27;</span>)</span><br><span class="line">f = <span class="number">3</span>*x**<span class="number">2</span> + <span class="number">2</span>*x*y + y**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">df_dx = diff(f, x)  <span class="comment"># x에 대한 편미분</span></span><br><span class="line">df_dy = diff(f, y)  <span class="comment"># y에 대한 편미분</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;df/dx: <span class="subst">&#123;df_dx&#125;</span>&#x27;</span>)    <span class="comment"># df/dx: 6*x + 2*y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;df/dy: <span class="subst">&#123;df_dy&#125;</span>&#x27;</span>)    <span class="comment"># df/dy: 2*x + 2*y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 임의의 시작점</span></span><br><span class="line">x_value = <span class="number">1.0</span></span><br><span class="line">y_value = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습률</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 100번 반복</span></span><br><span class="line">    gradient_x = df_dx.evalf(subs=&#123;x: x_value, y: y_value&#125;)  <span class="comment"># x 위치에서의 그래디언트 계산</span></span><br><span class="line">    gradient_y = df_dy.evalf(subs=&#123;x: x_value, y: y_value&#125;)  <span class="comment"># y 위치에서의 그래디언트 계산</span></span><br><span class="line"></span><br><span class="line">    x_value -= eta * gradient_x     <span class="comment"># 그래디언트의 반대 방향으로 이동</span></span><br><span class="line">    y_value -= eta * gradient_y     <span class="comment"># 그래디언트의 반대 방향으로 이동</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimized x: <span class="subst">&#123;x_value&#125;</span>&#x27;</span>)    <span class="comment"># Optimized x: -0.0627121858146143</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Optimized y: <span class="subst">&#123;y_value&#125;</span>&#x27;</span>)    <span class="comment"># Optimized y: 0.154295508359253</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>스칼라를 행렬로 미분</p>
<ul>
<li>스칼라 함수를 행렬 변수에 대해 미분하는 것을 나타냄</li>
<li>결과는 행렬이 됨</li>
<li>이 행렬의 각 요소는 해당 스칼라 함수를 행렬의 해당 요소로 편미분한 값, 스칼라 함수를 행렬로 미분한 결과는 자코비안 행렬이라고 부르며, 이는 함수의 지역적 변화율을 나타냄</li>
<li>$\frac{\partial f}{\partial x} &#x3D; \nabla_x f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_{1,1}  } ⋯ \frac{\partial f}{\partial x_{1,m}} \ ⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ \frac{\partial f}{\partial x_{n,1}  } ⋯ \frac{\partial f}{\partial x_{n,m}} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n \times m}$</li>
<li>스칼라 함수 f가 행렬 X에 의존한다고 하면, 이 함수를 행렬 X에 대해 미분한 결과는 행렬이 됨<ul>
<li>이 행렬의 (i, j)번째 요소는 f를 X의 (i, j)번째 요소에 대해 편미분한 값</li>
<li>스칼라 함수 f를 행렬 $X &#x3D; \begin{bmatrix} x_{11} &amp; x_{12} \ x_{21} &amp; x_{22} \end{bmatrix}$에 대해 미분하면 다음과 같은 형태의 행렬이 나옴<ul>
<li>$\nabla_X f &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_{11}} &amp; \frac{\partial f}{\partial x_{12}} \ \frac{\partial f}{\partial x_{21}} &amp; \frac{\partial f}{\partial x_{22}} \end{bmatrix}$</li>
<li>이때 $\frac{\partial f}{\partial x_{ij}}$는 함수 f를 행렬 X의 (i,j)번째 요소에 대해 편미분한 것으로, 이렇게 구해진 행렬을 자코비안 행렬(Jacobian matrix) 이라고 함</li>
<li>자코비안 행렬의 각 요소는 스칼라 함수가 각 변수를 조금씩 변화시킬 때, 함수의 출력이 얼마나 변화하는지를 나타냄, 따라서 자코비안 행렬은 함수의 지역적인 변화율을 설명하는 데 사용될 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>스칼라-행렬 미분은 딥러닝에서 매우 중요한 개념<ul>
<li>신경망의 <strong>가중치는 일반적으로 행렬 형태</strong>를 가지며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 함. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 오차 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산</li>
</ul>
</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>스칼라, 벡터, 행렬</strong></p>
<ul>
<li><strong>스칼라</strong>는 단일한 수치 값입니다. 예를 들어, 10이나 2.5와 같은 단일 숫자를 스칼라라고 합니다.</li>
<li><strong>벡터</strong>는 숫자들의 배열입니다. 예를 들어, [1, 2]와 같은 1차원 배열이 벡터입니다.</li>
<li><strong>행렬</strong>은 숫자들의 2차원 배열입니다. 예를 들어, [[1, 2], [3, 4]]와 같은 2차원 배열이 행렬입니다.</li>
</ul>
</blockquote>
<blockquote>
<p><strong>자코비안 행렬, 헤시안 행렬</strong></p>
<ul>
<li>자코비안 행렬(Jacobian matrix)<ul>
<li>자코비안 행렬은 벡터 값을 가진 함수를 벡터 변수에 대해 미분할 때 사용</li>
<li>함수의 입력과 출력이 모두 벡터일 때, 각 입력 변수에 대한 각 출력 변수의 편미분을 행렬 형태로 나타낸 것이 자코비안 행렬</li>
<li>즉, 다변수 벡터 함수의 첫 번째 도함수를 나타냅니다.</li>
</ul>
</li>
<li>헤시안 행렬(Hessian matrix)<ul>
<li>헤시안 행렬은 스칼라 값을 가진 함수를 행렬 변수에 대해 두 번 미분할 때 사용</li>
<li>즉, 함수의 두 번째 도함수(2차 미분)를 나타냄</li>
<li>헤시안 행렬의 각 성분은 원래 함수의 두 변수에 대한 두 번째 편미분</li>
<li>헤시안 행렬은 주로 함수의 곡률, 즉 최솟값, 최댓값, 또는 안장점(saddle point) 등을 판별하는 데 사용됨</li>
</ul>
</li>
</ul>
<p>따라서, 자코비안은 함수의 기울기(1차 도함수)를, 헤시안은 곡률(2차 도함수)을 나타내며, 두 행렬 모두 함수의 지역적인 동작을 이해하는 데 중요한 역할을 함</p>
</blockquote>
<h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><ul>
<li><p>상미분과 달리 미분 결과가 벡터</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61" alt="GradientDescent01"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61">https://github.com/shchoice/shchoice.github.io/assets/100276387/858207a4-1764-44d8-aff9-b66fb1d7ed61</a></p>
<p>$\nabla f(x,y) &#x3D; \begin{bmatrix} 2x+y \ x+3y^2 +10y\end{bmatrix} &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x} \ \frac{\partial f}{\partial y} \end{bmatrix}$</p>
</li>
</ul>
<h3 id="벡터를-스칼라로-벡터를-벡터로-미분"><a href="#벡터를-스칼라로-벡터를-벡터로-미분" class="headerlink" title="벡터를 스칼라로, 벡터를 벡터로 미분"></a>벡터를 스칼라로, 벡터를 벡터로 미분</h3><ol>
<li><p>벡터를 스칼라로 미분</p>
<p>$\frac{\partial f}{\partial x} &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x}, … \text { }, \frac{\partial f_n}{\partial x} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n}$</p>
</li>
<li><p>벡터를 벡터로 미분</p>
<p>$\frac{\partial f}{\partial x} &#x3D; \begin{bmatrix} \frac{\partial f}{\partial x_1} \ ⦙ \ \frac{\partial f}{\partial x_n} \end{bmatrix} &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x}, … \text { }, \frac{\partial f_m}{\partial x} \end{bmatrix}  &#x3D; \begin{bmatrix} \frac{\partial f_1}{\partial x_{1}  } ⋯ \frac{\partial f_m}{\partial x_{1}} \ ⦙ \text{ }\text{ }  ⋱ \text{ }\text{ } ⦙ \ \frac{\partial f_1}{\partial x_{n}  } ⋯ \frac{\partial f_m}{\partial x_{n}} \end{bmatrix}, \text {where }x \in \mathbb{R}^{n}\text{ } and\text{ } f(x) \in \mathbb{R}^m$</p>
</li>
</ol>
<h3 id="코드로-구현하기"><a href="#코드로-구현하기" class="headerlink" title="코드로 구현하기"></a>코드로 구현하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>]]).requires_grad_(<span class="literal">True</span>)</span><br><span class="line">x1 = x + <span class="number">2</span></span><br><span class="line">x2 = x - <span class="number">2</span></span><br><span class="line">x3 = x1 * x2</span><br><span class="line">y = x3.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="comment">#tensor([[3., 4.],</span></span><br><span class="line"><span class="comment">#       [5., 6.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="comment"># tensor([[-1.,  0.],</span></span><br><span class="line"><span class="comment">#         [ 1.,  2.]], grad_fn=&lt;SubBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"><span class="comment"># tensor([[-3.,  0.],</span></span><br><span class="line"><span class="comment">#        [ 5., 12.]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(14., grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 스칼라여야만 미분 가능하다. 스칼라 아니면 에러 반환 # grequired_grad_(True) 는 다 미분</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">.backward() 메서드는 PyTorch에서 제공하는 자동 미분 기능으로, 실제로는 스칼라 함수에 대한 그래디언트를 계산하며</span></span><br><span class="line"><span class="string">파이토치의 계산 그래프(computation graph)의 특성에서 비롯됨</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그래서 y.backward()에서 y는 보통 스칼라(scalar)가 됨</span></span><br><span class="line"><span class="string">그 이유는 우리가 관심을 갖는 대상이 보통 손실 함수(loss function)이기 때문이고, 이는 스칼라 값을 반환</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그런데 만약 y가 벡터나 행렬과 같은 스칼라가 아닌 텐서라면? </span></span><br><span class="line"><span class="string">이 경우에도 .backward() 메서드를 사용할 수 있지만, 이를 위해서는 인자로 벡터를 제공해야 함 </span></span><br><span class="line"><span class="string">이 벡터는 y의 각 요소에 대한 가중치를 나타내며, 이를 통해 스칼라 값을 얻을 수 있음</span></span><br><span class="line"><span class="string">예시) v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)  # 가중치 벡터, y.backward(v)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">이런 복잡성 때문에 대부분의 경우, .backward()는 손실 값과 같은 스칼라 텐서에 대해서만 호출되며, </span></span><br><span class="line"><span class="string">이는 각 파라미터에 대한 손실 함수의 그래디언트를 계산 </span></span><br><span class="line"><span class="string">이 그래디언트는 파라미터의 .grad 속성에 저장되며, 이를 사용해 파라미터를 업데이트하는 등의 작업을 수행</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[2., 4.],</span></span><br><span class="line"><span class="comment">#        [6., 8.]])</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">print(x3.numpy())</span></span><br><span class="line"><span class="string"># RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</span></span><br><span class="line"><span class="string">print(x3.detach_().numpy())</span></span><br><span class="line"><span class="string"># array([[-3.,  0.],</span></span><br><span class="line"><span class="string">#       [ 5., 12.]], dtype=float32)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">PyTorch에서 텐서는 기본적으로 requires_grad 속성이 False로 설정되나, </span></span><br><span class="line"><span class="string">이 속성이 True로 설정되면, </span></span><br><span class="line"><span class="string">해당 텐서에 연산이 수행될 때마다 그래디언트를 계산하는 계산 그래프에 이 정보가 추가됨</span></span><br><span class="line"><span class="string">따라서 역전파(backpropagation) 단계에서 그래디언트를 자동으로 계산할 수 있게됨.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">그러나 requires_grad가 True인 텐서는 Numpy 배열로 직접 변환할 수 없음. </span></span><br><span class="line"><span class="string">이는 PyTorch의 계산 그래프와 Numpy가 서로 호환되지 않기 때문. </span></span><br><span class="line"><span class="string">따라서 requires_grad가 True인 텐서를 Numpy 배열로 변환하려면 먼저 detach() 메서드를 사용하여</span></span><br><span class="line"><span class="string"> 계산 그래프에서 해당 텐서를 분리한 후 변환해야 함</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">따라서 에러 메시지에서 제안하는 것처럼, x3.detach().numpy()를 사용하면 x3를 Numpy 배열로 안전하게 변환할 수 있음. </span></span><br><span class="line"><span class="string">이렇게 하면 x3의 그래디언트가 필요하지 않은 새로운 텐서가 생성되고, 이 텐서는 Numpy 배열로 변환될 수 있음.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="Why-we-laern-this"><a href="#Why-we-laern-this" class="headerlink" title="Why we laern this?"></a>Why we laern this?</h3><ul>
<li><p>Loss 함수 결과값인 스칼라 힘수를 파라미터 행렬(𝜃)로 미분해야 한다면?</p>
</li>
<li><p>DNN의 중간 결과물 벡터(ℎ)를 파라미터 행렬(𝜃)로 미분해야 한다면?</p>
<p>딥러닝에서는 일반적으로 가중치(파라미터)가 행렬 형태를 가지고 있으며, 이러한 가중치를 업데이트하기 위해 그래디언트(미분값)를 계산해야 합니다. 이 때 사용되는 것이 바로 스칼라-행렬 미분으로, 손실 함수(스칼라 함수)를 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 또한, DNN에서는 각 레이어의 입력(중간 결과 값)을 가중치 행렬에 대해 미분하여 그래디언트를 계산합니다. 따라서, 이러한 미분 개념을 이해하는 것은 딥러닝 모델을 이해하고 최적화하는 데 매우 중요합니다. 따라서, 이러한 이유들로 인해 파라미터 행렬(𝜃)에 대한 스칼라 함수, 즉 손실 함수의 결과 값을 미분하거나, 파라미터 행렬(𝜃)에 대한 중간 결과 값 벡터 (ℎ)를 미분하는 개념을 배우는 것이 딥러닝에서 매우 중요합니다</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-19T14:56:59.000Z" title="7/19/2023, 11:56:59 PM">2023-07-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:03:13.000Z" title="9/6/2024, 12:03:13 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">5 minutes read (About 713 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/6%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%B4-%EC%9E%98-%ED%95%99%EC%8A%B5%EB%90%98%EB%8A%94%EC%A7%80-%ED%8C%90%EB%8B%A8%ED%95%98%EA%B8%B0-Loss-Function/">6장. 신경망이 잘 학습되는지 판단하기 - Loss Function</a></h1><div class="content"><h2 id="Again-Our-object-is"><a href="#Again-Our-object-is" class="headerlink" title="Again, Our object is"></a>Again, Our object is</h2><ul>
<li>데이터를 넣었을 때 출력을 반환하는 가상의 함수를 모사하는 것</li>
<li>Linear Layer 함수를 통해 원하는 함수를 모사해보자<ul>
<li>Linear Layer 함수가 얼마나 원하는 만큼 동작하는지 측정해 보자</li>
<li>얼마나 잘 동작하는지, 점수로 나타내보자</li>
</ul>
</li>
</ul>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><ul>
<li>Loss(손실값): 원하는 출력값(target,𝑦)가 실제 출력값(output, $\hat{y}$)의 차이의 합<ul>
<li>$\text{Loss} &#x3D; \sum_{i&#x3D;1}^{N} | y_i - \hat{y}<em>i | &#x3D; \sum</em>{i&#x3D;1}^{N} | y_i - f(x_i) |$</li>
</ul>
</li>
<li>그러므로 우리는 Loss가 작을수록 가상의 함수를 잘 모사하고 있다고 할 수 있음</li>
<li>Loss가 작은 Linear Layer를 선택하면 됨</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul>
<li><p>Linear Layer의 파라미터를 바꿀 때마다 Loss를 계산</p>
</li>
<li><p>Loss Function</p>
<ul>
<li>입력 : Linear Layer의 파라미터(𝜃, 즉, 𝑊,𝑏가 파라미터)</li>
<li>출력 : Looss<ul>
<li>𝐿(𝜃)&#x3D;$\sum_{i&#x3D;1}^{n} | y_i - f_{\theta}(x_j) |, \text{ where } \theta &#x3D; {W, b}$</li>
</ul>
</li>
</ul>
</li>
<li><p>종류</p>
<ul>
<li><p>Euclidean Distance</p>
<ul>
<li><p>$| y - \hat{y} |_2 (L2) &#x3D; \sqrt{(y_1 - \hat{y}_1)^2 + \ldots + (y_n - \hat{y}</p>
<p>n)^2} &#x3D; \sqrt{\sum</p>
<p>{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}, \text{ where } y \in \mathbb{R}^n \text{ and } \hat{y} \in \mathbb{R}^n$</p>
<ul>
<li><strong>딥러닝은 차원제약이 없어서 고차원으로 가면 차이가 굉장히 커질 수 있기 때문에 RMSE 가 등장</strong></li>
<li>cf) $| y - \hat{y} | : L1$, 절대값</li>
</ul>
</li>
</ul>
</li>
<li><p>RMSE(Root Mean Square Error)</p>
<ul>
<li>Euclidean Distance와 비슷한 개념</li>
<li>$\text{RMSE}(y, \hat{y}) &#x3D; \sqrt{\frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2}$</li>
</ul>
</li>
<li><p>&#96;&#96;&#96;<br>MSE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    (Mean Square Error)</span><br><span class="line"></span><br><span class="line">    - $\text&#123;MSE&#125;(y, \hat&#123;y&#125;) = \frac&#123;1&#125;&#123;n&#125; \sum_&#123;i=1&#125;^&#123;n&#125; (y_i - \hat&#123;y&#125;_i)^2 = \frac&#123;1&#125;&#123;n&#125;(\| y - \hat&#123;y&#125; \|_2)^2 = \frac&#123;1&#125;&#123;n&#125;\| y - \hat&#123;y&#125; \|_2^2 ∝ \| y - \hat&#123;y&#125; \|_2^2$</span><br><span class="line">    - Root와 상수를 뺏지만 크기 차이로 인한 순서 결과는 바뀌지 않음</span><br><span class="line">    - **손실함수로 가장 많이 사용**</span><br><span class="line"></span><br><span class="line">## 코드 구현하기</span><br><span class="line"></span><br><span class="line">- Loss Function 예제 (1) – 직접 구현하기</span><br><span class="line"></span><br><span class="line">  ```python</span><br><span class="line">  import torch</span><br><span class="line">  import torch.nn as nn</span><br><span class="line">  </span><br><span class="line">  def mse(x_hat, x):</span><br><span class="line">      # |x_hat| = (batch_size, dim)</span><br><span class="line">      # |x| = (batch_size, dim)</span><br><span class="line">      y = ((x - x_hat)**2).mean()</span><br><span class="line">      </span><br><span class="line">      return y</span><br><span class="line">  </span><br><span class="line">  x = torch.FloatTensor([[1, 1],</span><br><span class="line">                         [2, 2]])</span><br><span class="line">  x_hat = torch.FloatTensor([[0, 0],</span><br><span class="line">                             [0, 0]])</span><br><span class="line">  </span><br><span class="line">  print(x.size(), x_hat.size()) # torch.Size([2, 2]) torch.Size([2, 2])</span><br><span class="line">  mse(x_hat, x) # tensor(2.5000)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83" alt="LossFunction"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83">https://github.com/shchoice/shchoice.github.io/assets/100276387/c6689fb1-d9b7-4b33-a090-3474b05d1c83</a></p>
</li>
<li><p>Loss Function 예제 (2) – 라이브러리 활용</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">x_hat = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                           [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.size(), x_hat.size()) <span class="comment"># torch.Size([2, 2]) torch.Size([2, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x)) <span class="comment"># tensor(2.5000)</span></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x, reduction=<span class="string">&#x27;sum&#x27;</span>)) <span class="comment"># tensor(10.)</span></span><br><span class="line"><span class="built_in">print</span>(F.mse_loss(x_hat, x, reduction=<span class="string">&#x27;none&#x27;</span>)) <span class="comment"># tensor([[1., 1.], [4., 4.]])</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Loss Function 예제 (3) – 라이브러리 활용</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">mse_loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">print</span>(mse_loss(x_hat, x)) <span class="comment"># tensor(2.5000)</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><ul>
<li>우리는 목표로 하는 함수를 모사하기 위해<ul>
<li>학습용 입력 데이터들을 Linear Layer에 넣어 출력 값들을 구하고</li>
<li>출력값($\hat{y}$)들과 목표값(${y}$)들의 차이의 합(Loss)를 최소화 해야함</li>
</ul>
</li>
<li>결국, Linear Layer 파라미터(𝜃)를 바꾸면서 loss를 최소화 해야함</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-16T15:05:57.000Z" title="7/17/2023, 12:05:57 AM">2023-07-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:30.000Z" title="9/6/2024, 12:02:30 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">10 minutes read (About 1436 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-Linear-Layer/">5장. 신경망의 기본 구성요소 살펴보기 - Linear Layer</a></h1><div class="content"><h2 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h2><p>우리는 다음의 이미지를 통해 3이라고 머리가 인식하지만, 컴퓨터가 어떻게 이 이미지를 3으로 근사하도록 함수를 만들어야한다</p>
<p>우리는 $f^*$(f optimal)을 모사하는 최적의 $\hat{f}$ (f hat)을 찾아야한다</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/6098ece2-fe64-461f-a8fd-1a516e978c0d" alt="DigitMnist"></p>
<h3 id="Linear-Layer-란"><a href="#Linear-Layer-란" class="headerlink" title="Linear Layer 란"></a>Linear Layer 란</h3><ul>
<li><code>신경망의 가장 기본 구성 요소</code>, 딥러닝을 통해 모사하는 함수를 만들때 가장 기본이 되는 것이 Linear Layer</li>
<li>Fully-connected(FC) Layer 라고 불리기도 함<ul>
<li>입력의 모든 노드는 출력의 모든 노드와 컨넥션이 있음</li>
<li>Dense Layer 라고도 불리기도 함</li>
</ul>
</li>
<li>내부 파라미터에 따른 선형 변환을 수행하는 함수<ul>
<li>내부 파라미터를 잘 찾아내면 우리가 원하는 출력을 얻을 수 있음</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<h3 id="Linear-Layer-동작방식"><a href="#Linear-Layer-동작방식" class="headerlink" title="Linear Layer 동작방식"></a>Linear Layer 동작방식</h3><ul>
<li>각 입력 노드들에 weight(가중치)를 곱하고 모두 합친 뒤, bias(편향)을 더함</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/ed702032-9ad9-491c-b1cd-481a7a412907" alt="FCLayer02"></p>
<ul>
<li>|𝜃|&#x3D;(18,) , &#x2F;&#x2F; 18개의 파라미터가 있음! 𝑊 &#x3D; 5x3 &#x3D;15, 𝑏 &#x3D; 3</li>
</ul>
<h3 id="Linear-Layer-Equations"><a href="#Linear-Layer-Equations" class="headerlink" title="Linear Layer Equations"></a>Linear Layer Equations</h3><ul>
<li><p><strong>행렬 곱으로 구현 가능</strong></p>
</li>
<li><p>n차원에서 m차원으로의 <code>선형 변환 함수</code></p>
<ul>
<li>$x \in R^{k \times n}, w \in R^{n \times m} \rightarrow y \in R^{k \times m}$</li>
<li>$y &#x3D; f(k) &#x3D; x \cdot w + b$</li>
</ul>
</li>
<li><p>같은 표현</p>
<ul>
<li><p>𝑥를 미니배치에 관계없이 단순히 벡터로 볼 경우 : (m,n) x (n,1) &#x3D; (m,1)</p>
<ul>
<li><p>$y &#x3D; f(k) &#x3D; W^T \cdot x + b$</p>
<p>$\text{ where } x \in \mathbb{R}^n, W^T \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m \text{ and } y \in \mathbb{R}^m$</p>
</li>
</ul>
</li>
<li><p>𝑥를 미니배치(N개) 텐서로 표현할 경우 : (N,n) x (n,m) &#x3D; (N,m)</p>
<ul>
<li><p>$y &#x3D; f(k) &#x3D; x \cdot W + b$</p>
<p>$\text{ where } x \in \mathbb{R}^{k \times n}, W \in \mathbb{R}^{n \times m}, b \in \mathbb{R}^{n \times m} \text{ and } y \in \mathbb{R}^{n \times m}$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/704a682c-5897-48ba-bd4c-971327678942" alt="FCLayer01"></p>
<h3 id="코드로-구현해보기"><a href="#코드로-구현해보기" class="headerlink" title="코드로 구현해보기"></a>코드로 구현해보기</h3><ul>
<li><p>parameter 정보 확인 예제</p>
<ul>
<li>gradient에 관해서는 다음 gradient descent 파트에서 다룸</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 간단한 신경망 구조를 정의</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)  <span class="comment"># 10개의 입력을 받아 5개의 출력을 내는 선형 계층</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)  <span class="comment"># 5개의 입력을 받아 1개의 출력을 내는 선형 계층</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 신경망 객체를 생성</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 데이터</span></span><br><span class="line">input_data = torch.FloatTensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>, <span class="number">10.0</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;input_data : <span class="subst">&#123;input_data&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># input_data : tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 신경망을 통해 입력 데이터를 전달</span></span><br><span class="line">output_data = model(input_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;output: <span class="subst">&#123;output_data&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># output: tensor([[2.3264]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 파라미터들에 대한 정보를 출력</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;name: <span class="subst">&#123;name&#125;</span>, param.data: <span class="subst">&#123;param.data&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    name: fc1.weight, param.data: tensor([[-0.2895, -0.1230,  0.1624,  0.0381,  0.2252,  0.2265, -0.1498,  0.0806, -0.1704,  0.2421],</span></span><br><span class="line"><span class="string">        [-0.1162,  0.0786, -0.1140,  0.0178,  0.0470,  0.2920,  0.2933,  0.2919, 0.0493, -0.0025],</span></span><br><span class="line"><span class="string">        [ 0.0196,  0.0492, -0.2049,  0.1628, -0.1038,  0.1221,  0.0516, -0.1309, -0.2128, -0.3086],</span></span><br><span class="line"><span class="string">        [ 0.0129,  0.1872, -0.1641,  0.0406,  0.1779,  0.1346, -0.1623,  0.1618, 0.0410, -0.1538],</span></span><br><span class="line"><span class="string">        [ 0.1166, -0.0591,  0.0349, -0.0866,  0.2066, -0.0777,  0.3119, -0.1021, -0.2297,  0.2657]])</span></span><br><span class="line"><span class="string">    name: fc1.bias, param.data: tensor([ 0.0787, -0.0037, -0.2033,  0.0398, -0.1233])</span></span><br><span class="line"><span class="string">    name: fc2.weight, param.data: tensor([[0.0168, 0.2259, 0.2410, 0.0145, 0.2553]])</span></span><br><span class="line"><span class="string">    name: fc2.bias, param.data: tensor([0.2295])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Gradient 계산을 위한 랜덤 타깃 값 생성</span></span><br><span class="line">target = torch.FloatTensor([<span class="number">0.5</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;target <span class="subst">&#123;target&#125;</span>&quot;</span>) <span class="comment"># target tensor([[0.5000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실 함수로 평균 제곱 오차를 사용</span></span><br><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실 계산</span></span><br><span class="line">loss = loss_fn(output_data, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 역전파를 수행하여 그라디언트를 계산</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파라미터들의 그라디언트 정보를 출력</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;name: <span class="subst">&#123;name&#125;</span>, param.grad: <span class="subst">&#123;param.grad&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    name: fc1.weight, param.grad: tensor([[0.0614, 0.1229, 0.1843, 0.2458, 0.3072, 0.3687, 0.4301, 0.4915, 0.5530, 0.6144],</span></span><br><span class="line"><span class="string">        [0.8253, 1.6507, 2.4760, 3.3013, 4.1267, 4.9520, 5.7774, 6.6027, 7.4280, 8.2534],</span></span><br><span class="line"><span class="string">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0529, 0.1057, 0.1586, 0.2114, 0.2643, 0.3171, 0.3700, 0.4228, 0.4757, 0.5285],</span></span><br><span class="line"><span class="string">        [0.9324, 1.8648, 2.7972, 3.7296, 4.6621, 5.5945, 6.5269, 7.4593, 8.3917, 9.3241]])</span></span><br><span class="line"><span class="string">    name: fc1.bias, param.grad: tensor([0.0614, 0.8253, 0.0000, 0.0529, 0.9324])</span></span><br><span class="line"><span class="string">    name: fc2.weight, param.grad: tensor([[11.5118, 23.9645,  0.0000,  2.8603,  7.8742]])</span></span><br><span class="line"><span class="string">    name: fc2.bias, param.grad: tensor([3.6528])</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Raw Linear Layer 예제 (1) – nn.Module 추상 클래스를 활용</p>
<ul>
<li><p>$y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">W = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">                       [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b = torch.FloatTensor([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(W.size()) <span class="comment"># torch.Size([3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(b.size()) <span class="comment"># torch.Size([2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, W, b</span>):</span><br><span class="line">    y = torch.matmul(x, W) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">y = linear(x, W, b)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (2) – nn.Module 추상 클래스를 활용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, output_dim=<span class="number">2</span></span>):</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))</span><br><span class="line">        self.b = nn.Parameter(torch.FloatTensor(output_dim))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># |x| = (batch_size, input_dim)</span></span><br><span class="line">        y = torch.matmul(x, self.W) + self.b</span><br><span class="line">        <span class="comment"># |y| = (batch_size, input_dim) * (input_dim, output_dim)</span></span><br><span class="line">        <span class="comment">#     = (batch_size, output_dim)        </span></span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-3.7895e+32,  7.2868e-43],</span></span><br><span class="line"><span class="string">        [ 2.8026e-45,  0.0000e+00],</span></span><br><span class="line"><span class="string">        [-3.7896e+32,  7.2868e-43]], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (3) – nn.Linear 이용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-0.4061,  0.0483,  0.0804],</span></span><br><span class="line"><span class="string">        [ 0.0581,  0.0730,  0.4323]], requires_grad=True)</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0.4551, 0.4209], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Raw Linear Layer 예제 (4) – nn.Linear 이용</p>
<ul>
<li><p>$f(x) &#x3D; y &#x3D; x \cdot W + b, \text{ where } x \in \mathbb{R}^{N \times n}, y \in \mathbb{R}^{N \times m}, \text{ Thus, } W \in \mathbb{R}^{n \times m} \text{ and } b \in \mathbb{R}^{m}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">3</span>, output_dim=<span class="number">2</span></span>):</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.linear = nn.Linear(input_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># |x| = (batch_size, input_dim)</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="comment"># |y| = (batch_size, output_dim)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">                       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(x.size()) <span class="comment"># torch.Size([4, 3])</span></span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">y = linear(x)</span><br><span class="line"><span class="built_in">print</span>(y.size()) <span class="comment"># torch.Size([4, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> linear.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[-0.1267,  0.0563,  0.3951],</span></span><br><span class="line"><span class="string">        [ 0.2291,  0.3214,  0.2595]], requires_grad=True)</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0.3659, 0.4013], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Linear Layer 는 선형 함수</li>
<li>내부 가중치 파라미터(weight parameter) 𝑊와 𝑏에 의해 정의됨</li>
<li>우린 이 함수의 파라미터를 잘 조절하면, 주어진 입력에 대해 원하는 출력을 만들 수 있음</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-16T14:58:32.000Z" title="7/16/2023, 11:58:32 PM">2023-07-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-05T15:02:45.000Z" title="9/6/2024, 12:02:45 AM">2024-09-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/">딥러닝 개념</a><span> / </span><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">딥러닝 기본 개념</a></span><span class="level-item">4 minutes read (About 591 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%9C%EB%85%90/%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/%EA%B9%80%EA%B8%B0%ED%98%84%EC%9D%98%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D/5%EC%9E%A5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0-%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88%EA%B3%BC-%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/">5장. 신경망의 기본 구성요소 살펴보기 - 행렬의 곱셈과 벡터의 곱셈</a></h1><div class="content"><h2 id="행렬의-곱셈-Matrix-Multiplication"><a href="#행렬의-곱셈-Matrix-Multiplication" class="headerlink" title="행렬의 곱셈(Matrix Multiplication)"></a>행렬의 곱셈(Matrix Multiplication)</h2><ul>
<li><p>행렬의 곱셈</p>
<ul>
<li>2개의 행렬을 입력으로 받아 <code>새로운 행렬</code>을 생성</li>
<li>첫 번째 행렬의 각 행과 두 번째 행렬의 각 열 사이의 내적을 요소로 가지는 새로운 행렬을 만듬</li>
<li>행렬 곱셈은 내적의 총합을 사용하지만 자체적으로는 다른 연산</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행렬 곱셈</span></span><br><span class="line">M = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">N = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">matrix_product = torch.mm(M, N) <span class="comment"># 두 텐서가 모두 2차원 이상인 경우, &#x27;@&#x27;는 행렬곱(matrix multiplication)을 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Matrix Product:\\n <span class="subst">&#123;matrix_product&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Matrix Product:</span></span><br><span class="line"><span class="comment">#  tensor([[ 58,  64],</span></span><br><span class="line"><span class="comment">#         [139, 154]])</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-Matrix-Multiplication-행렬-곱"><a href="#1-Matrix-Multiplication-행렬-곱" class="headerlink" title="1. Matrix Multiplication(행렬 곱)"></a>1. Matrix Multiplication(행렬 곱)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/29973390-4e07-4bf5-a650-be12868b240b" alt="MatrixMultiplication"></p>
<h3 id="2-Vector-Matrix-Multiplication-벡터와-행렬의-곱"><a href="#2-Vector-Matrix-Multiplication-벡터와-행렬의-곱" class="headerlink" title="2. Vector Matrix Multiplication (벡터와 행렬의 곱)"></a>2. Vector Matrix Multiplication (벡터와 행렬의 곱)</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/321590d0-9da0-44bf-945f-90d828b4f084" alt="VectorMatrixMultiplication01"></p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4e1cd1aa-2d96-4e75-bb3e-6db31d46b176" alt="VectorMatrixMultiplication02"></p>
<h3 id="3-Batch-Matrix-Multiplication"><a href="#3-Batch-Matrix-Multiplication" class="headerlink" title="3. Batch Matrix Multiplication"></a>3. Batch Matrix Multiplication</h3><ul>
<li>같은 갯수의 행렬 쌍들에 대해서 병렬로 행렬 곱 실행</li>
<li>만약 4차원 텐서라면 (N1, N2, n, h) X (N1, N2, h, m) 이 됨</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/4baa6300-8ffc-4ee4-bee2-81c14936e11c" alt="BatchMatrixMultiplication"></p>
<h2 id="벡터의-곱셈-Vector-Multiplication"><a href="#벡터의-곱셈-Vector-Multiplication" class="headerlink" title="벡터의 곱셈(Vector Multiplication)"></a>벡터의 곱셈(Vector Multiplication)</h2><p>벡터의 곱셈에는 주로 2가지의 형태로 있음</p>
<ul>
<li><p>내적 (Dot Product, Inner Product, 점곱)</p>
<ul>
<li>두 개의 벡터를 입력으로 받아 <code>스칼라</code>(단일 수치) 값을 출력</li>
<li>벡터의 내적은 같은 위치에 있는 요소들끼리 곱한 후, 그 결과를 모두 더해서 하나의 숫자를 얻음</li>
<li>내적은 벡터들 사이의 <code>유사성</code>을 측정하는 데 사용</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 벡터 내적</span></span><br><span class="line">A = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">B = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">dot_product = torch.mm(A, B) <span class="comment"># 두 텐서가 모두 1차원인 경우, &#x27;@&#x27;는 벡터 내적(dot product)을 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dot Product: <span class="subst">&#123;dot_product&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># Dot Product: 32</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>외적 (Cross Product)</p>
<ul>
<li>3차원 벡터에 한정하며, 두 벡터의 외적은 새로운 벡터를 생성</li>
<li>새로운 벡터는 두 입력 벡터에 수직인 방향을 가지며, 그 크기는 두 입력 벡터 사이의 각도에 따라 달라짐</li>
<li>물리학에서 힘의 방향 계산 등에 사용</li>
</ul>
</li>
</ul>
<h2 id="벡터의-내적-vs-코사인-유사도"><a href="#벡터의-내적-vs-코사인-유사도" class="headerlink" title="벡터의 내적 vs 코사인 유사도"></a>벡터의 내적 vs 코사인 유사도</h2><ul>
<li>Dot Product<ul>
<li>$a \cdot b &#x3D; |a| |b| \cos \theta$</li>
<li>얼마나 같은 방향을 가지고 있는지 정보를 담으며,</li>
<li>벡터의 크기에도 영향을 받음</li>
</ul>
</li>
<li>Cosine Similarity<ul>
<li>$\text{cosine-similarity}(a, b) &#x3D; \frac{a \cdot b}{|a| |b|}$</li>
<li>방향성만 고려함</li>
<li>벡터의 크기 고려x</li>
</ul>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">Previous</a></div><div class="pagination-next"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/">1</a></li><li><a class="pagination-link is-current" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/page/2/">2</a></li><li><a class="pagination-link" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/matterhorn.jpg" alt="Shawn Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shawn Choi</p><p class="is-size-6 is-block">노력 백줌 열정 천줌의 소프트웨어 개발자</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, Republic of Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">138</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">64</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">110</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/shchoice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/shchoice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DevOps/"><span class="level-start"><span class="level-item">DevOps</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/"><span class="level-start"><span class="level-item">CI/CD 파이프라인</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/"><span class="level-start"><span class="level-item">Jenkins</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/"><span class="level-start"><span class="level-item">버전 관리</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DevOps/%EB%B2%84%EC%A0%84-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EB%B0%B0%ED%8F%AC-%EC%A0%84%EB%9E%B5/"><span class="level-start"><span class="level-item">버전 관리 및 배포 전략</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/MLOps/"><span class="level-start"><span class="level-item">MLOps</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/MLOps/Cuda/"><span class="level-start"><span class="level-item">Cuda</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/MLOps/MLflow/"><span class="level-start"><span class="level-item">MLflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Ops/Windows-CMD/"><span class="level-start"><span class="level-item">Windows CMD</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Java/%EB%82%B4-%EC%BD%94%EB%93%9C%EA%B0%80-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9D%B4%EC%83%81%ED%95%9C%EA%B0%80%EC%9A%94/"><span class="level-start"><span class="level-item">내 코드가 그렇게 이상한가요</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Spring/%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8/"><span class="level-start"><span class="level-item">핵심 원리 - 기본편</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/"><span class="level-start"><span class="level-item">기타</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/Github-Pages/"><span class="level-start"><span class="level-item">Github Pages</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B8%B0%ED%83%80/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/"><span class="level-start"><span class="level-item">데이터베이스 &amp; 검색엔진</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EA%B2%80%EC%83%89%EC%97%94%EC%A7%84/OpenSearch/"><span class="level-start"><span class="level-item">OpenSearch</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="level-start"><span class="level-item">딥러닝</span></span><span class="level-end"><span class="level-item tag">47</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/NLP/Text-Summarization/"><span class="level-start"><span class="level-item">Text Summarization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/"><span class="level-start"><span class="level-item">Transformers</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/Transformers/TainingArugments/"><span class="level-start"><span class="level-item">TainingArugments</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/"><span class="level-start"><span class="level-item">논문 리뷰</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 개념</span></span><span class="level-end"><span class="level-item tag">38</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝 기본 개념</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EB%85%90/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-NLP-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">딥러닝을 활용한 자연어 처리(NLP) 개념</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%86%B5%EA%B3%84%ED%95%99-%EB%B0%8F-%EC%88%98%ED%95%99/"><span class="level-start"><span class="level-item">딥러닝을 위한 통계학 및 수학</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/"><span class="level-start"><span class="level-item">성능과 튜닝</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%84%B1%EB%8A%A5%EA%B3%BC-%ED%8A%9C%EB%8B%9D/%ED%85%8C%EC%8A%A4%ED%8A%B8-%EB%B0%8F-%EB%B2%A4%EC%B9%98%EB%A7%88%ED%82%B9/"><span class="level-start"><span class="level-item">테스트 및 벤치마킹</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/UML/"><span class="level-start"><span class="level-item">UML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">소프트웨어 아키텍처</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/API-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">API 설계 및 아키텍처</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">마이크로서비스 아키텍처</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">웹 프로그래밍</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/FastAPI/"><span class="level-start"><span class="level-item">FastAPI</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/HTTP-%EB%B0%8F-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/"><span class="level-start"><span class="level-item">HTTP 및 네트워크</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Core/"><span class="level-start"><span class="level-item">Spring Core</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-Data-JPA/"><span class="level-start"><span class="level-item">Spring Data JPA</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Spring/Spring-MVC/"><span class="level-start"><span class="level-item">Spring MVC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/"><span class="level-start"><span class="level-item">개발 환경 설정</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/"><span class="level-start"><span class="level-item">보안</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%B3%B4%EC%95%88/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%95%94%ED%98%B8%ED%99%94/"><span class="level-start"><span class="level-item">데이터 암호화</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9B%B9-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%84%9C%EB%B2%84-%EB%B0%8F-%EC%9D%B8%ED%94%84%EB%9D%BC/"><span class="level-start"><span class="level-item">서버 및 인프라</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/"><span class="level-start"><span class="level-item">클라우드 컴퓨팅</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EB%8F%84%EC%BB%A4-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4/"><span class="level-start"><span class="level-item">도커 &amp; 쿠버네티스</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85/%EC%84%9C%EB%B2%84%EB%A6%AC%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/"><span class="level-start"><span class="level-item">서버리스 아키텍처</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">프로그래밍</span></span><span class="level-end"><span class="level-item tag">31</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Effective-Java/"><span class="level-start"><span class="level-item">Effective Java</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">함수형 프로그래밍</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/"><span class="level-start"><span class="level-item">Java&quot;</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Java/Java8/"><span class="level-start"><span class="level-item">Java8</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%8F%99%EC%8B%9C%EC%84%B1-%EB%B3%91%EB%A0%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">동시성 &amp; 병렬 프로그래밍</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/"><span class="level-start"><span class="level-item">소프트웨어 공학</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EA%B3%B5%ED%95%99/Agile/"><span class="level-start"><span class="level-item">Agile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%ED%81%B4%EB%A6%B0-%EC%BD%94%EB%93%9C/"><span class="level-start"><span class="level-item">클린 코드</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-27T14:37:53.000Z">2024-09-27</time></p><p class="title"><a href="/Jenkins-Notification-Teams-Email/">Jenkins Notification(Teams, Email)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-26T14:14:55.000Z">2024-09-26</time></p><p class="title"><a href="/Jenkins-SVM-%EC%97%B0%EB%8F%99-Multibranch-Pipeline-%EC%84%A4%EC%A0%95-%EB%B0%A9%EB%B2%95/">Jenkins - SVM 연동 &gt; Multibranch Pipeline 설정 방법</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-23T14:58:06.000Z">2024-09-23</time></p><p class="title"><a href="/Jenkins-SVM-%EC%97%B0%EB%8F%99-Freestyle-Project-%EC%84%A4%EC%A0%95-%EB%B0%A9%EB%B2%95/">Jenkins - SVM 연동 &gt; Freestyle Project 설정 방법</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-22T14:51:41.000Z">2024-09-22</time></p><p class="title"><a href="/Jenkins%EC%97%90%EC%84%9C-%EB%8B%A4%EC%96%91%ED%95%9C-%EB%B9%8C%EB%93%9C-%EC%98%B5%EC%85%98-%EC%84%A0%ED%83%9D%ED%95%98%EA%B8%B0-Multi-branch-Pipeline-vs-Freestyle-Project/">Jenkins에서 다양한 빌드 옵션 선택하기 &gt; Multi-branch Pipeline vs Freestyle Project</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/">CI/CD 파이프라인</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Jenkins/">Jenkins</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-20T17:29:08.000Z">2024-09-21</time></p><p class="title"><a href="/DevOps/CICD%20%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/Docker-Installation-CentOS-7/">Docker Installation - CentOS 7</a></p><p class="categories"><a href="/categories/DevOps/">DevOps</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/">CI/CD 파이프라인</a> / <a href="/categories/DevOps/CI-CD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8/Docker/">Docker</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/1%EA%B8%89-%EC%8B%9C%EB%AF%BC/"><span class="tag">1급 시민</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AES/"><span class="tag">AES</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASGI/"><span class="tag">ASGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Anonymous-Class/"><span class="tag">Anonymous Class</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoEncoder/"><span class="tag">AutoEncoder</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bind-Mounts/"><span class="tag">Bind Mounts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CGI/"><span class="tag">CGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CORS/"><span class="tag">CORS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Entropy-Loss/"><span class="tag">Cross Entropy Loss</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Curse-of-Dimensionality/"><span class="tag">Curse of Dimensionality</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Volume/"><span class="tag">Data Volume</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker-Orchestration-Tools/"><span class="tag">Docker Orchestration Tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Document-Embedding/"><span class="tag">Document Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-Vectors/"><span class="tag">Embedding Vectors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-vector/"><span class="tag">Embedding vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Entropy/"><span class="tag">Entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FLAN/"><span class="tag">FLAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FastAPI/"><span class="tag">FastAPI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Feature-Vector/"><span class="tag">Feature Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Forward-Proxy/"><span class="tag">Forward Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function-Interface/"><span class="tag">Function Interface</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradient-Descent/"><span class="tag">Gradient Descent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gunicorn/"><span class="tag">Gunicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Representation/"><span class="tag">Hidden Representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instruction-Finetuning/"><span class="tag">Instruction Finetuning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Jenkins/"><span class="tag">Jenkins</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL-Divergence/"><span class="tag">KL Divergence</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KoNLPy/"><span class="tag">KoNLPy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/L4-%EC%8A%A4%EC%9C%84%EC%B9%98/"><span class="tag">L4 스위치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lambda-Expression/"><span class="tag">Lambda Expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latent-Space/"><span class="tag">Latent Space</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learning-Rate/"><span class="tag">Learning Rate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Layer/"><span class="tag">Linear Layer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Load-Testing/"><span class="tag">Load Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Likelihood/"><span class="tag">Log-Likelihood</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MAP/"><span class="tag">MAP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLE/"><span class="tag">MLE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Manifold-hypothesis/"><span class="tag">Manifold hypothesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix/"><span class="tag">Matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mecab/"><span class="tag">Mecab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-Stage-Build/"><span class="tag">Multi Stage Build&quot;</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLL/"><span class="tag">NLL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OSI-7%EA%B3%84%EC%B8%B5/"><span class="tag">OSI 7계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Persistence-Data/"><span class="tag">Persistence Data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probabilistic-Perspective/"><span class="tag">Probabilistic Perspective</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RPS/"><span class="tag">RPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSA/"><span class="tag">RSA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Response-Time/"><span class="tag">Response Time</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reverse-Proxy/"><span class="tag">Reverse Proxy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SOLID/"><span class="tag">SOLID</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scalar/"><span class="tag">Scalar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spring%EC%9D%B4%EB%9E%80/"><span class="tag">Spring이란</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stress-Testing/"><span class="tag">Stress Testing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword-Embedding/"><span class="tag">Subword Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TCP-IP-4%EA%B3%84%EC%B8%B5/"><span class="tag">TCP/IP 4계층</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TPS/"><span class="tag">TPS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensor/"><span class="tag">Tensor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Testing-Types/"><span class="tag">Testing Types</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Throughput/"><span class="tag">Throughput</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tramsformers/"><span class="tag">Tramsformers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UML/"><span class="tag">UML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Uvicorn/"><span class="tag">Uvicorn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vector/"><span class="tag">Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WAS/"><span class="tag">WAS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WSGI/"><span class="tag">WSGI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/What-to-do/"><span class="tag">What to do</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cross-entropy/"><span class="tag">cross entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/default-method/"><span class="tag">default method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-commit-rule/"><span class="tag">git commit rule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-flow/"><span class="tag">git flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-merge/"><span class="tag">git merge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git-rebase/"><span class="tag">git rebase</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github-flow/"><span class="tag">github flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gitlab-flow/"><span class="tag">gitlab flow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/max-length/"><span class="tag">max_length</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mlflow/"><span class="tag">mlflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/one-hot-Encoding/"><span class="tag">one-hot Encoding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/packing/"><span class="tag">packing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/padding/"><span class="tag">padding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-%EC%84%A4%EC%B9%98/"><span class="tag">python 설치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/static-method/"><span class="tag">static method</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unpacking/"><span class="tag">unpacking</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5/"><span class="tag">객체지향</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8B%A4%ED%98%95%EC%84%B1/"><span class="tag">다형성</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%88%84%EC%88%98/"><span class="tag">데이터 누수</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%9E%8C%EB%8B%A4%EC%8B%9D/"><span class="tag">람다식</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A1%9C%EB%93%9C-%EB%B0%B8%EB%9F%B0%EC%8B%B1/"><span class="tag">로드 밸런싱</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A6%AC%EB%B2%84%EC%8A%A4-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">리버스 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">벡터의 곱셈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%EC%A7%80%ED%91%9C/"><span class="tag">성능 지표</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%B1%EB%8A%A5-%ED%85%8C%EC%8A%A4%ED%8A%B8/"><span class="tag">성능 테스트</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC/"><span class="tag">엔트로피</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9B%B9-%EC%84%9C%EB%B2%84/"><span class="tag">웹 서버</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EB%9F%89/"><span class="tag">정보량</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/"><span class="tag">정보이론</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%8B%A4%EC%9D%B4%EC%96%B4%EA%B7%B8%EB%9E%A8/"><span class="tag">클래스 다이어그램</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%8F%AC%EC%9B%8C%EB%93%9C-%ED%94%84%EB%A1%9D%EC%8B%9C/"><span class="tag">포워드 프록시</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"><span class="tag">함수형 인터페이스</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="tag">함수형 프로그래밍</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">행렬의 곱셈</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Seohwan Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>