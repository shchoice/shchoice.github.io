<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: 중급 개념 - Shawn&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Shawn&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon_sh.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Shawn&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:type" content="blog"><meta property="og:title" content="Shawn&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Shawn&#039;s Blog"><meta property="og:description" content="차분하고 겸손하지만 확실하게!!"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Shawn Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Shawn's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Shawn Choi"},"publisher":{"@type":"Organization","name":"Shawn's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"차분하고 겸손하지만 확실하게!!"}</script><link rel="icon" href="/img/favicon_sh.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-D7QRVGYDET" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-D7QRVGYDET');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/Deep-Learning/">Deep Learning</a></li><li class="is-active"><a href="#" aria-current="page">중급 개념</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-08T15:00:31.000Z" title="8/9/2023, 12:00:31 AM">2023-08-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:39:33.000Z" title="12/21/2023, 12:39:33 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">4 minutes read (About 637 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Geometric-Perspective-Manifold-Hypothesis/">3장. Geometric Perspective - Manifold Hypothesis</a></h1><div class="content"><h2 id="Manifold-가설"><a href="#Manifold-가설" class="headerlink" title="Manifold 가설"></a>Manifold 가설</h2><h3 id="데이터의-분포"><a href="#데이터의-분포" class="headerlink" title="데이터의 분포"></a>데이터의 분포</h3><ul>
<li>MNIST는 784(28x28) 차원의 벡터로 나타내어 진다.<ul>
<li>784 차원의 공간에 존재하는 데이터</li>
</ul>
</li>
<li>샘플들이 uniform 하게 분포되어 있을까?<ul>
<li>Not uniform, 한 군데에 몰려있을 확률이 높음</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/102a2208-d355-484d-99f5-7a63e518147d" alt="Manifold00"></p>
<h3 id="Manifold-Hypothesis-아직-증명되지-않음"><a href="#Manifold-Hypothesis-아직-증명되지-않음" class="headerlink" title="Manifold Hypothesis (아직 증명되지 않음)"></a>Manifold Hypothesis (아직 증명되지 않음)</h3><ul>
<li><p>실생활에서의 Manifold</p>
<ul>
<li>우리는 3차원의 좌표계에 살지만, 2차원 좌표계로 세상을 인식 (Mapping to Lower Dimensional Space) <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/aa171e80-a180-4b62-b3b9-d45ac513b1e6" alt="manifold02"></li>
</ul>
</li>
<li><p>고차원 공간의 샘플들이 다양체(manifold)의 형태로 분포해 있다는 가정</p>
<ul>
<li><p>따라서 다양체를 해당 차원의 공간에 mapping 할 수 있음</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/49cf6d71-984a-4116-a5c7-8ed0771f0e51" alt="manifold01"></p>
</li>
<li><p>고차원 공간에서의 두 점 사이의 거리는 저차원 공간으로의 맵핑 후 거리와 다름 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/86b931b9-afe3-48cf-b1b9-d5b2de0a8d95" alt="manifold03"></p>
</li>
</ul>
</li>
<li><p>MNIST 예제</p>
<ul>
<li>796 차원의 샘플들이 2D 공간 안에 맵핑이 됨 <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/8d1299c6-d941-4299-9d02-1894a5fdbcd3" alt="manifold-MNIST01"></li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/664b794d-0ac1-4c79-98f4-956eafe65160" alt="manifold-MNIST02"></p>
<ul>
<li><p>Non-linear Dimension Reduction 예제(Binary Classification ind 2-D)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f425bf37-0252-4beb-97bd-bd6eda06852c" alt="Manifold-NonLinear">]</p>
</li>
</ul>
</li>
<li><p>코드로 확인해보기</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> trainer <span class="keyword">import</span> Trainer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_image</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x.dim() == <span class="number">1</span>:</span><br><span class="line">        x = x.view(<span class="built_in">int</span>(x.size(<span class="number">0</span>) ** <span class="number">.5</span>), -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    plt.imshow(x, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> Namespace</span><br><span class="line"></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;train_ratio&#x27;</span>: <span class="number">.8</span>,</span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">50</span>,</span><br><span class="line">    <span class="string">&#x27;verbose&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;btl_size&#x27;</span>: <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">config = Namespace(**config)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(config)</span><br><span class="line"></span><br><span class="line">train_x, train_y = load_mnist(flatten=<span class="literal">True</span>)</span><br><span class="line">test_x, test_y = load_mnist(is_train=<span class="literal">False</span>, flatten=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_cnt = <span class="built_in">int</span>(train_x.size(<span class="number">0</span>) * config.train_ratio)</span><br><span class="line">valid_cnt = train_x.size(<span class="number">0</span>) - train_cnt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shuffle dataset to split into train/valid set.</span></span><br><span class="line">indices = torch.randperm(train_x.size(<span class="number">0</span>))</span><br><span class="line">train_x, valid_x = torch.index_select(</span><br><span class="line">    train_x,</span><br><span class="line">    dim=<span class="number">0</span>,</span><br><span class="line">    index=indices</span><br><span class="line">).split([train_cnt, valid_cnt], dim=<span class="number">0</span>)</span><br><span class="line">train_y, valid_y = torch.index_select(</span><br><span class="line">    train_y,</span><br><span class="line">    dim=<span class="number">0</span>,</span><br><span class="line">    index=indices</span><br><span class="line">).split([train_cnt, valid_cnt], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train:&quot;</span>, train_x.shape, train_y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Valid:&quot;</span>, valid_x.shape, valid_y.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test:&quot;</span>, test_x.shape, test_y.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Autoencoder</span><br><span class="line">model = Autoencoder(btl_size=config.btl_size)</span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">crit = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">trainer = Trainer(model, optimizer, crit)</span><br><span class="line">trainer.train((train_x, train_x), (valid_x, valid_x), config)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">    index1 = <span class="built_in">int</span>(random.random() * test_x.size(<span class="number">0</span>))</span><br><span class="line">    index2 = <span class="built_in">int</span>(random.random() * test_x.size(<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    z1 = model.encoder(test_x[index1].view(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    z2 = model.encoder(test_x[index2].view(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    recon = model.decoder((z1 + z2) / <span class="number">2</span>).squeeze()</span><br><span class="line"></span><br><span class="line">    show_image(test_x[index1])</span><br><span class="line">    show_image(test_x[index2])</span><br><span class="line">    show_image((test_x[index1] + test_x[index2]) / <span class="number">2</span>)</span><br><span class="line">    show_image(recon)</span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/194b7f53-93df-4458-9054-97554886bed1" alt="manifold-MNIST03"></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-07T14:57:13.000Z" title="8/7/2023, 11:57:13 PM">2023-08-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:39:28.000Z" title="12/21/2023, 12:39:28 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">a minute read (About 202 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Geometric-Perspective-Dimension-Reduction/">3장. Geometric Perspective - Dimension Reduction</a></h1><div class="content"><h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><h3 id="Linear-Dimension-Reduction-PCA-Principal-Component-Analysis"><a href="#Linear-Dimension-Reduction-PCA-Principal-Component-Analysis" class="headerlink" title="Linear Dimension Reduction: PCA(Principal Component Analysis)"></a><strong>Linear Dimension Reduction: PCA(Principal Component Analysis)</strong></h3><ul>
<li><p>n 차원의 공간에 샘플들의 분포가 주어져 있을 때, 분포를 잘 설명하기 위한 새로운 axis를 찾아내는 과정</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/3fb2833c-8225-4951-af80-18e67fc06f7a" alt="PCA01"></p>
</li>
<li><p>새로운 axis는 두 가지 조건을 만족해야 함</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/da195a58-f13c-4b2b-877a-c2c8af1f848d" alt="PCA02"></p>
</li>
<li><p><strong>새롭게 찾아낸 axis에 샘플들을 투사(projection)하면 차원 축소가 가능</strong></p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/50e81e35-f7d1-4f1e-955f-4ae3fee6f1af" alt="PCA03"></p>
<p>차원 축소(Dimension reduction)는 왜 필요할까?</p>
<ul>
<li><p>Binary Classifcaion in 2D</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5ca8d529-59a5-4962-a641-ec2637004312" alt="PCA04"></p>
</li>
</ul>
</li>
<li><p>차원 축소의 한계</p>
<ul>
<li><p>Binary Classification in 2D</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/ccdfbec0-5e75-4f65-9658-25f50605603b" alt="PCA05"></p>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-06T14:54:27.000Z" title="8/6/2023, 11:54:27 PM">2023-08-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:39:39.000Z" title="12/21/2023, 12:39:39 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">2 minutes read (About 283 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/3%E1%84%8C%E1%85%A1%E1%86%BC-Geometric-Perspective-%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%8B%E1%85%B4-%E1%84%8C%E1%85%A5%E1%84%8C%E1%85%AE-Curse-of-Dimensionality/">3장. Geometric Perspective - 차원의 저주(Curse of Dimensionality)</a></h1><div class="content"><h3 id="Sparseness-in-High-Dimensional-Space"><a href="#Sparseness-in-High-Dimensional-Space" class="headerlink" title="Sparseness in High Dimensional Space"></a>S<strong>parseness in High Dimensional Space</strong></h3><ul>
<li>d차원의 공간의 구(sphere) 안에 임의로 n개의 점을 흩뿌려보자</li>
<li>이 때, 구 테두리(회색 영역)와 안쪽에 위치한 점의 갯수를 살펴보자.</li>
<li>차원이 증가함에 따른 각 영역 별 점의 갯수의 차이는 어떻게 될까?</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c07019ea-66e7-4672-b257-4bd52842098f" alt="CurseOfDimension02"></p>
<h3 id="Curse-of-Dimensionality"><a href="#Curse-of-Dimensionality" class="headerlink" title="Curse of Dimensionality"></a>Curse of Dimensionality</h3><ul>
<li><p>차원이 높을수록 데이터는 희소하게 분포하게 되어 학습이 어려워짐</p>
<ul>
<li>모든 점들을 학습하기 위해서, 모든 구역들을 살펴봐야함</li>
<li>같은 구역 내의 점들은 구별할 수 없음</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/32cc9199-b7d7-463b-a29e-540826965e38" alt="Curse-of-Dimensionality01"></p>
</li>
<li><p>같은 정보의 데이터를 표현할 때, 차원이 높아질수록 희소성(sparseness)이 증가</p>
</li>
<li><p>희소성이 높을수록 modeling의 난이도가 높아짐</p>
<ul>
<li>Gaussian Mixture를 fitting하고자 할 때.</li>
<li>K-Means 클러스터링을 수행하고자 할 때.</li>
</ul>
</li>
<li><p>따라서 데이터의 특징(feature)을 더럽히지 않으면서 낮은 차원에서 표현해야 함</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-05T13:36:23.000Z" title="8/5/2023, 10:36:23 PM">2023-08-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:38:44.000Z" title="12/21/2023, 12:38:44 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">2 minutes read (About 238 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-%E2%80%93-MSE-with-Probabilistic-Perspective/">2장. Probabilistic Perspective – MSE with Probabilistic Perspective</a></h1><div class="content"><h3 id="MSE-with-Probabilistic-Perspective"><a href="#MSE-with-Probabilistic-Perspective" class="headerlink" title="MSE with Probabilistic Perspective"></a>MSE with Probabilistic Perspective</h3><ul>
<li>그 전에는 분류의 관점에서 봤지만 이번엔 MSE를 사용한 회귀의 문제로 봄</li>
<li>분류에서는 Multi-Nomial Distribtion을 사용해 Cross Entropy를 사용했지만, Gaussian Distribution을 가정하고 작성</li>
</ul>
<h3 id="Gaussian-PDF-가우시안-확률밀도함수"><a href="#Gaussian-PDF-가우시안-확률밀도함수" class="headerlink" title="Gaussian PDF (가우시안 확률밀도함수)"></a>Gaussian PDF (가우시안 확률밀도함수)</h3><ul>
<li><p>수식</p>
<ul>
<li><p>$p(x; \mu,\sigma) &#x3D; \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$</p>
<p>$\log{p(x;\mu,\sigma)} &#x3D; -\log{\sigma\sqrt{2\pi}}-\frac{1}{2}(\frac{x-\mu}{\sigma})^2$</p>
<p>$-\log{p(x;\mu,\sigma)} &#x3D; \log{\sigma\sqrt{2\pi}}-\frac{1}{2}(\frac{x-\mu}{\sigma})^2$</p>
</li>
</ul>
</li>
</ul>
<h3 id="MLE-with-Gradient-Descent"><a href="#MLE-with-Gradient-Descent" class="headerlink" title="MLE with Gradient Descent"></a>MLE with Gradient Descent</h3><ul>
<li><p>수식</p>
<ul>
<li><p>$D&#x3D;{(x_i, y_i)}_{i&#x3D;1}^N$</p>
<p>$\hat{\theta} &#x3D; \text{argmax}<em>{\theta \in \Theta} \sum</em>{i&#x3D;1}^{N} \log p(y_i | x_i; \theta)$    &#x2F;&#x2F;  $\log p(y_i | x_i; \theta)$ : log likelihood를 최대화</p>
<p>$L(\theta) &#x3D; -\sum_{i&#x3D;1}^{N} \log p(y_i | x_i; \theta)$               &#x2F;&#x2F; NLL로 바꾸어 loss function으로 삼고 minimize함, 그로 인해 gradient descent를 함</p>
<p>$\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)$</p>
</li>
</ul>
</li>
<li><p>Get Gradient of NLL</p>
<p>$\log{p(y_i,x_i;\phi,\psi)} &#x3D; \log{\sigma_{\psi}(x_i)\sqrt{2\pi}} +\frac{1}{2}(\frac{y_i-\mu_\phi(x_i)}{\sigma_{\psi}(x_i)})^2 \text{, where } \theta&#x3D;{\phi, \psi}$</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-03T14:50:28.000Z" title="8/3/2023, 11:50:28 PM">2023-08-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:38:53.000Z" title="12/21/2023, 12:38:53 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">7 minutes read (About 1034 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Information-Entropy/">2장. Probabilistic Perspective - Information &amp; Entropy</a></h1><div class="content"><h2 id="Information-amp-Entropy"><a href="#Information-amp-Entropy" class="headerlink" title="Information &amp; Entropy"></a>Information &amp; Entropy</h2><h3 id="Information-정보량"><a href="#Information-정보량" class="headerlink" title="Information(정보량)"></a>Information(정보량)</h3><ul>
<li><p>본디 통신이나 압축을 위해 주로 다루어지던 분야</p>
</li>
<li><p>Representation Learning에 관해서 다루다 보니 자연스럽게 연결됨</p>
<ul>
<li>Representation Learning에서는 정보 이론의 개념들을 이용하여 불확실성을 줄이고 정보를 최적으로 표현하려는 노력을 함(AutoEncoder의 특성 압축 등)</li>
</ul>
</li>
<li><p>불확실성(Uncertainty)을 나타내는 값</p>
<ul>
<li>정보가 높으면 불확실하다,</li>
<li>정보가 낮으면 정확하다.</li>
</ul>
</li>
<li><p>정보량 수식</p>
<ul>
<li>$I(\text{x})&#x3D;-\log P(\text{x})$     #  x라는 random variable에 대한 정보<ul>
<li>0≤𝑃(𝑥)≤1</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/fa29fa22-1bd0-4cf4-8aae-98df4911ad74" alt="Information"></p>
</li>
<li><p>정보량 예제</p>
<ul>
<li>예제1<ol>
<li>내일 아침 해는 동쪽 하늘에서 뜹니다. → 확률이 높을수록 정보량이 낮다.</li>
<li>내일 아침 해는 서쪽 하늘에서 뜹니다. → 확률이 낮을수록 엄청난 정보량을 가지고 있다.</li>
</ol>
</li>
<li>예제2<ol>
<li>올  여름 대한민국의 평균 여름 기온은 30도 입니다. → 정보량이 낮음</li>
<li>올 여름 대한민국의 평균 여름 기온은 10도 입니다.→ 정보량이 높음</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><ul>
<li><p>정보량의 기대값(평균)</p>
</li>
<li><p>분포의 평균적인 uncertainty를 나타내는 값</p>
<ul>
<li><strong>분포의 형태를 예측</strong>해볼 수 있음</li>
</ul>
</li>
<li><p>수식</p>
<ul>
<li><p>$H(P)&#x3D;-\mathbb{E}_{x \sim P(x)} [\log P(x)]$   # H(P) : P라고 하는 분포의 엔트로피</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5ad8f2bf-6f97-436d-800f-9a903171076f" alt="Entropy"></p>
<ul>
<li>P1: 엔트로피가 클수록 flat한 분포를 갖는다고 예측할 수 있음</li>
</ul>
</li>
</ul>
</li>
<li><p>P3: 엔트로피가 낮아질수록 sharp한 분포를 갖는다고 예측할 수 있음</p>
</li>
</ul>
<h3 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h3><ul>
<li><p>분포 P의 관점에서 본 분포 Q의 정보량의 평균</p>
</li>
<li><p>두 분포가 비슷할수록 작은 값을 갖음</p>
<ul>
<li>이런 성질 때문에 DNN을 최적화(Optimize)하는 데 사용함</li>
<li>Ground Truth P를 모사하고 싶은데, DNN이라는 weight parameter를 가진 확률분포 함수(Q)를 P와 가까이 하고 싶음.<ul>
<li>가까이 하기 위해 Cross Entropy로 측정하고 이를 줄이기 위해 Gradient Descent를 수행</li>
</ul>
</li>
<li>KL Divergence와 비슷<ul>
<li>범위 : 0~무한<ul>
<li>비슷하면 0에 가까고 다르면 무한대</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>수식</p>
<ul>
<li>$H(P, Q)&#x3D;-\mathbb{E}<em>{x \sim P(x)} [\log Q(x)]$      # 샘플링을 반복해 평균을 내겠음 $&#x3D; \int P(x) \log Q(x) dx$ $\approx -\frac{1}{n} \sum</em>{i&#x3D;1}^{n} \log Q(x_i)$</li>
</ul>
</li>
<li><p>DNN Optimization using Cross Entropy</p>
<ul>
<li>Classification 문제에서 Cross Entropy Loss를 사용하여 최소화(두 분포를 가까이 하기위함)</li>
<li>$CE(y_{1:N}, \hat{y}<em>{1:N}) &#x3D; -\frac{1}{N} \sum</em>{i&#x3D;1}^{N} y_i^T \cdot \log \hat{y}<em>i$ $&#x3D; -\frac{1}{N} \cdot \sum</em>{i&#x3D;1}^{N} \sum_{j&#x3D;1}^{d} y_{i,j} \times \log \hat{y}<em>{i,j}$ $&#x3D; -\frac{1}{N} \cdot \sum</em>{i&#x3D;1}^{N} \log P_{\theta} (y_i | x_i)$ $\text{where,} y_{1:N} \in \mathbb{R}^{N \times d} \text{ and } \hat{y}_{1:N} \in \mathbb{R}^{N \times d}$</li>
<li>$\mathcal{L}(\theta)&#x3D;-\mathbb{E}<em>{x \sim P(x)} [\mathbb{E}</em>{y \sim P(y|x)} [\log P(y|x;\theta)]]$<br> $\approx -\frac{1}{N\cdot k} \sum_{i&#x3D;1}^{N}\sum_{j&#x3D;1}^{k}\log P(y_{i,j}|x_i;\theta)$ $\approx -\frac{1}{N} \sum_{i&#x3D;1}^{N}\log P(y_i|x_i;\theta), \text{ if } k&#x3D;1$ $&#x3D; -\frac{1}{N} \sum_{i&#x3D;1}^{N} y_i^T \cdot \log \hat{y}<em>i$ &#x2F;&#x2F; $\mathbb{E}</em>{x \sim P(x)}$: Ground Truth P에 대해서 x를 샘플링 &#x2F;&#x2F; $\mathbb{E}_{y \sim P(y|x)}$: 샘플링한 x를 넣은 ground truth P(y|x)에서 를 샘플링 함 &#x2F;&#x2F; $P(y|x;\theta)$: x와 y를 DNN 확률 분포 함수에 넣음</li>
</ul>
</li>
<li><p>KL-Divergence &amp; Cross Entropy</p>
<ul>
<li><p>KL-Divergence와 Corss Entropy를 𝜽로 미분하면 같음</p>
<p>$KL(p||p_{\theta})&#x3D;-\mathbb{E}<em>{x \sim p(x)} [\log \frac{p</em>{\theta}(x)}{p(x)}]$ $&#x3D;-\int_{1} p(x)\log \frac{p_{\theta}(x)}{p(x)} dx$ $&#x3D;-\int_{1} p(x)\log p_{\theta} (x) dx+ \int_{1} p(x) \log p(x) dx$   &#x2F;&#x2F; corss enropy + entropy $&#x3D; H(p,p_{\theta})-H(p)$ &#x2F;&#x2F; croeeEntropy - entropy $\theta$ 로 미분하면 $\nabla_{\theta} KL(p||p_{\theta})&#x3D;\nabla_{\theta} H(p,p_{\theta})-\nabla_{\theta} H(p)$ &#x2F;&#x2F; H(p)는 𝜽로 미분 시 날아감</p>
</li>
</ul>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>Objective<ul>
<li>확률 분포 𝑃(𝑥)로부터 수집한 데이터셋 D를 통해, 확률 분포 함수 𝑃(𝑦|𝑥)를 근사하고 싶다.</li>
</ul>
</li>
<li>확률 분포 함수 신경망 $P_\theta(y|x)$를 통해 이를 수행하자</li>
<li>Cross Entropy(KL-Divergence)가 최소가 되도록 gradient descent 수행</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-01T15:08:39.000Z" title="8/2/2023, 12:08:39 AM">2023-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:38:27.000Z" title="12/21/2023, 12:38:27 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">4 minutes read (About 541 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Kullback-Leibler-Divergence/">2장. Probabilistic Perspective - Kullback-Leibler Divergence</a></h1><div class="content"><h3 id="Kullback-Leibler-Divergence"><a href="#Kullback-Leibler-Divergence" class="headerlink" title="Kullback-Leibler Divergence"></a>Kullback-Leibler Divergence</h3><ul>
<li><p>Kullback-Leibler Divergence 란</p>
<ul>
<li>KL Divergence(Kullback-Leibler Divergence, &#x3D; 상대 엔트로피(Relative Entropy))는 두 확률분포 P와 Q 간의 차이를 측정하는 방법</li>
<li>Q와 P를 근사하는 모델로 보았을 때, 모델이 실제 확률분포를 얼마나 잘 근사하고 있는지를 나타내는 지표로 활용</li>
</ul>
</li>
<li><p>KL Divergence 수식</p>
<ul>
<li>$D_{KL}(p || q) &#x3D; \mathbb{E}_{x \sim p(x)} [\log \frac{p(x)}{q(x)}] &#x3D; \int p(x)\log \frac{p(x)}{q(x)} dx$</li>
<li>$D_{KL}(p || q) \neq D_{KL}(q || p)$<ul>
<li>p(x)는 실제 확률분포에서 x라는 사건이 일어날 확률, q(x)는 모델이 추정한 x라는 사건이 일어날 확률</li>
</ul>
</li>
<li>KL Divergence는 항상 양수아며, p와 q가 비슷할수록 작은 값을 같게 된다.(완전히 같으면 최저가 0이 됨)</li>
</ul>
</li>
<li><p>사용 사례</p>
<ul>
<li>딥러닝에서는 종종 KL Divergence를 손실함수(loss function)로 사용하여 모델이 데이터의 실제 확률분포를 얼마나 잘 근사하는지를 측정</li>
<li>생성 모델(Generative Model)인 GAN(Generative Adversarial Networks)이나 VAE(Variational AutoEncoder)에서는 KL Divergence를 사용해 생성된 데이터의 확률분포와 실제 데이터의 확률분포가 얼마나 가까운지를 측정하는데 사용</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/94d5aa17-be42-4625-82ca-30a8ffceddcc" alt="KLDivergence"></p>
</li>
<li><p>KL-Divergence를 사용한 DNN 최적화(Optimization)</p>
<ul>
<li><p>$\mathcal{L}(\theta)&#x3D;-\mathbb{E}<em>{x \sim p(x)} [\mathbb{E}</em>{y \sim p(y|x)} [\log \frac{p_{\theta} (y|x)}{p(y|x)}]]$   &#x2F;&#x2F; $\mathbb{E}<em>{y \sim p(y|x)} [\log \frac{p</em>{\theta} (y|x)}{p(y|x)}]$ &#x3D; $D_{KL}(p(y|x)||p_\theta(y|x))$, ground truth인 확률분포 $p(y|x)$를 신경망의 확률 분포인 $p_{\theta}(y|x)$로 모사하기 위해 이 둘의 차이가 0이 되면 잘 최적화했음을 알 수 있음</p>
</li>
<li><p>$\mathcal{L}(\theta) \approx -\frac{1}{N\cdot k} \sum_{i&#x3D;1}^{N}\sum_{j&#x3D;1}^{k} \log \frac{p_{\theta} (y_{i,j}| x_i)}{p(y_{i,j}| x_i)}$ (Monte-Carlo 방법에 의함)</p>
<p>​          $\approx -\frac{1}{N} \sum_{i&#x3D;1}^{N} \log \frac{p_{\theta} (y_i| x_i)}{p(y_i| x_i)}$, if k&#x3D;1 # k&#x3D;1 : 즉 x는 여러개 뽑는 반면 y는 1개만 뽑음         </p>
<p>​          $\hat{\theta}&#x3D;\text{argmin}_{\theta \in \Theta} \mathcal{L}(\theta)$</p>
<p>​          $\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)$</p>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-01T15:03:48.000Z" title="8/2/2023, 12:03:48 AM">2023-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:39:13.000Z" title="12/21/2023, 12:39:13 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">5 minutes read (About 792 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-MAP/">2장. Probabilistic Perspective - MAP</a></h1><div class="content"><h3 id="MAP-Maximum-A-Posterior-Estimation"><a href="#MAP-Maximum-A-Posterior-Estimation" class="headerlink" title="MAP(Maximum A Posterior) Estimation"></a>MAP(Maximum A Posterior) Estimation</h3><ul>
<li><p>Data가 주어졌을 때 가설 h를 maximize 해야한다. 이를 위해 Posterior을 maximize할 수도 있고 likelihood로 maximize할 수도 있음</p>
</li>
<li><p>likelihood를 maximize 한다면 MLE</p>
</li>
<li><p>이 시간에는 poeteriror 를 maximize해서 h를 찾도록 하겠음 → MAP</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/396e25bd-8a54-4aa6-baab-2ee597481af8" alt="BayesTheorem"></p>
</li>
<li><p>MAP를 이해하기 위한 예제</p>
<ul>
<li><p>절도 사건의 범인은 발자국을 남겼습니다.</p>
</li>
<li><p>신발 사이즈 240일 때, 범인은 남자일까? 여자일까?</p>
<ul>
<li>$P(\text{y}│\text{x} &#x3D;240)$</li>
</ul>
</li>
<li><p>지인 중에 신발 사이즈가 240이었던 사람들을 떠올려보자</p>
<ul>
<li>여자 중에 많을까? 남자 중에 많을까?<ul>
<li>여자 중에서 신발 사이즈가 240인 사람 이라고 생각하면 likelihood(가정이 들어간 것</li>
<li>$P(\text{x}&#x3D;240|\text{y})$</li>
</ul>
</li>
</ul>
</li>
<li><p>그런데 범행 장소가 군부대라면?</p>
<ul>
<li><p>$P(\text{y}&#x3D;\text{male}) &gt; P(\text{y}&#x3D;\text{female})$</p>
<p>&#x2F;&#x2F; 99% &gt; 1%</p>
</li>
</ul>
</li>
<li><p>Likelihood는 여자일 가능성이 높지만, Prior를 고려하였을 때 범인은 남자일 가능성이 높음</p>
<ul>
<li>$P(\text{y} | \text{x} &#x3D; 240) &#x3D; \frac{P(\text{x} &#x3D; 240 | \text{y})P(\text{y})}{P(\text{x} &#x3D; 240)}$</li>
<li>$\frac{P(\text{x} &#x3D; 240 | \text{y&#x3D;male})P(\text{y&#x3D;male})}{P(\text{x} &#x3D; 240)} &gt; \frac{P(\text{x} &#x3D; 240 | \text{y&#x3D;female})P(\text{y&#x3D;female})}{P(\text{x} &#x3D; 240)}$</li>
</ul>
</li>
</ul>
</li>
<li><p>MAP Estimation</p>
<ul>
<li><p>Find $\hat{h}$, which maximizes posterior</p>
<ul>
<li><p>$\hat{h} &#x3D; \text{argmax}_{h \in H} P(h|D)$</p>
<p>$&#x3D; \text{argmax}_{h \in H} \frac{P(D|h)P(h)}{P(D)}$  &#x2F;&#x2F; 밑변은 h와 상관 없어 날려버림</p>
<p>$&#x3D; \text{argmax}_{h \in H}P(D|h)P(h)$  &#x2F;&#x2F; 𝑃(𝐷|ℎ) : likelihood, 𝑃(ℎ) : prior</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MAP Estimation (딥러닝이니 h를 𝜽로 바꿈)</p>
<ul>
<li><p>Bayesian 관점</p>
<p>$\hat{\theta} &#x3D; \text{argmax}<em>{\theta \in \Theta} P(\theta|D) &#x3D; \text{argmax}</em>{\theta \in \Theta} \frac{P(D|\theta)P(\theta)}{P(D)} &#x3D;  \text{argmax}_{\theta \in \Theta}P(D|\theta)P(\theta)$ &#x2F;&#x2F; Data가 주어졌을 때 𝜃가 궁금함</p>
<ul>
<li>파라미터 또한 Random Variable로 간주하며,</li>
<li>사전 정보(prior)를 통해 파라미터의 사전 분포를 설정 (prior에 대한 가정이 필요)</li>
<li>이후 관찰된 데이터를 통해 사후(posterior) 분포를 계산하며, 사후 분포가 최대가 되는 파라미터를 찾는 MAP 방법을 사용</li>
<li>미래의 uncertainty 까지 고려</li>
<li>likelihood를 maximize하는 동시에 prior까지 maximize 함</li>
</ul>
</li>
<li><p>Frequentist 관점</p>
<p>$\hat{\theta}  &#x3D;  \text{argmax}_{\theta \in \Theta}P(D;\theta)$ &#x2F;&#x2F;# 𝑃(𝜃,𝐷), likelihood를 maximize</p>
<ul>
<li>파라미터는 고정된 값이며 최적화의 대상으로  봄</li>
<li>관찰된 데이터를 기반으로 MLE 방법을 사용하여 파라미터를 추정</li>
<li>과적합(Overfitting)에 취약함</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>MAP를 통해 우리는 posterior를 최대화 하는 hypothesis를 찾을 수 있음<ul>
<li>마찬가지로 주어진 데이터셋에 대한 posterior를 최대화 하는 파라미터(𝜃)를 찾을 수 있음</li>
</ul>
</li>
<li>Bayesian 관점에서는 파라미터들을 랜덤 변수로 보고 prior에 대한 가정을 통해, 앞으로의 uncertainty 까지 고려<ul>
<li>이를 통해 overfitting 등의 문제도 해결할 수 있음</li>
</ul>
</li>
<li>Bayesian Deep Learning에 대한 다양한 시도들도 이어지고 있음<ul>
<li>베이지안 방법론은 계산량이 많이 필요하고 복잡한 딥러닝 모델에 적용하는 데에는 여전히 여러 도전적인 문제들이 존재</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-01T14:58:52.000Z" title="8/1/2023, 11:58:52 PM">2023-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:39:23.000Z" title="12/21/2023, 12:39:23 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">11 minutes read (About 1638 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%E1%84%80%E1%85%AA-MLE/">2장. Probabilistic Perspective - 신경망과 MLE</a></h1><div class="content"><h3 id="Again-our-object-is"><a href="#Again-our-object-is" class="headerlink" title="Again, our object is"></a>Again, our object is</h3><ul>
<li>데이터를 넣었을 때 출력을 반환하는 가상의 함수를 모사하는 것</li>
<li>확률 분포로부터 샘플링하여 데이터를 넣었을 때, 확률 분포를 반환하는 가상의 함수를 모사하는 것<ul>
<li>출력 분포에서 샘플링하면 원하는 출력 값을 얻을 수 있다.</li>
</ul>
</li>
<li>Example: 손 글씨가 주어졌을 때, 글씨의 클래스의 확률 분포<ul>
<li>$P(c|x) \text{, where } x \sim P(x)$</li>
</ul>
</li>
</ul>
<h3 id="Before-we-start"><a href="#Before-we-start" class="headerlink" title="Before we start,"></a>Before we start,</h3><ul>
<li>모두 같은 표현 (𝜃 를 𝑅𝑎𝑑𝑜𝑚 𝑉𝑎𝑟𝑖𝑎𝑏𝑙𝑒로 취급)<ul>
<li>$P_{\theta}(x) &#x3D; P(x; \theta) &#x3D; P(x|\theta)$ 𝜃 라는 파라미터를 갖는 확률분포에서의 𝑥의 확률값</li>
<li>$P_{\theta}(y|x) &#x3D; P(y|x; \theta) &#x3D; P(y|x, \theta)$ 𝜃 라는 파라미터를 갖는 확률분포에서의 𝑥가 주어졌을 때 𝑦의 확률값(</li>
</ul>
</li>
</ul>
<h3 id="Parameters-for-Probability-Distribution"><a href="#Parameters-for-Probability-Distribution" class="headerlink" title="Parameters for Probability Distribution"></a>Parameters for Probability Distribution</h3><ul>
<li>Bernoulli Distribution<ul>
<li>𝜃&#x3D;{𝑝} (동전던지기의 확률 같은)</li>
</ul>
</li>
<li>Gaussian Distribution<ul>
<li>𝜃&#x3D;{𝜇,𝜎} (대한민국 키(신장)의 분포를 알고 싶어요)</li>
</ul>
</li>
<li>딥러닝에서는 파라미터가 W와 b 가 되는 것이다!! 이 W와 b를 통해 확률 분포 함수를 정의할 수 있음</li>
</ul>
<h3 id="Parameters-for-Deep-Neural-Networks"><a href="#Parameters-for-Deep-Neural-Networks" class="headerlink" title="Parameters for Deep Neural Networks"></a>Parameters for Deep Neural Networks</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5f569c9f-d7f1-4df8-8745-be016e3fd5ff" alt="MLE-DNN"></p>
<ul>
<li>$\theta &#x3D; {W_1, b_1, W_2, b_2, \ldots, W_l, b_l}$</li>
<li>마찬가지로 Gradient Ascent를 통해 Likelihood를 최대로 하는 파라미터(𝜽)를 찾을 수 있다.<ul>
<li>$\sum_{i&#x3D;1}^{n} \log P(x_i | \theta)$ 를 Maximize 하는 𝜃를 찾을 수 있음</li>
</ul>
</li>
</ul>
<h3 id="Negative-Log-Likelihood-NLL"><a href="#Negative-Log-Likelihood-NLL" class="headerlink" title="Negative Log Likelihood(NLL)"></a>Negative Log Likelihood(NLL)</h3><ul>
<li>하지만 대부분의 딥러닝 프레임워크들은 Gradient Descent만 지원<ul>
<li>$\theta \leftarrow \theta - \alpha \cdot \frac{\partial \mathcal{L}(\theta)}{\partial \theta}$</li>
</ul>
</li>
<li>따라서 maximization 문제에서 minimization 문제로 접근 (-1만 곱하면 됨)</li>
</ul>
<h3 id="Deep-Neural-Networks-with-MLE"><a href="#Deep-Neural-Networks-with-MLE" class="headerlink" title="Deep Neural Networks with MLE"></a>Deep Neural Networks with MLE</h3><ul>
<li>MLE는 관측된 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 방법으로, 모델의 손실 함수를 설계하는 데 사용되며, 일반척으로 cross-entropy loss function 및 NLL이 MLE를 기반으로 함. 이 손실 함수를 최소화 함으로써 모델은 데이터를 가장 잘 설명하는 파라미터 값을 찾아가는 과정을 수행</li>
<li>MLE는 특정 데이터를 가장 잘 설명하는 확률 모델의 매개변수를 추정하는 방법</li>
<li>관찰된 데이터에 대한 모델의 적합성을 측정</li>
<li>분포 $P(\text{x})$로부터 샘플링한 데이터 𝒙가 주어졌을 때, 파라미터 𝜃를 갖는 DNN은 조건부 확률 분포를 나타냄<ul>
<li>$P(\text{y}|x;\theta) \text{, where } x \sim P(\text{x})$</li>
</ul>
</li>
<li>이때, 우리는 Gradient Descent를 통해 NLL을 최소화하는 𝜽를 찾을 수 있음<ul>
<li>$\hat{\theta} &#x3D; \text{argmin}<em>{\theta \in \Theta} \sum</em>{i&#x3D;1}^{N} -\log P(y_i | x_i; \theta)$</li>
</ul>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>MLE를 통해 수집한 데이터셋을 잘 설명하는 확률 분포의 파라미터를 찾을 수 있음<ul>
<li>하지만 비선형 함수에서는 베르누이나 가우시안 확률분포로는 설명할 수 없음</li>
<li>분류 작업의 경우에는 네트워크의 출력을 softmax 함수에 통과시켜 각 클래스에 속할 확률을 나타내는 확률 분포를 얻을 수 있음</li>
<li>이진 분류 모델에서는 시그모이드 함수를 사용할 수 있음(출력 범위는 0~1)</li>
</ul>
</li>
<li>Neural Networks 또한 확률 분포 함수이므로, MLE를 통해 파라미터(𝝁, 𝝈)를 찾을 것<ul>
<li>최대한 대신 최소화를 위해 Negative Log-Likelihood(NLL)을 Gradient Descent</li>
</ul>
</li>
<li>Gradient Descent를 수행하기 위해선, 파라미터에 대한 미분이 필요함<ul>
<li>이를 효율적으로 수행하기 위해 back-propagation을 활용</li>
</ul>
</li>
</ul>
<p>딥러닝이란 확률 분포 함수인 동시에 낮은 차원으로 차원을 축소하자</p>
<h3 id="MLE-Equation"><a href="#MLE-Equation" class="headerlink" title="MLE Equation"></a>MLE Equation</h3><ul>
<li>$D&#x3D;{(x_i, y_i)}_{i&#x3D;1}^{N} \text{, where } x_i: \sim P(\text{x}) \text{, } y_i&#x3D; \sim P(\text{y} ,|x_i)$</li>
<li>$\hat{\theta} &#x3D; \text{argmax}<em>{\theta \in \Theta} \sum</em>{i&#x3D;1}^{N} \log P(y_i | x_i; \theta)$$&#x3D; \text{argmin}<em>{\theta \in \Theta} -\sum</em>{i&#x3D;1}^{N} \log P(y_i | x_i; \theta)$</li>
<li>$\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)$</li>
</ul>
<h3 id="Connection-to-Deep-Neural-Networks"><a href="#Connection-to-Deep-Neural-Networks" class="headerlink" title="Connection to Deep Neural Networks"></a>Connection to Deep Neural Networks</h3><p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/612b11ed-5423-442c-bb79-2fbd01c9b250" alt="MLE-Softmax"></p>
<ul>
<li>$D&#x3D;{(x_i, y_i)}_{i&#x3D;1}^{N}$</li>
<li>$\hat{\theta} &#x3D; \text{argmin}<em>{\theta \in \Theta} -\sum</em>{i&#x3D;1}^{N} \log P(y_i | x_i; \theta)$</li>
<li>→  $\hat{y_i}&#x3D; f_\theta(x_i)$</li>
<li>$-\sum_{i&#x3D;1}^{N} \log P(y_i | x_i; \theta)&#x3D;-\sum_{i&#x3D;1}^{N} y_i^T \cdot \log\hat{y_i}$</li>
</ul>
<h3 id="MLE-NLL-amp-Cross-Entropy-Loss"><a href="#MLE-NLL-amp-Cross-Entropy-Loss" class="headerlink" title="MLE(NLL) &amp; Cross Entropy Loss"></a>MLE(NLL) &amp; Cross Entropy Loss</h3><ul>
<li><p>Minimizing Negative Log-Likelihood is equal to Minimizing Cross Entropy</p>
</li>
<li><p>$CE(y_{1:N},\hat{y}<em>{1:N}) &#x3D; -\frac{1}{N} \sum</em>{i&#x3D;1}^{N} y_i^T \cdot \log \hat{y}_i$   &#x2F;&#x2F; (1&#x2F;𝑁은 나중에 𝜃로 미분되어서 사라지기에 결국 MLE와 같음)</p>
<p>$&#x3D; -\frac{1}{N} \cdot \sum_{i&#x3D;1}^{N} \sum_{j&#x3D;1}^{d} y_{i,j} \times \log \hat{y}_{i,j}$</p>
<p>$&#x3D; -\frac{1}{N} \cdot \sum_{i&#x3D;1}^{N} \log P_\theta (y_i|x_i)$</p>
<p>$\text{where } y_{1:N} \in \mathbb{R}^{N \times d} \text{ and } \hat{y}_{1:N} \in \mathbb{R}^{N \times d}$</p>
</li>
<li><p>분류문제에서 Cross Entropy를 사용하고 CE Loss를 사용하여 최적화를 수행했는데, 이 모든 것은 결국 MLE를 수행하기 위한 것이다.(𝜽를 잘 찾는 과정(내가 수집한 데이터를 가장 잘 나타내기 위한 과정)을 하는 것)</p>
</li>
<li><p>Cross-Entropy 손실 함수는 두 확률 분포 사이의 차이를 측정</p>
<ul>
<li>딥러닝에서는 일반적으로 하나의 분포는 실제 레이블을 나타내고, 다른 하나는 모델의 예측을 나타냄</li>
<li>이 경우 Cross-Entropy 손실 함수는 모델의 예측이 실제 레이블을 얼마나 잘 반영하는지를 측정하는 데 사용</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2120b1fd-8916-459e-915e-57af2b4fef52" alt="MLE-Softmax2"></p>
</li>
</ul>
<blockquote>
<p>Negative Log-Likelihood (NLL)와 Cross-Entropy 손실 함수는 비슷한 개념을 포함하고 있지만, 엄밀히 말해서 동일한 것은 아님</p>
<p>이 둘의 차이점은 NLL이 단일 확률 분포에 대한 측정을 제공하는 반면, Cross-Entropy는 두 확률 분포 사이의 차이를 측정한다는 점입니다. 이 차이 때문에, 이 두 손실 함수는 다른 문맥에서 사용되며, 그 결과도 약간 다를 수 있음</p>
<p>그러나 분류 문제에서는 두 손실 함수가 동일한 결과를 제공하는 경우가 많습니다. 이는 각 클래스에 대한 실제 레이블 분포가 원-핫 인코딩 형식이기 때문 따라서 NLL과 Cross-Entropy 손실 함수는 종종 상호 교환적으로 사용</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-01T14:54:41.000Z" title="8/1/2023, 11:54:41 PM">2023-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:39:19.000Z" title="12/21/2023, 12:39:19 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">7 minutes read (About 1075 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Maximum-Likelihood-Estimation/">2장. Probabilistic Perspective - Maximum Likelihood Estimation</a></h1><div class="content"><h2 id="Maximum-Likelihood-Estimation-MLE"><a href="#Maximum-Likelihood-Estimation-MLE" class="headerlink" title="Maximum Likelihood Estimation(MLE)"></a>Maximum Likelihood Estimation(MLE)</h2><h3 id="Maximum-Likelihood-Estimation-란"><a href="#Maximum-Likelihood-Estimation-란" class="headerlink" title="Maximum Likelihood Estimation 란"></a>Maximum Likelihood Estimation 란</h3><ul>
<li><p>다음의 예시를 통해 왜 MLE가 필요한지 알아보자</p>
<ul>
<li><p>다음과 같이 대한민국 키의 평균을 얻기 위해 아래와 같은 샘플을 얻었다고 가정해보자. 이의 평균과 표준편차를 가장 잘 얻게 하려면 어떻게 해야할까?</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/abf815e8-2a1b-4ae7-a69a-ab6bb09ff5d3" alt="Gaussian01"></p>
</li>
<li><p>다음과 같이 3개의 가우시안 분포(Gaussian Distribution)를 살펴보자. 어떤 가우시안 분포가 가장 우리의 샘플을 잘 설명했다고 할 수 있을까? <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/dbbdb037-d4a8-4c2e-9e51-09c66d36b2c2" alt="Gaussian02"></p>
<ul>
<li>점선의 길이(Line’s length &#x3D; probabilty density)가 가장 긴 것이 제일 잘 표현한 것 같은데!(Approxmation 3) <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/aeca5859-2868-4efc-a179-ab3e83c008a8" alt="Gaussian03"></li>
</ul>
</li>
<li><p>따라서 우리의 샘플을 가장 잘 표현하는 가우시안 분포를 그리기 위한 𝜇와 𝜎를 찾자</p>
<ul>
<li>Likelihood 를 최대화 하는 MLE(Maximum Likelihood Estimation)을 구하자</li>
</ul>
</li>
</ul>
</li>
<li><p>Likelihood Function</p>
<ul>
<li>입력으로 <code>주어진 확률 분포(파라미터(𝜇,𝜎))가 데이터를 얼마나 잘 설명하는지 나타내는 점수(Likelihood)를 출력</code> 하는 함수<ul>
<li>입력 : 확률 분포를 표현하는 파라미터(𝜇,𝜎)</li>
<li>출력 : 데이터를 설명하는 정도 (Probability Density가 높아야)</li>
</ul>
</li>
<li>데이터를 잘 설명하는지 알 수 있는 방법<ul>
<li>데이터가 해당 확률 분포에서 <strong>높은 확률 값</strong>을 가질 것</li>
</ul>
</li>
<li>수식<ul>
<li>$L(\theta | x) &#x3D; P(x | \theta)$<ul>
<li>$x$ : 관찰된 데이터</li>
<li>$\theta$ : 모델의 파라미터</li>
<li>$P(x|\theta) : \theta$로 주어진 $x$의 확률</li>
<li>$L : \theta$ 에 대한 likjelihood function(우도 함수)</li>
</ul>
</li>
<li>$L(\theta | x_1, x_2, …, x_n) &#x3D; \prod_{i&#x3D;1}^{n} P(x_i | \theta)$ (주어진 데이터셋이 독립적이고 동일하게 분포(i.i.d)한 경우, 전체 데이터의 likelhood function은 각각의 데이터 포인트에 대한 확률의 곱으로 표현될 수 있음)<ul>
<li>하지만 실제 계산에서는 이렇게 곱셈 형태의 우도 함수는 계산이 어렵고, underflow 문제를 일으키기 때문에, 대부분의 경우 <code>로그 우도 함수(log-likelihood function)</code>을 사용 (중요)<ul>
<li>로그를 취하면 곱셈이 덧셈으로 바뀌어 계산이 쉬워짐 $\log L(\theta | x_1, x_2, …, x_n) &#x3D; \sum_{i&#x3D;1}^{n} \log P(x_i | \theta)$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>다음의 예제를 통해 MLE를 더 잘 이해해보자</p>
<ul>
<li>주사위의 확률 분포를 알고 싶다. <img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/a01e5190-5346-4d60-b6f1-7ac23528230f" alt="MLE-EX01"><ul>
<li>점수(likelihood)가 가장 높은 것은 $\theta_3$이다. 따라서 위 3개 중에서는 데이터를 잘 설명하는 파라미터라고 할 수 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Log-Likelihood"><a href="#Log-Likelihood" class="headerlink" title="Log-Likelihood"></a>Log-Likelihood</h3><ul>
<li>확률 분포의 가능성을 측정</li>
<li>앞에서 소개했다 싶이, Likelihood는 확률 곱으로 표현이 됨<ul>
<li>underflow의 가능성이 있음</li>
</ul>
</li>
<li>따라서 Log를 취하여 곱셈을 덧셈으로 바꾸고, Log Likelihood로 문제를 해결<ul>
<li>덧셈이 곱셈보다 연산도 빠름</li>
</ul>
</li>
<li>$\prod_{i&#x3D;1}^{n} P_{\theta}(\text{x} &#x3D; x_{i})$   → $\sum_{i&#x3D;1}^{n} \log P_{\theta}(\text{x} &#x3D; x_{i})$</li>
</ul>
<h3 id="MLE-via-Gradient-Ascent"><a href="#MLE-via-Gradient-Ascent" class="headerlink" title="MLE via Gradient Ascent"></a>MLE via Gradient Ascent</h3><ul>
<li><p>랜덤 생성 대신, Gradient Ascent를 통해 likelihood 값을 최대로 만드는 파라미터(𝜽)를 찾자</p>
<ul>
<li>$\theta \leftarrow \theta + \alpha \cdot \frac{\partial \mathcal{L}(\theta)}{\partial \theta}$</li>
</ul>
</li>
<li><p>예제</p>
<ul>
<li>n &#x3D; 100번 던졌을 때, k&#x3D;27번 앞면(True)이 나오는 동전이 있다. 이 동전의 확률 분포(파라미터 𝜽)를 추정하자.<ul>
<li>Binomial Distribution<ul>
<li>$\mathcal{L}(\theta) &#x3D; \frac{n!}{k!(n-k)!} \theta^k (1-\theta)^{n-k}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/03111cda-c739-44c4-bd54-e217e6367085" alt="MLE-GradientAscent"></p>
</li>
</ul>
<h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul>
<li>우리는 가상의 확률 분포를 모사하는 확률 분포의 파라미터(𝜽)를 찾고 싶다</li>
<li>목표 확률 분포로부터 데이터를 수집한 후, 데이터를 잘 설명하는 파라미터를 찾자<ul>
<li>Likelihood라는 값을 통해 얼마나 잘 설명하는지 알 수 있다</li>
<li>Likelihood function은 𝜃 을 입력으로 받아, 데이터들의 𝜃 에 대한 확률 값의 곱을 출력</li>
</ul>
</li>
<li>Likelihood를 최대화 하는 파라미터를 찾으면, 주어진 데이터를 가장 잘 설명한다.<ul>
<li>Gradient Ascent를 통해서 찾자.<ul>
<li>$\theta \leftarrow \theta + \alpha \cdot \frac{\partial \mathcal{L}(\theta)}{\partial \theta}$</li>
</ul>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-29T14:59:01.000Z" title="7/29/2023, 11:59:01 PM">2023-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T15:38:48.000Z" title="12/21/2023, 12:38:48 AM">2023-12-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a><span> / </span><a class="link-muted" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">중급 개념</a></span><span class="level-item">18 minutes read (About 2713 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Deep%20Learning/%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%80%E1%85%A2%E1%84%82%E1%85%A7%E1%86%B7/2%E1%84%8C%E1%85%A1%E1%86%BC-Probabilistic-Perspective-Basic-Statistics/">2장. Probabilistic Perspective - Basic Statistics</a></h1><div class="content"><h2 id="기본-통계학-Basic-Statistics"><a href="#기본-통계학-Basic-Statistics" class="headerlink" title="기본 통계학(Basic Statistics)"></a>기본 통계학(Basic Statistics)</h2><h3 id="Random-Variable-amp-Probability-Distribution"><a href="#Random-Variable-amp-Probability-Distribution" class="headerlink" title="Random Variable &amp; Probability Distribution"></a>Random Variable &amp; Probability Distribution</h3><ul>
<li>확률 변수(Random Variable)는 사건의 시행의 결과(확률)를 하나의 수치로 대응시킬 때의 확률 값, 일반적으로 대문자 X를 사용</li>
<li>어떤 변수(Random Variable) x가 𝒙 라는 값을 가질 확률<ul>
<li>$𝑃(\text{x}&#x3D;𝑥) &#x3D;𝑃(𝑥)$</li>
</ul>
</li>
<li>확률 분포 (함수)<ul>
<li>입력 : 확률 변수 x</li>
<li>출력 : x가 각 값에 해당될 때에 대한 확률값</li>
</ul>
</li>
<li>이산 확률 변수<ul>
<li>확률변수가 취할 수 있는 값의 수가 유한한 변수</li>
<li>ex) 동전 던지기, 주사위 던지기(X&#x3D;{0, 1, 2, 3})</li>
</ul>
</li>
<li>연속 확률 변수<ul>
<li>확률변수가 취할 수 있는 값의 수가 무한한 변수</li>
<li>ex) 키, 몸무게(P(50 ≤ X ≤ 60)) 등</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2aeefb28-c594-4c5c-806e-69d7d571f19f" alt="ProbabiltyDistibution"></p>
<h3 id="Function-or-Value"><a href="#Function-or-Value" class="headerlink" title="Function? or Value?"></a>Function? or Value?</h3><ul>
<li>확률값<ul>
<li>$𝑃(𝑥) &#x3D; P(\text{x}&#x3D;x)$</li>
</ul>
</li>
<li>확률 분포 함수<ul>
<li>$P(\text{x})$<br><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/47210d6e-73d3-4db3-8d14-817b51030869" alt="ProbabilityDensityFunction"></li>
<li>$P(y|x)$<ul>
<li>$P(y|x) &#x3D; P(\text{y}&#x3D;y|\text{x}&#x3D;x)$</li>
<li>Random Variable x가 𝑥 라는 값을 가졌을 때, Random Variable y가 𝑦일 확률 값</li>
</ul>
</li>
<li>$P(\text{y}|x)$<ul>
<li>$P(\text{y}|x) &#x3D; P(\text{y}|\text{x}&#x3D;x)$</li>
<li>Random Variable x가 𝑥 라는 값을 가졌을 때, Random Variable y의 분포</li>
</ul>
</li>
<li>$P(y|\text{x})$<ul>
<li>$P(y|\text{x})&#x3D;f(\text{x})&#x3D;P(\text{y}&#x3D;y|\text{x})$</li>
<li>어떤 Random Variable이 입력으로 주어졌을 때, Random Variable y가 𝑦일 확률 값을 뱉어내는 함수</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="확률-분포-Probability-Distribution"><a href="#확률-분포-Probability-Distribution" class="headerlink" title="확률 분포(Probability Distribution)"></a>확률 분포(Probability Distribution)</h3><p>확률 분포는 수치로 대응된 확률 변수의 개별 값들이 가지는 확률값의 분포</p>
<ul>
<li><p>이산 확률 분포(Discrete Probability Distribution) : 확률 변수가 취할 수 있는 값의 수가 유한한 확률 분포</p>
<ul>
<li><p>확률 질량함수(Probability Mass Function) : 확률 변수에서 특정 값에 대한 확률을 나타내는 함수</p>
<ul>
<li>확률값의 총 합은 1<ul>
<li>$\sum_{x} P(\text{x}&#x3D;x) &#x3D; 1, \text{ where } 0 \leq P(\text{x}&#x3D;x) \leq 1, \forall x \in \chi$</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/c555ab55-2167-44ae-b5a1-f67a393b9f46" alt="ProbabilityMassFunction"></p>
</li>
<li><p>주사위가 예시가 될 수 있음(이산 확률 분포)</p>
</li>
</ul>
</li>
<li><p>연속 확률 분포(Continuous Probability Distribution) : 확률 변수가 취할 수 있는 값의 수가 무한한 확률 분포</p>
<ul>
<li><p>확률 밀도 함수(robability Density Function , PDF)  : 확률 변수의 분포를 나타내는 함수</p>
<ul>
<li>면적의 합이 1<ul>
<li>$\int P(x) , dx &#x3D; 1, \text{ where } P(x) \geq 0, \forall x \in \mathbb{R}$</li>
</ul>
</li>
<li>함수값이 1보다 클 수 있음</li>
</ul>
<p>※ 주사위는 확률질량함수(이산), 확률밀도함수는 연속함수</p>
</li>
<li><p>연속 확률 변수의 경우, 어떤 샘플이 주어졌을 때, <strong>확률값을 알 수 없다</strong>. (높이는 단지 확률 밀도)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/f5c4716d-50e0-4c88-bc30-3b24db0869bc" alt="ProbabilityDensityFunction"></p>
</li>
</ul>
</li>
<li><p>결합 확률 분포(Joint Probability Distribution)</p>
<ul>
<li><p>두 개 이상의 Random Variable이 결합되었을 때의 확률 분포</p>
</li>
<li><p>두 이벤트 A와 B가 동시에 발생할 확률을 의미합니다. 이를 기호로 나타내면 P(A ∩ B) 또는 P(A, B)로 표현</p>
</li>
<li><p>P(A, B) &#x3D; P(A|B) * P(B) &#x3D; P(B|A) * P(A)</p>
<ul>
<li>P(A, B) : 사건 A와 사건 B가 동시에 발생할 확률</li>
<li>P(A|B) : 사건 B가 발생한 조건 하에서 사건 A가 발생할 확률</li>
<li>P(B) : 사건 B가 발생할 확률</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/2e67be1b-9289-44dc-a006-cd221bbec36c" alt="JointProbabilityDistribution"></p>
</li>
</ul>
</li>
</ul>
<h3 id="조건부-확률-Conditional-Probability"><a href="#조건부-확률-Conditional-Probability" class="headerlink" title="조건부 확률(Conditional Probability)"></a>조건부 확률(Conditional Probability)</h3><ul>
<li><p>어떤 사건 A가 일어났을 때 사건 B가 일어날 확률을 의미. 즉, 사건 A가 주어졌을 때의 사건 B의 확률</p>
</li>
<li><p>조건부 확률 분포</p>
<ul>
<li><p>랜덤 변수의 분포가 다른 랜덤 변수의 값에 의해 어떻게 영향을 받는지를 설명하는 확률 분포</p>
</li>
<li><p>즉, 하나의 랜덤 변수 X의 값이 주어졌을 때, 다른 랜덤 변수 Y의 분포를 의미</p>
</li>
<li><p>$P(\text{y}|\text{x}) &#x3D; \frac{P(\text{x}, \text{y})}{P(\text{x})}$ (x가 조건으로 주어졌을 때 y의 확률값)</p>
<p>P y given x 라고 읽음</p>
</li>
<li><p>조금더 친해져야할 형태(<strong>결합확률분포 형태</strong>)</p>
<ul>
<li>$<strong>P(\text{x}, \text{y}) &#x3D; P(\text{y} ,|, \text{x})P(\text{x})</strong>$</li>
</ul>
</li>
<li><p>예를 들어, 랜덤 변수 X가 ‘오늘 비가 올 확률’을 나타낸다고 할 때, 랜덤 변수 Y는 ‘비가 올 때 우산을 가지고 갈 확률’을 나타낼 수 있음. 이 경우, ‘우산을 가지고 갈 확률’은 ‘비가 올 확률’에 의해 영향을 받음. 이런 상황을 모델링한 것이 조건부 확률 분포이다.</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/cf745351-0dc1-492d-a970-9369752d8228" alt="CP"></p>
<h3 id="베이즈-정리-Bayes-Theorm"><a href="#베이즈-정리-Bayes-Theorm" class="headerlink" title="베이즈 정리(Bayes Theorm)"></a>베이즈 정리(Bayes Theorm)</h3><ul>
<li><p>베이즈 정리는 확률 이론과 통계학에서 굉장히 중요한 개념으로, 조건부 확률에 관한 정리</p>
</li>
<li><p>조건부 확률의 개념을 확장하고 이를 뒤집는 데 사용</p>
</li>
<li><p>주어진 사건 B가 발생했을 때, 사건 A가 발생할 확률을 알아내는 방법을 제공. 즉, 이미 알고 있는 정보에 기반하여 새로운 사건의 확률을 추정하는 방법</p>
</li>
<li><p>$P(A ,|, B) &#x3D; \frac{P(B,|,A)P(A)}{P(B)}$,</p>
<ul>
<li>(A|B)는 사건 B가 발생했을 때 사건 A가 발생할 조건부 확률을 의미하며, 이를 사후 확률(Posterior probability)이라고 함</li>
<li>P(B|A)는 사건 A가 발생했을 때 사건 B가 발생할 조건부 확률을 의미하며, 이를 가능도(Likelihood)라고 함</li>
<li>P(A)는 사건 A가 발생할 확률을 의미하며, 이를 사전 확률(Prior probability)라고 함</li>
<li>P(B)는 사건 B가 발생할 확률을 의미하며, 이를 증거(Evidence)라고 함</li>
</ul>
<blockquote>
<p>A를 “오늘 비가 올 것”이라는 사건, B를 “오늘 하늘이 흐림”이라는 사건이라고 가정합시다. 이 경우, 베이즈 정리는 다음과 같이 표현될 수 있습니다.</p>
<ul>
<li>사후 확률(P(A|B)), 즉 “하늘이 흐릴 때 비가 올 확률”은 알고 싶은 최종적인 확률입니다. 이는 우리가 이미 알고 있는 “하늘이 흐림”이라는 정보를 바탕으로 “오늘 비가 올 것”이라는 새로운 사건에 대한 확률을 업데이트하는 것을 의미합니다.</li>
<li>가능도(P(B|A)), 즉 “비가 올 때 하늘이 흐릴 확률”은 우리가 이미 알고 있는 “비가 올 것”이라는 사건이 주어졌을 때 “하늘이 흐림”이라는 사건의 확률입니다. 이는 사건 A가 주어졌을 때 사건 B가 발생할 확률을 나타냅니다.</li>
</ul>
</blockquote>
</li>
<li><p>$P(h ,|, D) &#x3D; \frac{P(D,|,h)P(h)}{P(D)}$</p>
<ul>
<li><p>$P(h ,|, D)P(D) &#x3D; P(D,|,h)P(h) &#x3D;P(h,D)$</p>
</li>
<li><p>가설이 있을 때 데이터가 있을 확률을 뒤집어서 알 수 있게 해준다.</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>조건부 확률과 베이즈 정리의 차이를 다음의 예시로 알아보자</strong>예시 : “비가 오는 날에 우산을 들고 나가는 확률”</p>
<p><strong>조건부 확률</strong> ‘비가 오는 날’이라는 사건이 이미 일어났을 때, 그 조건 하에서 ‘우산을 들고 나가는’ 사건이 일어날 확률을 계산하는 것이 조건부 확률</p>
<p><strong>베이즈 정리</strong> ”우산을 들고 나가는 사람 중에서 비가 오는 경우”</p>
</blockquote>
<h3 id="주변-분포-Marginal-Distribution"><a href="#주변-분포-Marginal-Distribution" class="headerlink" title="주변 분포(Marginal Distribution)"></a>주변 분포(Marginal Distribution)</h3><ul>
<li><p>여러 변수의 결합 확률 분포에서 한 개 또는 일부 변수의 분포를 나타냄, 이는 ‘주변화(marginalization)’라는 과정을 통해 얻어짐</p>
</li>
<li><p>주변화는 결합 확률 분포에서 관심 있는 변수를 제외한 나머지 변수들을 모두 합산(이산)하거나 적분(연속)하는 과정을 의미</p>
</li>
<li><p>주변 확률 분포를 통해, 특정 변수가 다른 변수들로부터 독립적으로 어떻게 분포하는지를 파악할 수 있음, 이를 통해 그 변수의 분포를 단독으로 볼 수 있게 됨</p>
</li>
<li><p>주변 분포는 확률론의 핵심 개념인 결합 확률, 조건부 확률, 그리고 이들의 관계를 연결해 보여주며, 주어진 데이터에서 특정 변수의 확률 분포를 독립적으로 파악하는 데 유용</p>
</li>
<li><p>Marginal Distribution의 공식</p>
<ul>
<li><p>$P(x) &#x3D; \int P(x,z) dz$ ( P(x, z)는 x와 z의 결합 확률 분포를 나타내고, 이를 z에 대해 적분하면 x의 주변 확률 분포 x를 얻게 됨)</p>
<p>$&#x3D; \int P(x|z)P(z) dz$ (결합 확률분포를 조건부 확률 P(x | z)와 z의 주변 확률 P(z)의 곱으로 분해하고 이를 z에 대해 적분하여 x의 주변확률 분포 P(x)를 얻게됨)</p>
<p>$&#x3D; \int P(z|x)P(x) dz &#x3D; P(x)\int P(z|x) dz &#x3D; P(x)$   $(∵\int P(z|x) dz&#x3D;1)$ (조건부 확률 $P(z | x)$가 z에 대해 적분하면 1이 됨, 왜냐하면 조건부 확률 P(z|x)는 z의 확률 분포를 나타내기 때문)</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/5780265a-73fa-41ca-9c58-f82a57308cff" alt="MarginalDistribution"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Expectation-and-Sampling"><a href="#Expectation-and-Sampling" class="headerlink" title="Expectation and Sampling"></a>Expectation and Sampling</h3><ul>
<li><p>기대값(expectation)은 확률 분포와 그에 대응하는 함수의 가중평균을 의미</p>
</li>
<li><p>기대값의 수식 (앞으로 자주 보게될 수식)</p>
<ul>
<li><p>$\mathbb{E}</p>
<p>{\text{x} \sim P(\text{x})}[f(x)] &#x3D; \sum</p>
<p>{x \in \chi} P(x) \cdot f(x)$</p>
<ul>
<li>$\chi$는 모든 가능한  $x$의 집합, $f(x)$는 $x$에 대한 함수, $P(x)$는 $x$의 확률 분포</li>
</ul>
</li>
</ul>
</li>
<li><p>f(x)라는 어떤 함수가 있는데, P(x)에서 샘플링한 x를 함수에 넣어봄</p>
<p><img src="https://github.com/shchoice/shchoice.github.io/assets/100276387/16640675-e6f2-4514-810a-2ff446075b79" alt="Expectation"></p>
</li>
<li><p>주사위의 기대값은?</p>
<ul>
<li><p>$\mathbb{E}<em>{x \sim P(x)}[f(x)] &#x3D; \sum</em>{x \in {1,2,3,4,5,6}} P(x &#x3D; x) \cdot f(x)$</p>
<p>$&#x3D;\frac{1}{6}×(f(1)+f(2)+f(3)+f(4)+f(5)+f(6))$</p>
<p>$&#x3D; \frac{1}{6}×(1+2+3+4+5+6)&#x3D;3.5 \text{, where } f(x)&#x3D;x$</p>
</li>
</ul>
<p>cf) 모든 기대값이 같으면 uniform distribution 따라서 시그마 1&#x2F;n *f(x)</p>
</li>
<li><p>$P(x) &#x3D; \int P(x,z) dz$</p>
<p>$&#x3D; \int P(x|z)P(z) dz$</p>
<p>$&#x3D; \mathbb{E}_{\text{z} \sim P(\text{z})}[P(x|\text{z})]$  (P(x)를 z에 대한 P(x|z)의 기대값으로 표현)</p>
</li>
</ul>
<h3 id="Monte-Carlo-방법"><a href="#Monte-Carlo-방법" class="headerlink" title="Monte-Carlo 방법"></a>Monte-Carlo 방법</h3><ul>
<li>무작위 표본을 사용하여 수학적 문제를 해결하는 통계적인 방법</li>
<li>주로 다음과 같은 상황에서 사용됨<ul>
<li>강화 학습에서는 Monte Carlo 방법을 사용하여 에이전트의 정책(policy)을 평가하거나 업데이트</li>
<li>딥러닝에서 복잡한 확률 모델에서 샘플을 추출하기 위해 Monte Carlo 샘플링 기법을 사용</li>
<li>복잡한 수학적 문제 해결<ul>
<li>복잡한 수학적 문제를 직접 해결하는 것이 어려울 때, Monte Carlo 방법을 사용하여 근사적인 해답을 찾을 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>확률 분포로부터 샘플링을 통해 𝒇의 가중 평균을 구해보자<ul>
<li>sample의 크기가 커질수록 보다 정확하게 구할 수 있음</li>
<li>$\mathbb{E}<em>{x \sim P(x)}[f(x)] \approx \frac{1}{n} \sum</em>{i&#x3D;1}^{n} f(x_i) \text{, where } x_i \sim P(x)$</li>
</ul>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/">1</a></li><li><a class="pagination-link" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/matterhorn.jpg" alt="Shawn Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shawn Choi</p><p class="is-size-6 is-block">노력 백줌 열정 천줌의 소프트웨어 개발자</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, Republic of Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">135</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">64</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">75</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/shchoice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/shchoice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">40</span></span></a><ul><li><a class="level is-mobile" href="/categories/Deep-Learning/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/Text-Summarization/"><span class="level-start"><span class="level-item">Text Summarization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/Transformers/"><span class="level-start"><span class="level-item">Transformers</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Deep-Learning/Transformers/TainingArugments/"><span class="level-start"><span class="level-item">TainingArugments</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Deep-Learning/%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">기본 개념</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/%EC%9E%90%EC%97%B0%EC%96%B4%EC%83%9D%EC%84%B1-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">자연어생성 개념</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/%EC%A4%91%EA%B8%89-%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">중급 개념</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ElasticSearch/"><span class="level-start"><span class="level-item">ElasticSearch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/FastAPI/"><span class="level-start"><span class="level-item">FastAPI</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Github-Pages/"><span class="level-start"><span class="level-item">Github Pages</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/JPA/"><span class="level-start"><span class="level-item">JPA</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/JPA/Basic/"><span class="level-start"><span class="level-item">Basic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">16</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Java8/"><span class="level-start"><span class="level-item">Java8</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Java/%ED%81%B4%EB%A6%B0%EC%BD%94%EB%93%9C/"><span class="level-start"><span class="level-item">클린코드</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Java/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">함수형 프로그래밍</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Kotlin/"><span class="level-start"><span class="level-item">Kotlin</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Kotlin/WebFlux/"><span class="level-start"><span class="level-item">WebFlux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Kotlin/%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">개념</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/MLOps/"><span class="level-start"><span class="level-item">MLOps</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/MLOps/MLflow/"><span class="level-start"><span class="level-item">MLflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/OPS/"><span class="level-start"><span class="level-item">OPS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/OpenSearch/"><span class="level-start"><span class="level-item">OpenSearch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Ops/Windows-CMD/"><span class="level-start"><span class="level-item">Windows CMD</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Ops/%EC%84%A4%EC%B9%98/"><span class="level-start"><span class="level-item">설치</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Agile/"><span class="level-start"><span class="level-item">Agile</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Agile/%ED%95%A8%EA%BB%98%EC%9E%90%EB%A6%AC%EA%B8%B0/"><span class="level-start"><span class="level-item">함께자리기</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/Clean-Code/"><span class="level-start"><span class="level-item">Clean Code</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/TDD/"><span class="level-start"><span class="level-item">TDD</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/UML/"><span class="level-start"><span class="level-item">UML</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/%EB%82%B4-%EC%BD%94%EB%93%9C%EA%B0%80-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9D%B4%EC%83%81%ED%95%9C%EA%B0%80%EC%9A%94/"><span class="level-start"><span class="level-item">내 코드가 그렇게 이상한가요</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Advanced-Concept/"><span class="level-start"><span class="level-item">Advanced Concept</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">개념</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">객체지향 프로그래밍</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/%EB%8F%99%EC%8B%9C%EC%84%B1-%EB%B0%8F-%EB%B9%84%EB%8F%99%EA%B8%B0/"><span class="level-start"><span class="level-item">동시성 및 비동기</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/%EB%AC%B8%EB%B2%95/"><span class="level-start"><span class="level-item">문법</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="level-start"><span class="level-item">함수형 프로그래밍</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/REST/"><span class="level-start"><span class="level-item">REST</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/Spring/Spring-Framework/"><span class="level-start"><span class="level-item">Spring Framework</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Spring/Spring-MVC/"><span class="level-start"><span class="level-item">Spring MVC</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Spring/%ED%95%B5%EC%8B%AC-%EC%9B%90%EB%A6%AC-%EA%B8%B0%EB%B3%B8%ED%8E%B8/"><span class="level-start"><span class="level-item">핵심 원리 - 기본편</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/TIL/1%EB%A7%8C-%EC%8B%9C%EA%B0%84%EC%9D%98-%EB%B2%95%EC%B9%99/"><span class="level-start"><span class="level-item">1만 시간의 법칙</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A7%88%EC%9D%B4%EB%8B%9D/"><span class="level-start"><span class="level-item">데이터 마이닝</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B8%B0%EB%B3%B8/"><span class="level-start"><span class="level-item">기본</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="level-start"><span class="level-item">딥러닝</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"><span class="level-start"><span class="level-item">자연어처리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="level-start"><span class="level-item">인공지능</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">개념</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EA%B0%9C%EB%85%90-%EC%A0%95%EB%A6%AC/"><span class="level-start"><span class="level-item">개념 정리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC/"><span class="level-start"><span class="level-item">자연어 처리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"><span class="level-start"><span class="level-item">자연어처리</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%98%A4%EC%BC%80%EC%8A%A4%ED%8A%B8%EB%A0%88%EC%9D%B4%EC%85%98/"><span class="level-start"><span class="level-item">컨테이너 오케스트레이션</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EC%98%A4%EC%BC%80%EC%8A%A4%ED%8A%B8%EB%A0%88%EC%9D%B4%EC%85%98/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%86%B5%EA%B3%84%ED%95%99/"><span class="level-start"><span class="level-item">통계학</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%86%B5%EA%B3%84%ED%95%99/%EA%B8%B0%EB%B3%B8/"><span class="level-start"><span class="level-item">기본</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/"><span class="level-start"><span class="level-item">파이썬</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/%ED%8C%8C%EC%9D%B4%EC%8D%AC/%EA%B0%9C%EB%85%90/"><span class="level-start"><span class="level-item">개념</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-26T14:52:30.000Z">2024-08-26</time></p><p class="title"><a href="/%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81/">모니터링</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-19T14:22:16.000Z">2024-08-19</time></p><p class="title"><a href="/%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-vs-%EB%B3%91%EB%A0%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/">동시성 프로그래밍 vs 병렬 프로그래밍</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-18T14:57:28.000Z">2024-08-18</time></p><p class="title"><a href="/2-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EA%B0%9C%EC%9A%94/">2. 아키텍처 개요</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-11T14:49:18.265Z">2024-08-11</time></p><p class="title"><a href="/CORS/"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-08-10T14:53:37.740Z">2024-08-10</time></p><p class="title"><a href="/AES%EC%99%80-RSA-%EC%95%94%ED%98%B8%ED%99%94/"> </a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/1%EA%B8%89-%EC%8B%9C%EB%AF%BC/"><span class="tag">1급 시민</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Anonymous-Class/"><span class="tag">Anonymous Class</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoEncoder/"><span class="tag">AutoEncoder</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bind-Mounts/"><span class="tag">Bind Mounts</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Entropy-Loss/"><span class="tag">Cross Entropy Loss</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Curse-of-Dimensionality/"><span class="tag">Curse of Dimensionality</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Volume/"><span class="tag">Data Volume</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker-Orchestration-Tools/"><span class="tag">Docker Orchestration Tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Document-Embedding/"><span class="tag">Document Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-Vectors/"><span class="tag">Embedding Vectors</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedding-vector/"><span class="tag">Embedding vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Entropy/"><span class="tag">Entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FLAN/"><span class="tag">FLAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Feature-Vector/"><span class="tag">Feature Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function-Interface/"><span class="tag">Function Interface</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gradient-Descent/"><span class="tag">Gradient Descent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Representation/"><span class="tag">Hidden Representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instruction-Finetuning/"><span class="tag">Instruction Finetuning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL-Divergence/"><span class="tag">KL Divergence</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KoNLPy/"><span class="tag">KoNLPy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lambda-Expression/"><span class="tag">Lambda Expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latent-Space/"><span class="tag">Latent Space</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learning-Rate/"><span class="tag">Learning Rate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Layer/"><span class="tag">Linear Layer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Likelihood/"><span class="tag">Log-Likelihood</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MAP/"><span class="tag">MAP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLE/"><span class="tag">MLE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Manifold-hypothesis/"><span class="tag">Manifold hypothesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matrix/"><span class="tag">Matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mecab/"><span class="tag">Mecab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-Stage-Build/"><span class="tag">Multi Stage Build&quot;</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLL/"><span class="tag">NLL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Persistence-Data/"><span class="tag">Persistence Data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probabilistic-Perspective/"><span class="tag">Probabilistic Perspective</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SOLID/"><span class="tag">SOLID</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scalar/"><span class="tag">Scalar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spring%EC%9D%B4%EB%9E%80/"><span class="tag">Spring이란</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword-Embedding/"><span class="tag">Subword Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensor/"><span class="tag">Tensor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tramsformers/"><span class="tag">Tramsformers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UML/"><span class="tag">UML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ubuntu/"><span class="tag">Ubuntu</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vector/"><span class="tag">Vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/What-to-do/"><span class="tag">What to do</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cross-entropy/"><span class="tag">cross entropy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/default-method/"><span class="tag">default method</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/max-length/"><span class="tag">max_length</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mlflow/"><span class="tag">mlflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/one-hot-Encoding/"><span class="tag">one-hot Encoding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/packing/"><span class="tag">packing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/padding/"><span class="tag">padding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python-%EC%84%A4%EC%B9%98/"><span class="tag">python 설치</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/static-method/"><span class="tag">static method</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unpacking/"><span class="tag">unpacking</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B0%9D%EC%B2%B4%EC%A7%80%ED%96%A5/"><span class="tag">객체지향</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8B%A4%ED%98%95%EC%84%B1/"><span class="tag">다형성</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%9E%8C%EB%8B%A4%EC%8B%9D/"><span class="tag">람다식</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%B2%A1%ED%84%B0%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">벡터의 곱셈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC/"><span class="tag">엔트로피</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EB%9F%89/"><span class="tag">정보량</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0/"><span class="tag">정보이론</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%8B%A4%EC%9D%B4%EC%96%B4%EA%B7%B8%EB%9E%A8/"><span class="tag">클래스 다이어그램</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/"><span class="tag">파라미터</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9D%B8%ED%84%B0%ED%8E%98%EC%9D%B4%EC%8A%A4/"><span class="tag">함수형 인터페이스</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"><span class="tag">함수형 프로그래밍</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%96%89%EB%A0%AC%EC%9D%98-%EA%B3%B1%EC%85%88/"><span class="tag">행렬의 곱셈</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Shawn&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Shawn Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>